{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"scPRINT-2: \ud83c\udfc3\ud83c\udfc3Your next-gen single cell foundation model scPRINT-2 is a single-cell RNA-seq foundation model built by J\u00e9r\u00e9mie Kalfon in the Cantini Lab. It uses novel architecture, encoding, decoding, training paradigms and losses. scPRINT-2 has been pretrained on more than 350 million cells from more than 22,000 datasets and 16 species. scPRINT-2 can be used to perform the following analyses in a zero-shot mode: expression denoising & imputation : increase the resolution of your scRNAseq data and discover un-measured genes' expression cell embedding and batch correction : generate a low-dimensional representation of your dataset at syntax level (organism, disease, cell type, sequencer, ...) label prediction : predict the cell type, disease, sequencer, sex, age, tissue of origin and ethnicity of your cells. gene network inference : generate a gene network from any cell or cell cluster in your scRNAseq dataset cross species integration : scPRINT-2 has been trained on 16 species and can be used to integrate data from different species. Example of scPRINT-2 finetuning exist for: new species : finetune scPRINT-2 on a new organism classification : finetune scPRINT-2 on your own cell type /disease / age labels / more... batch correction of your datasets / atlas : finetune scPRINT-2 to integrate data across species, technologies, and labs. scPRINT-2 is a foundation model and can be fine-tuned to perform many other analysis Read the manuscript! if you would like to know more about scPRINT-2 . Or have a look at some of my X-plainers . \ud83c\udf8a test scPRINT and scDataloader on this simple example google collab Table of Contents scPRINT-2: \ud83c\udfc3\ud83c\udfc3Your next-gen single cell foundation model Table of Contents Use scPRINT-2 try scPRINT-1 in superbio.ai! Try scPRINT-1 on a Google Colab notebook! To know about: lamin.ai install pytorch and GPUs Usage scPRINT-2's basic commands scPRINT-2's basic command line Example notebooks Documentation Docker Simple tests: FAQ I have a dataset and want a quick analysis: I have a dataset and want some more control over what is going on and which model to use: What does my anndata need to contain to be run with scPRINT-2 I want to generate an atlas-level embedding I need to generate gene tokens using pLLMs I want to re-train scPRINT-2 from scratch on my own data I want to regenerate the scPRINT-2 training corpus I want to fine-tune scPRINT-2 on my own data how can I find if scPRINT-2 was trained on my data? can I use scPRINT-2 on other organisms rather than humans? How long does scPRINT-2 take? What kind of resources do I need? (or in alternative: can I run scPRINT-2 locally?) I have different scRNASeq batches. Should I integrate my data before running scPRINT-2? I have new labels for my data that scPRINT-2 doesn't predict, how can I fine-tune it to predict them? where to find the input gene embeddings? Development dev install Reproducibility Building the Docker Image Pulling the Docker Image from Docker Hub Running the Docker Container Participate Use scPRINT-2 For the moment scPRINT-2 has been tested on MacOS and Linux (Ubuntu 20.04) with Python 3.10+. Its instalation takes on average 2 minutes in uv but much longer on conda . We highly recommend using uv to manage your python virtual environments!! Here is a link to our --still maintained-- previous generation model which contains larger size models: scPRINT-1 (don't forget to star it as well!): try scPRINT-1 in superbio.ai! HERE Try scPRINT-1 on a Google Colab notebook! To know about: lamin.ai To use scPRINT-2 , you will need to use lamin.ai . This is required to load biological information like genes, cell types, organisms.. (but also to manage the pre-training datasets if this is something you want to set up) install Here, is how to install uv uv venv <env-name> --python 3.11 source <env-name>/bin/activate #one of uv pip install scprint2 # OR uv pip install scprint2[dev] # for the dev dependencies (building etc..) # OR uv pip install scprint2[flash] # to use multiple flash attention functionalities written only in triton # : only if you have a compatible gpu (e.g. not available for apple GPUs for now, see https://github.com/triton-lang/triton?tab=readme-ov-file#compatibility), it is not useful if you just run inference using the main scPRINT, scPRINT-2 models #OR pip install scprint2[dev,flash] lamin init --storage ./testdb --name testdb --modules bionty scprint2 easy_setup \u26a0\ufe0f ./testdb is set in this example, but be mindful about where you want to store your data, this might get quite big as you use i,t and if you are on specific partition you want to consider this. If you start with lamin and have to do a lamin init , you will also do a scprint2 easy_setup . This populates your ontologies and adds some additional gene names. This is because scPRINT-2 is using ontologies to define its cell types, diseases, sexes, ethnicities, etc. ( link to view ontologies ) You can do it via the command: scdataloader populate all \u26a0\ufe0f It is ok to get warnings with this function or with this function: from scdataloader.utils import populate_my_ontology, _adding_scbasecamp_genes populate_my_ontology() #to populate everything (can take 2-10mns) populate_my_ontology( #the minimum for scPRINT-1 to run some inferences (denoising, grn inference) organisms: List[str] = [\"NCBITaxon:10090\", \"NCBITaxon:9606\"], sex: List[str] = [\"PATO:0000384\", \"PATO:0000383\"], celltypes = None, ethnicities = None, assays = None, tissues = None, diseases = None, dev_stages = None, ) _adding_scbasecamp_genes() #to add when using scPRINT-2 It will also download the default checkpoint of a pretrain scprint2 model from our hugging face page . But you can use other ones if you prefer: $ hf download jkobject/scPRINT v2-medium.ckpt --local-dir . A notebook for setting-up scPRINT-2 and lamin is also available here We make use of some additional packages we developed alongside scPRINT-2 (they are also shipped with scprint-2 already). Please refer to their documentation for more information: scDataLoader : a dataloader for training large cell models. GRnnData : a package to work with gene networks from single cell data. benGRN : a package to benchmark gene network inference methods from single cell data. simpler-flash : a package to easily use different versions of flash attention in pytorch models. hierarchical-classifier : a package to do hierarchical classification with pytorch when your labels can be mapped to a graph. pytorch and GPUs scPRINT-2 can run on machines without GPUs, but it will be slow. It is highly recommended to use a GPU for inference. Most of the time, everything works out of the box; otherwise, please send an issue model = scPRINT2.load_from_checkpoint( '../data/temp/last.ckpt', precpt_gene_emb=None, gene_pos_file=None,) You will know more by following the get-started notebook. Usage To get a sense of how scPRINT-2 works, have a look at our get-started notebook. scPRINT-2's basic commands This is a template of how you would go and use scPRINT most of the time: # import stuff from lightning.pytorch import Trainer from scprint2 import scPRINT2 from scdataloader import DataModule # setup a datamodule to train scprint2 from scratch datamodule = DataModule(...) # setup a model parameter model = scPRINT2(...) # to train / fit / test the model setup a trainer trainer = Trainer(...) # call the fit function trainer.fit(model, datamodule=datamodule) # to do predictions Denoiser, Embedder, GNInfer denoiser = Denoiser(...) adata = sc.read_h5ad(...) denoiser(model, adata=adata) ... scPRINT-2's basic command line Then fine-tune or analyse on your data $ scprint2 fit/train/predict/test/denoise/embed/gninfer/impute/gene_emb/generate/finetune --config config/[medium|large|vlarge] ... To denoise a dataset: $ scprint2 denoise --adata my_human_anndata.h5ad --ckpt_path v2-medium.ckpt --species \"NCBITaxon:9606\" --output_filename denoised.h5ad to do embedding and classification on a dataset: (the current version implies doing a PCA and Umap so it might need a lot of RAM if run as is) $ scprint2 embed --adata my_human_anndata.h5ad --ckpt_path v2-medium.ckpt --species \"NCBITaxon:9606\" --output_filename embedded.h5ad To do gene network inference on a dataset: $ scprint2 gninfer --adata my_human_anndata.h5ad --ckpt_path v2-medium.ckpt --species \"NCBITaxon:9606\" --cell_type 'cell_type_name_from-cell_type-obs_col' --output_filename grn.h5ad To re-train scPRINT-2 from scratch or from a checkpoint: $ scprint2 fit --config config/base_v2.yml --config config/pretrain_large.yml --ckpt_path large.ckpt find out more about the commands by running scprint2 --help or scprint2 [command] --help . more examples of using the command line are available in the docs . Example notebooks get-started : how to set things up run scPRINT-2 on a new species : how to fine-tune scPRINT-2 on a new organism. you will also need to generate embeddings and gene locations for your organism, see the FAQ below. do gene-network inference with scPRINT-2 : how to use scPRINT-2 to infer gene regulatory networks from your scRNAseq data (the first part is about getting ground truth data with benGRN) generate cell embeddings and cell label predictions from my data : how to use scPRINT-2 to generate cell embeddings and predict cell type generate gene output embeddings from my gene expressiond data : how to use scPRINT-2 to generate gene embeddings from your scRNAseq data do counterfactual gene expression prediction with scPRINT-2 : how to use scPRINT-2 to impute gene expression under different conditions do denoising with scPRINT-2 : how to use scPRINT-2 to denoise your scRNAseq data do imputation with scPRINT-2 (e.g. on Xenium Panel data) : how to use scPRINT-2 to impute missing genes in your scRNAseq data run scPRINT-2 on some Xenium spatial transcriptomics data : how to use scPRINT-2 to analyse spatial transcriptomics data fine-tune scPRINT-2 for cell type classification and/or batch correction : how to fine-tune scPRINT-2 on your own cell type labels Documentation For more information on usage, please see the documentation in https://www.jkobject.com/scPRINT-2/ Docker By using the scPRINT-2 Docker image , you can bypass the complexities of manual package installation, ensuring a consistent deployment environment. Included in this repository is a Dockerfile that lets you craft a container for the project; you have the choice to either build this image on your own or conveniently pull it from Docker Hub. Make sure that you have the docker command line interface installed on your system. A recommended way to install Docker with the correct NVIDIA drivers on Linux is to use this script /!\\ A MORE UP TO DATE DOCKER IMAGE is made as part of the open-problems benchmark and available on their GitHub for all tasks where scPRINT-2 is benchmarked Simple tests: An installation of scPRINT-2 and a simple test of the denoiser is performed during each commit to the main branch with a Github action and pytest workflow . It also provides an expected runtime for the installation and run of scPRINT-2. We now explore the different usages of scPRINT-2: FAQ I have a dataset and want a quick analysis: -> use superbio I have a dataset and want some more control over what is going on and which model to use: You will need to understand a few things, like lamindb, scdataloader, and scprint-2's inference tool. -> start with a quick intro using the google collab notebook -> look at the other FAQ element based on your desired use-case What does my anndata need to contain to be run with scPRINT-2 -> your anndata only needs to contain the species ontology id in its obs['organism_ontology_term_id'] (e.g. \"NCBITaxon:9606\"). It also needs to contain .var_names or .var.index with gene ids defined as ENSEMBL_IDs or HUGO_SYMBOL. -> That's it. You can then follow the preprocessing steps from various example notebooks to align your anndata to our gene set, make sure that it fits our requirements and then send it to the model! I want to generate an atlas-level embedding -> Refer to the notebook nice_umap_explain.ipynb . I need to generate gene tokens using pLLMs To run scPRINT-2, you can use the option to define the gene tokens using protein language model embeddings of genes. This is done by providing the path to a parquet file of the precomputed set of embeddings for each gene name to scPRINT-2 via \"precpt_gene_emb\" -> To generate this file please refer to the notebook generate_gene_embeddings . I want to re-train scPRINT-2 from scratch on my own data -> Refer to the documentation page pretrain scprint-2 I want to regenerate the scPRINT-2 training corpus -> Have a look at the scDataLoader 's README to understand how to do this. I want to fine-tune scPRINT-2 on my own data -> make sure that you a run of scPRINT-2's inference e.g. this one -> then please refine your question: do you want finetuning to predict labels? do batch correction? or make scprint work on your species? Have a look at the usage section and the rest of the FAQ to find the relevant information. how can I find if scPRINT-2 was trained on my data? If your data is available in cellxgene , or is listed in Arc's scBaseCount , scPRINT-2 was likely trained on it. However, some cells, and datasets were dropped due to low-quality data, and some were randomly removed to be part of the validation/test sets. can I use scPRINT-2 on other organisms rather than humans? scPRINT-2 has been pretrained on 16 organisms, check in the model.organisms or in our manuscript that yours isn't one of them, or highly related first. If so uses these and make sure that the gene names can be easily mapped by scdataloader's preprocess function. If not, scPRINT-2 can be used on other organisms that are not part of its training set, for this have a look at this notebook . You will also need to compute gene embeddings and gene locations for your organism's genetic data. Have a look at both notebooks. If you want to use scPRINT-2 on very different organisms than what it was trained on, you might need to then apply some finetuning, have a look at the finetuning notebook too. How long does scPRINT-2 take? What kind of resources do I need? (or in alternative: can I run scPRINT-2 locally?) Please look at our manuscript table 1 and supplementary Table 1 - 2 to know more about computational ressources. But know that you will likely need at least one high performance GPU. I have different scRNASeq batches. Should I integrate my data before running scPRINT-2? scPRINT-2 takes raw count as inputs, so please don't use integrated data. Just give the raw counts to scPRINT-2 and it will take care of the rest. For better results you can apply some finetuning of scPRINT-2 on your batches to better integrate them. See the finetuning notebook . You can replace the cross-species MMD loss with a cross-batch MMD loss. I have new labels for my data that scPRINT-2 doesn't predict, how can I fine-tune it to predict them? First have a look at scPRINT-2's inference capabilities and checkout the finetuning notebooks. In your case, what you will need to do is to reuse the finetuning notebook but also update the output layers of the classifier to predict your new labels. You can do this by changing the number of output classes in the classifier head to match the number of new labels you have. You will also need to update the scPRINT-2's mat_labels_hierarchy attribute to include your new labels and their relationships if they are hierarchical and you want this to happen, otherwise update it with an empty vector. Make sure also to update the label_decoders attribute in the model to include at the right index your new label decoder / classifier, the name of your new labels. Then you can proceed with the finetuning as usual, using your dataset with the new labels in the obs of your anndata. (I am sure chatgpt can help you with it too) where to find the input gene embeddings? If you think you need the gene embeddings file for loading the model from a checkpoint, you don't need to recompute them, as the embeddings are also stored in the model weights. You just need to load the weights like this: model = scPRINT2.load_from_checkpoint( '../../data/temp/last.ckpt', precpt_gene_emb=None, gene_pos_file=None, ) # to remove gene embeddings that scPRINT-2 was trained with but that are not found in the lamin ontology anymore missing = set(model.genes) - set(load_genes(model.organisms).index) if len(missing) > 0: print( \"Warning: some genes missmatch exist between model and ontology: solving...\", ) model._rm_genes(missing) But if you want to, you can also recreate the gene embedding file through this notebook . Just call the functions, and it should recreate the file itself. The file itself is also available on hugging face /!\\ Please understand that what I mean by gene embedding is the immutable input gene embeddings encoding the gene name. scPRINT-2 directly takes raw counts as input and takes care of doing the embedding on the fly. (it does similarly for a gene's location in the genome). Development dev install If you want to use the latest version of scPRINT-2 and work on the code yourself use git clone and pip -e instead of pip install . git clone https://github.com/cantinilab/scPRINT-2 git clone https://github.com/jkobject/scDataLoader git clone https://github.com/cantinilab/GRnnData git clone https://github.com/jkobject/benGRN pip install -e scprint2[dev] pip install -e scDataLoader[dev] pip install -e GRnnData[dev] pip install -e benGRN[dev] Reproducibility To reproduce the paper please use the version / tag 1.6.4 and you will have to git clone the repo to have access to all the pre-training functionalities! \u26a0\ufe0f When re-training scPRINT-2 from scratch, by default, every N epoch, the test() function will be called `. It is using a predownloadedtest datasets paths (see https://github.com/cantinilab/scPRINT-2/issues/12). Replace them with your own paths you want to use these test functions. They are also made available on hf.co: https://huggingface.co/jkobject/scPRINT-2/tree/main Building the Docker Image To build the Docker image from the provided Dockerfile , run the following command from the root directory of this repository: docker build -t scprint2:latest -f Dockerfile . Pulling the Docker Image from Docker Hub If you don't want to build the image yourself, you can pull it directly from Docker Hub: docker pull jkobject/scprint2:1.0.0 docker tag scprint2:latest jkobject/scprint2:1.0.0 Running the Docker Container Once you have the image (either by building it or pulling it), you can start a container with: docker run --gpus all --rm -it scprint2:latest bash Please note: When running the Docker container, ensure you mount any necessary folders using the -v option to access them inside the container. Participate Read the CONTRIBUTING.md file. Read the training runs document to know more about how pre-training was performed and the its behavior. code coverage is not right as I am using the command line interface for now. >50% of the code is covered by my current unit test. Acknowledgement: python template laminDB lightning Created by J\u00e9r\u00e9mie Kalfon","title":"Home"},{"location":"#scprint-2-your-next-gen-single-cell-foundation-model","text":"scPRINT-2 is a single-cell RNA-seq foundation model built by J\u00e9r\u00e9mie Kalfon in the Cantini Lab. It uses novel architecture, encoding, decoding, training paradigms and losses. scPRINT-2 has been pretrained on more than 350 million cells from more than 22,000 datasets and 16 species. scPRINT-2 can be used to perform the following analyses in a zero-shot mode: expression denoising & imputation : increase the resolution of your scRNAseq data and discover un-measured genes' expression cell embedding and batch correction : generate a low-dimensional representation of your dataset at syntax level (organism, disease, cell type, sequencer, ...) label prediction : predict the cell type, disease, sequencer, sex, age, tissue of origin and ethnicity of your cells. gene network inference : generate a gene network from any cell or cell cluster in your scRNAseq dataset cross species integration : scPRINT-2 has been trained on 16 species and can be used to integrate data from different species. Example of scPRINT-2 finetuning exist for: new species : finetune scPRINT-2 on a new organism classification : finetune scPRINT-2 on your own cell type /disease / age labels / more... batch correction of your datasets / atlas : finetune scPRINT-2 to integrate data across species, technologies, and labs. scPRINT-2 is a foundation model and can be fine-tuned to perform many other analysis Read the manuscript! if you would like to know more about scPRINT-2 . Or have a look at some of my X-plainers . \ud83c\udf8a test scPRINT and scDataloader on this simple example google collab","title":"scPRINT-2: \ud83c\udfc3\ud83c\udfc3Your next-gen single cell foundation model"},{"location":"#table-of-contents","text":"scPRINT-2: \ud83c\udfc3\ud83c\udfc3Your next-gen single cell foundation model Table of Contents Use scPRINT-2 try scPRINT-1 in superbio.ai! Try scPRINT-1 on a Google Colab notebook! To know about: lamin.ai install pytorch and GPUs Usage scPRINT-2's basic commands scPRINT-2's basic command line Example notebooks Documentation Docker Simple tests: FAQ I have a dataset and want a quick analysis: I have a dataset and want some more control over what is going on and which model to use: What does my anndata need to contain to be run with scPRINT-2 I want to generate an atlas-level embedding I need to generate gene tokens using pLLMs I want to re-train scPRINT-2 from scratch on my own data I want to regenerate the scPRINT-2 training corpus I want to fine-tune scPRINT-2 on my own data how can I find if scPRINT-2 was trained on my data? can I use scPRINT-2 on other organisms rather than humans? How long does scPRINT-2 take? What kind of resources do I need? (or in alternative: can I run scPRINT-2 locally?) I have different scRNASeq batches. Should I integrate my data before running scPRINT-2? I have new labels for my data that scPRINT-2 doesn't predict, how can I fine-tune it to predict them? where to find the input gene embeddings? Development dev install Reproducibility Building the Docker Image Pulling the Docker Image from Docker Hub Running the Docker Container Participate","title":"Table of Contents"},{"location":"#use-scprint-2","text":"For the moment scPRINT-2 has been tested on MacOS and Linux (Ubuntu 20.04) with Python 3.10+. Its instalation takes on average 2 minutes in uv but much longer on conda . We highly recommend using uv to manage your python virtual environments!! Here is a link to our --still maintained-- previous generation model which contains larger size models: scPRINT-1 (don't forget to star it as well!):","title":"Use scPRINT-2"},{"location":"#try-scprint-1-in-superbioai","text":"HERE","title":"try scPRINT-1 in superbio.ai!"},{"location":"#try-scprint-1-on-a-google-colab-notebook","text":"","title":"Try scPRINT-1 on a Google Colab notebook!"},{"location":"#to-know-about-laminai","text":"To use scPRINT-2 , you will need to use lamin.ai . This is required to load biological information like genes, cell types, organisms.. (but also to manage the pre-training datasets if this is something you want to set up)","title":"To know about: lamin.ai"},{"location":"#install","text":"Here, is how to install uv uv venv <env-name> --python 3.11 source <env-name>/bin/activate #one of uv pip install scprint2 # OR uv pip install scprint2[dev] # for the dev dependencies (building etc..) # OR uv pip install scprint2[flash] # to use multiple flash attention functionalities written only in triton # : only if you have a compatible gpu (e.g. not available for apple GPUs for now, see https://github.com/triton-lang/triton?tab=readme-ov-file#compatibility), it is not useful if you just run inference using the main scPRINT, scPRINT-2 models #OR pip install scprint2[dev,flash] lamin init --storage ./testdb --name testdb --modules bionty scprint2 easy_setup \u26a0\ufe0f ./testdb is set in this example, but be mindful about where you want to store your data, this might get quite big as you use i,t and if you are on specific partition you want to consider this. If you start with lamin and have to do a lamin init , you will also do a scprint2 easy_setup . This populates your ontologies and adds some additional gene names. This is because scPRINT-2 is using ontologies to define its cell types, diseases, sexes, ethnicities, etc. ( link to view ontologies ) You can do it via the command: scdataloader populate all \u26a0\ufe0f It is ok to get warnings with this function or with this function: from scdataloader.utils import populate_my_ontology, _adding_scbasecamp_genes populate_my_ontology() #to populate everything (can take 2-10mns) populate_my_ontology( #the minimum for scPRINT-1 to run some inferences (denoising, grn inference) organisms: List[str] = [\"NCBITaxon:10090\", \"NCBITaxon:9606\"], sex: List[str] = [\"PATO:0000384\", \"PATO:0000383\"], celltypes = None, ethnicities = None, assays = None, tissues = None, diseases = None, dev_stages = None, ) _adding_scbasecamp_genes() #to add when using scPRINT-2 It will also download the default checkpoint of a pretrain scprint2 model from our hugging face page . But you can use other ones if you prefer: $ hf download jkobject/scPRINT v2-medium.ckpt --local-dir . A notebook for setting-up scPRINT-2 and lamin is also available here We make use of some additional packages we developed alongside scPRINT-2 (they are also shipped with scprint-2 already). Please refer to their documentation for more information: scDataLoader : a dataloader for training large cell models. GRnnData : a package to work with gene networks from single cell data. benGRN : a package to benchmark gene network inference methods from single cell data. simpler-flash : a package to easily use different versions of flash attention in pytorch models. hierarchical-classifier : a package to do hierarchical classification with pytorch when your labels can be mapped to a graph.","title":"install"},{"location":"#pytorch-and-gpus","text":"scPRINT-2 can run on machines without GPUs, but it will be slow. It is highly recommended to use a GPU for inference. Most of the time, everything works out of the box; otherwise, please send an issue model = scPRINT2.load_from_checkpoint( '../data/temp/last.ckpt', precpt_gene_emb=None, gene_pos_file=None,) You will know more by following the get-started notebook.","title":"pytorch and GPUs"},{"location":"#usage","text":"To get a sense of how scPRINT-2 works, have a look at our get-started notebook.","title":"Usage"},{"location":"#scprint-2s-basic-commands","text":"This is a template of how you would go and use scPRINT most of the time: # import stuff from lightning.pytorch import Trainer from scprint2 import scPRINT2 from scdataloader import DataModule # setup a datamodule to train scprint2 from scratch datamodule = DataModule(...) # setup a model parameter model = scPRINT2(...) # to train / fit / test the model setup a trainer trainer = Trainer(...) # call the fit function trainer.fit(model, datamodule=datamodule) # to do predictions Denoiser, Embedder, GNInfer denoiser = Denoiser(...) adata = sc.read_h5ad(...) denoiser(model, adata=adata) ...","title":"scPRINT-2's basic commands"},{"location":"#scprint-2s-basic-command-line","text":"Then fine-tune or analyse on your data $ scprint2 fit/train/predict/test/denoise/embed/gninfer/impute/gene_emb/generate/finetune --config config/[medium|large|vlarge] ... To denoise a dataset: $ scprint2 denoise --adata my_human_anndata.h5ad --ckpt_path v2-medium.ckpt --species \"NCBITaxon:9606\" --output_filename denoised.h5ad to do embedding and classification on a dataset: (the current version implies doing a PCA and Umap so it might need a lot of RAM if run as is) $ scprint2 embed --adata my_human_anndata.h5ad --ckpt_path v2-medium.ckpt --species \"NCBITaxon:9606\" --output_filename embedded.h5ad To do gene network inference on a dataset: $ scprint2 gninfer --adata my_human_anndata.h5ad --ckpt_path v2-medium.ckpt --species \"NCBITaxon:9606\" --cell_type 'cell_type_name_from-cell_type-obs_col' --output_filename grn.h5ad To re-train scPRINT-2 from scratch or from a checkpoint: $ scprint2 fit --config config/base_v2.yml --config config/pretrain_large.yml --ckpt_path large.ckpt find out more about the commands by running scprint2 --help or scprint2 [command] --help . more examples of using the command line are available in the docs .","title":"scPRINT-2's basic command line"},{"location":"#example-notebooks","text":"get-started : how to set things up run scPRINT-2 on a new species : how to fine-tune scPRINT-2 on a new organism. you will also need to generate embeddings and gene locations for your organism, see the FAQ below. do gene-network inference with scPRINT-2 : how to use scPRINT-2 to infer gene regulatory networks from your scRNAseq data (the first part is about getting ground truth data with benGRN) generate cell embeddings and cell label predictions from my data : how to use scPRINT-2 to generate cell embeddings and predict cell type generate gene output embeddings from my gene expressiond data : how to use scPRINT-2 to generate gene embeddings from your scRNAseq data do counterfactual gene expression prediction with scPRINT-2 : how to use scPRINT-2 to impute gene expression under different conditions do denoising with scPRINT-2 : how to use scPRINT-2 to denoise your scRNAseq data do imputation with scPRINT-2 (e.g. on Xenium Panel data) : how to use scPRINT-2 to impute missing genes in your scRNAseq data run scPRINT-2 on some Xenium spatial transcriptomics data : how to use scPRINT-2 to analyse spatial transcriptomics data fine-tune scPRINT-2 for cell type classification and/or batch correction : how to fine-tune scPRINT-2 on your own cell type labels","title":"Example notebooks"},{"location":"#documentation","text":"For more information on usage, please see the documentation in https://www.jkobject.com/scPRINT-2/","title":"Documentation"},{"location":"#docker","text":"By using the scPRINT-2 Docker image , you can bypass the complexities of manual package installation, ensuring a consistent deployment environment. Included in this repository is a Dockerfile that lets you craft a container for the project; you have the choice to either build this image on your own or conveniently pull it from Docker Hub. Make sure that you have the docker command line interface installed on your system. A recommended way to install Docker with the correct NVIDIA drivers on Linux is to use this script /!\\ A MORE UP TO DATE DOCKER IMAGE is made as part of the open-problems benchmark and available on their GitHub for all tasks where scPRINT-2 is benchmarked","title":"Docker"},{"location":"#simple-tests","text":"An installation of scPRINT-2 and a simple test of the denoiser is performed during each commit to the main branch with a Github action and pytest workflow . It also provides an expected runtime for the installation and run of scPRINT-2. We now explore the different usages of scPRINT-2:","title":"Simple tests:"},{"location":"#faq","text":"","title":"FAQ"},{"location":"#i-have-a-dataset-and-want-a-quick-analysis","text":"-> use superbio","title":"I have a dataset and want a quick analysis:"},{"location":"#i-have-a-dataset-and-want-some-more-control-over-what-is-going-on-and-which-model-to-use","text":"You will need to understand a few things, like lamindb, scdataloader, and scprint-2's inference tool. -> start with a quick intro using the google collab notebook -> look at the other FAQ element based on your desired use-case","title":"I have a dataset and want some more control over what is going on and which model to use:"},{"location":"#what-does-my-anndata-need-to-contain-to-be-run-with-scprint-2","text":"-> your anndata only needs to contain the species ontology id in its obs['organism_ontology_term_id'] (e.g. \"NCBITaxon:9606\"). It also needs to contain .var_names or .var.index with gene ids defined as ENSEMBL_IDs or HUGO_SYMBOL. -> That's it. You can then follow the preprocessing steps from various example notebooks to align your anndata to our gene set, make sure that it fits our requirements and then send it to the model!","title":"What does my anndata need to contain to be run with scPRINT-2"},{"location":"#i-want-to-generate-an-atlas-level-embedding","text":"-> Refer to the notebook nice_umap_explain.ipynb .","title":"I want to generate an atlas-level embedding"},{"location":"#i-need-to-generate-gene-tokens-using-pllms","text":"To run scPRINT-2, you can use the option to define the gene tokens using protein language model embeddings of genes. This is done by providing the path to a parquet file of the precomputed set of embeddings for each gene name to scPRINT-2 via \"precpt_gene_emb\" -> To generate this file please refer to the notebook generate_gene_embeddings .","title":"I need to generate gene tokens using pLLMs"},{"location":"#i-want-to-re-train-scprint-2-from-scratch-on-my-own-data","text":"-> Refer to the documentation page pretrain scprint-2","title":"I want to re-train scPRINT-2 from scratch on my own data"},{"location":"#i-want-to-regenerate-the-scprint-2-training-corpus","text":"-> Have a look at the scDataLoader 's README to understand how to do this.","title":"I want to regenerate the scPRINT-2 training corpus"},{"location":"#i-want-to-fine-tune-scprint-2-on-my-own-data","text":"-> make sure that you a run of scPRINT-2's inference e.g. this one -> then please refine your question: do you want finetuning to predict labels? do batch correction? or make scprint work on your species? Have a look at the usage section and the rest of the FAQ to find the relevant information.","title":"I want to fine-tune scPRINT-2 on my own data"},{"location":"#how-can-i-find-if-scprint-2-was-trained-on-my-data","text":"If your data is available in cellxgene , or is listed in Arc's scBaseCount , scPRINT-2 was likely trained on it. However, some cells, and datasets were dropped due to low-quality data, and some were randomly removed to be part of the validation/test sets.","title":"how can I find if scPRINT-2 was trained on my data?"},{"location":"#can-i-use-scprint-2-on-other-organisms-rather-than-humans","text":"scPRINT-2 has been pretrained on 16 organisms, check in the model.organisms or in our manuscript that yours isn't one of them, or highly related first. If so uses these and make sure that the gene names can be easily mapped by scdataloader's preprocess function. If not, scPRINT-2 can be used on other organisms that are not part of its training set, for this have a look at this notebook . You will also need to compute gene embeddings and gene locations for your organism's genetic data. Have a look at both notebooks. If you want to use scPRINT-2 on very different organisms than what it was trained on, you might need to then apply some finetuning, have a look at the finetuning notebook too.","title":"can I use scPRINT-2 on other organisms rather than humans?"},{"location":"#how-long-does-scprint-2-take-what-kind-of-resources-do-i-need-or-in-alternative-can-i-run-scprint-2-locally","text":"Please look at our manuscript table 1 and supplementary Table 1 - 2 to know more about computational ressources. But know that you will likely need at least one high performance GPU.","title":"How long does scPRINT-2 take? What kind of resources do I need? (or in alternative: can I run scPRINT-2 locally?)"},{"location":"#i-have-different-scrnaseq-batches-should-i-integrate-my-data-before-running-scprint-2","text":"scPRINT-2 takes raw count as inputs, so please don't use integrated data. Just give the raw counts to scPRINT-2 and it will take care of the rest. For better results you can apply some finetuning of scPRINT-2 on your batches to better integrate them. See the finetuning notebook . You can replace the cross-species MMD loss with a cross-batch MMD loss.","title":"I have different scRNASeq batches. Should I integrate my data before running scPRINT-2?"},{"location":"#i-have-new-labels-for-my-data-that-scprint-2-doesnt-predict-how-can-i-fine-tune-it-to-predict-them","text":"First have a look at scPRINT-2's inference capabilities and checkout the finetuning notebooks. In your case, what you will need to do is to reuse the finetuning notebook but also update the output layers of the classifier to predict your new labels. You can do this by changing the number of output classes in the classifier head to match the number of new labels you have. You will also need to update the scPRINT-2's mat_labels_hierarchy attribute to include your new labels and their relationships if they are hierarchical and you want this to happen, otherwise update it with an empty vector. Make sure also to update the label_decoders attribute in the model to include at the right index your new label decoder / classifier, the name of your new labels. Then you can proceed with the finetuning as usual, using your dataset with the new labels in the obs of your anndata. (I am sure chatgpt can help you with it too)","title":"I have new labels for my data that scPRINT-2 doesn't predict, how can I fine-tune it to predict them?"},{"location":"#where-to-find-the-input-gene-embeddings","text":"If you think you need the gene embeddings file for loading the model from a checkpoint, you don't need to recompute them, as the embeddings are also stored in the model weights. You just need to load the weights like this: model = scPRINT2.load_from_checkpoint( '../../data/temp/last.ckpt', precpt_gene_emb=None, gene_pos_file=None, ) # to remove gene embeddings that scPRINT-2 was trained with but that are not found in the lamin ontology anymore missing = set(model.genes) - set(load_genes(model.organisms).index) if len(missing) > 0: print( \"Warning: some genes missmatch exist between model and ontology: solving...\", ) model._rm_genes(missing) But if you want to, you can also recreate the gene embedding file through this notebook . Just call the functions, and it should recreate the file itself. The file itself is also available on hugging face /!\\ Please understand that what I mean by gene embedding is the immutable input gene embeddings encoding the gene name. scPRINT-2 directly takes raw counts as input and takes care of doing the embedding on the fly. (it does similarly for a gene's location in the genome).","title":"where to find the input gene embeddings?"},{"location":"#development","text":"","title":"Development"},{"location":"#dev-install","text":"If you want to use the latest version of scPRINT-2 and work on the code yourself use git clone and pip -e instead of pip install . git clone https://github.com/cantinilab/scPRINT-2 git clone https://github.com/jkobject/scDataLoader git clone https://github.com/cantinilab/GRnnData git clone https://github.com/jkobject/benGRN pip install -e scprint2[dev] pip install -e scDataLoader[dev] pip install -e GRnnData[dev] pip install -e benGRN[dev]","title":"dev install"},{"location":"#reproducibility","text":"To reproduce the paper please use the version / tag 1.6.4 and you will have to git clone the repo to have access to all the pre-training functionalities! \u26a0\ufe0f When re-training scPRINT-2 from scratch, by default, every N epoch, the test() function will be called `. It is using a predownloadedtest datasets paths (see https://github.com/cantinilab/scPRINT-2/issues/12). Replace them with your own paths you want to use these test functions. They are also made available on hf.co: https://huggingface.co/jkobject/scPRINT-2/tree/main","title":"Reproducibility"},{"location":"#building-the-docker-image","text":"To build the Docker image from the provided Dockerfile , run the following command from the root directory of this repository: docker build -t scprint2:latest -f Dockerfile .","title":"Building the Docker Image"},{"location":"#pulling-the-docker-image-from-docker-hub","text":"If you don't want to build the image yourself, you can pull it directly from Docker Hub: docker pull jkobject/scprint2:1.0.0 docker tag scprint2:latest jkobject/scprint2:1.0.0","title":"Pulling the Docker Image from Docker Hub"},{"location":"#running-the-docker-container","text":"Once you have the image (either by building it or pulling it), you can start a container with: docker run --gpus all --rm -it scprint2:latest bash Please note: When running the Docker container, ensure you mount any necessary folders using the -v option to access them inside the container.","title":"Running the Docker Container"},{"location":"#participate","text":"Read the CONTRIBUTING.md file. Read the training runs document to know more about how pre-training was performed and the its behavior. code coverage is not right as I am using the command line interface for now. >50% of the code is covered by my current unit test. Acknowledgement: python template laminDB lightning Created by J\u00e9r\u00e9mie Kalfon","title":"Participate"},{"location":"cli/","text":"Documentation for the cli modules scprint2.__main__ Entry point for scprint. Classes: Name Description MySaveConfig MySaveConfig is a subclass of SaveConfigCallback to parametrize the wandb logger further in cli mode MySaveConfig Bases: SaveConfigCallback MySaveConfig is a subclass of SaveConfigCallback to parametrize the wandb logger further in cli mode scprint2.cli Classes: Name Description MyCLI MyCLI is a subclass of LightningCLI to add some missing params and create bindings between params of the model and the data. MyCLI Bases: LightningCLI MyCLI is a subclass of LightningCLI to add some missing params and create bindings between params of the model and the data. Used to allow calling denoise / embed / gninfer from the command line. Also to add more parameters and link parameters between the scdataloader and the scPRINT-2 model. Methods: Name Description instantiate_trainer Override to customize trainer instantiation instantiate_trainer Override to customize trainer instantiation Source code in scprint2/cli.py 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def instantiate_trainer ( self , ** kwargs ) -> Trainer : \"\"\"Override to customize trainer instantiation\"\"\" # Modify strategy if it's DDP trainer = super () . instantiate_trainer ( ** kwargs ) if \"fit\" in self . config and self . config [ \"fit\" ][ \"trainer\" ][ \"strategy\" ] in [ \"ddp\" , \"ddp_find_unused_parameters_true\" , \"fsdp\" , ]: # Create DDPStrategy with custom timeout from datetime import timedelta # Update the config print ( \"updating the config\" ) trainer . strategy . _timeout = timedelta ( seconds = TIMEOUT ) # 9 hours in seconds trainer . strategy . setup_distributed () # Call parent method to create trainer return trainer scprint2.trainer.trainer Classes: Name Description TrainingMode TrainingMode Bases: Callback TrainingMode a callback to set the training specific info to the model. This is because lightning is unfortunately setup this way. the model should be separated from training but at the same time it has training specific methods... so we have to do this. Parameters: noise ( List [ float ] , default: [0.6] ) \u2013 List of noise levels to apply if denoising is enabled. Defaults to [0.6], meaning only one forward path with 60% of the counts being dropped will happen. cce_temp ( float , default: 0.3 ) \u2013 Similarity threshold for CCE. Defaults to 0.5. cce_scale ( float , default: 0.2 ) \u2013 Scaling factor for CCE loss. Defaults to 0.002. ecs_threshold ( float , default: 0.4 ) \u2013 Threshold for ECS. Defaults to 0.3. ecs_scale ( float , default: 0.2 ) \u2013 Scaling factor for ECS loss. Defaults to 0.05. mvc_scale ( float , default: 0.0 ) \u2013 Scaling factor for MVC loss. Defaults to 1.0. do_generate ( bool , default: True ) \u2013 Whether to do the bottleneck learning task. Defaults to True. class_scale ( float , default: 1 ) \u2013 Scaling factor for classification loss. Defaults to 1.5. mask_ratio ( List [ float ] , default: [] ) \u2013 List of mask ratios to apply during training. Defaults to [], meaning no masking is applied during pretraining. warmup_duration ( int , default: 500 ) \u2013 Number of warmup steps for learning rate scheduling. Defaults to 500. fused_adam ( bool , default: False ) \u2013 Whether to use fused Adam optimizer. Defaults to True. adv_class_scale ( float , default: 1.0 ) \u2013 Scaling factor for adversarial classification loss. Defaults to 0.1. lr_reduce_patience ( int , default: 2 ) \u2013 Number of epochs with no improvement after which learning rate will be reduced. Defaults to 1. lr_reduce_factor ( float , default: 0.6 ) \u2013 Factor by which the learning rate will be reduced. Defaults to 0.6. lr_reduce_monitor ( str , default: 'val_loss' ) \u2013 Quantity to be monitored for learning rate reduction. Defaults to \"val_loss\". run_full_forward ( bool , default: False ) \u2013 Whether to run a second forward pass without masking or denoising for the bottleneck learning / MVC case. Defaults to False. lr ( float , default: 0.0001 ) \u2013 Initial learning rate. Defaults to 0.001. optim ( str , default: 'adamW' ) \u2013 Optimizer to use during training. Defaults to \"adamW\". weight_decay ( float , default: 0.01 ) \u2013 Weight decay to apply during optimization. Defaults to 0.01. name ( str , default: '' ) \u2013 Name of the training mode. Defaults to an empty string. should be an ID for the model test_every ( int , default: 5 ) \u2013 Number of epochs between testing. Defaults to 1. class_embd_diss_scale ( float , default: 0.3 ) \u2013 Scaling factor for the class embedding dissimilarity loss. Defaults to 0.1. zinb_and_mse ( bool , default: False ) \u2013 Whether to use ZINB and MSE loss. Defaults to False. var_context_length ( bool , default: False ) \u2013 Whether to use variable context length. Defaults to False. dropout ( float , default: 0.1 ) \u2013 Dropout rate for the model. Defaults to 0.1. set_step ( int , default: None ) \u2013 Set the global step for the model. Defaults to None. vae_kl_scale ( float , default: 0.001 ) \u2013 Scaling factor for the VAE KL loss. Defaults to 0.3. randsamp ( bool , default: True ) \u2013 Whether to use random sampling for the noise amount at each training step. Defaults to True. vae_kl_warmup_steps ( int , default: 80000 ) \u2013 Number of warmup steps for the VAE KL loss. Defaults to 20_000. mask_zeros ( bool , default: False ) \u2013 Whether to mask zeros in the expression matrix. Defaults to False. Source code in scprint2/trainer/trainer.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def __init__ ( self , noise : List [ float ] = [ 0.6 ], cce_temp : float = 0.3 , # .6 cce_scale : float = 0.2 , # .01 ecs_threshold : float = 0.4 , class_embd_diss_scale : float = 0.3 , ecs_scale : float = 0.2 , # .1 mvc_scale : float = 0.0 , do_next_tp : bool = False , do_generate : bool = True , class_scale : float = 1 , mask_ratio : List [ float | str ] = [], # 0.3 test_every : int = 5 , randsamp : bool = True , warmup_duration : int = 500 , fused_adam : bool = False , adv_class_scale : float = 1.0 , lr_reduce_patience : int = 2 , lr_reduce_factor : float = 0.6 , lr_reduce_monitor : str = \"val_loss\" , run_full_forward : bool = False , lr : float = 0.0001 , dropout : float = 0.1 , optim : str = \"adamW\" , weight_decay : float = 0.01 , zinb_and_mse : bool = False , var_context_length : bool = False , vae_kl_warmup_steps : int = 80_000 , vae_kl_scale : float = 0.001 , name : str = \"\" , set_step : Optional [ int ] = None , mask_zeros : bool = False , ): \"\"\" TrainingMode a callback to set the training specific info to the model. This is because lightning is unfortunately setup this way. the model should be separated from training but at the same time it has training specific methods... so we have to do this. Args: noise (List[float]): List of noise levels to apply if denoising is enabled. Defaults to [0.6], meaning only one forward path with 60% of the counts being dropped will happen. cce_temp (float): Similarity threshold for CCE. Defaults to 0.5. cce_scale (float): Scaling factor for CCE loss. Defaults to 0.002. ecs_threshold (float): Threshold for ECS. Defaults to 0.3. ecs_scale (float): Scaling factor for ECS loss. Defaults to 0.05. mvc_scale (float): Scaling factor for MVC loss. Defaults to 1.0. do_generate (bool): Whether to do the bottleneck learning task. Defaults to True. class_scale (float): Scaling factor for classification loss. Defaults to 1.5. mask_ratio (List[float]): List of mask ratios to apply during training. Defaults to [], meaning no masking is applied during pretraining. warmup_duration (int): Number of warmup steps for learning rate scheduling. Defaults to 500. fused_adam (bool): Whether to use fused Adam optimizer. Defaults to True. adv_class_scale (float): Scaling factor for adversarial classification loss. Defaults to 0.1. lr_reduce_patience (int): Number of epochs with no improvement after which learning rate will be reduced. Defaults to 1. lr_reduce_factor (float): Factor by which the learning rate will be reduced. Defaults to 0.6. lr_reduce_monitor (str): Quantity to be monitored for learning rate reduction. Defaults to \"val_loss\". run_full_forward (bool): Whether to run a second forward pass without masking or denoising for the bottleneck learning / MVC case. Defaults to False. lr (float): Initial learning rate. Defaults to 0.001. optim (str): Optimizer to use during training. Defaults to \"adamW\". weight_decay (float): Weight decay to apply during optimization. Defaults to 0.01. name (str): Name of the training mode. Defaults to an empty string. should be an ID for the model test_every (int): Number of epochs between testing. Defaults to 1. class_embd_diss_scale (float): Scaling factor for the class embedding dissimilarity loss. Defaults to 0.1. zinb_and_mse (bool): Whether to use ZINB and MSE loss. Defaults to False. var_context_length (bool): Whether to use variable context length. Defaults to False. dropout (float): Dropout rate for the model. Defaults to 0.1. set_step (int, optional): Set the global step for the model. Defaults to None. vae_kl_scale (float): Scaling factor for the VAE KL loss. Defaults to 0.3. randsamp (bool): Whether to use random sampling for the noise amount at each training step. Defaults to True. vae_kl_warmup_steps (int): Number of warmup steps for the VAE KL loss. Defaults to 20_000. mask_zeros (bool): Whether to mask zeros in the expression matrix. Defaults to False. \"\"\" super () . __init__ () self . noise = noise self . cce_temp = cce_temp self . cce_scale = cce_scale self . ecs_threshold = ecs_threshold self . ecs_scale = ecs_scale self . vae_kl_scale = vae_kl_scale self . do_next_tp = do_next_tp self . do_generate = do_generate self . class_scale = class_scale self . mask_ratio = mask_ratio self . warmup_duration = warmup_duration self . fused_adam = fused_adam self . mvc_scale = mvc_scale self . adv_class_scale = adv_class_scale self . lr_reduce_patience = lr_reduce_patience self . lr_reduce_factor = lr_reduce_factor self . lr_reduce_monitor = lr_reduce_monitor self . lr = lr self . optim = optim self . weight_decay = weight_decay self . run_full_forward = run_full_forward self . name = name self . test_every = test_every self . class_embd_diss_scale = class_embd_diss_scale self . zinb_and_mse = zinb_and_mse self . var_context_length = var_context_length self . dropout = dropout self . set_step = set_step self . randsamp = randsamp self . mask_zeros = mask_zeros self . vae_kl_warmup_steps = vae_kl_warmup_steps","title":"cli"},{"location":"cli/#documentation-for-the-cli-modules","text":"","title":"Documentation for the cli modules"},{"location":"cli/#scprint2.__main__","text":"Entry point for scprint. Classes: Name Description MySaveConfig MySaveConfig is a subclass of SaveConfigCallback to parametrize the wandb logger further in cli mode","title":"__main__"},{"location":"cli/#scprint2.__main__.MySaveConfig","text":"Bases: SaveConfigCallback MySaveConfig is a subclass of SaveConfigCallback to parametrize the wandb logger further in cli mode","title":"MySaveConfig"},{"location":"cli/#scprint2.cli","text":"Classes: Name Description MyCLI MyCLI is a subclass of LightningCLI to add some missing params and create bindings between params of the model and the data.","title":"cli"},{"location":"cli/#scprint2.cli.MyCLI","text":"Bases: LightningCLI MyCLI is a subclass of LightningCLI to add some missing params and create bindings between params of the model and the data. Used to allow calling denoise / embed / gninfer from the command line. Also to add more parameters and link parameters between the scdataloader and the scPRINT-2 model. Methods: Name Description instantiate_trainer Override to customize trainer instantiation","title":"MyCLI"},{"location":"cli/#scprint2.cli.MyCLI.instantiate_trainer","text":"Override to customize trainer instantiation Source code in scprint2/cli.py 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def instantiate_trainer ( self , ** kwargs ) -> Trainer : \"\"\"Override to customize trainer instantiation\"\"\" # Modify strategy if it's DDP trainer = super () . instantiate_trainer ( ** kwargs ) if \"fit\" in self . config and self . config [ \"fit\" ][ \"trainer\" ][ \"strategy\" ] in [ \"ddp\" , \"ddp_find_unused_parameters_true\" , \"fsdp\" , ]: # Create DDPStrategy with custom timeout from datetime import timedelta # Update the config print ( \"updating the config\" ) trainer . strategy . _timeout = timedelta ( seconds = TIMEOUT ) # 9 hours in seconds trainer . strategy . setup_distributed () # Call parent method to create trainer return trainer","title":"instantiate_trainer"},{"location":"cli/#scprint2.trainer.trainer","text":"Classes: Name Description TrainingMode","title":"trainer"},{"location":"cli/#scprint2.trainer.trainer.TrainingMode","text":"Bases: Callback TrainingMode a callback to set the training specific info to the model. This is because lightning is unfortunately setup this way. the model should be separated from training but at the same time it has training specific methods... so we have to do this. Parameters: noise ( List [ float ] , default: [0.6] ) \u2013 List of noise levels to apply if denoising is enabled. Defaults to [0.6], meaning only one forward path with 60% of the counts being dropped will happen. cce_temp ( float , default: 0.3 ) \u2013 Similarity threshold for CCE. Defaults to 0.5. cce_scale ( float , default: 0.2 ) \u2013 Scaling factor for CCE loss. Defaults to 0.002. ecs_threshold ( float , default: 0.4 ) \u2013 Threshold for ECS. Defaults to 0.3. ecs_scale ( float , default: 0.2 ) \u2013 Scaling factor for ECS loss. Defaults to 0.05. mvc_scale ( float , default: 0.0 ) \u2013 Scaling factor for MVC loss. Defaults to 1.0. do_generate ( bool , default: True ) \u2013 Whether to do the bottleneck learning task. Defaults to True. class_scale ( float , default: 1 ) \u2013 Scaling factor for classification loss. Defaults to 1.5. mask_ratio ( List [ float ] , default: [] ) \u2013 List of mask ratios to apply during training. Defaults to [], meaning no masking is applied during pretraining. warmup_duration ( int , default: 500 ) \u2013 Number of warmup steps for learning rate scheduling. Defaults to 500. fused_adam ( bool , default: False ) \u2013 Whether to use fused Adam optimizer. Defaults to True. adv_class_scale ( float , default: 1.0 ) \u2013 Scaling factor for adversarial classification loss. Defaults to 0.1. lr_reduce_patience ( int , default: 2 ) \u2013 Number of epochs with no improvement after which learning rate will be reduced. Defaults to 1. lr_reduce_factor ( float , default: 0.6 ) \u2013 Factor by which the learning rate will be reduced. Defaults to 0.6. lr_reduce_monitor ( str , default: 'val_loss' ) \u2013 Quantity to be monitored for learning rate reduction. Defaults to \"val_loss\". run_full_forward ( bool , default: False ) \u2013 Whether to run a second forward pass without masking or denoising for the bottleneck learning / MVC case. Defaults to False. lr ( float , default: 0.0001 ) \u2013 Initial learning rate. Defaults to 0.001. optim ( str , default: 'adamW' ) \u2013 Optimizer to use during training. Defaults to \"adamW\". weight_decay ( float , default: 0.01 ) \u2013 Weight decay to apply during optimization. Defaults to 0.01. name ( str , default: '' ) \u2013 Name of the training mode. Defaults to an empty string. should be an ID for the model test_every ( int , default: 5 ) \u2013 Number of epochs between testing. Defaults to 1. class_embd_diss_scale ( float , default: 0.3 ) \u2013 Scaling factor for the class embedding dissimilarity loss. Defaults to 0.1. zinb_and_mse ( bool , default: False ) \u2013 Whether to use ZINB and MSE loss. Defaults to False. var_context_length ( bool , default: False ) \u2013 Whether to use variable context length. Defaults to False. dropout ( float , default: 0.1 ) \u2013 Dropout rate for the model. Defaults to 0.1. set_step ( int , default: None ) \u2013 Set the global step for the model. Defaults to None. vae_kl_scale ( float , default: 0.001 ) \u2013 Scaling factor for the VAE KL loss. Defaults to 0.3. randsamp ( bool , default: True ) \u2013 Whether to use random sampling for the noise amount at each training step. Defaults to True. vae_kl_warmup_steps ( int , default: 80000 ) \u2013 Number of warmup steps for the VAE KL loss. Defaults to 20_000. mask_zeros ( bool , default: False ) \u2013 Whether to mask zeros in the expression matrix. Defaults to False. Source code in scprint2/trainer/trainer.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def __init__ ( self , noise : List [ float ] = [ 0.6 ], cce_temp : float = 0.3 , # .6 cce_scale : float = 0.2 , # .01 ecs_threshold : float = 0.4 , class_embd_diss_scale : float = 0.3 , ecs_scale : float = 0.2 , # .1 mvc_scale : float = 0.0 , do_next_tp : bool = False , do_generate : bool = True , class_scale : float = 1 , mask_ratio : List [ float | str ] = [], # 0.3 test_every : int = 5 , randsamp : bool = True , warmup_duration : int = 500 , fused_adam : bool = False , adv_class_scale : float = 1.0 , lr_reduce_patience : int = 2 , lr_reduce_factor : float = 0.6 , lr_reduce_monitor : str = \"val_loss\" , run_full_forward : bool = False , lr : float = 0.0001 , dropout : float = 0.1 , optim : str = \"adamW\" , weight_decay : float = 0.01 , zinb_and_mse : bool = False , var_context_length : bool = False , vae_kl_warmup_steps : int = 80_000 , vae_kl_scale : float = 0.001 , name : str = \"\" , set_step : Optional [ int ] = None , mask_zeros : bool = False , ): \"\"\" TrainingMode a callback to set the training specific info to the model. This is because lightning is unfortunately setup this way. the model should be separated from training but at the same time it has training specific methods... so we have to do this. Args: noise (List[float]): List of noise levels to apply if denoising is enabled. Defaults to [0.6], meaning only one forward path with 60% of the counts being dropped will happen. cce_temp (float): Similarity threshold for CCE. Defaults to 0.5. cce_scale (float): Scaling factor for CCE loss. Defaults to 0.002. ecs_threshold (float): Threshold for ECS. Defaults to 0.3. ecs_scale (float): Scaling factor for ECS loss. Defaults to 0.05. mvc_scale (float): Scaling factor for MVC loss. Defaults to 1.0. do_generate (bool): Whether to do the bottleneck learning task. Defaults to True. class_scale (float): Scaling factor for classification loss. Defaults to 1.5. mask_ratio (List[float]): List of mask ratios to apply during training. Defaults to [], meaning no masking is applied during pretraining. warmup_duration (int): Number of warmup steps for learning rate scheduling. Defaults to 500. fused_adam (bool): Whether to use fused Adam optimizer. Defaults to True. adv_class_scale (float): Scaling factor for adversarial classification loss. Defaults to 0.1. lr_reduce_patience (int): Number of epochs with no improvement after which learning rate will be reduced. Defaults to 1. lr_reduce_factor (float): Factor by which the learning rate will be reduced. Defaults to 0.6. lr_reduce_monitor (str): Quantity to be monitored for learning rate reduction. Defaults to \"val_loss\". run_full_forward (bool): Whether to run a second forward pass without masking or denoising for the bottleneck learning / MVC case. Defaults to False. lr (float): Initial learning rate. Defaults to 0.001. optim (str): Optimizer to use during training. Defaults to \"adamW\". weight_decay (float): Weight decay to apply during optimization. Defaults to 0.01. name (str): Name of the training mode. Defaults to an empty string. should be an ID for the model test_every (int): Number of epochs between testing. Defaults to 1. class_embd_diss_scale (float): Scaling factor for the class embedding dissimilarity loss. Defaults to 0.1. zinb_and_mse (bool): Whether to use ZINB and MSE loss. Defaults to False. var_context_length (bool): Whether to use variable context length. Defaults to False. dropout (float): Dropout rate for the model. Defaults to 0.1. set_step (int, optional): Set the global step for the model. Defaults to None. vae_kl_scale (float): Scaling factor for the VAE KL loss. Defaults to 0.3. randsamp (bool): Whether to use random sampling for the noise amount at each training step. Defaults to True. vae_kl_warmup_steps (int): Number of warmup steps for the VAE KL loss. Defaults to 20_000. mask_zeros (bool): Whether to mask zeros in the expression matrix. Defaults to False. \"\"\" super () . __init__ () self . noise = noise self . cce_temp = cce_temp self . cce_scale = cce_scale self . ecs_threshold = ecs_threshold self . ecs_scale = ecs_scale self . vae_kl_scale = vae_kl_scale self . do_next_tp = do_next_tp self . do_generate = do_generate self . class_scale = class_scale self . mask_ratio = mask_ratio self . warmup_duration = warmup_duration self . fused_adam = fused_adam self . mvc_scale = mvc_scale self . adv_class_scale = adv_class_scale self . lr_reduce_patience = lr_reduce_patience self . lr_reduce_factor = lr_reduce_factor self . lr_reduce_monitor = lr_reduce_monitor self . lr = lr self . optim = optim self . weight_decay = weight_decay self . run_full_forward = run_full_forward self . name = name self . test_every = test_every self . class_embd_diss_scale = class_embd_diss_scale self . zinb_and_mse = zinb_and_mse self . var_context_length = var_context_length self . dropout = dropout self . set_step = set_step self . randsamp = randsamp self . mask_zeros = mask_zeros self . vae_kl_warmup_steps = vae_kl_warmup_steps","title":"TrainingMode"},{"location":"model/","text":"Documentation for the model model description scprint2.model.model Classes: Name Description scPRINT2 scPRINT2 Bases: LightningModule , PyTorchModelHubMixin scPRINT-2: Single-Cell Pretrained Regulatory Inference Network Transformer. A foundation model for single-cell biology that learns cell and gene representations through self-supervised learning on large-scale single-cell RNA-seq data. The model can be used for: - Cell type classification and annotation - Gene expression denoising and imputation - Cell embedding generation for downstream analysis - Gene regulatory network inference via attention patterns - Multi-species gene expression modeling Architecture Overview Gene Encoder: Embeds gene identities (optionally with pretrained embeddings) Expression Encoder: Encodes expression values (continuous, binned, or metacell) Position Encoder: Optional genomic position encoding Transformer: Main attention-based encoder (various attention mechanisms) Cell Transformer: Optional separate transformer for cell embeddings Decoders: Expression reconstruction, classification, and MVC decoders The model supports multiple training objectives Masked expression prediction (like BERT) Denoising autoencoding Cell embedding contrastive learning (ECS and CCE losses) Multi-class cell type classification with hierarchical labels Multi-view coding (MVC) for robust representations Parameters: genes ( list | dict ) \u2013 Gene vocabulary. Either a list of gene names or a dict mapping organism names to lists of genes for multi-species models. organisms ( list [ str ] ) \u2013 List of organism ontology term IDs the model supports. d_model ( int , default: 256 ) \u2013 Hidden dimension of the transformer. Defaults to 256. nhead ( int , default: 4 ) \u2013 Number of attention heads. Defaults to 4. nlayers ( int , default: 8 ) \u2013 Number of transformer layers. Defaults to 8. precpt_gene_emb ( str , default: None ) \u2013 Path to parquet file with pretrained gene embeddings. Index should match gene names. Defaults to None. memmap_gene_emb ( bool , default: False ) \u2013 Memory-map gene embeddings for large files. Defaults to False. finetune_gene_emb ( bool , default: False ) \u2013 Add trainable adapter layers on top of frozen pretrained embeddings. Defaults to False. freeze_embeddings ( bool , default: True ) \u2013 Freeze gene embeddings during training. Defaults to True. gene_pos_file ( str , default: None ) \u2013 Path to parquet file with genomic positions. Must have 'pos' column with integer positions. Defaults to None. normalization ( str , default: 'sum' ) \u2013 Expression normalization method. One of: - \"sum\": Divide by total counts (TPM-like) - \"log\": Log2(1 + x) transform - \"both\": Sum normalization then log transform - \"raw\": No normalization Defaults to \"sum\". attn_bias ( str , default: None ) \u2013 Path to sparse matrix (.npz) with attention biases (e.g., gene-gene regulatory priors). Defaults to None. expr_encoder_layers ( int , default: 3 ) \u2013 Number of layers in expression encoder MLP. Defaults to 3. attention ( str , default: 'normal' ) \u2013 Attention mechanism type. One of: - \"normal\": Standard PyTorch attention - \"legacy-flash\": Flash attention via simpler-flash - \"performer\": Performer linear attention - \"hyper\": Compressed hyperbolic attention - \"criss-cross\": Criss-cross attention Defaults to \"normal\". expr_emb_style ( str , default: 'continuous' ) \u2013 Expression embedding approach. One of: - \"continuous\": MLP on continuous expression values - \"binned\": Learned embeddings for discretized expression bins - \"metacell\": DeepSet encoder aggregating KNN neighbors Defaults to \"continuous\". n_input_bins ( int , default: 0 ) \u2013 Number of expression bins when using binned embedding. Required if expr_emb_style=\"binned\". Defaults to 0. mvc_decoder ( str , default: None ) \u2013 Multi-view coding decoder architecture. One of: - None: No MVC decoder - \"inner product\": Dot product between cell and gene embeddings - \"concat query\": Concatenate cell embedding with gene queries - \"sum query\": Add cell embedding to gene queries Defaults to None. pred_embedding ( list [ str ] , default: None ) \u2013 Class names to use for cell embeddings during prediction/logging. Defaults to None (use all). layers_cls ( list [ int ] , default: [256, 128] ) \u2013 Hidden layer sizes for classification heads. Defaults to [256, 128]. classes ( dict [ str , int ] , default: None ) \u2013 Classification targets mapping class names to number of categories. E.g., {\"cell_type_ontology_term_id\": 100}. Defaults to None. labels_hierarchy ( dict [ str , dict [ int , list [ int ]]] , default: {} ) \u2013 Hierarchical label structure for ontology-based classes. Maps parent indices to lists of children indices. Defaults to {}. label_decoders ( dict [ str , dict [ int , str ]] , default: None ) \u2013 Mapping from encoded integers back to label strings for each class. Used for logging/plotting. Defaults to None. compress_class_dim ( dict [ str , int ] , default: None ) \u2013 Compressed embedding dimension for each class. Uses VAE or FSQ compression. Defaults to None. cell_specific_blocks ( bool , default: False ) \u2013 Use separate transformer for cell embeddings with cross-attention to gene transformer. Defaults to False. zinb ( bool , default: True ) \u2013 Use Zero-Inflated Negative Binomial distribution for expression reconstruction. If False, uses MSE loss. Defaults to True. splicing_head ( bool , default: False ) \u2013 Add separate decoder for spliced/unspliced expression. Defaults to False. do_adv_cls ( bool , default: False ) \u2013 Use adversarial classification to remove batch effects from cell type embeddings. Defaults to False. dropout ( float , default: 0.1 ) \u2013 Dropout rate throughout the model. Defaults to 0.1. use_metacell_token ( bool , default: False ) \u2013 Add learnable metacell token to distinguish single cells from metacells. Defaults to False. lr ( float , default: 0.0001 ) \u2013 Base learning rate. Defaults to 0.0001. nb_features ( int , default: None ) \u2013 Number of random features for Performer attention. Defaults to None. sketcher_size ( int , default: 200 ) \u2013 Sketch size for sparse attention methods. Defaults to 200. feature_redraw_interval ( int , default: None ) \u2013 Steps between random feature redraws for Performer. Defaults to None. num_heads_kv ( int , default: 4 ) \u2013 Number of key-value heads (for MQA/GQA). Defaults to 4. d_model_cell ( int , default: 128 ) \u2013 Hidden dim for cell transformer when using cell_specific_blocks. Defaults to 128. nhead_cell ( int , default: 4 ) \u2013 Attention heads for cell transformer. Defaults to 4. nlayers_cell ( int , default: 6 ) \u2013 Layers in cell transformer. Defaults to 6. num_heads_kv_cell ( int , default: 4 ) \u2013 KV heads for cell transformer. Defaults to 4. drop_path_rate ( float , default: 0.0 ) \u2013 Stochastic depth rate. Defaults to 0.0. **attention_kwargs ( dict , default: {} ) \u2013 Additional arguments passed to FlashTransformer. Attributes: Training ( Configuration (set these before training ) \u2013 noise (list[float]): Dropout rates for denoising task. E.g., [0.6]. mask_ratio (list[float]): Mask ratios for masked prediction. E.g., [0.15]. cce_temp (float): Temperature for contrastive loss. cce_scale (float): Weight for contrastive cell embedding loss. ecs_scale (float): Weight for elastic cell similarity loss. ecs_threshold (float): Similarity threshold for ECS loss. mvc_scale (float): Weight for MVC reconstruction loss. class_scale (float): Weight for classification loss. lr_reduce_patience (int): Epochs before reducing learning rate. lr_reduce_factor (float): Factor to reduce learning rate by. warmup_duration (int): Steps for learning rate warmup. Prediction ( Configuration (set before predict ) \u2013 predict_mode (str): \"none\" or \"generate\" for expression generation. pred_embedding (list[str]): Classes to include in cell embeddings. get_attention_layer (list[int]): Layers to extract attention from. predict_depth_mult (float): Multiplier for depth in generation. pred_log_adata (bool): Whether to log predictions as AnnData. Example Initialize model model = scPrint2( ... genes=gene_list, ... organisms=[\"NCBITaxon:9606\"], ... d_model=512, ... nlayers=12, ... classes={\"cell_type_ontology_term_id\": 100}, ... ) Configure training model.noise = [0.4, 0.6] model.mask_ratio = [0.15, 0.3] Train with PyTorch Lightning trainer = L.Trainer(max_epochs=100) trainer.fit(model, datamodule) Generate embeddings model.pred_embedding = [\"cell_type_ontology_term_id\"] predictions = trainer.predict(model, datamodule) Note The model is designed to work with scDataLoader's DataModule and Collator. Gene order must match between model initialization and data loading. Methods: Name Description add_organism Add a new organism to an existing model for transfer learning. configure_optimizers @see pl.LightningModule forward Complete forward pass through the scPRINT-2 log_adata log_adata will log an adata from predictions. on_fit_start @see pl.LightningModule on_load_checkpoint Handle checkpoint loading with backward compatibility. on_predict_epoch_end @see pl.LightningModule will on_predict_epoch_start @see pl.LightningModule on_test_start @see pl.LightningModule on_validation_epoch_end @see pl.LightningModule optimizer_step @see pl.LightningModule predict_step embed given gene expression, encode the gene embedding and cell embedding. training_step training_step defines the train loop. It is independent of forward validation_step validation_step defines the validation loop. It is independent of forward Attributes: genes ( list [ str ] ) \u2013 Get flattened list of all genes in the model's vocabulary. Source code in scprint2/model/model.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 def __init__ ( self , genes , organisms : list [ str ], d_model : int = 256 , nhead : int = 4 , nlayers : int = 8 , precpt_gene_emb : Optional [ str ] = None , memmap_gene_emb : bool = False , finetune_gene_emb : bool = False , freeze_embeddings : bool = True , gene_pos_file : Optional [ str ] = None , normalization : str = \"sum\" , # log, sum, both, raw attn_bias : Optional [ str ] = None , expr_encoder_layers : int = 3 , attention : str = \"normal\" , # \"performer\", \"legacy-flash\", \"normal\", \"criss-cross\", \"hyper\", \"adasplash\", \"softpick\", \"softpick-flash\" expr_emb_style : str = \"continuous\" , # \"binned\", \"continuous\", \"metacell\" n_input_bins : int = 0 , mvc_decoder : Optional [ str ] = None , # \"inner product\", \"concat query\", \"sum query\" pred_embedding : Optional [ list [ str ]] = None , layers_cls : list [ int ] = [ 256 , 128 ], classes : Optional [ Dict [ str , int ]] = None , labels_hierarchy : Dict [ str , Dict [ int , list [ int ]]] = {}, label_decoders : Optional [ Dict [ str , Dict [ int , str ]]] = None , compress_class_dim : Optional [ Dict [ str , int ]] = None , cell_specific_blocks : bool = False , zinb : bool = True , splicing_head : bool = False , do_adv_cls : bool = False , dropout : float = 0.1 , use_metacell_token : bool = False , lr : float = 0.0001 , nb_features : Optional [ int ] = None , sketcher_size : int = 200 , feature_redraw_interval : Optional [ int ] = None , num_heads_kv : int = 4 , d_model_cell : int = 128 , nhead_cell : int = 4 , nlayers_cell : int = 6 , num_heads_kv_cell : int = 4 , transformer = None , drop_path_rate : float = 0.0 , # unused args from older versions kept for loading old models gene_pos_enc = None , max_cont_len = None , residual_in_fp32 = None , checkpointing = None , fused_dropout_add_ln = None , strict_loading = None , optim = None , weight_decay = None , prenorm = None , domain_spec_batchnorm = None , use_flash_attn = None , cell_emb_style = None , num_batch_labels = None , fused_mlp = None , fused_bias_fc = None , ** attention_kwargs : dict , ): \"\"\" scPRINT-2: Single-Cell Pretrained Regulatory Inference Network Transformer. A foundation model for single-cell biology that learns cell and gene representations through self-supervised learning on large-scale single-cell RNA-seq data. The model can be used for: - Cell type classification and annotation - Gene expression denoising and imputation - Cell embedding generation for downstream analysis - Gene regulatory network inference via attention patterns - Multi-species gene expression modeling Architecture Overview: 1. Gene Encoder: Embeds gene identities (optionally with pretrained embeddings) 2. Expression Encoder: Encodes expression values (continuous, binned, or metacell) 3. Position Encoder: Optional genomic position encoding 4. Transformer: Main attention-based encoder (various attention mechanisms) 5. Cell Transformer: Optional separate transformer for cell embeddings 6. Decoders: Expression reconstruction, classification, and MVC decoders The model supports multiple training objectives: - Masked expression prediction (like BERT) - Denoising autoencoding - Cell embedding contrastive learning (ECS and CCE losses) - Multi-class cell type classification with hierarchical labels - Multi-view coding (MVC) for robust representations Args: genes (list | dict): Gene vocabulary. Either a list of gene names or a dict mapping organism names to lists of genes for multi-species models. organisms (list[str]): List of organism ontology term IDs the model supports. d_model (int, optional): Hidden dimension of the transformer. Defaults to 256. nhead (int, optional): Number of attention heads. Defaults to 4. nlayers (int, optional): Number of transformer layers. Defaults to 8. precpt_gene_emb (str, optional): Path to parquet file with pretrained gene embeddings. Index should match gene names. Defaults to None. memmap_gene_emb (bool, optional): Memory-map gene embeddings for large files. Defaults to False. finetune_gene_emb (bool, optional): Add trainable adapter layers on top of frozen pretrained embeddings. Defaults to False. freeze_embeddings (bool, optional): Freeze gene embeddings during training. Defaults to True. gene_pos_file (str, optional): Path to parquet file with genomic positions. Must have 'pos' column with integer positions. Defaults to None. normalization (str, optional): Expression normalization method. One of: - \"sum\": Divide by total counts (TPM-like) - \"log\": Log2(1 + x) transform - \"both\": Sum normalization then log transform - \"raw\": No normalization Defaults to \"sum\". attn_bias (str, optional): Path to sparse matrix (.npz) with attention biases (e.g., gene-gene regulatory priors). Defaults to None. expr_encoder_layers (int, optional): Number of layers in expression encoder MLP. Defaults to 3. attention (str, optional): Attention mechanism type. One of: - \"normal\": Standard PyTorch attention - \"legacy-flash\": Flash attention via simpler-flash - \"performer\": Performer linear attention - \"hyper\": Compressed hyperbolic attention - \"criss-cross\": Criss-cross attention Defaults to \"normal\". expr_emb_style (str, optional): Expression embedding approach. One of: - \"continuous\": MLP on continuous expression values - \"binned\": Learned embeddings for discretized expression bins - \"metacell\": DeepSet encoder aggregating KNN neighbors Defaults to \"continuous\". n_input_bins (int, optional): Number of expression bins when using binned embedding. Required if expr_emb_style=\"binned\". Defaults to 0. mvc_decoder (str, optional): Multi-view coding decoder architecture. One of: - None: No MVC decoder - \"inner product\": Dot product between cell and gene embeddings - \"concat query\": Concatenate cell embedding with gene queries - \"sum query\": Add cell embedding to gene queries Defaults to None. pred_embedding (list[str], optional): Class names to use for cell embeddings during prediction/logging. Defaults to None (use all). layers_cls (list[int], optional): Hidden layer sizes for classification heads. Defaults to [256, 128]. classes (dict[str, int], optional): Classification targets mapping class names to number of categories. E.g., {\"cell_type_ontology_term_id\": 100}. Defaults to None. labels_hierarchy (dict[str, dict[int, list[int]]], optional): Hierarchical label structure for ontology-based classes. Maps parent indices to lists of children indices. Defaults to {}. label_decoders (dict[str, dict[int, str]], optional): Mapping from encoded integers back to label strings for each class. Used for logging/plotting. Defaults to None. compress_class_dim (dict[str, int], optional): Compressed embedding dimension for each class. Uses VAE or FSQ compression. Defaults to None. cell_specific_blocks (bool, optional): Use separate transformer for cell embeddings with cross-attention to gene transformer. Defaults to False. zinb (bool, optional): Use Zero-Inflated Negative Binomial distribution for expression reconstruction. If False, uses MSE loss. Defaults to True. splicing_head (bool, optional): Add separate decoder for spliced/unspliced expression. Defaults to False. do_adv_cls (bool, optional): Use adversarial classification to remove batch effects from cell type embeddings. Defaults to False. dropout (float, optional): Dropout rate throughout the model. Defaults to 0.1. use_metacell_token (bool, optional): Add learnable metacell token to distinguish single cells from metacells. Defaults to False. lr (float, optional): Base learning rate. Defaults to 0.0001. nb_features (int, optional): Number of random features for Performer attention. Defaults to None. sketcher_size (int, optional): Sketch size for sparse attention methods. Defaults to 200. feature_redraw_interval (int, optional): Steps between random feature redraws for Performer. Defaults to None. num_heads_kv (int, optional): Number of key-value heads (for MQA/GQA). Defaults to 4. d_model_cell (int, optional): Hidden dim for cell transformer when using cell_specific_blocks. Defaults to 128. nhead_cell (int, optional): Attention heads for cell transformer. Defaults to 4. nlayers_cell (int, optional): Layers in cell transformer. Defaults to 6. num_heads_kv_cell (int, optional): KV heads for cell transformer. Defaults to 4. drop_path_rate (float, optional): Stochastic depth rate. Defaults to 0.0. **attention_kwargs (dict): Additional arguments passed to FlashTransformer. Attributes: Training Configuration (set these before training): noise (list[float]): Dropout rates for denoising task. E.g., [0.6]. mask_ratio (list[float]): Mask ratios for masked prediction. E.g., [0.15]. cce_temp (float): Temperature for contrastive loss. cce_scale (float): Weight for contrastive cell embedding loss. ecs_scale (float): Weight for elastic cell similarity loss. ecs_threshold (float): Similarity threshold for ECS loss. mvc_scale (float): Weight for MVC reconstruction loss. class_scale (float): Weight for classification loss. lr_reduce_patience (int): Epochs before reducing learning rate. lr_reduce_factor (float): Factor to reduce learning rate by. warmup_duration (int): Steps for learning rate warmup. Prediction Configuration (set before predict): predict_mode (str): \"none\" or \"generate\" for expression generation. pred_embedding (list[str]): Classes to include in cell embeddings. get_attention_layer (list[int]): Layers to extract attention from. predict_depth_mult (float): Multiplier for depth in generation. pred_log_adata (bool): Whether to log predictions as AnnData. Example: >>> # Initialize model >>> model = scPrint2( ... genes=gene_list, ... organisms=[\"NCBITaxon:9606\"], ... d_model=512, ... nlayers=12, ... classes={\"cell_type_ontology_term_id\": 100}, ... ) >>> >>> # Configure training >>> model.noise = [0.4, 0.6] >>> model.mask_ratio = [0.15, 0.3] >>> >>> # Train with PyTorch Lightning >>> trainer = L.Trainer(max_epochs=100) >>> trainer.fit(model, datamodule) >>> >>> # Generate embeddings >>> model.pred_embedding = [\"cell_type_ontology_term_id\"] >>> predictions = trainer.predict(model, datamodule) Note: The model is designed to work with scDataLoader's DataModule and Collator. Gene order must match between model initialization and data loading. \"\"\" super () . __init__ () self . save_hyperparameters () # training flags self . noise = [ 0.6 ] self . cce_temp = 0.3 self . lr = lr self . cce_scale = 0.2 self . ecs_threshold = 0.4 self . ecs_scale = 0.2 self . mvc_scale = 1.0 self . class_embd_diss_scale = 0.3 self . adv_class_scale = 1.0 self . do_adv_cls = do_adv_cls self . run_full_forward = True self . class_scale = 1 self . zinb_and_mse = False self . do_next_tp = False self . do_generate = False self . var_context_length = False self . mask_ratio = [] self . warmup_duration = 500 self . weight_decay = 0.01 self . optim = \"adamW\" self . fused_adam = False self . lr_reduce_patience = 2 self . lr_reduce_factor = 0.6 self . test_every = 20 self . randsamp = True self . lr_reduce_monitor = \"val_loss\" self . name = \"\" self . set_step = None self . lrfinder_steps = 0 self . doplot = False self . get_attention_layer = None self . embs = None self . pred_log_adata = True self . predict_depth_mult = 3 self . predict_mode = \"none\" self . keep_all_labels_pred = False self . mask_zeros = False self . vae_kl_scale = 0.05 self . vae_kl_warmup_steps = 40_000 # Default value, can be adjusted self . save_expr = False self . counter = 0 # should be stored somehow self . d_model = d_model self . normalization = normalization self . attn_bias = attn_bias if attn_bias != \"none\" else None self . organisms = organisms self . nlayers = nlayers self . use_metacell_token = use_metacell_token self . mvc_decoder = mvc_decoder # need to store self . n_input_bins = n_input_bins self . attention = attention if classes is None : classes = {} self . label_counts = classes self . classes = list ( classes . keys ()) self . label_decoders = label_decoders self . pred_embedding = pred_embedding self . _genes = genes self . expr_emb_style = expr_emb_style if labels_hierarchy is None : labels_hierarchy = {} self . labels_hierarchy = labels_hierarchy self . hparams [ \"classes\" ] = classes self . hparams [ \"label_decoders\" ] = label_decoders self . hparams [ \"organisms\" ] = organisms self . hparams [ \"use_metacell_token\" ] = use_metacell_token # 20x more likely to drop a non TF compared to a TF self . tf_masker = WeightedMasker ( self . genes , tf_weight = 0.05 ) self . attn = utils . Attention ( len ( self . genes ), additional_tokens = ( ( 1 if self . use_metacell_token else 0 ) + (( len ( classes ) + 1 ) if not cell_specific_blocks else 0 ) ), ) self . mat_labels_hierarchy = {} for k , v in labels_hierarchy . items (): tens = torch . zeros (( len ( v ), classes [ k ])) for k2 , v2 in v . items (): tens [ k2 - classes [ k ], v2 ] = 1 self . mat_labels_hierarchy [ k ] = tens . to ( bool ) # encoder # gene encoder if gene_pos_file is not None : gene_pos_enc = pd . read_parquet ( gene_pos_file ) if len ( gene_pos_enc ) < len ( self . genes ): print ( \"Warning: only a subset of the genes available in the loc file.\" ) for k , v in self . _genes . items (): tokeep = set ( gene_pos_enc . index . tolist ()) self . _genes [ k ] = [ u for u in v if u in tokeep ] if len ( self . _genes [ k ]) < 100 : raise ValueError ( f \"the gene pos file { gene_pos_file } does not match most of the genes given to the model for species { k } \" ) gene_pos_enc = gene_pos_enc . loc [ self . genes , [ \"pos\" ]] if precpt_gene_emb is not None : embeddings = pd . read_parquet ( precpt_gene_emb ) if len ( embeddings ) < len ( self . genes ): print ( \"Warning: only a subset of the genes available in the embeddings file.\" ) for k , v in self . _genes . items (): tokeep = set ( embeddings . index . tolist ()) self . _genes [ k ] = [ u for u in v if u in tokeep ] if len ( self . _genes [ k ]) < 100 : raise ValueError ( f \"the gene embeddings file { precpt_gene_emb } does not match most of the genes given to the model for species { k } \" ) embeddings = embeddings . loc [ self . genes ] print ( \"number of genes: \" , len ( embeddings )) if not memmap_gene_emb : sembeddings = torch . nn . AdaptiveAvgPool1d ( d_model )( torch . tensor ( embeddings . values , dtype = torch . float32 ) ) else : embeddings = None gene_encoder = encoders . GeneEncoder ( len ( self . genes ), d_model , weights_file = precpt_gene_emb if memmap_gene_emb else None , weights = sembeddings if not memmap_gene_emb else None , freeze = freeze_embeddings , ) else : gene_encoder = encoders . GeneEncoder ( len ( self . genes ), d_model , freeze = freeze_embeddings ) if finetune_gene_emb : if not freeze_embeddings : raise ValueError ( \"finetune_gene_emb is True but freeze_embeddings is False\" ) # Create adapter layers after the frozen base encoder self . gene_encoder = torch . nn . Sequential ( gene_encoder , torch . nn . Linear ( d_model , d_model ), torch . nn . ReLU (), torch . nn . Linear ( d_model , d_model ), ) else : self . gene_encoder = gene_encoder # Positional Encoding if gene_pos_file is not None : # redoing it just in case some were dropped with embbeding file step gene_pos_enc = gene_pos_enc . loc [ self . genes , \"pos\" ] . astype ( int ) . tolist () self . pos_encoder = encoders . PositionalEncoding ( d_model , gene_pos_enc = gene_pos_enc ) else : self . pos_encoder = None # Value Encoder, NOTE: the scaling style is also handled in _encode method expr_d_model = d_model # // 8 if finetune_gene_emb else d_model if expr_emb_style in \"continuous\" : expr_encoder = encoders . ContinuousValueEncoder ( expr_d_model , dropout , layers = expr_encoder_layers ) elif expr_emb_style == \"binned\" : assert n_input_bins > 0 assert normalization == \"raw\" , \"shouldn't use normalization\" expr_encoder = encoders . CategoryValueEncoder ( n_input_bins , expr_d_model ) elif expr_emb_style == \"metacell\" : expr_encoder = encoders . EasyExprGNN ( self_dim = expr_d_model * 2 , output_dim = expr_d_model , shared_layers = expr_encoder_layers , dropout = dropout , ) else : raise ValueError ( f \"expr_emb_style should be one of binned, continuous, metacell, \" f \"got { expr_emb_style } \" ) if finetune_gene_emb and False : self . expr_encoder = encoders . ExprBasedFT ( d_model , gene_encoder , expr_encoder , dropout , layers = expr_encoder_layers , intermediary_d = int ( d_model * 1.5 ), ) else : self . expr_encoder = expr_encoder # Class Encoder # always have [base_cell_emb, time_embedding, depth_embedding] + any other class info # base cell embedding will store other cell specific information self . class_encoder = encoders . CategoryValueEncoder ( len ( self . classes ) + 1 , d_model if not cell_specific_blocks else d_model_cell , ) if self . use_metacell_token : self . metacell_encoder = encoders . CategoryValueEncoder ( 2 , d_model ) # compute tensor for mat_labels_hierarchy # old parameters that can still be passed when loading older models (managed in the _on_load_ckpt function) for i in [ \"strict_loading\" , \"optim\" , \"weight_decay\" , \"d_hid\" , \"edge_dim\" , \"prenorm\" , \"domain_spec_batchnorm\" , \"use_flash_attn\" , \"cell_emb_style\" , \"num_batch_labels\" , \"transformer\" , \"residual_in_fp32\" , \"max_cont_len\" , ]: if i in attention_kwargs : attention_kwargs . pop ( i ) # attention # Linear if attention == \"linear\" : # linear attention using the fast attention package # self.attention = FastattentionEncoder( # d_model, nhead, d_hid, nlayers, dropout, \"linear\" # ) raise NotImplementedError ( \"Linear attention is not implemented\" ) elif attention == \"performer\" : self . transformer = Performer ( dim = d_model , depth = nlayers , heads = nhead , dim_head = d_model // nhead , causal = False , attn_dropout = dropout , ff_dropout = dropout , qkv_bias = True , nb_features = nb_features , feature_redraw_interval = feature_redraw_interval , ) else : self . transformer = FlashTransformer ( d_model = d_model , nhead = nhead , dropout = dropout , attn_dropout = dropout , nlayers = nlayers , cross_attn = cell_specific_blocks , cross_dim = d_model_cell , attn_type = \"flash\" if attention == \"legacy-flash\" else attention , num_heads_kv = num_heads_kv , sketcher_size = sketcher_size , drop_path_rate = drop_path_rate , ** attention_kwargs , ) if cell_specific_blocks : attention_kwargs . pop ( \"num_heads_kv\" , None ) self . cell_transformer = FlashTransformer ( d_model = d_model_cell , nhead = nhead_cell , num_heads_kv = num_heads_kv_cell , nlayers = nlayers_cell , dropout = dropout , cross_attn = True , cross_dim = d_model , attn_type = \"flash\" if attention == \"legacy-flash\" else \"normal\" , ** attention_kwargs , ) else : self . cell_transformer = None # decoders # expression self . splicing_head = None if expr_emb_style == \"binned\" : self . expr_decoder = decoders . ClsDecoder ( d_model , n_input_bins , layers = [ d_model // 2 , d_model // 4 ], dropout = dropout , ) else : self . expr_decoder = decoders . ExprDecoder ( d_model , dropout = dropout , zinb = zinb , use_depth = True , ) if splicing_head : self . splicing_head = decoders . ExprDecoder ( d_model , dropout = dropout , zinb = zinb , use_depth = True , ) # cls decoder self . cls_decoders = torch . nn . ModuleDict () # should be a very simple classifier for most things # (maybe scale with the number of classes) should be 1 layer... for clss , n_cls in classes . items (): mdim = d_model_cell if cell_specific_blocks else self . d_model dim = compress_class_dim [ clss ] if compress_class_dim is not None else mdim self . cls_decoders [ clss ] = decoders . ClsDecoder ( dim , n_cls , layers = layers_cls , dropout = dropout , ) if \"cell_type_ontology_term_id\" in classes and self . do_adv_cls : mdim = d_model_cell if cell_specific_blocks else self . d_model dim = ( compress_class_dim [ \"cell_type_ontology_term_id\" ] if compress_class_dim is not None else mdim ) if \"assay_ontology_term_id\" in classes : self . assay_relab = utils . relabel_assay_for_adv ( self . label_decoders , self . labels_hierarchy ) self . adv_assay_decoder = decoders . ClsDecoder ( dim , len ( set ( self . assay_relab . values ())), layers = layers_cls , dropout = dropout , ) if len ( self . organisms ) > 1 : self . adv_organism_decoder = decoders . ClsDecoder ( dim , len ( self . organisms ), layers = layers_cls , dropout = dropout , ) # expression decoder from batch embbedding if mvc_decoder is not None : if cell_specific_blocks : raise ValueError ( \"MVC decoder is not supported for cell specific blocks\" ) self . mvc_decoder = decoders . MVCDecoder ( d_model , arch_style = mvc_decoder , zinb = zinb , use_depth = True ) else : self . mvc_decoder = None if compress_class_dim is not None : self . compressor = torch . nn . ModuleDict () dim = d_model_cell if cell_specific_blocks else self . d_model for k , v in compress_class_dim . items (): if v >= 8 : self . compressor [ k ] = decoders . VAEDecoder ( dim , layers = [ 128 , v , ], dropout = dropout , return_latent = True , ) else : self . compressor [ k ] = fsq . FSQ ( levels = [ 2 ] * v , dim = dim ) else : self . compressor = None self . apply ( partial ( utils . _init_weights , n_layer = nlayers , ) ) for i , dec in self . cls_decoders . items (): torch . nn . init . constant_ ( dec . out_layer . bias , - 0.13 ) self . expr_encoder . _init_weights () genes property Get flattened list of all genes in the model's vocabulary. For multi-organism models, concatenates genes from all organisms in consistent order. Returns: list [ str ] \u2013 list[str]: Gene names in model vocabulary order. add_organism Add a new organism to an existing model for transfer learning. Extends the gene vocabulary and embeddings to include genes from a new organism. Useful for applying a pretrained model to a new species. Parameters: organism ( str ) \u2013 Organism ontology term ID (e.g., \"NCBITaxon:10090\" for mouse). genes ( Index ) \u2013 Gene names/IDs for the new organism. emb ( DataFrame ) \u2013 Gene embeddings DataFrame with genes as index. Will be resized to match model's d_model. locs ( DataFrame , default: None ) \u2013 Genomic positions with 'pos' column. Required if model uses positional encoding. Defaults to None. Raises: ValueError \u2013 If model requires gene locations but none provided. ValueError \u2013 If gene positions exceed model's maximum position encoding. Note Only genes present in both genes and emb (and locs if provided) will be added. The model's gene encoder is expanded in-place. Source code in scprint2/model/model.py 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 def add_organism ( self , organism : str , genes : pd . Index , emb : pd . DataFrame , locs = None ): \"\"\" Add a new organism to an existing model for transfer learning. Extends the gene vocabulary and embeddings to include genes from a new organism. Useful for applying a pretrained model to a new species. Args: organism (str): Organism ontology term ID (e.g., \"NCBITaxon:10090\" for mouse). genes (pd.Index): Gene names/IDs for the new organism. emb (pd.DataFrame): Gene embeddings DataFrame with genes as index. Will be resized to match model's d_model. locs (pd.DataFrame, optional): Genomic positions with 'pos' column. Required if model uses positional encoding. Defaults to None. Raises: ValueError: If model requires gene locations but none provided. ValueError: If gene positions exceed model's maximum position encoding. Note: Only genes present in both `genes` and `emb` (and `locs` if provided) will be added. The model's gene encoder is expanded in-place. \"\"\" if self . pos_encoder is not None and locs is None : raise ValueError ( \"this model needs gene locations to add a new organism\" ) self . organisms . append ( organism ) if locs is not None : overlap = set ( locs . index ) & set ( emb . index ) & set ( genes . index ) genes = genes [ genes . index . isin ( overlap )] locs = locs . loc [ genes . index ] pos = locs [ \"pos\" ] token_to_pos = { token : pos for token , pos in enumerate ( pos )} if self . pos_encoder . pe . shape [ 0 ] < max ( pos ): raise ValueError ( f \"the number of gene locs in the added organism needs to be less than { self . pos_encoder . pe . shape [ 0 ] } \" ) token_to_pos = { token : pos for token , pos in enumerate ( pos )} arr = [] for _ , v in token_to_pos . items (): arr . append ( self . pos_encoder . pe [ v - 1 ] . to ( \"cpu\" ) . numpy ()) pe = torch . Tensor ( np . array ( arr )) . to ( self . pos_encoder . pe . device ) self . pos_encoder . pe = torch . cat ([ self . pos_encoder . pe , pe ], dim = 0 ) else : overlap = set ( emb . index ) & set ( genes . index ) genes = genes [ genes . index . isin ( overlap )] emb = emb . loc [ genes . index ] self . _genes [ organism ] = genes . index . tolist () if self . gene_encoder is None : genc = self . expr_encoder . gene_encoder else : genc = self . gene_encoder if type ( genc ) is torch . nn . Sequential : enc = genc [ 0 ] else : enc = genc semb = torch . nn . AdaptiveAvgPool1d ( self . d_model )( torch . tensor ( emb . values , dtype = torch . float32 ) ) . to ( enc . embeddings . weight . data . device ) if enc . memmap : print ( \"todev.. will fail for now\" ) embs = torch . cat ([ enc . embeddings . weight . data , semb ], dim = 0 ) enc . embeddings = nn . Embedding ( embs . shape [ 0 ], embs . shape [ 1 ], padding_idx = None , _freeze = enc . embeddings . weight . requires_grad , ) enc . embeddings . weight . data . copy_ ( embs ) enc . embeddings . weight . data = enc . embeddings . weight . data . to ( self . device ) if type ( genc ) is torch . nn . Sequential : genc [ 0 ] = enc else : genc = enc if self . gene_encoder is None : self . expr_encoder . gene_encoder = genc else : self . gene_encoder = genc configure_optimizers @see pl.LightningModule Source code in scprint2/model/model.py 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 def configure_optimizers ( self ): \"\"\"@see pl.LightningModule\"\"\" # https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam # not working because of poor weight decay implem if self . optim == \"adam\" : optimizer = optim . Adam ( self . parameters (), lr = self . hparams . lr , betas = ( 0.9 , 0.999 ), eps = 1e-7 , # 1e-5 to 1e-8 weight_decay = self . weight_decay , amsgrad = False , fused = self . fused_adam , ) elif self . optim == \"adamW\" : optimizer = optim . AdamW ( self . parameters (), lr = self . hparams . lr , betas = ( 0.9 , 0.999 ), eps = 1e-7 , # 1e-5 to 1e-8 weight_decay = self . weight_decay , amsgrad = False , fused = self . fused_adam , ) elif self . optim == \"galore\" : raise NotImplementedError ( \"Galore optimizer not implemented\" ) # param_groups = [ # { # \"params\": [ # v for k, v in self.named_parameters() if \"transformer\" not in k # ] # }, # { # \"params\": [ # v for k, v in self.named_parameters() if \"transformer\" in k # ], # \"rank\": 128, # \"update_proj_gap\": 200, # \"scale\": 0.25, # \"proj_type\": \"std\", # }, # ] # optimizer = GaLoreAdamW(param_groups, lr=self.hparams.lr) else : raise ValueError ( f \"Unknown optimizer: { self . optim } \" ) if self . lr_reduce_monitor is None : print ( \"no lr reduce factor\" ) return [ optimizer ] # lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts( # optimizer, # T_0=20000, # T_mult=2, # eta_min=1e-8, # ) # interval = \"step\" # frequency = 10 # lr_scheduler = optim.lr_scheduler.ExponentialLR( # optimizer, # gamma=0.85, # ) lr_scheduler = optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = \"min\" , patience = self . lr_reduce_patience , factor = self . lr_reduce_factor , ) interval = \"epoch\" frequency = 1 # lr_scheduler = StepwiseCAWRWithWD( # optimizer, # T_0=20_000, # T_mult=2, # eta_min=1e-8, # wd_decay=0.9 # ) lr_dict = { \"scheduler\" : lr_scheduler , # The unit of the scheduler's step size, could also be 'step'. # 'epoch' updates the scheduler on epoch end whereas 'step' # updates it after a optimizer update. \"interval\" : interval , # How many epochs/steps should pass between calls to # `scheduler.step()`. 1 corresponds to updating the learning # rate after every epoch/step. \"frequency\" : frequency , # Metric to to monitor for schedulers like `ReduceLROnPlateau` \"monitor\" : self . lr_reduce_monitor , } self . lrfinder_steps = 0 for val in self . trainer . callbacks : if type ( val ) is _LRCallback : self . lrfinder_steps = val . num_training if type ( val ) is LearningRateFinder : self . lrfinder_steps = val . _num_training_steps return [ optimizer ], [ lr_dict ] forward Complete forward pass through the scPRINT-2 Encodes input expression data, processes through transformer(s), and decodes into expression predictions and cell classifications. Parameters: gene_pos ( Tensor ) \u2013 Gene indices of shape (batch, seq_len) mapping to positions in the model's gene vocabulary. expression ( Tensor , default: None ) \u2013 Expression values of shape (batch, seq_len). Can be raw counts or normalized depending on model config. Defaults to None. neighbors ( Tensor , default: None ) \u2013 KNN neighbor expressions of shape (batch, n_neighbors, seq_len) for metacell-style encoding. Defaults to None. neighbors_info ( Tensor , default: None ) \u2013 Neighbor weights of shape (batch, n_neighbors). Defaults to None. mask ( Tensor , default: None ) \u2013 Boolean mask of shape (batch, seq_len) where True indicates positions to mask (set to zero). Defaults to None. req_depth ( Tensor , default: None ) \u2013 Target sequencing depth of shape (batch,) for depth-conditional generation. Defaults to None. get_gene_emb ( bool , default: False ) \u2013 Return gene embeddings from transformer. Defaults to False. metacell_token ( Tensor , default: None ) \u2013 Binary metacell indicators of shape (batch,). Defaults to None. depth_mult ( Tensor , default: None ) \u2013 Expression depth multiplier. If None, uses sum of expression values. Defaults to None. do_sample ( bool , default: False ) \u2013 Sample from predicted distribution. Currently unused. Defaults to False. do_mvc ( bool , default: False ) \u2013 Compute multi-view coding predictions. Defaults to False. do_class ( bool , default: False ) \u2013 Compute classification predictions. Defaults to False. get_attention_layer ( list [ int ] , default: None ) \u2013 Layer indices to extract attention weights from. Defaults to None. mask_zeros ( Tensor , default: None ) \u2013 Boolean mask for zero-expression genes of shape (batch, seq_len + num_special_tokens). Defaults to None. Returns: Dict [ str , Tensor ] | tuple [ Dict [ str , Tensor ], list ] \u2013 dict[str, Tensor] | tuple[dict, list]: Model outputs containing: - \"mean\": Predicted expression (batch, seq_len) - \"disp\": Dispersion parameters (batch, seq_len) [if ZINB] - \"zero_logits\": Zero-inflation logits (batch, seq_len) [if ZINB] - \"input_cell_embs\": Cell embeddings (batch, n_classes+1, d_model) - \"input_cell_emb\": Mean cell embedding (batch, d_model) - \"output_cell_embs\": Processed cell embeddings - \"output_cell_emb\": Final cell embedding - \"cls_output_{class}\": Classification logits for each class - \"gene_embedding\": Gene embeddings [if get_gene_emb] - \"mvc_*\": MVC predictions [if do_mvc] If get_attention_layer is not None, returns (outputs_dict, attention_list) where attention_list contains QKV tensors from specified layers. Example output = model( ... gene_pos=batch[\"genes\"], ... expression=batch[\"x\"], ... req_depth=batch[\"depth\"], ... do_class=True, ... ) predictions = output[\"mean\"] cell_types = output[\"cls_output_cell_type_ontology_term_id\"].argmax(-1) Source code in scprint2/model/model.py 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 def forward ( self , gene_pos : Tensor , expression : Optional [ Tensor ] = None , neighbors : Optional [ Tensor ] = None , neighbors_info : Optional [ Tensor ] = None , mask : Optional [ Tensor ] = None , req_depth : Optional [ Tensor ] = None , get_gene_emb : bool = False , metacell_token : Optional [ Tensor ] = None , # (minibatch, 1) depth_mult : Optional [ Tensor ] = None , do_sample : bool = False , do_mvc : bool = False , do_class : bool = False , get_attention_layer : Optional [ list ] = None , mask_zeros : Optional [ Tensor ] = None , ) -> Dict [ str , Tensor ] | tuple [ Dict [ str , Tensor ], list ]: \"\"\" Complete forward pass through the scPRINT-2 Encodes input expression data, processes through transformer(s), and decodes into expression predictions and cell classifications. Args: gene_pos (Tensor): Gene indices of shape (batch, seq_len) mapping to positions in the model's gene vocabulary. expression (Tensor, optional): Expression values of shape (batch, seq_len). Can be raw counts or normalized depending on model config. Defaults to None. neighbors (Tensor, optional): KNN neighbor expressions of shape (batch, n_neighbors, seq_len) for metacell-style encoding. Defaults to None. neighbors_info (Tensor, optional): Neighbor weights of shape (batch, n_neighbors). Defaults to None. mask (Tensor, optional): Boolean mask of shape (batch, seq_len) where True indicates positions to mask (set to zero). Defaults to None. req_depth (Tensor, optional): Target sequencing depth of shape (batch,) for depth-conditional generation. Defaults to None. get_gene_emb (bool, optional): Return gene embeddings from transformer. Defaults to False. metacell_token (Tensor, optional): Binary metacell indicators of shape (batch,). Defaults to None. depth_mult (Tensor, optional): Expression depth multiplier. If None, uses sum of expression values. Defaults to None. do_sample (bool, optional): Sample from predicted distribution. Currently unused. Defaults to False. do_mvc (bool, optional): Compute multi-view coding predictions. Defaults to False. do_class (bool, optional): Compute classification predictions. Defaults to False. get_attention_layer (list[int], optional): Layer indices to extract attention weights from. Defaults to None. mask_zeros (Tensor, optional): Boolean mask for zero-expression genes of shape (batch, seq_len + num_special_tokens). Defaults to None. Returns: dict[str, Tensor] | tuple[dict, list]: Model outputs containing: - \"mean\": Predicted expression (batch, seq_len) - \"disp\": Dispersion parameters (batch, seq_len) [if ZINB] - \"zero_logits\": Zero-inflation logits (batch, seq_len) [if ZINB] - \"input_cell_embs\": Cell embeddings (batch, n_classes+1, d_model) - \"input_cell_emb\": Mean cell embedding (batch, d_model) - \"output_cell_embs\": Processed cell embeddings - \"output_cell_emb\": Final cell embedding - \"cls_output_{class}\": Classification logits for each class - \"gene_embedding\": Gene embeddings [if get_gene_emb] - \"mvc_*\": MVC predictions [if do_mvc] If get_attention_layer is not None, returns (outputs_dict, attention_list) where attention_list contains QKV tensors from specified layers. Example: >>> output = model( ... gene_pos=batch[\"genes\"], ... expression=batch[\"x\"], ... req_depth=batch[\"depth\"], ... do_class=True, ... ) >>> predictions = output[\"mean\"] >>> cell_types = output[\"cls_output_cell_type_ontology_term_id\"].argmax(-1) \"\"\" cell_embs , encoding = self . _encoder ( gene_pos , expression , neighbors , neighbors_info , mask , metacell_token = metacell_token , ) # attention_bias num = ( 1 if self . use_metacell_token else 0 ) + ( ( len ( self . classes ) + 1 ) if not self . cell_transformer else 0 ) if self . attn_bias is not None : if not hasattr ( self , \"nbias_sparse\" ): bias_path = os . path . join ( self . attn_bias ) # Keep as sparse matrix - much more memory efficient self . nbias_sparse = load_npz ( bias_path ) bias = torch . zeros ( ( gene_pos . shape [ 0 ], gene_pos . shape [ 1 ] + num , gene_pos . shape [ 1 ] + num , ), device = gene_pos . device , dtype = torch . float16 , ) fade_factor = 100 # Extract only the needed values from sparse matrix batch_size = gene_pos . shape [ 0 ] # Vectorized extraction from sparse matrix for b in range ( batch_size ): indices = gene_pos [ b ] . cpu () . numpy () # Get submatrix for this batch's genes submatrix = self . nbias_sparse [ np . ix_ ( indices , indices )] bias [ b , num :, num :] = ( torch . tensor ( submatrix . toarray (), device = gene_pos . device , dtype = torch . float16 ) * fade_factor ) bias [:, num :, : num ] = - 10_000 if not self . cell_transformer : encoding = torch . cat ([ cell_embs , encoding ], dim = 1 ) if type ( self . transformer ) is FlashTransformer : transformer_output = self . transformer ( encoding , return_qkv = get_attention_layer , bias = bias if self . attn_bias is not None else None , bias_layer = list ( range ( self . nlayers - 1 )), mask_zeros = mask_zeros , ) elif type ( self . transformer ) is Performer : transformer_output = self . transformer ( encoding ) else : raise ValueError ( f \"Unknown transformer: { type ( self . transformer ) } \" ) if get_attention_layer is not None : transformer_output , qkvs = transformer_output if self . cell_transformer : cell_embs = self . cell_transformer ( cell_embs , x_kv = transformer_output ) else : cell_embs , transformer_output = transformer_output . split ( [ len ( self . classes ) + 1 , transformer_output . shape [ 1 ] - ( len ( self . classes ) + 1 ), ], dim = 1 , ) # if not provided we will mult by the current expression sum depth_mult = expression . sum ( 1 ) if depth_mult is None else depth_mult req_depth = torch . log2 ( 1 + req_depth ) res = self . _expr_decoder ( transformer_output [:, ( 1 if self . use_metacell_token else 0 ) :, :], depth_mult , req_depth , get_gene_emb , ) res . update ( self . _cell_decoder ( cell_embs , do_mvc , do_class , depth_mult , req_depth , gene_pos if do_mvc else None , ) ) return ( res , qkvs ) if get_attention_layer is not None else res log_adata log_adata will log an adata from predictions. It will log to tensorboard and wandb if available see @utils.log_adata Source code in scprint2/model/model.py 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 def log_adata ( self , gtclass = None , name = \"\" ): \"\"\" log_adata will log an adata from predictions. It will log to tensorboard and wandb if available see @utils.log_adata \"\"\" try : mdir = self . logger . save_dir if self . logger . save_dir is not None else \"/tmp\" except : mdir = \"data/\" if not os . path . exists ( mdir ): os . makedirs ( mdir ) adata , fig = utils . make_adata ( genes = self . genes , embs = self . embs , pos = self . pos if self . save_expr else None , expr_pred = self . expr_pred if self . save_expr else None , classes = self . classes , pred = self . pred if not self . keep_all_labels_pred else None , label_decoders = self . label_decoders , labels_hierarchy = self . labels_hierarchy , gtclass = gtclass , doplot = self . doplot , ) adata . write ( str ( mdir ) + \"/step_\" + str ( self . global_step ) + \"_\" + str ( self . name ) + \"_\" + str ( name ) + \"_\" + str ( self . global_rank ) + \".h5ad\" ) if self . doplot and fig is not None : logged = False try : self . logger . experiment . add_figure ( fig ) logged = True except : print ( \"couldn't log to tensorboard\" ) try : self . logger . log_image ( key = \"umaps\" , images = [ fig ], step = self . global_step ) logged = True except : print ( \"couldn't log to wandb\" ) if not logged : fig . savefig ( mdir + \"/umap_\" + self . name + \"_\" + name + \".png\" ) return adata on_fit_start @see pl.LightningModule Source code in scprint2/model/model.py 1599 1600 1601 1602 1603 1604 1605 def on_fit_start ( self ): \"\"\"@see pl.LightningModule\"\"\" if type ( self . transformer ) is FlashTransformer : for encoder_layers in self . transformer . blocks : encoder_layers . set_seq_parallel ( True ) for k , v in self . mat_labels_hierarchy . items (): self . mat_labels_hierarchy [ k ] = v . to ( self . device ) on_load_checkpoint Handle checkpoint loading with backward compatibility. Automatically handles: - Different class configurations between checkpoint and current model - Legacy parameter names and structures - Encoder/decoder mismatches with datamodule - Gene vocabulary differences - Early stopping callback state Called automatically by PyTorch Lightning during checkpoint loading. Parameters: checkpoints ( dict ) \u2013 Checkpoint dictionary from torch.load(). Note Prints warnings when configurations differ between checkpoint and current model. These should be reviewed to ensure expected behavior. Source code in scprint2/model/model.py 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 def on_load_checkpoint ( self , checkpoints ): \"\"\" Handle checkpoint loading with backward compatibility. Automatically handles: - Different class configurations between checkpoint and current model - Legacy parameter names and structures - Encoder/decoder mismatches with datamodule - Gene vocabulary differences - Early stopping callback state Called automatically by PyTorch Lightning during checkpoint loading. Args: checkpoints (dict): Checkpoint dictionary from torch.load(). Note: Prints warnings when configurations differ between checkpoint and current model. These should be reviewed to ensure expected behavior. \"\"\" # if not the same number of labels (due to diff datasets) for name , clss in self . cls_decoders . items (): size = checkpoints [ \"state_dict\" ][ \"cls_decoders.\" + name + \".out_layer.bias\" ] . shape [ 0 ] if size != clss . out_layer . bias . shape [ 0 ]: self . cls_decoders [ name ] . out_layer = torch . nn . Linear ( clss . out_layer . weight . shape [ 1 ], size ) # from older model versions self . normalization = checkpoints [ \"hyper_parameters\" ] . get ( \"normalization\" , \"sum\" ) if ( checkpoints [ \"state_dict\" ] . get ( \"gene_encoder.0.embedding.weight\" , None ) is not None ): # replace it with the new one gene_encoder.0.embeddings.weight in the state_dict checkpoints [ \"state_dict\" ][ \"gene_encoder.0.embeddings.weight\" ] = checkpoints [ \"state_dict\" ][ \"gene_encoder.0.embedding.weight\" ] del checkpoints [ \"state_dict\" ][ \"gene_encoder.0.embedding.weight\" ] # same # when doing batch effect correction and input dataset is not the same if ( \"grad_reverse_discriminator_loss.out_layer.bias\" in checkpoints [ \"state_dict\" ] ): for k in list ( checkpoints [ \"state_dict\" ] . keys ()): if \"grad_reverse_discriminator_loss\" in k : del checkpoints [ \"state_dict\" ][ k ] print ( \"the discriminator for batch effect correction has been removed. \" \"dropping the legacy key.\" ) # same if ( checkpoints [ \"state_dict\" ] . get ( \"gene_encoder.embedding.weight\" , None ) is not None ): # replace it with the new one gene_encoder.embeddings.weight in the state_dict checkpoints [ \"state_dict\" ][ \"gene_encoder.embeddings.weight\" ] = checkpoints [ \"state_dict\" ][ \"gene_encoder.embedding.weight\" ] del checkpoints [ \"state_dict\" ][ \"gene_encoder.embedding.weight\" ] if \"classes\" in checkpoints [ \"hyper_parameters\" ]: if self . label_counts != checkpoints [ \"hyper_parameters\" ][ \"classes\" ]: if \"label_counts\" in checkpoints [ \"hyper_parameters\" ] and set ( checkpoints [ \"hyper_parameters\" ][ \"label_counts\" ] . keys () ) == set ( checkpoints [ \"hyper_parameters\" ][ \"classes\" ]): if self . classes != checkpoints [ \"hyper_parameters\" ][ \"classes\" ]: print ( \"classes have changed, be careful\" ) self . classes = checkpoints [ \"hyper_parameters\" ][ \"classes\" ] self . label_counts = checkpoints [ \"hyper_parameters\" ][ \"label_counts\" ] if self . classes == self . label_counts : raise ValueError ( \"classes and label_counts are the same, this is not allowed, please use another checkpoint\" ) else : self . label_counts = checkpoints [ \"hyper_parameters\" ][ \"classes\" ] if self . classes != list ( checkpoints [ \"hyper_parameters\" ][ \"classes\" ] . keys () ): print ( \"classes have changed, be careful\" ) self . classes = list ( checkpoints [ \"hyper_parameters\" ][ \"classes\" ] . keys () ) # else it is all good as expected else : print ( \"no classes in the checkpoint, be careful\" ) if checkpoints [ \"state_dict\" ] . get ( \"pos_encoder.pe\" ) is not None : if self . pos_encoder is None : self . pos_encoder = encoders . PositionalEncoding ( self . d_model , gene_pos_enc = [ 0 , 1 , 2 ] ) self . pos_encoder . pe = checkpoints [ \"state_dict\" ][ \"pos_encoder.pe\" ] if self . label_decoders != checkpoints [ \"hyper_parameters\" ][ \"label_decoders\" ] or self . labels_hierarchy != checkpoints [ \"hyper_parameters\" ] . get ( \"labels_hierarchy\" , {} ): print ( \"label decoders have changed, be careful\" ) self . label_decoders = checkpoints [ \"hyper_parameters\" ][ \"label_decoders\" ] self . labels_hierarchy = checkpoints [ \"hyper_parameters\" ] . get ( \"labels_hierarchy\" , {} ) for k , v in self . labels_hierarchy . items (): tens = torch . zeros (( len ( v ), self . label_counts [ k ])) for k2 , v2 in v . items (): tens [ k2 - self . label_counts [ k ], v2 ] = 1 self . mat_labels_hierarchy [ k ] = tens . to ( bool ) if ( \"gene_pos_enc\" in checkpoints [ \"hyper_parameters\" ] and checkpoints [ \"hyper_parameters\" ][ \"gene_pos_enc\" ] is not None ): if ( self . pos_encoder is None or self . pos_encoder . gene_pos_enc != checkpoints [ \"hyper_parameters\" ][ \"gene_pos_enc\" ] ): print ( \"Gene position encoding has changed in the dataloader compared to last time, trying to revert\" ) self . pos_encoder = encoders . PositionalEncoding ( self . d_model , gene_pos_enc = checkpoints [ \"hyper_parameters\" ][ \"gene_pos_enc\" ], ) checkpoints [ \"hyper_parameters\" ] . pop ( \"gene_pos_enc\" ) mencoders = {} if type ( checkpoints [ \"hyper_parameters\" ][ \"genes\" ]) is list : print ( \"converting a gene list-based model\" ) org = checkpoints [ \"hyper_parameters\" ] . get ( \"organisms\" , self . organisms ) genedf = load_genes ( org ) checkpoints [ \"hyper_parameters\" ][ \"genes\" ] = { i : genedf . index [ ( genedf . organism == i ) & genedf . index . isin ( checkpoints [ \"hyper_parameters\" ][ \"genes\" ]) ] . tolist () for i in org } if \"precpt_gene_emb\" in checkpoints [ \"hyper_parameters\" ]: checkpoints [ \"hyper_parameters\" ] . pop ( \"precpt_gene_emb\" ) if \"gene_pos_file\" in checkpoints [ \"hyper_parameters\" ]: checkpoints [ \"hyper_parameters\" ] . pop ( \"gene_pos_file\" ) if \"transformer\" in checkpoints [ \"hyper_parameters\" ]: checkpoints [ \"hyper_parameters\" ][ \"attention\" ] = checkpoints [ \"hyper_parameters\" ] . pop ( \"transformer\" ) try : if self . trainer . datamodule . decoders != self . label_decoders : print ( \"label decoders have changed, be careful\" ) # if we don't have the same decoders, we need to update the one on the datamodule side for k , v in self . label_decoders . items (): mencoders [ k ] = { va : ke for ke , va in v . items ()} self . trainer . datamodule . encoders = mencoders es = None for k in self . trainer . callbacks : if isinstance ( k , EarlyStopping ): es = k if es is not None : prev = checkpoints [ \"callbacks\" ] . get ( \"EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}\" ) if prev is not None : prev = prev [ \"patience\" ] if prev != es . patience : print ( \"updating the early stopping parameter to {} \" . format ( es . patience ) ) checkpoints [ \"callbacks\" ][ \"EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}\" ][ \"patience\" ] = es . patience if prev < es . patience : checkpoints [ \"callbacks\" ][ \"EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}\" ][ \"stopped_epoch\" ] = 0 except RuntimeError as e : if \"scPRINT2 is not attached to a `Trainer`.\" in str ( e ): print ( \"FYI: scPRINT2 is not attached to a `Trainer`.\" ) else : raise e if ( self . mvc_decoder is None and checkpoints [ \"state_dict\" ] . get ( \"mvc_decoder.gene2query.weight\" ) is not None ): for i in [ \"mvc_decoder.gene2query.weight\" , \"mvc_decoder.gene2query.bias\" , \"mvc_decoder.norm.weight\" , \"mvc_decoder.norm.bias\" , \"mvc_decoder.pred_var_zero.weight\" , ]: if i in checkpoints [ \"state_dict\" ]: del checkpoints [ \"state_dict\" ][ i ] org = checkpoints [ \"hyper_parameters\" ] . get ( \"organisms\" ) if self . organisms != org and org is not None : self . organisms = org try : self . trainer . datamodule . organisms = self . organisms except RuntimeError as e : if \"scPRINT2 is not attached to a `Trainer`.\" not in str ( e ): raise e if self . _genes != checkpoints [ \"hyper_parameters\" ][ \"genes\" ]: self . _genes = checkpoints [ \"hyper_parameters\" ][ \"genes\" ] try : self . trainer . datamodule . set_valid_genes_collator ( self . genes ) except RuntimeError as e : if \"scPRINT2 is not attached to a `Trainer`.\" not in str ( e ): raise e if not is_interactive (): self . save_hyperparameters () on_predict_epoch_end @see pl.LightningModule will Source code in scprint2/model/model.py 2634 2635 2636 2637 2638 2639 2640 def on_predict_epoch_end ( self ): \"\"\"@see pl.LightningModule will\"\"\" if self . pos . shape [ 0 ] < 100 : return if self . pred_log_adata : print ( \"adding on disk\" ) return self . log_adata ( name = \"predict_part_\" + str ( self . counter )) on_predict_epoch_start @see pl.LightningModule Source code in scprint2/model/model.py 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 def on_predict_epoch_start ( self ): \"\"\"@see pl.LightningModule\"\"\" print ( \"predict epoch start\" ) self . embs = None self . attn . data = None self . attn . attn = None self . counter = 0 if type ( self . transformer ) is FlashTransformer : for encoder_layers in self . transformer . blocks : encoder_layers . set_seq_parallel ( False ) on_test_start @see pl.LightningModule Source code in scprint2/model/model.py 2324 2325 2326 2327 2328 def on_test_start ( self ): \"\"\"@see pl.LightningModule\"\"\" print ( \"test start\" ) for k , v in self . mat_labels_hierarchy . items (): self . mat_labels_hierarchy [ k ] = v . to ( self . device ) on_validation_epoch_end @see pl.LightningModule Source code in scprint2/model/model.py 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 def on_validation_epoch_end ( self ): \"\"\"@see pl.LightningModule\"\"\" self . pos = None self . expr_pred = None self . do_adv_cls = self . _store_adv_cls gathered_embs = self . all_gather ( self . embs ) # Merge the dictionaries from all processes for key in self . embs . keys (): self . embs [ key ] = gathered_embs [ key ] . view ( - 1 , gathered_embs [ key ] . shape [ - 1 ]) self . info = self . all_gather ( self . info ) . view ( - 1 , self . info . shape [ - 1 ]) self . pred = ( self . all_gather ( self . pred ) . view ( - 1 , self . pred . shape [ - 1 ]) if self . pred is not None else None ) # self.pos = self.all_gather(self.pos).view(-1, self.pos.shape[-1]) # self.expr_pred[0] = self.all_gather(self.expr_pred[0]).view( # -1, self.expr_pred[0].shape[-1] # ) # if len(self.expr_pred) > 1: # self.expr_pred[1] = self.all_gather(self.expr_pred[1]).view( # -1, self.expr_pred[1].shape[-1] # ) # self.expr_pred[2] = self.all_gather(self.expr_pred[2]).view( # -1, self.expr_pred[2].shape[-1] # ) if self . trainer . state . stage != \"sanity_check\" : if self . trainer . is_global_zero : print ( \"logging anndata\" ) sch = self . lr_schedulers () if sch is not None : sch . step ( self . trainer . callback_metrics [ \"val_loss\" ]) # run the test function on specific dataset if self . embs is not None : self . log_adata ( gtclass = self . info , name = \"validation_part_\" + str ( self . counter ) ) if ( self . current_epoch + 1 ) % self . test_every == 0 : self . on_test_epoch_end () # Synchronize all processes with a timeout if torch . distributed . is_initialized (): # Set a timeout that's longer than your test typically takes # Write rank to file for debugging self . trainer . strategy . barrier () self . pred = None optimizer_step @see pl.LightningModule Source code in scprint2/model/model.py 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 def optimizer_step ( self , epoch , batch_idx , optimizer , optimizer_closure ): \"\"\"@see pl.LightningModule\"\"\" # update params # manually warm up lr without a scheduler # making sure that we don't do this during lrfinder lr_scale = None prev_lr = None if ( self . trainer . global_step < self . warmup_duration + self . lrfinder_steps ) and self . lrfinder_steps <= self . trainer . global_step : for i , pg in enumerate ( optimizer . param_groups ): lr_scale = min ( 1.0 , float ( self . trainer . global_step + 1 ) / self . warmup_duration ) prev_lr = pg [ \"lr\" ] pg [ \"lr\" ] = lr_scale * self . hparams . lr for i , pg in enumerate ( optimizer . param_groups ): # if pg[\"lr\"] < 2e-5: # pg[\"lr\"] = 2e-5 self . log ( \"lr_\" + str ( i ), pg [ \"lr\" ]) if optimizer . param_groups [ 0 ][ \"lr\" ] > self . hparams . lr : if prev_lr is not None : pg [ \"lr\" ] = prev_lr else : print ( \"OPTIMIZER HAS INCREASED LR. WHYY?\" ) print ( optimizer . param_groups [ 0 ][ \"lr\" ], self . hparams . lr ) optimizer . param_groups [ 0 ][ \"lr\" ] = self . hparams . lr optimizer . step ( closure = optimizer_closure ) predict_step embed given gene expression, encode the gene embedding and cell embedding. Parameters: batch ( Dict [ str , Tensor ] ) \u2013 Dictionary containing 'genes', 'x', 'depth', and optionally 'knn_cells'. batch_idx ( int ) \u2013 Index of the batch. Returns: Dict [ str , Tensor ] \u2013 Dict[str, Tensor]: Dictionary containing model predictions. Source code in scprint2/model/model.py 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 def predict_step ( self , batch : Dict [ str , Tensor ], batch_idx : int ) -> Dict [ str , Tensor ]: \"\"\" embed given gene expression, encode the gene embedding and cell embedding. Args: batch (Dict[str, Tensor]): Dictionary containing 'genes', 'x', 'depth', and optionally 'knn_cells'. batch_idx: Index of the batch. Returns: Dict[str, Tensor]: Dictionary containing model predictions. \"\"\" return self . _predict ( batch [ \"genes\" ], batch [ \"x\" ], batch [ \"depth\" ], batch . get ( \"knn_cells\" , None ), batch . get ( \"knn_cells_info\" , None ), self . predict_mode , self . pred_embedding , self . get_attention_layer , self . predict_depth_mult , ) training_step training_step defines the train loop. It is independent of forward @see pl.LightningModule Returns: Tensor ( Tensor ) \u2013 Total loss value for the training step. Source code in scprint2/model/model.py 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 def training_step ( self , batch : Dict [ str , Tensor ], batch_idx : int , ) -> Tensor : \"\"\" training_step defines the train loop. It is independent of forward @see pl.LightningModule Returns: Tensor: Total loss value for the training step. \"\"\" total_loss , losses = self . _full_training ( batch = batch , noise = self . noise , do_next_tp = self . do_next_tp , cce_temp = self . cce_temp , do_generate = self . do_generate , run_full_forward = self . run_full_forward , mask_ratio = self . mask_ratio , ) if total_loss is None or torch . isnan ( total_loss ): raise ValueError ( \"Loss is NaN\" ) try : self . log ( \"train_loss\" , total_loss , prog_bar = True , sync_dist = True ) self . log_dict ( losses , prog_bar = True , sync_dist = True ) except Exception as e : print ( e ) print ( losses ) return total_loss validation_step validation_step defines the validation loop. It is independent of forward @see pl.LightningModule Parameters: batch ( list [ Tensor ] ) \u2013 @see training_step Source code in scprint2/model/model.py 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 def validation_step ( self , batch , batch_idx , ): \"\"\" validation_step defines the validation loop. It is independent of forward @see pl.LightningModule Args: batch (list[Tensor]): @see training_step \"\"\" val_loss , losses = self . _full_training ( batch = batch , noise = self . noise , do_next_tp = self . do_next_tp , cce_temp = self . cce_temp , do_vae_kl = False , do_generate = self . do_generate , run_full_forward = self . run_full_forward , mask_ratio = self . mask_ratio , ) expression = batch [ \"x\" ] gene_pos = batch [ \"genes\" ] depth = batch [ \"depth\" ] metacell_token = batch . get ( \"is_meta\" , None ) knn_cells = batch . get ( \"knn_cells\" , None ) knn_cells_info = batch . get ( \"knn_cells_info\" , None ) # TODO: make this faster by only calling val loss if self . embs is not None : if self . pos . shape [ 0 ] < 100_000 / self . trainer . world_size : self . info = torch . cat ([ self . info , batch [ \"class\" ]]) self . _predict ( gene_pos , expression , depth , knn_cells = knn_cells , knn_cells_info = knn_cells_info , pred_embedding = self . pred_embedding , max_size_in_mem = 120_000 , metacell_token = metacell_token , ) else : self . info = batch [ \"class\" ] self . _predict ( gene_pos , expression , depth , knn_cells = knn_cells , knn_cells_info = knn_cells_info , pred_embedding = self . pred_embedding , max_size_in_mem = 120_000 , metacell_token = metacell_token , ) self . log ( \"val_loss\" , val_loss , sync_dist = True ) expr_loss = mean ( [ v . cpu () . item () if type ( v ) is Tensor else v for k , v in losses . items () if \"expr\" in k ] ) self . log ( \"val_loss_expr\" , expr_loss , sync_dist = True ) cls_loss = mean ( [ v . cpu () . item () if type ( v ) is Tensor else v for k , v in losses . items () if \"cls\" in k ] ) self . log ( \"val_loss_cls\" , cls_loss , sync_dist = True ) # self.log_dict(losses, sync_dist=True) return val_loss losses scprint2.model.loss Classes: Name Description AdversarialDiscriminatorLoss Functions: Name Description contrastive_loss Computes NT-Xent loss (InfoNCE) between two sets of vectors. criterion_neg_log_bernoulli Compute the negative log-likelihood of Bernoulli distribution ecs ecs Computes the similarity of cell embeddings based on a threshold. grad_reverse grad_reverse Reverses the gradient of the input tensor. hierarchical_classification Computes the classification loss for a given batch of predictions and ground truth labels. masked_mae Compute the masked MAE loss between input and target. masked_mse Compute the masked MSE loss between input and target. masked_nb Compute the masked negative binomial loss between input and target. masked_relative_error Compute the masked relative error between input and target. mse Compute the MSE loss between input and target. nb Computes the negative binomial (NB) loss. within_sample Compute dissimilarity between embeddings within each sample zinb Computes zero-inflated negative binomial (ZINB) loss. AdversarialDiscriminatorLoss Bases: Module Discriminator for the adversarial training for batch correction. Parameters: d_model ( int ) \u2013 The size of the input tensor. n_cls ( int ) \u2013 The number of classes. nlayers ( int , default: 3 ) \u2013 The number of layers in the discriminator. Defaults to 3. activation ( callable , default: LeakyReLU ) \u2013 The activation function. Defaults to nn.LeakyReLU. reverse_grad ( bool , default: True ) \u2013 Whether to reverse the gradient. Defaults Methods: Name Description forward Args: Source code in scprint2/model/loss.py 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 def __init__ ( self , d_model : int , n_cls : int , nlayers : int = 3 , activation : Callable = nn . LeakyReLU , reverse_grad : bool = True , ): \"\"\" Discriminator for the adversarial training for batch correction. Args: d_model (int): The size of the input tensor. n_cls (int): The number of classes. nlayers (int, optional): The number of layers in the discriminator. Defaults to 3. activation (callable, optional): The activation function. Defaults to nn.LeakyReLU. reverse_grad (bool, optional): Whether to reverse the gradient. Defaults \"\"\" super () . __init__ () # module list self . decoder = nn . ModuleList () for _ in range ( nlayers - 1 ): self . decoder . append ( nn . Linear ( d_model , d_model )) self . decoder . append ( nn . LayerNorm ( d_model )) self . decoder . append ( activation ()) self . out_layer = nn . Linear ( d_model , n_cls ) self . reverse_grad = reverse_grad forward Parameters: x ( Tensor ) \u2013 Tensor, shape [batch_size, embsize] batch_labels ( Tensor ) \u2013 Tensor, shape [batch_size] Source code in scprint2/model/loss.py 348 349 350 351 352 353 354 355 356 357 358 359 def forward ( self , x : Tensor , batch_labels : Tensor ) -> Tensor : \"\"\" Args: x: Tensor, shape [batch_size, embsize] batch_labels: Tensor, shape [batch_size] \"\"\" if self . reverse_grad : x = grad_reverse ( x , lambd = 1.0 ) for layer in self . decoder : x = layer ( x ) x = self . out_layer ( x ) return F . cross_entropy ( x , batch_labels ) contrastive_loss Computes NT-Xent loss (InfoNCE) between two sets of vectors. Parameters: x ( Tensor ) \u2013 Tensor of shape [batch_size, feature_dim] y ( Tensor ) \u2013 Tensor of shape [batch_size, feature_dim] temperature ( float , default: 0.1 ) \u2013 Temperature parameter to scale the similarities. Lower values make the model more confident/selective. Typical values are between 0.1 and 0.5. Returns: Tensor ( Tensor ) \u2013 NT-Xent loss value Note Assumes x[i] and y[i] are positive pairs All other combinations are considered negative pairs Uses cosine similarity scaled by temperature Source code in scprint2/model/loss.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def contrastive_loss ( x : Tensor , y : Tensor , temperature : float = 0.1 ) -> Tensor : \"\"\" Computes NT-Xent loss (InfoNCE) between two sets of vectors. Args: x: Tensor of shape [batch_size, feature_dim] y: Tensor of shape [batch_size, feature_dim] temperature: Temperature parameter to scale the similarities. Lower values make the model more confident/selective. Typical values are between 0.1 and 0.5. Returns: Tensor: NT-Xent loss value Note: - Assumes x[i] and y[i] are positive pairs - All other combinations are considered negative pairs - Uses cosine similarity scaled by temperature \"\"\" # Check input dimensions assert x . shape == y . shape , \"Input tensors must have the same shape\" batch_size = x . shape [ 0 ] # Compute cosine similarity matrix # x_unsqueeze: [batch_size, 1, feature_dim] # y_unsqueeze: [1, batch_size, feature_dim] # -> similarities: [batch_size, batch_size] similarities = ( F . cosine_similarity ( x . unsqueeze ( 1 ), y . unsqueeze ( 0 ), dim = 2 ) / temperature ) # The positive pairs are on the diagonal labels = torch . arange ( batch_size , device = x . device ) # Cross entropy loss return F . cross_entropy ( similarities , labels ) criterion_neg_log_bernoulli Compute the negative log-likelihood of Bernoulli distribution Source code in scprint2/model/loss.py 141 142 143 144 145 146 147 148 def criterion_neg_log_bernoulli ( input : Tensor , target : Tensor , mask : Tensor ) -> Tensor : \"\"\" Compute the negative log-likelihood of Bernoulli distribution \"\"\" mask = mask . float () bernoulli = torch . distributions . Bernoulli ( probs = input ) masked_log_probs = bernoulli . log_prob (( target > 0 ) . float ()) * mask return - masked_log_probs . sum () / mask . sum () ecs ecs Computes the similarity of cell embeddings based on a threshold. Parameters: cell_emb ( Tensor ) \u2013 A tensor representing cell embeddings. ecs_threshold ( float , default: 0.5 ) \u2013 A threshold for determining similarity. Defaults to 0.5. Returns: Tensor ( Tensor ) \u2013 A tensor representing the mean of 1 minus the square of the difference between the cosine similarity and the threshold. Source code in scprint2/model/loss.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def ecs ( cell_emb : Tensor , ecs_threshold : float = 0.5 ) -> Tensor : \"\"\" ecs Computes the similarity of cell embeddings based on a threshold. Args: cell_emb (Tensor): A tensor representing cell embeddings. ecs_threshold (float, optional): A threshold for determining similarity. Defaults to 0.5. Returns: Tensor: A tensor representing the mean of 1 minus the square of the difference between the cosine similarity and the threshold. \"\"\" # Here using customized cosine similarity instead of F.cosine_similarity # to avoid the pytorch issue of similarity larger than 1.0, pytorch # 78064 # normalize the embedding cell_emb_normed = F . normalize ( cell_emb , p = 2 , dim = 1 ) cos_sim = torch . mm ( cell_emb_normed , cell_emb_normed . t ()) # mask out diagnal elements mask = torch . eye ( cos_sim . size ( 0 )) . bool () . to ( cos_sim . device ) cos_sim = cos_sim . masked_fill ( mask , 0.0 ) # only optimize positive similarities cos_sim = F . relu ( cos_sim ) return torch . mean ( 1 - ( cos_sim - ecs_threshold ) ** 2 ) grad_reverse grad_reverse Reverses the gradient of the input tensor. Parameters: x ( Tensor ) \u2013 The input tensor whose gradient is to be reversed. lambd ( float , default: 1.0 ) \u2013 The scaling factor for the reversed gradient. Defaults to 1.0. Returns: Tensor ( Tensor ) \u2013 The input tensor with its gradient reversed during the backward pass. Source code in scprint2/model/loss.py 373 374 375 376 377 378 379 380 381 382 383 384 def grad_reverse ( x : Tensor , lambd : float = 1.0 ) -> Tensor : \"\"\" grad_reverse Reverses the gradient of the input tensor. Args: x (Tensor): The input tensor whose gradient is to be reversed. lambd (float, optional): The scaling factor for the reversed gradient. Defaults to 1.0. Returns: Tensor: The input tensor with its gradient reversed during the backward pass. \"\"\" return GradReverse . apply ( x , lambd ) hierarchical_classification Computes the classification loss for a given batch of predictions and ground truth labels. Parameters: pred ( Tensor ) \u2013 The predicted logits for the batch. Shape: (batch_size, n_labels) cl ( Tensor ) \u2013 The ground truth labels for the batch. Shape: (batch_size,) labels_hierarchy ( Tensor , default: None ) \u2013 The hierarchical structure of the labels. Defaults to None. A binary tensor of shape (number of parents, n_labels) if not given, will act as a regular classification loss see gist for more details of how one can compute it https://gist.github.com/jkobject/5b36bc4807edb440b86644952a49781e Raises: ValueError \u2013 If the labels_hierarchy is not found while the number of predicted labels is smaller than the number of ground truth labels. Returns: Tensor ( Tensor ) \u2013 The computed binary cross entropy loss for the given batch. Source code in scprint2/model/loss.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def hierarchical_classification ( pred : torch . Tensor , cl : torch . Tensor , labels_hierarchy : Optional [ torch . Tensor ] = None , ) -> torch . Tensor : \"\"\" Computes the classification loss for a given batch of predictions and ground truth labels. Args: pred (Tensor): The predicted logits for the batch. Shape: (batch_size, n_labels) cl (Tensor): The ground truth labels for the batch. Shape: (batch_size,) labels_hierarchy (Tensor, optional): The hierarchical structure of the labels. Defaults to None. A binary tensor of shape (number of parents, n_labels) if not given, will act as a regular classification loss see gist for more details of how one can compute it https://gist.github.com/jkobject/5b36bc4807edb440b86644952a49781e Raises: ValueError: If the labels_hierarchy is not found while the number of predicted labels is smaller than the number of ground truth labels. Returns: Tensor: The computed binary cross entropy loss for the given batch. \"\"\" maxsize = pred . shape [ 1 ] newcl = torch . zeros ( ( pred . shape [ 0 ], maxsize ), device = cl . device ) # batchsize * n_labels # if we don't know the label we set the weight to 0 else to 1 valid_indices = ( cl != - 1 ) & ( cl < maxsize ) valid_cl = cl [ valid_indices ] newcl [ valid_indices , valid_cl ] = 1 weight = torch . ones_like ( newcl , device = cl . device ) # if we don't know the label we set the weight to 0 for all labels weight [ cl == - 1 , :] = 0 # if we have non leaf values, we don't know so we don't compute grad and set weight to 0 # and add labels that won't be counted but so that we can still use them if labels_hierarchy is not None and ( cl >= maxsize ) . any (): is_parent = cl >= maxsize subset_parent_weight = weight [ is_parent ] # we set the weight of the leaf elements for pred where we don't know the leaf, to 0 # i.e. the elements where we will compute the max # in cl, parents are values past the maxsize # (if there is 10 leafs labels, the label 10,14, or 15 is a parent at position # row 0, 4, or 5 in the hierarchy matrix subset_parent_weight [ labels_hierarchy [ cl [ is_parent ] - maxsize ]] = 0 weight [ is_parent ] = subset_parent_weight # we set their lead to 1 (since the weight will be zero, not really usefull..) subset_parent_newcl = newcl [ is_parent ] subset_parent_newcl [ labels_hierarchy [ cl [ is_parent ] - maxsize ]] = 1 newcl [ is_parent ] = subset_parent_newcl # all parental nodes that have a 1 in the labels_hierarchy matrix are set to 1 # for each parent label / row in labels_hierarchy matrix, the addnewcl is # the max of the newcl values where the parent label is 1 newcl_expanded = newcl . unsqueeze ( - 1 ) . expand ( - 1 , - 1 , labels_hierarchy . shape [ 0 ]) addnewcl = torch . max ( newcl_expanded * labels_hierarchy . T , dim = 1 )[ 0 ] # for their weight, it is decreasing based on number of children they have # it is the same here as for parental labels, we don't want to compute # gradients when they are 0 meaning not parents of the true leaf label. # for now we weight related to how many labels they contain. addweight = addnewcl . clone () / ( labels_hierarchy . sum ( 1 ) ** 0.5 ) # except if it is the cl label we know about? subset_parent_weight = addweight [ is_parent ] subset_parent_weight [:, cl [ is_parent ] - maxsize ] = 1 addweight [ is_parent ] = subset_parent_weight # we apply the same mask to the pred but now we want to compute # logsumexp instead of max since we want to keep the gradients # we also set to -inf since it is a more neutral element for logsumexp pred_expanded = ( pred . clone () . unsqueeze ( - 1 ) . expand ( - 1 , - 1 , labels_hierarchy . shape [ 0 ]) ) pred_expanded = pred_expanded * labels_hierarchy . T pred_expanded [ pred_expanded == 0 ] = torch . finfo ( pred . dtype ) . min addpred = torch . logsumexp ( pred_expanded , dim = 1 ) # we add the new labels to the cl newcl = torch . cat ([ newcl , addnewcl ], dim = 1 ) weight = torch . cat ([ weight , addweight ], dim = 1 ) pred = torch . cat ([ pred , addpred ], dim = 1 ) elif labels_hierarchy is None and ( cl >= maxsize ) . any (): raise ValueError ( \"need to use labels_hierarchy for this usecase\" ) myloss = torch . nn . functional . binary_cross_entropy_with_logits ( pred , target = newcl , weight = weight ) return myloss masked_mae Compute the masked MAE loss between input and target. MAE = mean absolute error Source code in scprint2/model/loss.py 39 40 41 42 43 44 45 46 def masked_mae ( input : Tensor , target : Tensor , mask : Tensor ) -> Tensor : \"\"\" Compute the masked MAE loss between input and target. MAE = mean absolute error \"\"\" mask = mask . float () loss = F . l1_loss ( input * mask , target * mask , reduction = \"sum\" ) return loss / mask . sum () masked_mse Compute the masked MSE loss between input and target. Source code in scprint2/model/loss.py 13 14 15 16 17 18 19 20 21 22 23 def masked_mse ( input : Tensor , target : Tensor , mask : Tensor ) -> Tensor : \"\"\" Compute the masked MSE loss between input and target. \"\"\" mask = mask . float () input = torch . log2 ( input + 1 ) input = ( input / torch . sum ( input , dim = 1 , keepdim = True )) * 10000 target = torch . log2 ( target + 1 ) target = ( target / torch . sum ( target , dim = 1 , keepdim = True )) * 10000 loss = F . mse_loss ( input * mask , target * mask , reduction = \"sum\" ) return loss / mask . sum () masked_nb Compute the masked negative binomial loss between input and target. Source code in scprint2/model/loss.py 49 50 51 52 53 54 55 56 def masked_nb ( input : Tensor , target : Tensor , mask : Tensor ) -> Tensor : \"\"\" Compute the masked negative binomial loss between input and target. \"\"\" mask = mask . float () nb = torch . distributions . NegativeBinomial ( total_count = target , probs = input ) masked_log_probs = nb . log_prob ( target ) * mask return - masked_log_probs . sum () / mask . sum () masked_relative_error Compute the masked relative error between input and target. Source code in scprint2/model/loss.py 151 152 153 154 155 156 157 158 159 def masked_relative_error ( input : Tensor , target : Tensor , mask : torch . LongTensor ) -> Tensor : \"\"\" Compute the masked relative error between input and target. \"\"\" assert mask . any () loss = torch . abs ( input [ mask ] - target [ mask ]) / ( target [ mask ] + 1e-5 ) return loss . mean () mse Compute the MSE loss between input and target. Source code in scprint2/model/loss.py 26 27 28 29 30 31 32 33 34 35 36 def mse ( input : Tensor , target : Tensor , mask = False ) -> Tensor : \"\"\" Compute the MSE loss between input and target. \"\"\" if mask : return masked_mse ( input , target , ( target > 0 )) input = torch . log2 ( input + 1 ) input = ( input / torch . sum ( input , dim = 1 , keepdim = True )) * 10000 target = torch . log2 ( target + 1 ) target = ( target / torch . sum ( target , dim = 1 , keepdim = True )) * 10000 return F . mse_loss ( input , target , reduction = \"mean\" ) nb Computes the negative binomial (NB) loss. This function was adapted from scvi-tools. Parameters: target ( Tensor ) \u2013 Ground truth data. mu ( Tensor ) \u2013 Means of the negative binomial distribution (must have positive support). theta ( Tensor ) \u2013 Inverse dispersion parameter (must have positive support). eps ( float , default: 0.0001 ) \u2013 Numerical stability constant. Defaults to 1e-4. Returns: Tensor ( Tensor ) \u2013 NB loss value. Source code in scprint2/model/loss.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def nb ( target : Tensor , mu : Tensor , theta : Tensor , eps = 1e-4 ) -> Tensor : \"\"\" Computes the negative binomial (NB) loss. This function was adapted from scvi-tools. Args: target (Tensor): Ground truth data. mu (Tensor): Means of the negative binomial distribution (must have positive support). theta (Tensor): Inverse dispersion parameter (must have positive support). eps (float, optional): Numerical stability constant. Defaults to 1e-4. Returns: Tensor: NB loss value. \"\"\" if theta . ndimension () == 1 : theta = theta . view ( 1 , theta . size ( 0 )) log_theta_mu_eps = torch . log ( theta + mu + eps ) res = ( theta * ( torch . log ( theta + eps ) - log_theta_mu_eps ) + target * ( torch . log ( mu + eps ) - log_theta_mu_eps ) + torch . lgamma ( target + theta ) - torch . lgamma ( theta ) - torch . lgamma ( target + 1 ) ) return - res . mean () within_sample Compute dissimilarity between embeddings within each sample using a combination of cosine and L2 distance Parameters: cell_embs ( Tensor ) \u2013 tensor of shape [batch_size, num_embeddings, embedding_dim] Source code in scprint2/model/loss.py 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 def within_sample ( cell_embs : Tensor ): \"\"\" Compute dissimilarity between embeddings within each sample using a combination of cosine and L2 distance Args: cell_embs: tensor of shape [batch_size, num_embeddings, embedding_dim] \"\"\" batch_size , num_embeddings , emb_dim = cell_embs . shape # Normalize embeddings for cosine similarity cell_embs_norm = F . normalize ( cell_embs , p = 2 , dim =- 1 ) # Compute pairwise cosine similarities cos_sim = torch . bmm ( cell_embs_norm , cell_embs_norm . transpose ( 1 , 2 )) # Compute pairwise L2 distances (normalized by embedding dimension) l2_dist = torch . cdist ( cell_embs , cell_embs , p = 2 ) / np . sqrt ( emb_dim ) # Create mask for pairs (excluding self-similarity) mask = 1 - torch . eye ( num_embeddings , device = cos_sim . device ) mask = mask . unsqueeze ( 0 ) . expand ( batch_size , - 1 , - 1 ) # Combine losses: # - High cosine similarity should be penalized # - Small L2 distance should be penalized cos_loss = ( cos_sim * mask ) . pow ( 2 ) . mean () l2_loss = 1.0 / ( l2_dist * mask + 1e-3 ) . mean () return 0.5 * cos_loss + 0.5 * l2_loss zinb Computes zero-inflated negative binomial (ZINB) loss. This function was modified from scvi-tools. Parameters: target ( Tensor ) \u2013 Torch Tensor of ground truth data. mu ( Tensor ) \u2013 Torch Tensor of means of the negative binomial (must have positive support). theta ( Tensor ) \u2013 Torch Tensor of inverse dispersion parameter (must have positive support). pi ( Tensor ) \u2013 Torch Tensor of logits of the dropout parameter (real support). eps ( float , default: 0.0001 ) \u2013 Numerical stability constant. Defaults to 1e-4. Returns: Tensor ( Tensor ) \u2013 ZINB loss value. Source code in scprint2/model/loss.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def zinb ( target : Tensor , mu : Tensor , theta : Tensor , pi : Tensor , eps = 1e-4 , mask = False , ) -> Tensor : \"\"\" Computes zero-inflated negative binomial (ZINB) loss. This function was modified from scvi-tools. Args: target (Tensor): Torch Tensor of ground truth data. mu (Tensor): Torch Tensor of means of the negative binomial (must have positive support). theta (Tensor): Torch Tensor of inverse dispersion parameter (must have positive support). pi (Tensor): Torch Tensor of logits of the dropout parameter (real support). eps (float, optional): Numerical stability constant. Defaults to 1e-4. Returns: Tensor: ZINB loss value. \"\"\" # uses log(sigmoid(x)) = -softplus(-x) softplus_pi = F . softplus ( - pi ) # eps to make it positive support and taking the log log_theta_mu_eps = torch . log ( theta + mu + eps ) pi_theta_log = - pi + theta * ( torch . log ( theta + eps ) - log_theta_mu_eps ) case_zero = F . softplus ( pi_theta_log ) - softplus_pi mul_case_zero = torch . mul (( target < eps ) . type ( torch . float32 ), case_zero ) case_non_zero = ( - softplus_pi + pi_theta_log + target * ( torch . log ( mu + eps ) - log_theta_mu_eps ) + torch . lgamma ( target + theta ) - torch . lgamma ( theta ) - torch . lgamma ( target + 1 ) ) mul_case_non_zero = torch . mul (( target > eps ) . type ( torch . float32 ), case_non_zero ) res = mul_case_zero + mul_case_non_zero # we want to minize the loss but maximize the log likelyhood if mask : mask = ( target > 0 ) . float () res = res * mask return - res . sum () / mask . sum () return - res . mean () utils scprint2.model.utils Classes: Name Description Attention WeightedMasker Functions: Name Description downsample_profile This function downsamples the expression profile of a given single cell RNA matrix. make_adata This function creates an AnnData object from the given input parameters. simple_masker Randomly mask a batch of data. test Test the given model on the full set of benchmarks and save the results to JSON files. zinb_sample zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution. Attention Initialize the Attention class. Parameters: gene_dim ( int ) \u2013 The dimension of the gene. additional_tokens ( int , default: 0 ) \u2013 The number of additional tokens to add. precomp_attn ( bool , default: False ) \u2013 Whether to compute attention or it is precomputed apply_softmax ( bool , default: True ) \u2013 Whether to apply softmax to the attention. sum_heads ( bool , default: True ) \u2013 Whether to sum the heads. Methods: Name Description add_attn Aggregate the attention or data based on the precomp_attn flag. add_qk Add data to the internal storage. get Get the aggregated attention or data. Source code in scprint2/model/utils.py 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 def __init__ ( self , gene_dim : int , precomp_attn : bool = False , apply_softmax : bool = True , sum_heads : bool = True , additional_tokens : int = 0 , ): \"\"\" Initialize the Attention class. Args: gene_dim (int): The dimension of the gene. additional_tokens (int): The number of additional tokens to add. precomp_attn (bool): Whether to compute attention or it is precomputed apply_softmax (bool): Whether to apply softmax to the attention. sum_heads (bool): Whether to sum the heads. \"\"\" self . data : Optional [ Tensor ] = None self . gene_dim : int = gene_dim self . additional_tokens : int = additional_tokens self . div : Optional [ Tensor ] = None self . apply_softmax : bool = apply_softmax self . sum_heads : bool = sum_heads self . precomp_attn : bool = precomp_attn self . speciesloc : int = 0 add_attn Aggregate the attention or data based on the precomp_attn flag. Parameters: x ( List [ Tensor ] ) \u2013 List of tensors to aggregate. Tensor of size (batch, seq_len, 2, heads, emb) pos ( Tensor ) \u2013 Position tensor. Source code in scprint2/model/utils.py 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 def add_attn ( self , x : List [ Tensor ], pos : Tensor , expr : Optional [ Tensor ] = None ) -> None : \"\"\" Aggregate the attention or data based on the precomp_attn flag. Args: x (List[Tensor]): List of tensors to aggregate. Tensor of size (batch, seq_len, 2, heads, emb) pos (Tensor): Position tensor. \"\"\" if self . data is None : self . data = torch . zeros ( [ self . gene_dim + self . additional_tokens , self . gene_dim + self . additional_tokens , ], device = pos . device , dtype = torch . float32 , ) self . div = torch . zeros ( 1 , device = pos . device , dtype = torch . float32 ) for i , elem in enumerate ( x ): if self . apply_softmax : attn = torch . nn . functional . softmax ( elem [:, :, 0 , :, :] . permute ( 0 , 2 , 1 , 3 ) @ elem [:, :, 1 , :, :] . permute ( 0 , 2 , 3 , 1 ), dim =- 1 , ) if expr is not None : attn [:, :, self . additional_tokens :, self . additional_tokens :] = ( attn [:, :, self . additional_tokens :, self . additional_tokens :] * ( expr > 0 ) . float () . unsqueeze ( 1 ) . unsqueeze ( - 1 ) * ( expr > 0 ) . float () . unsqueeze ( 1 ) . unsqueeze ( 2 ) ) self . data += attn . sum ( 0 ) . mean ( 0 ) else : self . data [:, :] += ( ( elem [:, :, 0 , :, :] . permute ( 0 , 2 , 1 , 3 ) @ elem [:, :, 1 , :, :] . permute ( 0 , 2 , 3 , 1 ) ) . sum ( 0 ) . mean ( 0 ) ) self . div += 1 add_qk Add data to the internal storage. Parameters: x ( List [ Tensor ] ) \u2013 List of tensors to add. pos ( Tensor ) \u2013 Position tensor. Source code in scprint2/model/utils.py 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 def add_qk ( self , x : List [ Tensor ], pos : Tensor , expr : Optional [ Tensor ] = None ) -> None : \"\"\" Add data to the internal storage. Args: x (List[Tensor]): List of tensors to add. pos (Tensor): Position tensor. \"\"\" # this is a debugger line if self . data is None : self . data = torch . zeros ( [ len ( x ), self . gene_dim + self . additional_tokens ] + list ( x [ 0 ] . shape [ 2 :]), device = pos . device , ) self . div = torch . zeros ( self . gene_dim + self . additional_tokens , device = pos . device ) for i in range ( x [ 0 ] . shape [ 0 ]): # batch size loc = torch . cat ( [ torch . arange ( self . additional_tokens , device = pos . device ), pos [ i ] + self . additional_tokens - self . speciesloc , ] ) . int () for j in range ( len ( x )): # number of layers * heads self . data [ j , loc , :, :, :] += x [ j ][ i ] self . div [ loc ] += 1 get Get the aggregated attention or data. Returns: Optional [ ndarray ] \u2013 Optional[np.ndarray]: The aggregated attention or data. Source code in scprint2/model/utils.py 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 def get ( self ) -> Optional [ np . ndarray ]: \"\"\" Get the aggregated attention or data. Returns: Optional[np.ndarray]: The aggregated attention or data. \"\"\" if not self . precomp_attn : if self . data is None : return None # shape is (layers, genes, qkv, heads, emb) return self . data / self . div . view ( 1 , self . div . shape [ 0 ], 1 , 1 , 1 ) else : if self . data is None : return None self . data . div_ ( self . div ) return self . data WeightedMasker Randomly mask a batch of data. Parameters: genes ( List [ str ] ) \u2013 The list of genes the model might see. TFs ( List [ str ] , default: fileToList ( FILEDIR + '/../../data/main/TFs.txt') ) \u2013 The list of TFs the model can drop. tf_weight ( float , default: 10 ) \u2013 How likely it is to drop a non TF compared to a TF. Source code in scprint2/model/utils.py 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def __init__ ( self , genes : List [ str ], TFs : List [ str ] = utils . fileToList ( FILEDIR + \"/../../data/main/TFs.txt\" ), tf_weight : float = 10 , ): \"\"\" Randomly mask a batch of data. Args: genes (List[str]): The list of genes the model might see. TFs (List[str]): The list of TFs the model can drop. tf_weight (float): How likely it is to drop a non TF compared to a TF. \"\"\" TFs = set ( TFs ) self . weights = torch . tensor ([ tf_weight if gene in TFs else 1 for gene in genes ]) self . max_to_drop = ( self . weights == tf_weight ) . sum () self . tf_weight = tf_weight downsample_profile This function downsamples the expression profile of a given single cell RNA matrix. The noise is applied based on the renoise parameter, the total counts of the matrix, and the number of genes. The function first calculates the noise threshold (scaler) based on the renoise parameter. It then generates an initial matrix count by applying a Poisson distribution to a random tensor scaled by the total counts and the number of genes. The function then models the sampling zeros by applying a Poisson distribution to a random tensor scaled by the noise threshold, the total counts, and the number of genes. The function also models the technical zeros by generating a random tensor and comparing it to the noise threshold. The final matrix count is calculated by subtracting the sampling zeros from the initial matrix count and multiplying by the technical zeros. The function ensures that the final matrix count is not less than zero by taking the maximum of the final matrix count and a tensor of zeros. The function returns the final matrix count. Parameters: mat ( Tensor ) \u2013 The input matrix. dropout ( float ) \u2013 The renoise parameter. Returns: Tensor \u2013 torch.Tensor: The matrix count after applying noise. Source code in scprint2/model/utils.py 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 def downsample_profile ( mat : Tensor , dropout : float , method = \"new\" , randsamp = False ) -> Tensor : \"\"\" This function downsamples the expression profile of a given single cell RNA matrix. The noise is applied based on the renoise parameter, the total counts of the matrix, and the number of genes. The function first calculates the noise threshold (scaler) based on the renoise parameter. It then generates an initial matrix count by applying a Poisson distribution to a random tensor scaled by the total counts and the number of genes. The function then models the sampling zeros by applying a Poisson distribution to a random tensor scaled by the noise threshold, the total counts, and the number of genes. The function also models the technical zeros by generating a random tensor and comparing it to the noise threshold. The final matrix count is calculated by subtracting the sampling zeros from the initial matrix count and multiplying by the technical zeros. The function ensures that the final matrix count is not less than zero by taking the maximum of the final matrix count and a tensor of zeros. The function returns the final matrix count. Args: mat (torch.Tensor): The input matrix. dropout (float): The renoise parameter. Returns: torch.Tensor: The matrix count after applying noise. \"\"\" # Randomly drop on average N counts to each element of expression using a heavy tail Gaussian distribution # here we try to get the scale of the distribution so as to remove the right number of counts from each gene # https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02601-5#:~:text=Zero%20measurements%20in%20scRNA%2Dseq,generation%20of%20scRNA%2Dseq%20data. if randsamp : dropout = torch . rand ( mat . shape [ 0 ], device = mat . device ) * dropout dropout = ( dropout . unsqueeze ( 1 ) if len ( mat . shape ) == 2 else dropout . unsqueeze ( 1 ) . unsqueeze ( 1 ) ) if method == \"old\" : totcounts = mat . sum ( - 1 ) ngenes = mat . shape [ - 1 ] tnoise = 1 - ( 1 - dropout ) ** ( 1 / 2 ) # we model the sampling zeros (dropping 30% of the reads) res = torch . poisson ( torch . rand ( mat . shape , device = mat . device ) * (( tnoise * totcounts . unsqueeze ( - 1 )) / ( 0.5 * ngenes )) ) . int () # we model the technical zeros (dropping 50% of the genes) drop = ( torch . rand ( mat . shape , device = mat . device ) > tnoise ) . int () mat = ( mat - res ) * drop return torch . maximum ( mat , torch . zeros ( ( 1 , 1 ) if len ( mat . shape ) == 2 else ( 1 , 1 , 1 ), device = mat . device , dtype = torch . int , ), ) elif method == \"jules\" : scaler = ( 1 - dropout ) ** ( 1 / 2 ) notdrop = ( torch . rand ( mat . shape , device = mat . device , ) < scaler ) . int () notdrop [ mat == 0 ] = 0 # apply the dropout after the poisson, right? return notdrop * torch . poisson ( mat * scaler ) elif method == \"new\" : dropout = dropout * 1.1 # we model the sampling zeros (dropping 30% of the reads) res = torch . poisson (( mat * ( dropout / 2 ))) . int () # we model the technical zeros (dropping 50% of the genes) notdrop = ( torch . rand ( mat . shape , device = mat . device ) >= ( dropout / 2 )) . int () mat = ( mat - res ) * notdrop return torch . maximum ( mat , torch . zeros ( ( 1 , 1 ) if len ( mat . shape ) == 2 else ( 1 , 1 , 1 ), device = mat . device , dtype = torch . int , ), ) else : raise ValueError ( f \"method { method } not recognized\" ) make_adata This function creates an AnnData object from the given input parameters. Parameters: genes ( list ) \u2013 List of genes that will be used as variable names. embs ( Tensor | Dict ) \u2013 Embeddings of the cells. The shape of the tensor is (n_cells, n_features). if multiple, it is a dict of name -> tensor pos ( Tensor , default: None ) \u2013 Positions of the cells. The shape of the tensor is (n_cells,). expr_pred ( List [ Tensor ] , default: None ) \u2013 Predicted expression. The shape of the tensors are (n_cells, n_genes). the first is mu, the second theta, the third pi if present classes ( list , default: None ) \u2013 List of classes, the order should be the same as in the pred and gtclass tensors. pred ( Tensor , default: None ) \u2013 Predicted labels. The shape of the tensor is (n_cells, n_classes). Default is None. label_decoders ( dict , default: None ) \u2013 Dictionary to map class codes to class names. Default is None. labels_hierarchy ( dict , default: None ) \u2013 Dictionary representing the hierarchy of labels. Default is {}. see the model for defintion. gtclass ( Tensor , default: None ) \u2013 Ground truth class values. Default is None. doplot ( bool , default: True ) \u2013 Whether to generate plots. Default is True. Returns: AnnData \u2013 anndata.AnnData: The created AnnData object. Source code in scprint2/model/utils.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def make_adata ( genes : List [ str ], embs : Union [ Tensor , Dict [ str , Tensor ]], pos : Tensor = None , expr_pred : List [ Tensor ] = None , classes : List [ str ] = None , pred : Tensor = None , label_decoders : Optional [ Dict ] = None , labels_hierarchy : Optional [ Dict ] = None , gtclass : Optional [ Tensor ] = None , doplot : bool = True , ) -> AnnData : \"\"\" This function creates an AnnData object from the given input parameters. Args: genes (list): List of genes that will be used as variable names. embs (torch.Tensor|Dict): Embeddings of the cells. The shape of the tensor is (n_cells, n_features). if multiple, it is a dict of name -> tensor pos (torch.Tensor): Positions of the cells. The shape of the tensor is (n_cells,). expr_pred (List[torch.Tensor]): Predicted expression. The shape of the tensors are (n_cells, n_genes). the first is mu, the second theta, the third pi if present classes (list): List of classes, the order should be the same as in the pred and gtclass tensors. pred (torch.Tensor, optional): Predicted labels. The shape of the tensor is (n_cells, n_classes). Default is None. label_decoders (dict, optional): Dictionary to map class codes to class names. Default is None. labels_hierarchy (dict, optional): Dictionary representing the hierarchy of labels. Default is {}. see the model for defintion. gtclass (torch.Tensor, optional): Ground truth class values. Default is None. doplot (bool, optional): Whether to generate plots. Default is True. Returns: anndata.AnnData: The created AnnData object. \"\"\" print ( \"logging the anndata\" ) colname = [ \"pred_\" + i for i in classes ] if pred is not None : obs = np . array ( pred . to ( device = \"cpu\" , dtype = torch . int32 )) # label decoders is not cls_decoders. one is a dict to map class codes (ints) # to class names the other is the module the predict the class if label_decoders is not None : obs = np . array ( [ [ label_decoders [ classes [ i ]][ n ] for n in name ] for i , name in enumerate ( obs . T ) ] ) . T if gtclass is not None : colname += classes nobs = np . array ( gtclass . to ( device = \"cpu\" , dtype = torch . int32 )) if label_decoders is not None : nobs = np . array ( [ [ label_decoders [ classes [ i ]][ n ] for n in name ] for i , name in enumerate ( nobs . T ) ] ) . T obs = np . hstack ([ obs , nobs ]) n_cells = embs [ list ( embs . keys ())[ 0 ]] . shape [ 0 ] layers = None size = len ( genes ) if pos is not None : minval = pos . min () maxval = pos . max () genes = genes [ minval : maxval + 1 ] size = len ( genes ) pos = pos - minval mu_array = np . zeros (( n_cells , size ), dtype = np . float32 ) pos = pos . cpu () . numpy () # Create empty array with same shape as expr_pred[0] # Fill array with values from expr_pred[0] for idx in range ( n_cells ): mu_array [ idx , pos [ idx ]] = expr_pred [ 0 ][ idx ] . cpu () . numpy () + 1 exist = mu_array . sum ( 0 ) != 0 mu_array = mu_array [:, exist ] mu_array [ mu_array == 1 ] = 0 layers = { \"scprint_mu\" : mu_array , # \"used_scprint\": csr_matrix(pos), } if len ( expr_pred ) > 1 : theta_array = np . zeros (( n_cells , size ), dtype = np . float32 ) # Fill array with values from expr_pred[0] for idx in range ( n_cells ): theta_array [ idx , pos [ idx ]] = expr_pred [ 1 ][ idx ] . cpu () . numpy () layers [ \"scprint_theta\" ] = theta_array [:, exist ] pi_array = np . zeros (( n_cells , size ), dtype = np . float32 ) # Fill array with values from expr_pred[0] for idx in range ( n_cells ): pi_array [ idx , pos [ idx ]] = expr_pred [ 2 ][ idx ] . cpu () . numpy () layers [ \"scprint_pi\" ] = pi_array [:, exist ] genes = [ n for i , n in enumerate ( genes ) if exist [ i ] > 0 ] else : genes = [] adata = AnnData ( X = csr_matrix (( n_cells , len ( genes ))), layers = layers , obs = ( pd . DataFrame ( obs , columns = colname , ) if pred is not None else None ), var = pd . DataFrame ( index = genes ), ) for k , v in embs . items (): adata . obsm [ \"scprint_emb_\" + k ] = v . cpu () . numpy () rep = \"scprint_emb_\" + k del embs accuracy = {} if labels_hierarchy is None : labels_hierarchy = {} if pred is not None : for clss in classes : if gtclass is not None : tr = translate ( set ( adata . obs [ clss ]), clss ) if tr is not None : adata . obs [ \"conv_\" + clss ] = adata . obs [ clss ] . replace ( tr ) tr = translate ( set ( adata . obs [ \"pred_\" + clss ]), clss ) if tr is not None : adata . obs [ \"conv_pred_\" + clss ] = adata . obs [ \"pred_\" + clss ] . replace ( tr ) res = [] if label_decoders is not None and gtclass is not None : class_topred = label_decoders [ clss ] . values () if clss in labels_hierarchy : cur_labels_hierarchy = { label_decoders [ clss ][ k ]: [ label_decoders [ clss ][ i ] for i in v ] for k , v in labels_hierarchy [ clss ] . items () } else : cur_labels_hierarchy = {} for pred , true in adata . obs [[ \"pred_\" + clss , clss ]] . values : if pred == true : res . append ( True ) continue if len ( labels_hierarchy ) > 0 : if true in cur_labels_hierarchy : res . append ( pred in cur_labels_hierarchy [ true ]) elif true not in class_topred : raise ValueError ( f \"true label { true } not in available classes\" ) elif true != \"unknown\" : res . append ( False ) elif true not in class_topred : raise ValueError ( f \"true label { true } not in available classes\" ) elif true != \"unknown\" : res . append ( False ) else : pass accuracy [ \"pred_\" + clss ] = sum ( res ) / len ( res ) if len ( res ) > 0 else 0 adata . obs = adata . obs . astype ( \"category\" ) print ( adata ) if doplot and adata . shape [ 0 ] > 100 : sc . pp . neighbors ( adata , use_rep = rep ) sc . tl . umap ( adata ) sc . tl . leiden ( adata , key_added = \"sprint_leiden\" ) if gtclass is not None : color = [ i for pair in zip ( [ \"conv_\" + i if \"conv_\" + i in adata . obs . columns else i for i in classes ], [ ( \"conv_pred_\" + i if \"conv_pred_\" + i in adata . obs . columns else \"pred_\" + i ) for i in classes ], ) for i in pair ] fig , axs = plt . subplots ( int ( len ( color ) / 2 ), 2 , figsize = ( 24 , len ( color ) * 4 ) ) plt . subplots_adjust ( wspace = 1 ) if len ( color ) > 2 : for i , col in enumerate ( color ): sc . pl . umap ( adata , color = col , ax = axs [ i // 2 , i % 2 ], show = False , ) acc = \"\" if \"pred_\" in col and col . split ( \"conv_\" )[ - 1 ] in accuracy : acc = \" (accuracy: {:.2f} )\" . format ( accuracy [ col . split ( \"conv_\" )[ - 1 ]] ) axs [ i // 2 , i % 2 ] . set_title ( col + \" UMAP\" + acc ) if \"cell_type\" in col : axs [ i // 2 , i % 2 ] . legend ( fontsize = \"x-small\" ) axs [ i // 2 , i % 2 ] . set_xlabel ( \"UMAP1\" ) axs [ i // 2 , i % 2 ] . set_ylabel ( \"UMAP2\" ) else : for i , col in enumerate ( color ): sc . pl . umap ( adata , color = col , ax = axs [ i % 2 ], show = False , ) acc = \"\" if \"pred_\" in col and col . split ( \"conv_\" )[ - 1 ] in accuracy : acc = \" (accuracy: {:.2f} )\" . format ( accuracy [ col . split ( \"conv_\" )[ - 1 ]] ) axs [ i % 2 ] . set_title ( col + \" UMAP\" + acc ) if \"cell_type\" in col : axs [ i % 2 ] . legend ( fontsize = \"x-small\" ) axs [ i % 2 ] . set_xlabel ( \"UMAP1\" ) axs [ i % 2 ] . set_ylabel ( \"UMAP2\" ) else : color = [ ( \"conv_pred_\" + i if \"conv_pred_\" + i in adata . obs . columns else \"pred_\" + i ) for i in classes ] if len ( color ) > 1 : fig , axs = plt . subplots ( len ( color ), 1 , figsize = ( 16 , len ( color ) * 8 )) for i , col in enumerate ( color ): sc . pl . umap ( adata , color = col , ax = axs [ i ], show = False , ) acc = \"\" if \"pred_\" in col and col . split ( \"conv_\" )[ - 1 ] in accuracy : acc = \" (accuracy: {:.2f} )\" . format ( accuracy [ col . split ( \"conv_\" )[ - 1 ]] ) axs [ i ] . set_title ( col + \" UMAP of \" + rep + \" embedding \" + acc ) axs [ i ] . set_xlabel ( \"UMAP1\" ) axs [ i ] . set_ylabel ( \"UMAP2\" ) else : fig = sc . pl . umap ( adata , color = color , show = False , return_fig = True , ) plt . show () else : fig = None return adata , fig simple_masker Randomly mask a batch of data. Parameters: shape ( List [ int ] ) \u2013 The shape of the data. mask_ratio ( float , default: 0.15 ) \u2013 The ratio of genes to mask, default to 0.15. Returns: Tensor \u2013 torch.Tensor: A tensor of masked data. Source code in scprint2/model/utils.py 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 def simple_masker ( shape : List [ int ], mask_ratio : float = 0.15 , ) -> torch . Tensor : \"\"\" Randomly mask a batch of data. Args: shape (List[int]): The shape of the data. mask_ratio (float): The ratio of genes to mask, default to 0.15. Returns: torch.Tensor: A tensor of masked data. \"\"\" return torch . rand ( shape ) < mask_ratio test Test the given model on the full set of benchmarks and save the results to JSON files. Parameters: model ( Module ) \u2013 The model to be tested. filedir ( str ) \u2013 The directory where the data files are located. do_class ( bool , default: True ) \u2013 Whether to perform classification. Defaults to True. maxcells_grn ( int , default: 1024 ) \u2013 Maximum cells for GRN analysis. Defaults to 1024. Returns: None \u2013 None Source code in scprint2/model/utils.py 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 def test ( model : torch . nn . Module , filedir : str , do_class : bool = True , maxcells_grn : int = 1024 , ) -> None : \"\"\" Test the given model on the full set of benchmarks and save the results to JSON files. Args: model (torch.nn.Module): The model to be tested. filedir (str): The directory where the data files are located. do_class (bool): Whether to perform classification. Defaults to True. maxcells_grn (int): Maximum cells for GRN analysis. Defaults to 1024. Returns: None \"\"\" metrics = {} tot = {} for dataset , path in EMBEDDING_DATASETS . items (): res = embbed_task . default_benchmark ( model , dataset = path , do_class = do_class , coarse = False , ) tot [ \"embed_\" + dataset ] = res metrics . update ( { \"emb_\" + dataset + \"/scib\" : float ( res [ \"scib\" ][ \"Total\" ]), \"emb_\" + dataset + \"/scib_bio\" : float ( res [ \"scib\" ][ \"Bio conservation\" ]), \"emb_\" + dataset + \"/scib_batch\" : float ( res [ \"scib\" ][ \"Batch correction\" ]), \"emb_\" + dataset + \"/ct_class\" : float ( res [ \"classif\" ] . get ( \"cell_type_ontology_term_id\" , {}) . get ( \"macro\" , 0 ) if do_class else 0 ), \"emb_\" + dataset + \"/ct_class_macro\" : float ( res [ \"classif\" ] . get ( \"cell_type_ontology_term_id\" , {}) . get ( \"macro\" , 0 ) if do_class else 0 ), } ) print ( metrics ) gc . collect () for dataset , filepath in DENOISE_DATASETS . items (): res = denoise_task . default_benchmark ( model , dataset = filepath ) tot [ \"denoise_\" + dataset ] = res metrics . update ( { \"denoise_\" + dataset + \"/reco2full_vs_noisy2full\" : float ( res [ \"reco2full\" ] - res [ \"noisy2full\" ] ), } ) print ( metrics ) gc . collect () res = grn_task . default_benchmark ( model , \"gwps\" , batch_size = 32 if model . d_model <= 512 else 8 , maxcells = maxcells_grn , ) tot [ \"grn_gwps\" ] = res metrics . update ( { \"grn_gwps/auprc_self\" : float ( res [ \"self\" ][ \"auprc\" ]), \"grn_gwps/epr_self\" : float ( res [ \"self\" ][ \"epr\" ]), \"grn_gwps/auprc_omni\" : float ( res [ \"omni\" ][ \"auprc\" ]), \"grn_gwps/epr_omni\" : float ( res [ \"omni\" ][ \"epr\" ]), \"grn_gwps/auprc\" : float ( res [ \"mean\" ][ \"auprc\" ]), \"grn_gwps/epr\" : float ( res [ \"mean\" ][ \"epr\" ]), } ) print ( metrics ) gc . collect () for dataset , filepath in { \"old_kidney\" : \"https://datasets.cellxgene.cziscience.com/ede85b09-454b-4374-bf60-5f675e989b64.h5ad\" , # \"kidney\": \"https://datasets.cellxgene.cziscience.com/01bc7039-961f-4c24-b407-d535a2a7ba2c.h5ad\", \"lung_smart\" : \"https://datasets.cellxgene.cziscience.com/6ebba0e0-a159-406f-8095-451115673a2c.h5ad\" , # filedir + \"/../../data/yBCKp6HmXuHa0cZptMo7.h5ad\", } . items (): res = grn_task . default_benchmark ( model , filepath , # kidney dataset (2.87, 1.27) (0.00147, 0.00133) batch_size = 32 if model . d_model <= 512 else 8 , maxcells = maxcells_grn , maxgenes = 4000 , ) tot [ \"grn_omni_\" + dataset ] = res metrics . update ( { \"grn_omni_\" + dataset + \"/auprc_class\" : float ( np . mean ([ i [ \"auprc\" ] for k , i in res . items () if \"_class\" in k ]) ), \"grn_omni_\" + dataset + \"/or_class\" : float ( np . mean ([ i [ \"odd_ratio\" ] for k , i in res . items () if \"_class\" in k ]) ), \"grn_omni_\" + dataset + \"/tf_enr_class\" : float ( np . sum ( [ i . get ( \"TF_enr\" , False ) for k , i in res . items () if \"_class\" in k ] ) ), \"grn_omni_\" + dataset + \"/tf_targ_enr_class\" : float ( np . mean ( [ i [ \"significant_enriched_TFtargets\" ] for k , i in res . items () if \"_class\" in k ] ) ), \"grn_omni_\" + dataset + \"/auprc\" : float ( np . mean ([ i [ \"auprc\" ] for k , i in res . items () if \"_mean\" in k ]) ), \"grn_omni_\" + dataset + \"/epr\" : float ( np . mean ([ i [ \"epr\" ] for k , i in res . items () if \"_mean\" in k ]) ), \"grn_omni_\" + dataset + \"/or\" : float ( np . mean ([ i [ \"odd_ratio\" ] for k , i in res . items () if \"_mean\" in k ]) ), \"grn_omni_\" + dataset + \"/tf_enr\" : float ( np . sum ( [ i . get ( \"TF_enr\" , False ) for k , i in res . items () if \"_mean\" in k ] ) ), \"grn_omni_\" + dataset + \"/tf_targ_enr\" : float ( np . mean ( [ i [ \"significant_enriched_TFtargets\" ] for k , i in res . items () if \"_mean\" in k ] ) ), # 'grn_omni/ct': res['classif']['cell_type_ontology_term_id']['accuracy'], } ) print ( metrics ) gc . collect () return metrics , tot zinb_sample zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution. Parameters: mu ( Tensor ) \u2013 The mean of the Negative Binomial (NB) distribution. theta ( Tensor ) \u2013 The dispersion parameter of the NB distribution. zi_probs ( Tensor ) \u2013 The zero-inflation probabilities. sample_shape ( Size , default: Size ([]) ) \u2013 The output shape. Defaults to torch.Size([]). Returns: Tensor \u2013 torch.Tensor: A sample from the ZINB distribution. Source code in scprint2/model/utils.py 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 def zinb_sample ( mu : torch . Tensor , theta : torch . Tensor , zi_probs : torch . Tensor , sample_shape : torch . Size = torch . Size ([]), ) -> torch . Tensor : \"\"\" zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution. Args: mu (torch.Tensor): The mean of the Negative Binomial (NB) distribution. theta (torch.Tensor): The dispersion parameter of the NB distribution. zi_probs (torch.Tensor): The zero-inflation probabilities. sample_shape (torch.Size, optional): The output shape. Defaults to torch.Size([]). Returns: torch.Tensor: A sample from the ZINB distribution. \"\"\" concentration = theta rate = theta / mu # Important remark: Gamma is parametrized by the rate = 1/scale! gamma_d = Gamma ( concentration = concentration , rate = rate ) p_means = gamma_d . sample ( sample_shape ) # Clamping as distributions objects can have buggy behaviors when # their parameters are too high l_train = torch . clamp ( p_means , max = 1e8 ) samp = Poisson ( l_train ) . sample () # Shape : (n_samples, n_cells_batch, n_vars) is_zero = torch . rand_like ( samp ) <= zi_probs samp_ = torch . where ( is_zero , torch . zeros_like ( samp ), samp ) return samp_ encoder and decoder modules scprint2.model.encoders Classes: Name Description CategoryValueEncoder ContinuousValueEncoder DPositionalEncoding The PositionalEncoding module applies a positional encoding to a sequence of vectors. EasyExprGNN ExprBasedFT GNN GeneEncoder PositionalEncoding CategoryValueEncoder Bases: Module Encodes categorical values into a vector using an embedding layer and layer normalization. Parameters: num_embeddings ( int ) \u2013 The number of possible values. embedding_dim ( int ) \u2013 The dimension of the output vectors. padding_idx ( int , default: None ) \u2013 The index of the padding token. Defaults to None. Note: not used in the current version of scprint-2. Source code in scprint2/model/encoders.py 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 def __init__ ( self , num_embeddings : int , embedding_dim : int , padding_idx : Optional [ int ] = None , ): \"\"\" Encodes categorical values into a vector using an embedding layer and layer normalization. Args: num_embeddings (int): The number of possible values. embedding_dim (int): The dimension of the output vectors. padding_idx (int, optional): The index of the padding token. Defaults to None. Note: not used in the current version of scprint-2. \"\"\" super ( CategoryValueEncoder , self ) . __init__ () self . embedding = nn . Embedding ( num_embeddings , embedding_dim , padding_idx = padding_idx ) ContinuousValueEncoder Bases: Module Encode real number values to a vector using neural nets projection. Parameters: d_model ( int ) \u2013 The dimension of the input vectors. dropout ( float , default: 0.1 ) \u2013 The dropout rate to apply to the output of the positional encoding. max_value ( int , default: 100000 ) \u2013 The maximum value of the input. Defaults to 100_000. layers ( int , default: 1 ) \u2013 The number of layers in the encoder. Defaults to 1. size ( int , default: 1 ) \u2013 The size of the input. Defaults to 1. Methods: Name Description forward Args: Source code in scprint2/model/encoders.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def __init__ ( self , d_model : int , dropout : float = 0.1 , max_value : int = 100_000 , layers : int = 1 , size : int = 1 , ): \"\"\" Encode real number values to a vector using neural nets projection. Args: d_model (int): The dimension of the input vectors. dropout (float, optional): The dropout rate to apply to the output of the positional encoding. max_value (int, optional): The maximum value of the input. Defaults to 100_000. layers (int, optional): The number of layers in the encoder. Defaults to 1. size (int, optional): The size of the input. Defaults to 1. \"\"\" super ( ContinuousValueEncoder , self ) . __init__ () self . max_value = max_value self . encoder = nn . ModuleList () self . output_dim = d_model # self.mask_value = nn.Embedding(1, d_model) self . encoder . append ( nn . Linear ( size , d_model )) for _ in range ( layers - 1 ): self . encoder . append ( nn . LayerNorm ( d_model )) self . encoder . append ( nn . ReLU ()) self . encoder . append ( nn . Dropout ( p = dropout )) self . encoder . append ( nn . Linear ( d_model , d_model )) forward Parameters: x ( Tensor ) \u2013 Tensor, shape [batch_size, seq_len] Source code in scprint2/model/encoders.py 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def forward ( self , x : Tensor , mask : Tensor = None ) -> Tensor : \"\"\" Args: x: Tensor, shape [batch_size, seq_len] \"\"\" # expand last dimension x = x . unsqueeze ( - 1 ) # use the mask embedding when x=-1 # mask = (x == -1).float() x = torch . clamp ( x , min = 0 , max = self . max_value ) for val in self . encoder : x = val ( x ) if mask is not None : x = x . masked_fill_ ( mask . unsqueeze ( - 1 ), 0 ) # x = x.masked_fill_(mask.unsqueeze(-1), self.mask_value(0)) return x DPositionalEncoding Bases: Module The PositionalEncoding module applies a positional encoding to a sequence of vectors. This is necessary for the Transformer model, which does not have any inherent notion of position in a sequence. The positional encoding is added to the input embeddings and allows the model to attend to positions in the sequence. Parameters: d_model ( int ) \u2013 The dimension of the input vectors. max_len_x ( int ) \u2013 The maximum length in the x dimension. max_len_y ( int ) \u2013 The maximum length in the y dimension. maxvalue_x ( float , default: 10000.0 ) \u2013 Maximum value for x dimension scaling. Defaults to 10000.0. maxvalue_y ( float , default: 10000.0 ) \u2013 Maximum value for y dimension scaling. Defaults to 10000.0. Note: not used in the current version of scprint-2. Methods: Name Description forward Args: Source code in scprint2/model/encoders.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def __init__ ( self , d_model : int , max_len_x : int , max_len_y : int , maxvalue_x = 10000.0 , maxvalue_y = 10000.0 , ): super ( DPositionalEncoding , self ) . __init__ () position2 = torch . arange ( max_len_y ) . unsqueeze ( 1 ) position1 = torch . arange ( max_len_x ) . unsqueeze ( 1 ) half_n = d_model // 2 div_term2 = torch . exp ( torch . arange ( 0 , half_n , 2 ) * ( - math . log ( maxvalue_y ) / d_model ) ) div_term1 = torch . exp ( torch . arange ( 0 , half_n , 2 ) * ( - math . log ( maxvalue_x ) / d_model ) ) pe1 = torch . zeros ( max_len_x , 1 , d_model ) pe2 = torch . zeros ( max_len_y , 1 , d_model ) pe1 [:, 0 , 0 : half_n : 2 ] = torch . sin ( position1 * div_term1 ) pe1 [:, 0 , 1 : half_n : 2 ] = torch . cos ( position1 * div_term1 ) pe2 [:, 0 , half_n :: 2 ] = torch . sin ( position2 * div_term2 ) pe2 [:, 0 , 1 + half_n :: 2 ] = torch . cos ( position2 * div_term2 ) # https://github.com/tatp22/multidim-positional-encoding/blob/master/positional_encodings/torch_encodings.py self . register_buffer ( \"pe1\" , pe1 ) self . register_buffer ( \"pe2\" , pe2 ) forward Parameters: x ( Tensor ) \u2013 Tensor, shape [seq_len, batch_size, embedding_dim] Source code in scprint2/model/encoders.py 234 235 236 237 238 239 240 241 def forward ( self , x : Tensor , pos_x : Tensor , pos_y : Tensor ) -> Tensor : \"\"\" Args: x: Tensor, shape [seq_len, batch_size, embedding_dim] \"\"\" x = x + self . pe1 [ pos_x ] x = x + self . pe2 [ pos_y ] return x EasyExprGNN Bases: Module Easy Expression Graph Neural Network The main GNN used in scPRINT-2 for expression encoding. It is inspired from the DeepSets architecture to aggregate neighbor information. Parameters: self_dim ( int , default: 64 ) \u2013 Dimension of the self features output_dim ( int , default: 32 ) \u2013 Output dimension self_layers ( int , default: 2 ) \u2013 Number of layers for self features dropout ( float , default: 0.1 ) \u2013 Dropout rate shared_layers ( int , default: 2 ) \u2013 Number of shared layers neighbors_layers ( int , default: 2 ) \u2013 Number of layers for neighbors features Methods: Name Description forward Forward pass of the Easy Expression GNN Source code in scprint2/model/encoders.py 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 def __init__ ( self , self_dim : int = 64 , output_dim : int = 32 , self_layers : int = 2 , dropout : float = 0.1 , shared_layers : int = 2 , neighbors_layers : int = 2 , ): \"\"\" Easy Expression Graph Neural Network The main GNN used in scPRINT-2 for expression encoding. It is inspired from the DeepSets architecture to aggregate neighbor information. Args: self_dim (int): Dimension of the self features output_dim (int): Output dimension self_layers (int): Number of layers for self features dropout (float): Dropout rate shared_layers (int): Number of shared layers neighbors_layers (int): Number of layers for neighbors features \"\"\" super ( EasyExprGNN , self ) . __init__ () self . output_dim = output_dim self . self_dim = self_dim # neighbors self . neighbors_layers = nn . ModuleList () self . neighbors_layers . append ( nn . Linear ( 2 , self_dim // 2 )) for i in range ( neighbors_layers - 1 ): self . neighbors_layers . append ( nn . LayerNorm ( self_dim // 2 )) self . neighbors_layers . append ( nn . ReLU ()) self . neighbors_layers . append ( nn . Dropout ( p = dropout )) self . neighbors_layers . append ( nn . Linear ( self_dim // 2 , self_dim // 2 )) # self self . self_layers = nn . ModuleList () self . self_layers . append ( nn . Linear ( 1 , self_dim // 2 )) for i in range ( self_layers - 1 ): self . self_layers . append ( nn . LayerNorm ( self_dim // 2 )) self . self_layers . append ( nn . ReLU ()) self . self_layers . append ( nn . Dropout ( p = dropout )) self . self_layers . append ( nn . Linear ( self_dim // 2 , self_dim // 2 )) # shared self . shared_layers = nn . ModuleList () for i in range ( shared_layers - 1 ): self . shared_layers . append ( nn . Linear ( self_dim , self_dim )) self . shared_layers . append ( nn . LayerNorm ( self_dim )) self . shared_layers . append ( nn . ReLU ()) self . shared_layers . append ( nn . Dropout ( p = dropout )) self . shared_layers . append ( nn . Linear ( self_dim , output_dim )) forward Forward pass of the Easy Expression GNN Parameters: expr ( Optional [ Tensor ] , default: None ) \u2013 Tensor of shape (batch, seq_len) representing expression values neighbors ( Optional [ Tensor ] , default: None ) \u2013 Tensor of shape (batch, seq_len, n_neighbors) representing neighbor indices edge_info ( Optional [ Tensor ] , default: None ) \u2013 Tensor of shape (batch, seq_len, n_neighbors) representing edge information mask ( Optional [ Tensor ] , default: None ) \u2013 Tensor of shape (batch, seq_len) representing mask for the input Returns: Tensor \u2013 Tensor of shape (batch, seq_len, output_dim) representing the output features Source code in scprint2/model/encoders.py 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def forward ( self , expr : Optional [ Tensor ] = None , neighbors : Optional [ Tensor ] = None , edge_info : Optional [ Tensor ] = None , mask : Optional [ Tensor ] = None , ) -> Tensor : \"\"\" Forward pass of the Easy Expression GNN Args: expr: Tensor of shape (batch, seq_len) representing expression values neighbors: Tensor of shape (batch, seq_len, n_neighbors) representing neighbor indices edge_info: Tensor of shape (batch, seq_len, n_neighbors) representing edge information mask: Tensor of shape (batch, seq_len) representing mask for the input Returns: Tensor of shape (batch, seq_len, output_dim) representing the output features \"\"\" # batch, seq_len, neighbs if neighbors is None : neighbors = torch . zeros ( ( expr . shape [ 0 ], expr . shape [ 1 ], self . self_dim // 2 ), device = expr . device ) else : neighbors = neighbors . transpose ( 1 , 2 ) neighbors = torch . cat ( [ neighbors . unsqueeze ( - 1 ), edge_info . unsqueeze ( - 1 )], dim =- 1 ) for i , layer in enumerate ( self . neighbors_layers ): # batch, seq_len, neighbs, hidden_dim neighbors = layer ( neighbors ) neighbors = neighbors . sum ( - 2 ) if expr is None : expr = torch . zeros ( ( neighbors . shape [ 0 ], neighbors . shape [ 1 ], 1 ), device = neighbors . device ) else : expr = expr . unsqueeze ( - 1 ) for i , layer in enumerate ( self . self_layers ): expr = layer ( expr ) x = torch . cat ([ expr , neighbors ], dim =- 1 ) for layer in self . shared_layers : # batch, seq_len, neighbs, hidden_dim x = layer ( x ) if mask is not None : x = x . masked_fill ( mask . unsqueeze ( - 1 ), 0 ) return x ExprBasedFT Bases: Module Encode real number values to a vector using neural nets projection. Parameters: d_model ( int ) \u2013 The dimension of the input vectors. gene_encoder ( Module ) \u2013 The gene name encoder module. expr_encoder ( Module , default: Identity () ) \u2013 The expression encoder module. Defaults to nn.Identity. dropout ( float , default: 0.1 ) \u2013 The dropout rate to apply to the output of the positional encoding. layers ( int , default: 2 ) \u2013 The number of layers in the encoder. Defaults to 2. intermediary_d ( int , default: 256 + 64 ) \u2013 The dimension of the intermediary layers. Defaults to 256 + 64. Methods: Name Description forward Args: Source code in scprint2/model/encoders.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 def __init__ ( self , d_model : int , gene_encoder : nn . Module , expr_encoder : nn . Module = nn . Identity (), dropout : float = 0.1 , layers : int = 2 , intermediary_d : int = 256 + 64 , ): \"\"\" Encode real number values to a vector using neural nets projection. Args: d_model (int): The dimension of the input vectors. gene_encoder (nn.Module): The gene name encoder module. expr_encoder (nn.Module, optional): The expression encoder module. Defaults to nn.Identity. dropout (float, optional): The dropout rate to apply to the output of the positional encoding. layers (int, optional): The number of layers in the encoder. Defaults to 2. intermediary_d (int, optional): The dimension of the intermediary layers. Defaults to 256 + 64. \"\"\" super ( ExprBasedFT , self ) . __init__ () self . encoder = nn . ModuleList () # self.mask_value = nn.Embedding(1, d_model) self . add_module ( \"gene_encoder\" , gene_encoder ) self . add_module ( \"expr_encoder\" , expr_encoder ) expr_shape , gene_shape = ( self . expr_encoder . output_dim , self . gene_encoder . output_dim , ) self . encoder . append ( nn . Linear ( expr_shape + gene_shape , intermediary_d )) for i in range ( layers - 1 ): self . encoder . append ( nn . LayerNorm ( intermediary_d )) self . encoder . append ( nn . ReLU ()) self . encoder . append ( nn . Dropout ( p = dropout )) self . encoder . append ( nn . Linear ( intermediary_d , intermediary_d if i < layers - 2 else d_model ) ) forward Parameters: gene_pos ( Tensor [ batch_size , seq_len ] ) \u2013 Gene position indices input to the gene encoder expr ( ( Tensor [ batch_size , seq_len ], Optional ) , default: None ) \u2013 Expression values input to the expression encoder mask ( ( Tensor [ batch_size , seq_len ], Optional ) , default: None ) \u2013 Mask for the input input to the expression encoder neighbors ( ( Tensor [ batch_size , seq_len , n_neighbors ], Optional ) , default: None ) \u2013 Neighbors indices input to the expression encoder when it is a GNN neighbors_info ( ( Tensor [ batch_size , seq_len , n_neighbors ], Optional ) , default: None ) \u2013 optional additional information about the neighbors input to the expression encoder when it is a GNN Source code in scprint2/model/encoders.py 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 def forward ( self , gene_pos : Tensor , expr : Optional [ Tensor ] = None , mask : Optional [ Tensor ] = None , neighbors : Optional [ Tensor ] = None , neighbors_info : Optional [ Tensor ] = None , ) -> Tensor : \"\"\" Args: gene_pos (Tensor[batch_size, seq_len]): Gene position indices input to the gene encoder expr (Tensor[batch_size, seq_len], Optional): Expression values input to the expression encoder mask (Tensor[batch_size, seq_len], Optional): Mask for the input input to the expression encoder neighbors (Tensor[batch_size, seq_len, n_neighbors], Optional): Neighbors indices input to the expression encoder when it is a GNN neighbors_info (Tensor[batch_size, seq_len, n_neighbors], Optional): optional additional information about the neighbors input to the expression encoder when it is a GNN \"\"\" # expand last dimension if neighbors is None and expr is None : expr = torch . zeros ( ( gene_pos . shape [ 0 ], gene_pos . shape [ 1 ], self . expr_encoder . output_dim ), dtype = torch . float32 , device = gene_pos . device , ) # if no expr information: consider that it is all masked else : expr = ( self . expr_encoder ( expr , mask = mask ) if neighbors is None else self . expr_encoder ( expr , neighbors , neighbors_info , mask = mask ) ) gene_pos = self . gene_encoder ( gene_pos ) x = torch . cat ([ expr , gene_pos ], dim =- 1 ) for val in self . encoder : x = val ( x ) return x GNN Bases: Module Graph Neural Network model Another implementation of a GNN layer that can be used for expression encoding. Supports GCN, GAT, GraphSAGE, and DeepSets architectures. Parameters: input_dim ( int , default: 1 ) \u2013 Dimension of input node features output_dim ( int , default: 256 ) \u2013 Dimension of output node features num_layers ( int , default: 2 ) \u2013 Number of GNN layers dropout ( float , default: 0.1 ) \u2013 Dropout probability gnn_type ( str , default: 'deepset' ) \u2013 Type of GNN layer ('gcn', 'gat', 'sage', or 'deepset') Methods: Name Description forward Forward pass Source code in scprint2/model/encoders.py 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 def __init__ ( self , input_dim : int = 1 , # here, 1 or 2 merge_dim : int = 32 , output_dim : int = 256 , num_layers : int = 2 , dropout : float = 0.1 , gnn_type : str = \"deepset\" , add_connection_feature : bool = False , ): \"\"\" Graph Neural Network model Another implementation of a GNN layer that can be used for expression encoding. Supports GCN, GAT, GraphSAGE, and DeepSets architectures. Args: input_dim: Dimension of input node features output_dim: Dimension of output node features num_layers: Number of GNN layers dropout: Dropout probability gnn_type: Type of GNN layer ('gcn', 'gat', 'sage', or 'deepset') \"\"\" super () . __init__ () self . input_dim = input_dim self . output_dim = output_dim if num_layers == 1 : raise ValueError ( \"num_layers must be greater than 1\" ) self . num_layers = num_layers self . dropout = dropout self . gnn_type = gnn_type self . add_connection_feature = add_connection_feature if gnn_type == \"deepset\" : # Local MLP (phi) for processing individual nodes self . input_nn_layer = MLP ( in_channels = input_dim , hidden_channels = merge_dim , out_channels = merge_dim , num_layers = num_layers , dropout = dropout , act = \"relu\" , norm = \"layer_norm\" , ) self . input_self_layer = MLP ( in_channels = input_dim , hidden_channels = merge_dim + 2 , out_channels = merge_dim , num_layers = num_layers - 1 , dropout = dropout , act = \"relu\" , norm = \"layer_norm\" , ) # Global MLP (rho) for processing aggregated features self . output_layer = MLP ( in_channels = ( ( merge_dim * 2 ) + 1 if add_connection_feature else merge_dim * 2 ), hidden_channels = output_dim , out_channels = output_dim , num_layers = num_layers , dropout = dropout , act = \"relu\" , norm = \"layer_norm\" , ) return # Select GNN layer type for other architectures else : if gnn_type == \"gcn\" : gnn_layer = GCNConv elif gnn_type == \"gat\" : gnn_layer = GATConv elif gnn_type == \"sage\" : gnn_layer = SAGEConv else : raise ValueError ( f \"Unknown GNN type: { gnn_type } \" ) self . gnn_layer = gnn_layer ( output_dim , output_dim , add_self_loops = False , normalize = False , aggr = \"mean\" , ) forward Forward pass Parameters: x ( Tensor ) \u2013 Node features [minibatch_size, ngenes] neighbors ( Tensor ) \u2013 Neighbor nodes [minibatch_size, ngenes, n_neighbors] or [minibatch_size, ngenes, n_neighbors, 2] edge_info ( Tensor , default: None ) \u2013 Graph connectivity [2, num_edges] if gnn_type != deepset, Edge features [num_edges, 1] if gnn_type == deepset, or None if gnn_type == deepset and no edge features. batch ( Tensor , default: None ) \u2013 Batch assignment vector [num_nodes] mask ( Tensor , default: None ) \u2013 Mask tensor for the nodes. Returns: Tensor ( Tensor ) \u2013 Node embeddings [num_nodes, hidden_dim] Source code in scprint2/model/encoders.py 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 def forward ( self , x : Tensor , neighbors : Tensor , edge_info : Optional [ Tensor ] = None , batch : Optional [ Tensor ] = None , mask : Optional [ Tensor ] = None , ) -> Tensor : \"\"\" Forward pass Args: x (Tensor): Node features [minibatch_size, ngenes] neighbors (Tensor): Neighbor nodes [minibatch_size, ngenes, n_neighbors] or [minibatch_size, ngenes, n_neighbors, 2] edge_info (Tensor, optional): Graph connectivity [2, num_edges] if gnn_type != deepset, Edge features [num_edges, 1] if gnn_type == deepset, or None if gnn_type == deepset and no edge features. batch (Tensor, optional): Batch assignment vector [num_nodes] mask (Tensor, optional): Mask tensor for the nodes. Returns: Tensor: Node embeddings [num_nodes, hidden_dim] \"\"\" # Standard GNN forward pass x = x . unsqueeze ( - 1 ) neighbors = neighbors . unsqueeze ( - 1 ) if self . gnn_type == \"deepset\" : neighbors = self . input_nn_layer ( neighbors ) . sum ( dim =- 3 ) x = self . input_self_layer ( x ) x = torch . cat ([ x , neighbors ], dim =- 1 ) else : x = self . gnn_layer ( x , edge_info ) neighbors = self . gnn_layer ( neighbors , edge_info ) for layer in self . layers : x = layer ( x , edge_info ) x = F . relu ( x ) x = F . dropout ( x , p = self . dropout , training = self . training ) # TODO: to finish x = self . output_layer ( x ) if mask is not None : x = x . masked_fill_ ( mask . unsqueeze ( - 1 ), 0 ) return x GeneEncoder Bases: Module Encodes gene sequences into a continuous vector space using an embedding layer. Uses memory mapping for efficient access to large embedding files. Parameters: num_embeddings ( int ) \u2013 The number of possible values embedding_dim ( int ) \u2013 The dimension of the output vectors padding_idx ( int , default: None ) \u2013 The index of the padding token weights ( Tensor , default: None ) \u2013 The initial weights for the embedding layer weights_file ( str , default: None ) \u2013 Path to parquet file containing embeddings freeze ( bool , default: False ) \u2013 Whether to freeze the weights of the embedding layer Methods: Name Description __del__ Cleanup method to ensure proper handling of memory-mapped file. forward Forward pass of the encoder. Source code in scprint2/model/encoders.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , num_embeddings : int , embedding_dim : int , padding_idx : Optional [ int ] = None , weights : Optional [ Tensor ] = None , weights_file : Optional [ str ] = None , freeze : bool = False , ): \"\"\" Encodes gene sequences into a continuous vector space using an embedding layer. Uses memory mapping for efficient access to large embedding files. Args: num_embeddings (int): The number of possible values embedding_dim (int): The dimension of the output vectors padding_idx (int, optional): The index of the padding token weights (Tensor, optional): The initial weights for the embedding layer weights_file (str, optional): Path to parquet file containing embeddings freeze (bool, optional): Whether to freeze the weights of the embedding layer \"\"\" super ( GeneEncoder , self ) . __init__ () self . output_dim = embedding_dim if weights_file is not None : self . memmap = True if not freeze : raise ValueError ( \"freeze must be True when using memory-mapped embeddings\" ) # Load the parquet file and create memory-mapped array import os import pandas as pd # Create memory-mapped file path self . mmap_file = f \" { weights_file } .mmap\" self . loc = None self . enc = None # Only create the memory-mapped file if it doesn't exist if not os . path . exists ( self . mmap_file ): print ( f \"Creating memory-mapped file for embeddings at { self . mmap_file } \" ) df = pd . read_parquet ( weights_file ) embeddings = torch . nn . AdaptiveAvgPool1d ( self . output_dim )( torch . tensor ( df . values ) ) # Create memory-mapped array self . embeddings = np . memmap ( self . mmap_file , dtype = \"float32\" , mode = \"w+\" , shape = embeddings . shape ) # Copy data to memory-mapped array self . embeddings [:] = embeddings . numpy () # self . embeddings . flush () # Clean up memory del df del embeddings else : print ( f \"Loading existing memory-mapped embeddings from { self . mmap_file } \" ) # Load existing memory-mapped file self . embeddings = np . memmap ( self . mmap_file , dtype = \"float32\" , mode = \"r\" , # Read-only mode since we don't need to modify shape = ( num_embeddings , embedding_dim ), ) else : self . memmap = False self . embeddings = nn . Embedding ( num_embeddings , embedding_dim , padding_idx = padding_idx , _freeze = freeze ) if weights is not None : self . embeddings . weight . data . copy_ ( torch . Tensor ( weights )) __del__ Cleanup method to ensure proper handling of memory-mapped file. Source code in scprint2/model/encoders.py 116 117 118 119 120 121 122 def __del__ ( self ): \"\"\"Cleanup method to ensure proper handling of memory-mapped file.\"\"\" if hasattr ( self , \"embeddings\" ) and self . embeddings is not None : try : self . embeddings . _mmap . close () except : pass forward Forward pass of the encoder. Parameters: x ( Tensor ) \u2013 Input tensor of indices [batch_size, seq_len] Returns: Tensor ( Tensor ) \u2013 Embedded vectors [batch_size, seq_len, embedding_dim] Source code in scprint2/model/encoders.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def forward ( self , x : Tensor ) -> Tensor : \"\"\" Forward pass of the encoder. Args: x (Tensor): Input tensor of indices [batch_size, seq_len] Returns: Tensor: Embedded vectors [batch_size, seq_len, embedding_dim] \"\"\" if self . memmap : if self . loc is None or not torch . all ( x . sum ( 1 ) == self . loc ): self . enc = ( torch . from_numpy ( self . embeddings [ x . reshape ( - 1 ) . cpu () . numpy ()] . copy () ) . reshape ( x . shape + ( - 1 ,)) . to ( x . device ) ) self . loc = x . sum ( 1 ) return self . enc . clone () else : return self . embeddings ( x ) PositionalEncoding Bases: Module The PositionalEncoding module applies a positional encoding to a sequence of vectors. This is necessary for the Transformer model, which does not have any inherent notion of position in a sequence. The positional encoding is added to the input embeddings and allows the model to attend to positions in the sequence. Parameters: d_model ( int ) \u2013 The dimension of the input vectors. gene_pos_enc ( list [ str ] , default: [] ) \u2013 The gene position encoding to use. Note: not used in the current version of scprint-2. Methods: Name Description forward Args: Source code in scprint2/model/encoders.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def __init__ ( self , d_model : int , gene_pos_enc : list [ str ] = [], ): \"\"\" The PositionalEncoding module applies a positional encoding to a sequence of vectors. This is necessary for the Transformer model, which does not have any inherent notion of position in a sequence. The positional encoding is added to the input embeddings and allows the model to attend to positions in the sequence. Args: d_model (int): The dimension of the input vectors. gene_pos_enc (list[str], optional): The gene position encoding to use. Note: not used in the current version of scprint-2. \"\"\" super ( PositionalEncoding , self ) . __init__ () self . gene_pos_enc = gene_pos_enc max_len = max ( gene_pos_enc ) position = torch . arange ( max_len ) . unsqueeze ( 1 ) token_to_pos = { token : pos for token , pos in enumerate ( gene_pos_enc )} # Create a dictionary to convert token to position div_term = torch . exp ( torch . arange ( 0 , d_model , 2 ) * ( - math . log ( float ( 10_000 )) / d_model ) ) pe = torch . zeros ( max_len , 1 , d_model ) pe [:, 0 , 0 :: 2 ] = torch . sin ( position * div_term ) pe [:, 0 , 1 :: 2 ] = torch . cos ( position * div_term ) # we reorder them and map them to gene_id (position) arr = [] for _ , v in token_to_pos . items (): arr . append ( pe [ v - 1 ] . numpy ()) pe = torch . Tensor ( np . array ( arr )) # Remove the unnecessary middle dimension since pe should be [m, d] # pe = pe.squeeze(1) self . register_buffer ( \"pe\" , pe ) forward Parameters: gene_pos ( Tensor ) \u2013 Gene position indices, shape [seq_len, batch_size] or [seq_len] Returns: Tensor ( Tensor ) \u2013 Positional encodings, shape [*gene_pos.shape, embedding_dim] Source code in scprint2/model/encoders.py 169 170 171 172 173 174 175 176 177 178 179 def forward ( self , gene_pos : Tensor ) -> Tensor : \"\"\" Args: gene_pos (Tensor): Gene position indices, shape [seq_len, batch_size] or [seq_len] Returns: Tensor: Positional encodings, shape [*gene_pos.shape, embedding_dim] \"\"\" return torch . index_select ( self . pe , 0 , gene_pos . reshape ( - 1 )) . reshape ( gene_pos . shape + ( - 1 ,) ) scprint2.model.decoders Classes: Name Description ClsDecoder ExprDecoder GraphSDEExprDecoder MVCDecoder VAEDecoder ClsDecoder Bases: Module ClsDecoder Decoder for classification task. Parameters: d_model ( int ) \u2013 Dimension of the input. n_cls ( int ) \u2013 Number of classes. layers ( List [ int ] , default: [256, 128] ) \u2013 List of hidden layers. activation ( Callable , default: ReLU ) \u2013 Activation function. dropout ( float , default: 0.1 ) \u2013 Dropout rate. Methods: Name Description forward Args: Source code in scprint2/model/decoders.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 def __init__ ( self , d_model : int , n_cls : int , layers : List [ int ] = [ 256 , 128 ], activation : Callable = nn . ReLU , dropout : float = 0.1 , ): \"\"\" ClsDecoder Decoder for classification task. Args: d_model (int): Dimension of the input. n_cls (int): Number of classes. layers (List[int]): List of hidden layers. activation (Callable): Activation function. dropout (float): Dropout rate. \"\"\" super ( ClsDecoder , self ) . __init__ () # module List layers = [ d_model ] + layers self . decoder = nn . Sequential () self . n_cls = n_cls for i , l in enumerate ( layers [ 1 :]): self . decoder . append ( nn . Linear ( layers [ i ], l )) self . decoder . append ( nn . LayerNorm ( l )) self . decoder . append ( activation ()) self . decoder . append ( nn . Dropout ( dropout )) self . out_layer = nn . Linear ( layers [ - 1 ], n_cls ) forward Parameters: x ( Tensor ) \u2013 Tensor, shape [batch_size, embsize] Source code in scprint2/model/decoders.py 259 260 261 262 263 264 265 def forward ( self , x : Tensor ) -> Tensor : \"\"\" Args: x: Tensor, shape [batch_size, embsize] \"\"\" x = self . decoder ( x ) return self . out_layer ( x ) ExprDecoder Bases: Module ExprDecoder Decoder for the gene expression prediction. Will output the mean, variance and zero logits, parameters of a zero inflated negative binomial distribution. Parameters: d_model ( int ) \u2013 The dimension of the model. This is the size of the input feature vector. nfirst_tokens_to_skip ( int , default: 0 ) \u2013 The number of initial labels to skip in the sequence. Defaults to 0. dropout ( float , default: 0.1 ) \u2013 The dropout rate applied during training to prevent overfitting. Defaults to 0.1. zinb ( bool , default: True ) \u2013 Whether to use a zero inflated negative binomial distribution. Defaults to True. use_depth ( bool , default: False ) \u2013 Whether to use depth as an additional feature. Defaults to False. Methods: Name Description forward x is the output of the transformer, (batch, seq_len, d_model) Source code in scprint2/model/decoders.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __init__ ( self , d_model : int , nfirst_tokens_to_skip : int = 0 , dropout : float = 0.1 , zinb : bool = True , use_depth : bool = False , ): \"\"\" ExprDecoder Decoder for the gene expression prediction. Will output the mean, variance and zero logits, parameters of a zero inflated negative binomial distribution. Args: d_model (int): The dimension of the model. This is the size of the input feature vector. nfirst_tokens_to_skip (int, optional): The number of initial labels to skip in the sequence. Defaults to 0. dropout (float, optional): The dropout rate applied during training to prevent overfitting. Defaults to 0.1. zinb (bool, optional): Whether to use a zero inflated negative binomial distribution. Defaults to True. use_depth (bool, optional): Whether to use depth as an additional feature. Defaults to False. \"\"\" super ( ExprDecoder , self ) . __init__ () self . fc = nn . Sequential ( nn . Linear ( d_model if not use_depth else d_model + 1 , d_model ), nn . LayerNorm ( d_model ), nn . LeakyReLU (), nn . Dropout ( dropout ), nn . Linear ( d_model , d_model ), nn . LayerNorm ( d_model ), nn . LeakyReLU (), ) self . pred_var_zero = nn . Linear ( d_model , 3 if zinb else 1 ) self . zinb = zinb forward x is the output of the transformer, (batch, seq_len, d_model) Source code in scprint2/model/decoders.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def forward ( self , x : Tensor , req_depth : Optional [ Tensor ] = None ) -> Dict [ str , Tensor ]: \"\"\"x is the output of the transformer, (batch, seq_len, d_model)\"\"\" # we don't do it on the labels if req_depth is not None : x = torch . cat ( [ x , req_depth . unsqueeze ( 1 ) . unsqueeze ( - 1 ) . expand ( - 1 , x . shape [ 1 ], - 1 )], dim =- 1 , ) x = self . fc ( x ) if self . zinb : pred_value , var_value , zero_logits = self . pred_var_zero ( x ) . split ( 1 , dim =- 1 ) # (batch, seq_len) # The sigmoid function is used to map the zero_logits to a probability between 0 and 1. return dict ( mean = F . softmax ( pred_value . squeeze ( - 1 ), dim =- 1 ), disp = torch . exp ( torch . clamp ( var_value . squeeze ( - 1 ), max = 15 )), zero_logits = zero_logits . squeeze ( - 1 ), ) else : pred_value = self . pred_var_zero ( x ) return dict ( mean = F . softmax ( pred_value . squeeze ( - 1 ), dim =- 1 )) GraphSDEExprDecoder Bases: Module Initialize the ExprNeuralSDEDecoder module. Parameters: d_model ( int ) \u2013 The dimension of the model. drift ( Module ) \u2013 The drift component of the SDE. diffusion ( Module ) \u2013 The diffusion component of the SDE. Source code in scprint2/model/decoders.py 9 10 11 12 13 14 15 16 17 18 19 20 21 def __init__ ( self , d_model : int , drift : nn . Module , diffusion : nn . Module ): \"\"\" Initialize the ExprNeuralSDEDecoder module. Args: d_model (int): The dimension of the model. drift (nn.Module): The drift component of the SDE. diffusion (nn.Module): The diffusion component of the SDE. \"\"\" super () . __init__ () self . d_model = d_model self . drift = drift self . diffusion = diffusion MVCDecoder Bases: Module MVCDecoder Decoder for masked value prediction of cell embeddings. Uses gene embeddings with cell embeddings to predict mean, variance, and zero logits parameters of a zero-inflated negative binomial distribution. Parameters: d_model ( int ) \u2013 Dimension of the gene embedding. arch_style ( str , default: 'inner product' ) \u2013 Architecture style of the decoder. Options: \"inner product\": Uses inner product between cell and gene embeddings, \"concat query\": Concatenates cell and gene embeddings, \"sum query\": Sums cell and gene embeddings. Defaults to \"inner product\". tot_labels ( int , default: 1 ) \u2013 Total number of labels in the input. Defaults to 1. query_activation ( Module , default: Sigmoid ) \u2013 Activation function for query vectors. Defaults to nn.Sigmoid. hidden_activation ( Module , default: PReLU ) \u2013 Activation function for hidden layers. Defaults to nn.PReLU. use_depth ( bool , default: False ) \u2013 Whether to use depth as an additional feature. Defaults to False. zinb ( bool , default: True ) \u2013 Whether to use a zero-inflated negative binomial distribution. Defaults to True. Methods: Name Description forward Args: Source code in scprint2/model/decoders.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def __init__ ( self , d_model : int , arch_style : str = \"inner product\" , tot_labels : int = 1 , query_activation : nn . Module = nn . Sigmoid , hidden_activation : nn . Module = nn . PReLU , use_depth : bool = False , zinb : bool = True , ) -> None : \"\"\" MVCDecoder Decoder for masked value prediction of cell embeddings. Uses gene embeddings with cell embeddings to predict mean, variance, and zero logits parameters of a zero-inflated negative binomial distribution. Args: d_model (int): Dimension of the gene embedding. arch_style (str, optional): Architecture style of the decoder. Options: \"inner product\": Uses inner product between cell and gene embeddings, \"concat query\": Concatenates cell and gene embeddings, \"sum query\": Sums cell and gene embeddings. Defaults to \"inner product\". tot_labels (int, optional): Total number of labels in the input. Defaults to 1. query_activation (nn.Module, optional): Activation function for query vectors. Defaults to nn.Sigmoid. hidden_activation (nn.Module, optional): Activation function for hidden layers. Defaults to nn.PReLU. use_depth (bool, optional): Whether to use depth as an additional feature. Defaults to False. zinb (bool, optional): Whether to use a zero-inflated negative binomial distribution. Defaults to True. \"\"\" super ( MVCDecoder , self ) . __init__ () if arch_style == \"inner product\" : self . gene2query = nn . Linear ( d_model if not use_depth else d_model + 1 , d_model ) self . norm = nn . LayerNorm ( d_model ) self . query_activation = query_activation () self . pred_var_zero = nn . Linear ( d_model , d_model * ( 3 if zinb else 1 ), bias = False ) elif arch_style == \"concat query\" : self . gene2query = nn . Linear ( d_model if not use_depth else d_model + 1 , d_model ) self . query_activation = query_activation () self . fc1 = nn . Linear ( d_model * ( 1 + tot_labels ), d_model // 2 ) self . hidden_activation = hidden_activation () self . fc2 = nn . Linear ( d_model // 2 , ( 3 if zinb else 1 )) elif arch_style == \"sum query\" : self . gene2query = nn . Linear ( d_model if not use_depth else d_model + 1 , d_model ) self . query_activation = query_activation () self . fc1 = nn . Linear ( d_model , 64 ) self . hidden_activation = hidden_activation () self . fc2 = nn . Linear ( 64 , ( 3 if zinb else 1 )) else : raise ValueError ( f \"Unknown arch_style: { arch_style } \" ) self . arch_style = arch_style self . do_detach = arch_style . endswith ( \"detach\" ) self . d_model = d_model self . zinb = zinb forward Parameters: cell_emb ( Tensor ) \u2013 Tensor, shape (batch, embsize=d_model) gene_embs ( Tensor ) \u2013 Tensor, shape (batch, seq_len, embsize=d_model) req_depth ( Optional [ Tensor ] , default: None ) \u2013 Tensor, shape (batch,), optional depth information. Returns: Union [ Tensor , Dict [ str , Tensor ]] \u2013 Dict[str, Tensor]: A dictionary containing the predicted mean, variance, and zero logits (if zinb is True). Source code in scprint2/model/decoders.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def forward ( self , cell_emb : Tensor , gene_embs : Tensor , req_depth : Optional [ Tensor ] = None , ) -> Union [ Tensor , Dict [ str , Tensor ]]: \"\"\" Args: cell_emb: Tensor, shape (batch, embsize=d_model) gene_embs: Tensor, shape (batch, seq_len, embsize=d_model) req_depth: Tensor, shape (batch,), optional depth information. Returns: Dict[str, Tensor]: A dictionary containing the predicted mean, variance, and zero logits (if zinb is True). \"\"\" if req_depth is not None : gene_embs = torch . cat ( [ gene_embs , req_depth . unsqueeze ( 1 ) . unsqueeze ( - 1 ) . expand ( - 1 , gene_embs . shape [ 1 ], - 1 ), ], dim =- 1 , ) if self . arch_style == \"inner product\" : query_vecs = self . query_activation ( self . norm ( self . gene2query ( gene_embs ))) if self . zinb : pred , var , zero_logits = self . pred_var_zero ( query_vecs ) . split ( self . d_model , dim =- 1 ) else : pred = self . pred_var_zero ( query_vecs ) cell_emb = cell_emb . unsqueeze ( 2 ) if self . zinb : pred , var , zero_logits = ( torch . bmm ( pred , cell_emb ) . squeeze ( 2 ), torch . bmm ( var , cell_emb ) . squeeze ( 2 ), torch . bmm ( zero_logits , cell_emb ) . squeeze ( 2 ), ) else : pred = torch . bmm ( pred , cell_emb ) . squeeze ( 2 ) # zero logits need to based on the cell_emb, because of input exprs elif self . arch_style == \"concat query\" : query_vecs = self . query_activation ( self . gene2query ( gene_embs )) # expand cell_emb to (batch, seq_len, embsize) cell_emb = cell_emb . unsqueeze ( 1 ) . expand ( - 1 , gene_embs . shape [ 1 ], - 1 ) h = self . hidden_activation ( self . fc1 ( torch . cat ([ cell_emb , query_vecs ], dim = 2 )) ) if self . zinb : pred , var , zero_logits = self . fc2 ( h ) . split ( 1 , dim =- 1 ) else : pred = self . fc2 ( h ) elif self . arch_style == \"sum query\" : query_vecs = self . query_activation ( self . gene2query ( gene_embs )) cell_emb = cell_emb . unsqueeze ( 1 ) h = self . hidden_activation ( self . fc1 ( cell_emb + query_vecs )) if self . zinb : pred , var , zero_logits = self . fc2 ( h ) . split ( 1 , dim =- 1 ) else : pred = self . fc2 ( h ) if self . zinb : return dict ( mvc_mean = F . softmax ( pred , dim =- 1 ), mvc_disp = torch . exp ( torch . clamp ( var , max = 15 )), mvc_zero_logits = zero_logits , ) else : return dict ( mvc_mean = F . softmax ( pred , dim =- 1 )) VAEDecoder Bases: Module VAEDecoder for variational autoencoding of cell embeddings. Parameters: d_model ( int ) \u2013 Input dimension (original embedding size) layers ( List [ int ] , default: [64, 64] ) \u2013 List of hidden layer sizes for encoder and decoder activation ( Callable , default: ReLU ) \u2013 Activation function to use dropout ( float , default: 0.1 ) \u2013 Dropout rate return_latent ( bool , default: False ) \u2013 Whether to return the latent vectors Methods: Name Description forward Forward pass through VAE. kl_divergence Compute KL divergence between N(mu, var) and N(0, 1). reparameterize Reparameterization trick to sample from N(mu, var) from N(0,1). Source code in scprint2/model/decoders.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 def __init__ ( self , d_model : int , layers : List [ int ] = [ 64 , 64 ], activation : Callable = nn . ReLU , dropout : float = 0.1 , return_latent : bool = False , ): \"\"\" VAEDecoder for variational autoencoding of cell embeddings. Args: d_model (int): Input dimension (original embedding size) layers (List[int]): List of hidden layer sizes for encoder and decoder activation (Callable): Activation function to use dropout (float): Dropout rate return_latent (bool): Whether to return the latent vectors \"\"\" super ( VAEDecoder , self ) . __init__ () # Encoder layers self . return_latent = return_latent encoder_layers = [ d_model ] + layers self . encoder = nn . Sequential () for i , ( in_size , out_size ) in enumerate ( zip ( encoder_layers [: - 1 ], encoder_layers [ 1 :]) ): self . encoder . append ( nn . Linear ( in_size , out_size )) self . encoder . append ( nn . LayerNorm ( out_size )) self . encoder . append ( activation ()) self . encoder . append ( nn . Dropout ( dropout )) # VAE latent parameters self . fc_mu = nn . Linear ( encoder_layers [ - 1 ], encoder_layers [ - 1 ]) self . fc_var = nn . Linear ( encoder_layers [ - 1 ], encoder_layers [ - 1 ]) # Decoder layers decoder_layers = [ encoder_layers [ - 1 ]] + list ( reversed ( layers [: - 1 ])) + [ d_model ] self . decoder = nn . Sequential () for i , ( in_size , out_size ) in enumerate ( zip ( decoder_layers [: - 1 ], decoder_layers [ 1 :] ) # Changed to include final layer ): self . decoder . append ( nn . Linear ( in_size , out_size )) if ( i < len ( decoder_layers ) - 2 ): # Don't apply activation/norm to final layer self . decoder . append ( nn . LayerNorm ( out_size )) self . decoder . append ( activation ()) self . decoder . append ( nn . Dropout ( dropout )) forward Forward pass through VAE. Parameters: x ( Tensor ) \u2013 Input tensor of shape [batch_size, d_model] Returns: Union [ Tensor , Tuple [ Tensor , Tensor , Tensor , Tensor , Tensor ]] \u2013 If self.return_latent is True: Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]: - reconstructed_x (Tensor): Reconstructed input, shape [batch_size, d_model] - mu (Tensor): Mean of the latent Gaussian, shape [batch_size, latent_dim] - log_var (Tensor): Log variance of the latent Gaussian, shape [batch_size, latent_dim] - kl_loss (Tensor): KL divergence loss (scalar tensor) Else ( Union [ Tensor , Tuple [ Tensor , Tensor , Tensor , Tensor , Tensor ]] ) \u2013 Tensor: reconstructed_x of shape [batch_size, d_model] Source code in scprint2/model/decoders.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 def forward ( self , x : Tensor ) -> Union [ Tensor , Tuple [ Tensor , Tensor , Tensor , Tensor , Tensor ]]: \"\"\" Forward pass through VAE. Args: x (Tensor): Input tensor of shape [batch_size, d_model] Returns: If self.return_latent is True: Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]: - reconstructed_x (Tensor): Reconstructed input, shape [batch_size, d_model] - mu (Tensor): Mean of the latent Gaussian, shape [batch_size, latent_dim] - log_var (Tensor): Log variance of the latent Gaussian, shape [batch_size, latent_dim] - kl_loss (Tensor): KL divergence loss (scalar tensor) Else: Tensor: reconstructed_x of shape [batch_size, d_model] \"\"\" # Encode encoded = self . encoder ( x ) # Get latent parameters mu = self . fc_mu ( encoded ) log_var = self . fc_var ( encoded ) log_var = torch . clamp ( log_var , min =- 10 ) # Sample latent vector kl_loss = self . kl_divergence ( mu , log_var ) # free_bits = 2.0 # per latent dim # kl_loss = torch.clamp(kl_loss / mu.size(-1), min=free_bits) * mu.size(-1) z = self . reparameterize ( mu , log_var ) # Decode decoded = self . decoder ( z ) if self . return_latent : return decoded , mu , log_var , encoded , kl_loss return decoded , kl_loss kl_divergence Compute KL divergence between N(mu, var) and N(0, 1). Parameters: mu ( Tensor ) \u2013 Mean of the latent Gaussian log_var ( Tensor ) \u2013 Log variance of the latent Gaussian Returns: Tensor ( Tensor ) \u2013 KL divergence loss Source code in scprint2/model/decoders.py 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def kl_divergence ( self , mu : Tensor , log_var : Tensor ) -> Tensor : \"\"\" Compute KL divergence between N(mu, var) and N(0, 1). Args: mu (Tensor): Mean of the latent Gaussian log_var (Tensor): Log variance of the latent Gaussian Returns: Tensor: KL divergence loss \"\"\" # KL(N(mu, var) || N(0, 1)) = -0.5 * sum(1 + log(var) - mu^2 - var) kl_loss = - 0.5 * torch . sum ( 1 + log_var - mu . pow ( 2 ) - log_var . exp (), dim = 1 ) return kl_loss . mean () reparameterize Reparameterization trick to sample from N(mu, var) from N(0,1). Parameters: mu ( Tensor ) \u2013 Mean of the latent Gaussian log_var ( Tensor ) \u2013 Log variance of the latent Gaussian Returns: Tensor ( Tensor ) \u2013 Sampled latent vector Source code in scprint2/model/decoders.py 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def reparameterize ( self , mu : Tensor , log_var : Tensor ) -> Tensor : \"\"\" Reparameterization trick to sample from N(mu, var) from N(0,1). Args: mu (Tensor): Mean of the latent Gaussian log_var (Tensor): Log variance of the latent Gaussian Returns: Tensor: Sampled latent vector \"\"\" std = torch . exp ( 0.5 * log_var ) eps = torch . randn_like ( std ) return mu + eps * std scprint2.model.fsq Finite Scalar Quantization: VQ-VAE Made Simple - https://arxiv.org/abs/2309.15505 Code adapted from Jax version in Appendix A.1 Classes: Name Description FSQ Functions: Name Description round_ste Round with straight through gradients. FSQ Bases: Module Methods: Name Description bound Bound z , an array of shape (..., d). codes_to_indices Converts a code to an index in the codebook. forward einstein notation indices_to_codes Inverse of codes_to_indices . quantize Quantizes z, returns quantized zhat, same shape as z. Source code in scprint2/model/fsq.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __init__ ( self , levels : List [ int ], dim : Optional [ int ] = None , num_codebooks = 1 , keep_num_codebooks_dim : Optional [ bool ] = None , scale : Optional [ float ] = None , ): super () . __init__ () _levels = torch . tensor ( levels , dtype = int32 ) self . register_buffer ( \"_levels\" , _levels , persistent = False ) _basis = torch . cumprod ( torch . tensor ([ 1 ] + levels [: - 1 ]), dim = 0 , dtype = int32 ) self . register_buffer ( \"_basis\" , _basis , persistent = False ) self . scale = scale codebook_dim = len ( levels ) self . codebook_dim = codebook_dim effective_codebook_dim = codebook_dim * num_codebooks self . num_codebooks = num_codebooks self . effective_codebook_dim = effective_codebook_dim keep_num_codebooks_dim = default ( keep_num_codebooks_dim , num_codebooks > 1 ) assert not ( num_codebooks > 1 and not keep_num_codebooks_dim ) self . keep_num_codebooks_dim = keep_num_codebooks_dim self . dim = default ( dim , len ( _levels ) * num_codebooks ) has_projections = self . dim != effective_codebook_dim self . project_in = ( nn . Linear ( self . dim , effective_codebook_dim ) if has_projections else nn . Identity () ) self . project_out = ( nn . Linear ( effective_codebook_dim , self . dim ) if has_projections else nn . Identity () ) self . has_projections = has_projections self . codebook_size = self . _levels . prod () . item () implicit_codebook = self . indices_to_codes ( torch . arange ( self . codebook_size ), project_out = False ) self . register_buffer ( \"implicit_codebook\" , implicit_codebook , persistent = False ) bound Bound z , an array of shape (..., d). Source code in scprint2/model/fsq.py 99 100 101 102 103 104 def bound ( self , z : Tensor , eps : float = 1e-3 ) -> Tensor : \"\"\"Bound `z`, an array of shape (..., d).\"\"\" half_l = ( self . _levels - 1 ) * ( 1 - eps ) / 2 offset = torch . where ( self . _levels % 2 == 0 , 0.5 , 0.0 ) shift = ( offset / half_l ) . tan () return ( z + shift ) . tanh () * half_l - offset codes_to_indices Converts a code to an index in the codebook. Source code in scprint2/model/fsq.py 120 121 122 123 124 def codes_to_indices ( self , zhat : Tensor ) -> Tensor : \"\"\"Converts a `code` to an index in the codebook.\"\"\" assert zhat . shape [ - 1 ] == self . codebook_dim zhat = self . _scale_and_shift ( zhat ) return ( zhat * self . _basis ) . sum ( dim =- 1 ) . to ( int32 ) forward einstein notation b - batch n - sequence (or flattened spatial dimensions) d - feature dimension, which is also log2(codebook size) c - number of codebook dim Source code in scprint2/model/fsq.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def forward ( self , z : Tensor ) -> Tensor : \"\"\" einstein notation b - batch n - sequence (or flattened spatial dimensions) d - feature dimension, which is also log2(codebook size) c - number of codebook dim \"\"\" assert z . shape [ - 1 ] == self . dim , ( f \"expected dimension of { self . dim } but found dimension of { z . shape [ - 1 ] } \" ) small = self . project_in ( z ) z = rearrange ( small , \"b (c d) -> b c d\" , c = self . num_codebooks ) codes = self . quantize ( z ) indices = self . codes_to_indices ( codes ) codes = rearrange ( codes , \"b c d -> b (c d)\" ) out = self . project_out ( codes ) if not self . keep_num_codebooks_dim : indices = rearrange ( indices , \"... 1 -> ...\" ) return out , indices , small indices_to_codes Inverse of codes_to_indices . Source code in scprint2/model/fsq.py 126 127 128 129 130 131 132 133 134 135 136 137 def indices_to_codes ( self , indices : Tensor , project_out = True ) -> Tensor : \"\"\"Inverse of `codes_to_indices`.\"\"\" indices = rearrange ( indices , \"... -> ... 1\" ) codes_non_centered = ( indices // self . _basis ) % self . _levels codes = self . _scale_and_shift_inverse ( codes_non_centered ) if self . keep_num_codebooks_dim : codes = rearrange ( codes , \"... c d -> ... (c d)\" ) if project_out : codes = self . project_out ( codes ) return codes quantize Quantizes z, returns quantized zhat, same shape as z. Source code in scprint2/model/fsq.py 106 107 108 109 110 def quantize ( self , z : Tensor ) -> Tensor : \"\"\"Quantizes z, returns quantized zhat, same shape as z.\"\"\" quantized = round_ste ( self . bound ( z )) half_width = self . _levels // 2 # Renormalize to [-1, 1]. return quantized / half_width round_ste Round with straight through gradients. Source code in scprint2/model/fsq.py 39 40 41 42 def round_ste ( z : Tensor ) -> Tensor : \"\"\"Round with straight through gradients.\"\"\" zhat = z . round () return z + ( zhat - z ) . detach ()","title":"model"},{"location":"model/#documentation-for-the-model","text":"","title":"Documentation for the model"},{"location":"model/#model-description","text":"","title":"model description"},{"location":"model/#scprint2.model.model","text":"Classes: Name Description scPRINT2","title":"model"},{"location":"model/#scprint2.model.model.scPRINT2","text":"Bases: LightningModule , PyTorchModelHubMixin scPRINT-2: Single-Cell Pretrained Regulatory Inference Network Transformer. A foundation model for single-cell biology that learns cell and gene representations through self-supervised learning on large-scale single-cell RNA-seq data. The model can be used for: - Cell type classification and annotation - Gene expression denoising and imputation - Cell embedding generation for downstream analysis - Gene regulatory network inference via attention patterns - Multi-species gene expression modeling Architecture Overview Gene Encoder: Embeds gene identities (optionally with pretrained embeddings) Expression Encoder: Encodes expression values (continuous, binned, or metacell) Position Encoder: Optional genomic position encoding Transformer: Main attention-based encoder (various attention mechanisms) Cell Transformer: Optional separate transformer for cell embeddings Decoders: Expression reconstruction, classification, and MVC decoders The model supports multiple training objectives Masked expression prediction (like BERT) Denoising autoencoding Cell embedding contrastive learning (ECS and CCE losses) Multi-class cell type classification with hierarchical labels Multi-view coding (MVC) for robust representations Parameters: genes ( list | dict ) \u2013 Gene vocabulary. Either a list of gene names or a dict mapping organism names to lists of genes for multi-species models. organisms ( list [ str ] ) \u2013 List of organism ontology term IDs the model supports. d_model ( int , default: 256 ) \u2013 Hidden dimension of the transformer. Defaults to 256. nhead ( int , default: 4 ) \u2013 Number of attention heads. Defaults to 4. nlayers ( int , default: 8 ) \u2013 Number of transformer layers. Defaults to 8. precpt_gene_emb ( str , default: None ) \u2013 Path to parquet file with pretrained gene embeddings. Index should match gene names. Defaults to None. memmap_gene_emb ( bool , default: False ) \u2013 Memory-map gene embeddings for large files. Defaults to False. finetune_gene_emb ( bool , default: False ) \u2013 Add trainable adapter layers on top of frozen pretrained embeddings. Defaults to False. freeze_embeddings ( bool , default: True ) \u2013 Freeze gene embeddings during training. Defaults to True. gene_pos_file ( str , default: None ) \u2013 Path to parquet file with genomic positions. Must have 'pos' column with integer positions. Defaults to None. normalization ( str , default: 'sum' ) \u2013 Expression normalization method. One of: - \"sum\": Divide by total counts (TPM-like) - \"log\": Log2(1 + x) transform - \"both\": Sum normalization then log transform - \"raw\": No normalization Defaults to \"sum\". attn_bias ( str , default: None ) \u2013 Path to sparse matrix (.npz) with attention biases (e.g., gene-gene regulatory priors). Defaults to None. expr_encoder_layers ( int , default: 3 ) \u2013 Number of layers in expression encoder MLP. Defaults to 3. attention ( str , default: 'normal' ) \u2013 Attention mechanism type. One of: - \"normal\": Standard PyTorch attention - \"legacy-flash\": Flash attention via simpler-flash - \"performer\": Performer linear attention - \"hyper\": Compressed hyperbolic attention - \"criss-cross\": Criss-cross attention Defaults to \"normal\". expr_emb_style ( str , default: 'continuous' ) \u2013 Expression embedding approach. One of: - \"continuous\": MLP on continuous expression values - \"binned\": Learned embeddings for discretized expression bins - \"metacell\": DeepSet encoder aggregating KNN neighbors Defaults to \"continuous\". n_input_bins ( int , default: 0 ) \u2013 Number of expression bins when using binned embedding. Required if expr_emb_style=\"binned\". Defaults to 0. mvc_decoder ( str , default: None ) \u2013 Multi-view coding decoder architecture. One of: - None: No MVC decoder - \"inner product\": Dot product between cell and gene embeddings - \"concat query\": Concatenate cell embedding with gene queries - \"sum query\": Add cell embedding to gene queries Defaults to None. pred_embedding ( list [ str ] , default: None ) \u2013 Class names to use for cell embeddings during prediction/logging. Defaults to None (use all). layers_cls ( list [ int ] , default: [256, 128] ) \u2013 Hidden layer sizes for classification heads. Defaults to [256, 128]. classes ( dict [ str , int ] , default: None ) \u2013 Classification targets mapping class names to number of categories. E.g., {\"cell_type_ontology_term_id\": 100}. Defaults to None. labels_hierarchy ( dict [ str , dict [ int , list [ int ]]] , default: {} ) \u2013 Hierarchical label structure for ontology-based classes. Maps parent indices to lists of children indices. Defaults to {}. label_decoders ( dict [ str , dict [ int , str ]] , default: None ) \u2013 Mapping from encoded integers back to label strings for each class. Used for logging/plotting. Defaults to None. compress_class_dim ( dict [ str , int ] , default: None ) \u2013 Compressed embedding dimension for each class. Uses VAE or FSQ compression. Defaults to None. cell_specific_blocks ( bool , default: False ) \u2013 Use separate transformer for cell embeddings with cross-attention to gene transformer. Defaults to False. zinb ( bool , default: True ) \u2013 Use Zero-Inflated Negative Binomial distribution for expression reconstruction. If False, uses MSE loss. Defaults to True. splicing_head ( bool , default: False ) \u2013 Add separate decoder for spliced/unspliced expression. Defaults to False. do_adv_cls ( bool , default: False ) \u2013 Use adversarial classification to remove batch effects from cell type embeddings. Defaults to False. dropout ( float , default: 0.1 ) \u2013 Dropout rate throughout the model. Defaults to 0.1. use_metacell_token ( bool , default: False ) \u2013 Add learnable metacell token to distinguish single cells from metacells. Defaults to False. lr ( float , default: 0.0001 ) \u2013 Base learning rate. Defaults to 0.0001. nb_features ( int , default: None ) \u2013 Number of random features for Performer attention. Defaults to None. sketcher_size ( int , default: 200 ) \u2013 Sketch size for sparse attention methods. Defaults to 200. feature_redraw_interval ( int , default: None ) \u2013 Steps between random feature redraws for Performer. Defaults to None. num_heads_kv ( int , default: 4 ) \u2013 Number of key-value heads (for MQA/GQA). Defaults to 4. d_model_cell ( int , default: 128 ) \u2013 Hidden dim for cell transformer when using cell_specific_blocks. Defaults to 128. nhead_cell ( int , default: 4 ) \u2013 Attention heads for cell transformer. Defaults to 4. nlayers_cell ( int , default: 6 ) \u2013 Layers in cell transformer. Defaults to 6. num_heads_kv_cell ( int , default: 4 ) \u2013 KV heads for cell transformer. Defaults to 4. drop_path_rate ( float , default: 0.0 ) \u2013 Stochastic depth rate. Defaults to 0.0. **attention_kwargs ( dict , default: {} ) \u2013 Additional arguments passed to FlashTransformer. Attributes: Training ( Configuration (set these before training ) \u2013 noise (list[float]): Dropout rates for denoising task. E.g., [0.6]. mask_ratio (list[float]): Mask ratios for masked prediction. E.g., [0.15]. cce_temp (float): Temperature for contrastive loss. cce_scale (float): Weight for contrastive cell embedding loss. ecs_scale (float): Weight for elastic cell similarity loss. ecs_threshold (float): Similarity threshold for ECS loss. mvc_scale (float): Weight for MVC reconstruction loss. class_scale (float): Weight for classification loss. lr_reduce_patience (int): Epochs before reducing learning rate. lr_reduce_factor (float): Factor to reduce learning rate by. warmup_duration (int): Steps for learning rate warmup. Prediction ( Configuration (set before predict ) \u2013 predict_mode (str): \"none\" or \"generate\" for expression generation. pred_embedding (list[str]): Classes to include in cell embeddings. get_attention_layer (list[int]): Layers to extract attention from. predict_depth_mult (float): Multiplier for depth in generation. pred_log_adata (bool): Whether to log predictions as AnnData. Example","title":"scPRINT2"},{"location":"model/#scprint2.model.model.scPRINT2--initialize-model","text":"model = scPrint2( ... genes=gene_list, ... organisms=[\"NCBITaxon:9606\"], ... d_model=512, ... nlayers=12, ... classes={\"cell_type_ontology_term_id\": 100}, ... )","title":"Initialize model"},{"location":"model/#scprint2.model.model.scPRINT2--configure-training","text":"model.noise = [0.4, 0.6] model.mask_ratio = [0.15, 0.3]","title":"Configure training"},{"location":"model/#scprint2.model.model.scPRINT2--train-with-pytorch-lightning","text":"trainer = L.Trainer(max_epochs=100) trainer.fit(model, datamodule)","title":"Train with PyTorch Lightning"},{"location":"model/#scprint2.model.model.scPRINT2--generate-embeddings","text":"model.pred_embedding = [\"cell_type_ontology_term_id\"] predictions = trainer.predict(model, datamodule) Note The model is designed to work with scDataLoader's DataModule and Collator. Gene order must match between model initialization and data loading. Methods: Name Description add_organism Add a new organism to an existing model for transfer learning. configure_optimizers @see pl.LightningModule forward Complete forward pass through the scPRINT-2 log_adata log_adata will log an adata from predictions. on_fit_start @see pl.LightningModule on_load_checkpoint Handle checkpoint loading with backward compatibility. on_predict_epoch_end @see pl.LightningModule will on_predict_epoch_start @see pl.LightningModule on_test_start @see pl.LightningModule on_validation_epoch_end @see pl.LightningModule optimizer_step @see pl.LightningModule predict_step embed given gene expression, encode the gene embedding and cell embedding. training_step training_step defines the train loop. It is independent of forward validation_step validation_step defines the validation loop. It is independent of forward Attributes: genes ( list [ str ] ) \u2013 Get flattened list of all genes in the model's vocabulary. Source code in scprint2/model/model.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 def __init__ ( self , genes , organisms : list [ str ], d_model : int = 256 , nhead : int = 4 , nlayers : int = 8 , precpt_gene_emb : Optional [ str ] = None , memmap_gene_emb : bool = False , finetune_gene_emb : bool = False , freeze_embeddings : bool = True , gene_pos_file : Optional [ str ] = None , normalization : str = \"sum\" , # log, sum, both, raw attn_bias : Optional [ str ] = None , expr_encoder_layers : int = 3 , attention : str = \"normal\" , # \"performer\", \"legacy-flash\", \"normal\", \"criss-cross\", \"hyper\", \"adasplash\", \"softpick\", \"softpick-flash\" expr_emb_style : str = \"continuous\" , # \"binned\", \"continuous\", \"metacell\" n_input_bins : int = 0 , mvc_decoder : Optional [ str ] = None , # \"inner product\", \"concat query\", \"sum query\" pred_embedding : Optional [ list [ str ]] = None , layers_cls : list [ int ] = [ 256 , 128 ], classes : Optional [ Dict [ str , int ]] = None , labels_hierarchy : Dict [ str , Dict [ int , list [ int ]]] = {}, label_decoders : Optional [ Dict [ str , Dict [ int , str ]]] = None , compress_class_dim : Optional [ Dict [ str , int ]] = None , cell_specific_blocks : bool = False , zinb : bool = True , splicing_head : bool = False , do_adv_cls : bool = False , dropout : float = 0.1 , use_metacell_token : bool = False , lr : float = 0.0001 , nb_features : Optional [ int ] = None , sketcher_size : int = 200 , feature_redraw_interval : Optional [ int ] = None , num_heads_kv : int = 4 , d_model_cell : int = 128 , nhead_cell : int = 4 , nlayers_cell : int = 6 , num_heads_kv_cell : int = 4 , transformer = None , drop_path_rate : float = 0.0 , # unused args from older versions kept for loading old models gene_pos_enc = None , max_cont_len = None , residual_in_fp32 = None , checkpointing = None , fused_dropout_add_ln = None , strict_loading = None , optim = None , weight_decay = None , prenorm = None , domain_spec_batchnorm = None , use_flash_attn = None , cell_emb_style = None , num_batch_labels = None , fused_mlp = None , fused_bias_fc = None , ** attention_kwargs : dict , ): \"\"\" scPRINT-2: Single-Cell Pretrained Regulatory Inference Network Transformer. A foundation model for single-cell biology that learns cell and gene representations through self-supervised learning on large-scale single-cell RNA-seq data. The model can be used for: - Cell type classification and annotation - Gene expression denoising and imputation - Cell embedding generation for downstream analysis - Gene regulatory network inference via attention patterns - Multi-species gene expression modeling Architecture Overview: 1. Gene Encoder: Embeds gene identities (optionally with pretrained embeddings) 2. Expression Encoder: Encodes expression values (continuous, binned, or metacell) 3. Position Encoder: Optional genomic position encoding 4. Transformer: Main attention-based encoder (various attention mechanisms) 5. Cell Transformer: Optional separate transformer for cell embeddings 6. Decoders: Expression reconstruction, classification, and MVC decoders The model supports multiple training objectives: - Masked expression prediction (like BERT) - Denoising autoencoding - Cell embedding contrastive learning (ECS and CCE losses) - Multi-class cell type classification with hierarchical labels - Multi-view coding (MVC) for robust representations Args: genes (list | dict): Gene vocabulary. Either a list of gene names or a dict mapping organism names to lists of genes for multi-species models. organisms (list[str]): List of organism ontology term IDs the model supports. d_model (int, optional): Hidden dimension of the transformer. Defaults to 256. nhead (int, optional): Number of attention heads. Defaults to 4. nlayers (int, optional): Number of transformer layers. Defaults to 8. precpt_gene_emb (str, optional): Path to parquet file with pretrained gene embeddings. Index should match gene names. Defaults to None. memmap_gene_emb (bool, optional): Memory-map gene embeddings for large files. Defaults to False. finetune_gene_emb (bool, optional): Add trainable adapter layers on top of frozen pretrained embeddings. Defaults to False. freeze_embeddings (bool, optional): Freeze gene embeddings during training. Defaults to True. gene_pos_file (str, optional): Path to parquet file with genomic positions. Must have 'pos' column with integer positions. Defaults to None. normalization (str, optional): Expression normalization method. One of: - \"sum\": Divide by total counts (TPM-like) - \"log\": Log2(1 + x) transform - \"both\": Sum normalization then log transform - \"raw\": No normalization Defaults to \"sum\". attn_bias (str, optional): Path to sparse matrix (.npz) with attention biases (e.g., gene-gene regulatory priors). Defaults to None. expr_encoder_layers (int, optional): Number of layers in expression encoder MLP. Defaults to 3. attention (str, optional): Attention mechanism type. One of: - \"normal\": Standard PyTorch attention - \"legacy-flash\": Flash attention via simpler-flash - \"performer\": Performer linear attention - \"hyper\": Compressed hyperbolic attention - \"criss-cross\": Criss-cross attention Defaults to \"normal\". expr_emb_style (str, optional): Expression embedding approach. One of: - \"continuous\": MLP on continuous expression values - \"binned\": Learned embeddings for discretized expression bins - \"metacell\": DeepSet encoder aggregating KNN neighbors Defaults to \"continuous\". n_input_bins (int, optional): Number of expression bins when using binned embedding. Required if expr_emb_style=\"binned\". Defaults to 0. mvc_decoder (str, optional): Multi-view coding decoder architecture. One of: - None: No MVC decoder - \"inner product\": Dot product between cell and gene embeddings - \"concat query\": Concatenate cell embedding with gene queries - \"sum query\": Add cell embedding to gene queries Defaults to None. pred_embedding (list[str], optional): Class names to use for cell embeddings during prediction/logging. Defaults to None (use all). layers_cls (list[int], optional): Hidden layer sizes for classification heads. Defaults to [256, 128]. classes (dict[str, int], optional): Classification targets mapping class names to number of categories. E.g., {\"cell_type_ontology_term_id\": 100}. Defaults to None. labels_hierarchy (dict[str, dict[int, list[int]]], optional): Hierarchical label structure for ontology-based classes. Maps parent indices to lists of children indices. Defaults to {}. label_decoders (dict[str, dict[int, str]], optional): Mapping from encoded integers back to label strings for each class. Used for logging/plotting. Defaults to None. compress_class_dim (dict[str, int], optional): Compressed embedding dimension for each class. Uses VAE or FSQ compression. Defaults to None. cell_specific_blocks (bool, optional): Use separate transformer for cell embeddings with cross-attention to gene transformer. Defaults to False. zinb (bool, optional): Use Zero-Inflated Negative Binomial distribution for expression reconstruction. If False, uses MSE loss. Defaults to True. splicing_head (bool, optional): Add separate decoder for spliced/unspliced expression. Defaults to False. do_adv_cls (bool, optional): Use adversarial classification to remove batch effects from cell type embeddings. Defaults to False. dropout (float, optional): Dropout rate throughout the model. Defaults to 0.1. use_metacell_token (bool, optional): Add learnable metacell token to distinguish single cells from metacells. Defaults to False. lr (float, optional): Base learning rate. Defaults to 0.0001. nb_features (int, optional): Number of random features for Performer attention. Defaults to None. sketcher_size (int, optional): Sketch size for sparse attention methods. Defaults to 200. feature_redraw_interval (int, optional): Steps between random feature redraws for Performer. Defaults to None. num_heads_kv (int, optional): Number of key-value heads (for MQA/GQA). Defaults to 4. d_model_cell (int, optional): Hidden dim for cell transformer when using cell_specific_blocks. Defaults to 128. nhead_cell (int, optional): Attention heads for cell transformer. Defaults to 4. nlayers_cell (int, optional): Layers in cell transformer. Defaults to 6. num_heads_kv_cell (int, optional): KV heads for cell transformer. Defaults to 4. drop_path_rate (float, optional): Stochastic depth rate. Defaults to 0.0. **attention_kwargs (dict): Additional arguments passed to FlashTransformer. Attributes: Training Configuration (set these before training): noise (list[float]): Dropout rates for denoising task. E.g., [0.6]. mask_ratio (list[float]): Mask ratios for masked prediction. E.g., [0.15]. cce_temp (float): Temperature for contrastive loss. cce_scale (float): Weight for contrastive cell embedding loss. ecs_scale (float): Weight for elastic cell similarity loss. ecs_threshold (float): Similarity threshold for ECS loss. mvc_scale (float): Weight for MVC reconstruction loss. class_scale (float): Weight for classification loss. lr_reduce_patience (int): Epochs before reducing learning rate. lr_reduce_factor (float): Factor to reduce learning rate by. warmup_duration (int): Steps for learning rate warmup. Prediction Configuration (set before predict): predict_mode (str): \"none\" or \"generate\" for expression generation. pred_embedding (list[str]): Classes to include in cell embeddings. get_attention_layer (list[int]): Layers to extract attention from. predict_depth_mult (float): Multiplier for depth in generation. pred_log_adata (bool): Whether to log predictions as AnnData. Example: >>> # Initialize model >>> model = scPrint2( ... genes=gene_list, ... organisms=[\"NCBITaxon:9606\"], ... d_model=512, ... nlayers=12, ... classes={\"cell_type_ontology_term_id\": 100}, ... ) >>> >>> # Configure training >>> model.noise = [0.4, 0.6] >>> model.mask_ratio = [0.15, 0.3] >>> >>> # Train with PyTorch Lightning >>> trainer = L.Trainer(max_epochs=100) >>> trainer.fit(model, datamodule) >>> >>> # Generate embeddings >>> model.pred_embedding = [\"cell_type_ontology_term_id\"] >>> predictions = trainer.predict(model, datamodule) Note: The model is designed to work with scDataLoader's DataModule and Collator. Gene order must match between model initialization and data loading. \"\"\" super () . __init__ () self . save_hyperparameters () # training flags self . noise = [ 0.6 ] self . cce_temp = 0.3 self . lr = lr self . cce_scale = 0.2 self . ecs_threshold = 0.4 self . ecs_scale = 0.2 self . mvc_scale = 1.0 self . class_embd_diss_scale = 0.3 self . adv_class_scale = 1.0 self . do_adv_cls = do_adv_cls self . run_full_forward = True self . class_scale = 1 self . zinb_and_mse = False self . do_next_tp = False self . do_generate = False self . var_context_length = False self . mask_ratio = [] self . warmup_duration = 500 self . weight_decay = 0.01 self . optim = \"adamW\" self . fused_adam = False self . lr_reduce_patience = 2 self . lr_reduce_factor = 0.6 self . test_every = 20 self . randsamp = True self . lr_reduce_monitor = \"val_loss\" self . name = \"\" self . set_step = None self . lrfinder_steps = 0 self . doplot = False self . get_attention_layer = None self . embs = None self . pred_log_adata = True self . predict_depth_mult = 3 self . predict_mode = \"none\" self . keep_all_labels_pred = False self . mask_zeros = False self . vae_kl_scale = 0.05 self . vae_kl_warmup_steps = 40_000 # Default value, can be adjusted self . save_expr = False self . counter = 0 # should be stored somehow self . d_model = d_model self . normalization = normalization self . attn_bias = attn_bias if attn_bias != \"none\" else None self . organisms = organisms self . nlayers = nlayers self . use_metacell_token = use_metacell_token self . mvc_decoder = mvc_decoder # need to store self . n_input_bins = n_input_bins self . attention = attention if classes is None : classes = {} self . label_counts = classes self . classes = list ( classes . keys ()) self . label_decoders = label_decoders self . pred_embedding = pred_embedding self . _genes = genes self . expr_emb_style = expr_emb_style if labels_hierarchy is None : labels_hierarchy = {} self . labels_hierarchy = labels_hierarchy self . hparams [ \"classes\" ] = classes self . hparams [ \"label_decoders\" ] = label_decoders self . hparams [ \"organisms\" ] = organisms self . hparams [ \"use_metacell_token\" ] = use_metacell_token # 20x more likely to drop a non TF compared to a TF self . tf_masker = WeightedMasker ( self . genes , tf_weight = 0.05 ) self . attn = utils . Attention ( len ( self . genes ), additional_tokens = ( ( 1 if self . use_metacell_token else 0 ) + (( len ( classes ) + 1 ) if not cell_specific_blocks else 0 ) ), ) self . mat_labels_hierarchy = {} for k , v in labels_hierarchy . items (): tens = torch . zeros (( len ( v ), classes [ k ])) for k2 , v2 in v . items (): tens [ k2 - classes [ k ], v2 ] = 1 self . mat_labels_hierarchy [ k ] = tens . to ( bool ) # encoder # gene encoder if gene_pos_file is not None : gene_pos_enc = pd . read_parquet ( gene_pos_file ) if len ( gene_pos_enc ) < len ( self . genes ): print ( \"Warning: only a subset of the genes available in the loc file.\" ) for k , v in self . _genes . items (): tokeep = set ( gene_pos_enc . index . tolist ()) self . _genes [ k ] = [ u for u in v if u in tokeep ] if len ( self . _genes [ k ]) < 100 : raise ValueError ( f \"the gene pos file { gene_pos_file } does not match most of the genes given to the model for species { k } \" ) gene_pos_enc = gene_pos_enc . loc [ self . genes , [ \"pos\" ]] if precpt_gene_emb is not None : embeddings = pd . read_parquet ( precpt_gene_emb ) if len ( embeddings ) < len ( self . genes ): print ( \"Warning: only a subset of the genes available in the embeddings file.\" ) for k , v in self . _genes . items (): tokeep = set ( embeddings . index . tolist ()) self . _genes [ k ] = [ u for u in v if u in tokeep ] if len ( self . _genes [ k ]) < 100 : raise ValueError ( f \"the gene embeddings file { precpt_gene_emb } does not match most of the genes given to the model for species { k } \" ) embeddings = embeddings . loc [ self . genes ] print ( \"number of genes: \" , len ( embeddings )) if not memmap_gene_emb : sembeddings = torch . nn . AdaptiveAvgPool1d ( d_model )( torch . tensor ( embeddings . values , dtype = torch . float32 ) ) else : embeddings = None gene_encoder = encoders . GeneEncoder ( len ( self . genes ), d_model , weights_file = precpt_gene_emb if memmap_gene_emb else None , weights = sembeddings if not memmap_gene_emb else None , freeze = freeze_embeddings , ) else : gene_encoder = encoders . GeneEncoder ( len ( self . genes ), d_model , freeze = freeze_embeddings ) if finetune_gene_emb : if not freeze_embeddings : raise ValueError ( \"finetune_gene_emb is True but freeze_embeddings is False\" ) # Create adapter layers after the frozen base encoder self . gene_encoder = torch . nn . Sequential ( gene_encoder , torch . nn . Linear ( d_model , d_model ), torch . nn . ReLU (), torch . nn . Linear ( d_model , d_model ), ) else : self . gene_encoder = gene_encoder # Positional Encoding if gene_pos_file is not None : # redoing it just in case some were dropped with embbeding file step gene_pos_enc = gene_pos_enc . loc [ self . genes , \"pos\" ] . astype ( int ) . tolist () self . pos_encoder = encoders . PositionalEncoding ( d_model , gene_pos_enc = gene_pos_enc ) else : self . pos_encoder = None # Value Encoder, NOTE: the scaling style is also handled in _encode method expr_d_model = d_model # // 8 if finetune_gene_emb else d_model if expr_emb_style in \"continuous\" : expr_encoder = encoders . ContinuousValueEncoder ( expr_d_model , dropout , layers = expr_encoder_layers ) elif expr_emb_style == \"binned\" : assert n_input_bins > 0 assert normalization == \"raw\" , \"shouldn't use normalization\" expr_encoder = encoders . CategoryValueEncoder ( n_input_bins , expr_d_model ) elif expr_emb_style == \"metacell\" : expr_encoder = encoders . EasyExprGNN ( self_dim = expr_d_model * 2 , output_dim = expr_d_model , shared_layers = expr_encoder_layers , dropout = dropout , ) else : raise ValueError ( f \"expr_emb_style should be one of binned, continuous, metacell, \" f \"got { expr_emb_style } \" ) if finetune_gene_emb and False : self . expr_encoder = encoders . ExprBasedFT ( d_model , gene_encoder , expr_encoder , dropout , layers = expr_encoder_layers , intermediary_d = int ( d_model * 1.5 ), ) else : self . expr_encoder = expr_encoder # Class Encoder # always have [base_cell_emb, time_embedding, depth_embedding] + any other class info # base cell embedding will store other cell specific information self . class_encoder = encoders . CategoryValueEncoder ( len ( self . classes ) + 1 , d_model if not cell_specific_blocks else d_model_cell , ) if self . use_metacell_token : self . metacell_encoder = encoders . CategoryValueEncoder ( 2 , d_model ) # compute tensor for mat_labels_hierarchy # old parameters that can still be passed when loading older models (managed in the _on_load_ckpt function) for i in [ \"strict_loading\" , \"optim\" , \"weight_decay\" , \"d_hid\" , \"edge_dim\" , \"prenorm\" , \"domain_spec_batchnorm\" , \"use_flash_attn\" , \"cell_emb_style\" , \"num_batch_labels\" , \"transformer\" , \"residual_in_fp32\" , \"max_cont_len\" , ]: if i in attention_kwargs : attention_kwargs . pop ( i ) # attention # Linear if attention == \"linear\" : # linear attention using the fast attention package # self.attention = FastattentionEncoder( # d_model, nhead, d_hid, nlayers, dropout, \"linear\" # ) raise NotImplementedError ( \"Linear attention is not implemented\" ) elif attention == \"performer\" : self . transformer = Performer ( dim = d_model , depth = nlayers , heads = nhead , dim_head = d_model // nhead , causal = False , attn_dropout = dropout , ff_dropout = dropout , qkv_bias = True , nb_features = nb_features , feature_redraw_interval = feature_redraw_interval , ) else : self . transformer = FlashTransformer ( d_model = d_model , nhead = nhead , dropout = dropout , attn_dropout = dropout , nlayers = nlayers , cross_attn = cell_specific_blocks , cross_dim = d_model_cell , attn_type = \"flash\" if attention == \"legacy-flash\" else attention , num_heads_kv = num_heads_kv , sketcher_size = sketcher_size , drop_path_rate = drop_path_rate , ** attention_kwargs , ) if cell_specific_blocks : attention_kwargs . pop ( \"num_heads_kv\" , None ) self . cell_transformer = FlashTransformer ( d_model = d_model_cell , nhead = nhead_cell , num_heads_kv = num_heads_kv_cell , nlayers = nlayers_cell , dropout = dropout , cross_attn = True , cross_dim = d_model , attn_type = \"flash\" if attention == \"legacy-flash\" else \"normal\" , ** attention_kwargs , ) else : self . cell_transformer = None # decoders # expression self . splicing_head = None if expr_emb_style == \"binned\" : self . expr_decoder = decoders . ClsDecoder ( d_model , n_input_bins , layers = [ d_model // 2 , d_model // 4 ], dropout = dropout , ) else : self . expr_decoder = decoders . ExprDecoder ( d_model , dropout = dropout , zinb = zinb , use_depth = True , ) if splicing_head : self . splicing_head = decoders . ExprDecoder ( d_model , dropout = dropout , zinb = zinb , use_depth = True , ) # cls decoder self . cls_decoders = torch . nn . ModuleDict () # should be a very simple classifier for most things # (maybe scale with the number of classes) should be 1 layer... for clss , n_cls in classes . items (): mdim = d_model_cell if cell_specific_blocks else self . d_model dim = compress_class_dim [ clss ] if compress_class_dim is not None else mdim self . cls_decoders [ clss ] = decoders . ClsDecoder ( dim , n_cls , layers = layers_cls , dropout = dropout , ) if \"cell_type_ontology_term_id\" in classes and self . do_adv_cls : mdim = d_model_cell if cell_specific_blocks else self . d_model dim = ( compress_class_dim [ \"cell_type_ontology_term_id\" ] if compress_class_dim is not None else mdim ) if \"assay_ontology_term_id\" in classes : self . assay_relab = utils . relabel_assay_for_adv ( self . label_decoders , self . labels_hierarchy ) self . adv_assay_decoder = decoders . ClsDecoder ( dim , len ( set ( self . assay_relab . values ())), layers = layers_cls , dropout = dropout , ) if len ( self . organisms ) > 1 : self . adv_organism_decoder = decoders . ClsDecoder ( dim , len ( self . organisms ), layers = layers_cls , dropout = dropout , ) # expression decoder from batch embbedding if mvc_decoder is not None : if cell_specific_blocks : raise ValueError ( \"MVC decoder is not supported for cell specific blocks\" ) self . mvc_decoder = decoders . MVCDecoder ( d_model , arch_style = mvc_decoder , zinb = zinb , use_depth = True ) else : self . mvc_decoder = None if compress_class_dim is not None : self . compressor = torch . nn . ModuleDict () dim = d_model_cell if cell_specific_blocks else self . d_model for k , v in compress_class_dim . items (): if v >= 8 : self . compressor [ k ] = decoders . VAEDecoder ( dim , layers = [ 128 , v , ], dropout = dropout , return_latent = True , ) else : self . compressor [ k ] = fsq . FSQ ( levels = [ 2 ] * v , dim = dim ) else : self . compressor = None self . apply ( partial ( utils . _init_weights , n_layer = nlayers , ) ) for i , dec in self . cls_decoders . items (): torch . nn . init . constant_ ( dec . out_layer . bias , - 0.13 ) self . expr_encoder . _init_weights ()","title":"Generate embeddings"},{"location":"model/#scprint2.model.model.scPRINT2.genes","text":"Get flattened list of all genes in the model's vocabulary. For multi-organism models, concatenates genes from all organisms in consistent order. Returns: list [ str ] \u2013 list[str]: Gene names in model vocabulary order.","title":"genes"},{"location":"model/#scprint2.model.model.scPRINT2.add_organism","text":"Add a new organism to an existing model for transfer learning. Extends the gene vocabulary and embeddings to include genes from a new organism. Useful for applying a pretrained model to a new species. Parameters: organism ( str ) \u2013 Organism ontology term ID (e.g., \"NCBITaxon:10090\" for mouse). genes ( Index ) \u2013 Gene names/IDs for the new organism. emb ( DataFrame ) \u2013 Gene embeddings DataFrame with genes as index. Will be resized to match model's d_model. locs ( DataFrame , default: None ) \u2013 Genomic positions with 'pos' column. Required if model uses positional encoding. Defaults to None. Raises: ValueError \u2013 If model requires gene locations but none provided. ValueError \u2013 If gene positions exceed model's maximum position encoding. Note Only genes present in both genes and emb (and locs if provided) will be added. The model's gene encoder is expanded in-place. Source code in scprint2/model/model.py 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 def add_organism ( self , organism : str , genes : pd . Index , emb : pd . DataFrame , locs = None ): \"\"\" Add a new organism to an existing model for transfer learning. Extends the gene vocabulary and embeddings to include genes from a new organism. Useful for applying a pretrained model to a new species. Args: organism (str): Organism ontology term ID (e.g., \"NCBITaxon:10090\" for mouse). genes (pd.Index): Gene names/IDs for the new organism. emb (pd.DataFrame): Gene embeddings DataFrame with genes as index. Will be resized to match model's d_model. locs (pd.DataFrame, optional): Genomic positions with 'pos' column. Required if model uses positional encoding. Defaults to None. Raises: ValueError: If model requires gene locations but none provided. ValueError: If gene positions exceed model's maximum position encoding. Note: Only genes present in both `genes` and `emb` (and `locs` if provided) will be added. The model's gene encoder is expanded in-place. \"\"\" if self . pos_encoder is not None and locs is None : raise ValueError ( \"this model needs gene locations to add a new organism\" ) self . organisms . append ( organism ) if locs is not None : overlap = set ( locs . index ) & set ( emb . index ) & set ( genes . index ) genes = genes [ genes . index . isin ( overlap )] locs = locs . loc [ genes . index ] pos = locs [ \"pos\" ] token_to_pos = { token : pos for token , pos in enumerate ( pos )} if self . pos_encoder . pe . shape [ 0 ] < max ( pos ): raise ValueError ( f \"the number of gene locs in the added organism needs to be less than { self . pos_encoder . pe . shape [ 0 ] } \" ) token_to_pos = { token : pos for token , pos in enumerate ( pos )} arr = [] for _ , v in token_to_pos . items (): arr . append ( self . pos_encoder . pe [ v - 1 ] . to ( \"cpu\" ) . numpy ()) pe = torch . Tensor ( np . array ( arr )) . to ( self . pos_encoder . pe . device ) self . pos_encoder . pe = torch . cat ([ self . pos_encoder . pe , pe ], dim = 0 ) else : overlap = set ( emb . index ) & set ( genes . index ) genes = genes [ genes . index . isin ( overlap )] emb = emb . loc [ genes . index ] self . _genes [ organism ] = genes . index . tolist () if self . gene_encoder is None : genc = self . expr_encoder . gene_encoder else : genc = self . gene_encoder if type ( genc ) is torch . nn . Sequential : enc = genc [ 0 ] else : enc = genc semb = torch . nn . AdaptiveAvgPool1d ( self . d_model )( torch . tensor ( emb . values , dtype = torch . float32 ) ) . to ( enc . embeddings . weight . data . device ) if enc . memmap : print ( \"todev.. will fail for now\" ) embs = torch . cat ([ enc . embeddings . weight . data , semb ], dim = 0 ) enc . embeddings = nn . Embedding ( embs . shape [ 0 ], embs . shape [ 1 ], padding_idx = None , _freeze = enc . embeddings . weight . requires_grad , ) enc . embeddings . weight . data . copy_ ( embs ) enc . embeddings . weight . data = enc . embeddings . weight . data . to ( self . device ) if type ( genc ) is torch . nn . Sequential : genc [ 0 ] = enc else : genc = enc if self . gene_encoder is None : self . expr_encoder . gene_encoder = genc else : self . gene_encoder = genc","title":"add_organism"},{"location":"model/#scprint2.model.model.scPRINT2.configure_optimizers","text":"@see pl.LightningModule Source code in scprint2/model/model.py 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 def configure_optimizers ( self ): \"\"\"@see pl.LightningModule\"\"\" # https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam # not working because of poor weight decay implem if self . optim == \"adam\" : optimizer = optim . Adam ( self . parameters (), lr = self . hparams . lr , betas = ( 0.9 , 0.999 ), eps = 1e-7 , # 1e-5 to 1e-8 weight_decay = self . weight_decay , amsgrad = False , fused = self . fused_adam , ) elif self . optim == \"adamW\" : optimizer = optim . AdamW ( self . parameters (), lr = self . hparams . lr , betas = ( 0.9 , 0.999 ), eps = 1e-7 , # 1e-5 to 1e-8 weight_decay = self . weight_decay , amsgrad = False , fused = self . fused_adam , ) elif self . optim == \"galore\" : raise NotImplementedError ( \"Galore optimizer not implemented\" ) # param_groups = [ # { # \"params\": [ # v for k, v in self.named_parameters() if \"transformer\" not in k # ] # }, # { # \"params\": [ # v for k, v in self.named_parameters() if \"transformer\" in k # ], # \"rank\": 128, # \"update_proj_gap\": 200, # \"scale\": 0.25, # \"proj_type\": \"std\", # }, # ] # optimizer = GaLoreAdamW(param_groups, lr=self.hparams.lr) else : raise ValueError ( f \"Unknown optimizer: { self . optim } \" ) if self . lr_reduce_monitor is None : print ( \"no lr reduce factor\" ) return [ optimizer ] # lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts( # optimizer, # T_0=20000, # T_mult=2, # eta_min=1e-8, # ) # interval = \"step\" # frequency = 10 # lr_scheduler = optim.lr_scheduler.ExponentialLR( # optimizer, # gamma=0.85, # ) lr_scheduler = optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = \"min\" , patience = self . lr_reduce_patience , factor = self . lr_reduce_factor , ) interval = \"epoch\" frequency = 1 # lr_scheduler = StepwiseCAWRWithWD( # optimizer, # T_0=20_000, # T_mult=2, # eta_min=1e-8, # wd_decay=0.9 # ) lr_dict = { \"scheduler\" : lr_scheduler , # The unit of the scheduler's step size, could also be 'step'. # 'epoch' updates the scheduler on epoch end whereas 'step' # updates it after a optimizer update. \"interval\" : interval , # How many epochs/steps should pass between calls to # `scheduler.step()`. 1 corresponds to updating the learning # rate after every epoch/step. \"frequency\" : frequency , # Metric to to monitor for schedulers like `ReduceLROnPlateau` \"monitor\" : self . lr_reduce_monitor , } self . lrfinder_steps = 0 for val in self . trainer . callbacks : if type ( val ) is _LRCallback : self . lrfinder_steps = val . num_training if type ( val ) is LearningRateFinder : self . lrfinder_steps = val . _num_training_steps return [ optimizer ], [ lr_dict ]","title":"configure_optimizers"},{"location":"model/#scprint2.model.model.scPRINT2.forward","text":"Complete forward pass through the scPRINT-2 Encodes input expression data, processes through transformer(s), and decodes into expression predictions and cell classifications. Parameters: gene_pos ( Tensor ) \u2013 Gene indices of shape (batch, seq_len) mapping to positions in the model's gene vocabulary. expression ( Tensor , default: None ) \u2013 Expression values of shape (batch, seq_len). Can be raw counts or normalized depending on model config. Defaults to None. neighbors ( Tensor , default: None ) \u2013 KNN neighbor expressions of shape (batch, n_neighbors, seq_len) for metacell-style encoding. Defaults to None. neighbors_info ( Tensor , default: None ) \u2013 Neighbor weights of shape (batch, n_neighbors). Defaults to None. mask ( Tensor , default: None ) \u2013 Boolean mask of shape (batch, seq_len) where True indicates positions to mask (set to zero). Defaults to None. req_depth ( Tensor , default: None ) \u2013 Target sequencing depth of shape (batch,) for depth-conditional generation. Defaults to None. get_gene_emb ( bool , default: False ) \u2013 Return gene embeddings from transformer. Defaults to False. metacell_token ( Tensor , default: None ) \u2013 Binary metacell indicators of shape (batch,). Defaults to None. depth_mult ( Tensor , default: None ) \u2013 Expression depth multiplier. If None, uses sum of expression values. Defaults to None. do_sample ( bool , default: False ) \u2013 Sample from predicted distribution. Currently unused. Defaults to False. do_mvc ( bool , default: False ) \u2013 Compute multi-view coding predictions. Defaults to False. do_class ( bool , default: False ) \u2013 Compute classification predictions. Defaults to False. get_attention_layer ( list [ int ] , default: None ) \u2013 Layer indices to extract attention weights from. Defaults to None. mask_zeros ( Tensor , default: None ) \u2013 Boolean mask for zero-expression genes of shape (batch, seq_len + num_special_tokens). Defaults to None. Returns: Dict [ str , Tensor ] | tuple [ Dict [ str , Tensor ], list ] \u2013 dict[str, Tensor] | tuple[dict, list]: Model outputs containing: - \"mean\": Predicted expression (batch, seq_len) - \"disp\": Dispersion parameters (batch, seq_len) [if ZINB] - \"zero_logits\": Zero-inflation logits (batch, seq_len) [if ZINB] - \"input_cell_embs\": Cell embeddings (batch, n_classes+1, d_model) - \"input_cell_emb\": Mean cell embedding (batch, d_model) - \"output_cell_embs\": Processed cell embeddings - \"output_cell_emb\": Final cell embedding - \"cls_output_{class}\": Classification logits for each class - \"gene_embedding\": Gene embeddings [if get_gene_emb] - \"mvc_*\": MVC predictions [if do_mvc] If get_attention_layer is not None, returns (outputs_dict, attention_list) where attention_list contains QKV tensors from specified layers. Example output = model( ... gene_pos=batch[\"genes\"], ... expression=batch[\"x\"], ... req_depth=batch[\"depth\"], ... do_class=True, ... ) predictions = output[\"mean\"] cell_types = output[\"cls_output_cell_type_ontology_term_id\"].argmax(-1) Source code in scprint2/model/model.py 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 def forward ( self , gene_pos : Tensor , expression : Optional [ Tensor ] = None , neighbors : Optional [ Tensor ] = None , neighbors_info : Optional [ Tensor ] = None , mask : Optional [ Tensor ] = None , req_depth : Optional [ Tensor ] = None , get_gene_emb : bool = False , metacell_token : Optional [ Tensor ] = None , # (minibatch, 1) depth_mult : Optional [ Tensor ] = None , do_sample : bool = False , do_mvc : bool = False , do_class : bool = False , get_attention_layer : Optional [ list ] = None , mask_zeros : Optional [ Tensor ] = None , ) -> Dict [ str , Tensor ] | tuple [ Dict [ str , Tensor ], list ]: \"\"\" Complete forward pass through the scPRINT-2 Encodes input expression data, processes through transformer(s), and decodes into expression predictions and cell classifications. Args: gene_pos (Tensor): Gene indices of shape (batch, seq_len) mapping to positions in the model's gene vocabulary. expression (Tensor, optional): Expression values of shape (batch, seq_len). Can be raw counts or normalized depending on model config. Defaults to None. neighbors (Tensor, optional): KNN neighbor expressions of shape (batch, n_neighbors, seq_len) for metacell-style encoding. Defaults to None. neighbors_info (Tensor, optional): Neighbor weights of shape (batch, n_neighbors). Defaults to None. mask (Tensor, optional): Boolean mask of shape (batch, seq_len) where True indicates positions to mask (set to zero). Defaults to None. req_depth (Tensor, optional): Target sequencing depth of shape (batch,) for depth-conditional generation. Defaults to None. get_gene_emb (bool, optional): Return gene embeddings from transformer. Defaults to False. metacell_token (Tensor, optional): Binary metacell indicators of shape (batch,). Defaults to None. depth_mult (Tensor, optional): Expression depth multiplier. If None, uses sum of expression values. Defaults to None. do_sample (bool, optional): Sample from predicted distribution. Currently unused. Defaults to False. do_mvc (bool, optional): Compute multi-view coding predictions. Defaults to False. do_class (bool, optional): Compute classification predictions. Defaults to False. get_attention_layer (list[int], optional): Layer indices to extract attention weights from. Defaults to None. mask_zeros (Tensor, optional): Boolean mask for zero-expression genes of shape (batch, seq_len + num_special_tokens). Defaults to None. Returns: dict[str, Tensor] | tuple[dict, list]: Model outputs containing: - \"mean\": Predicted expression (batch, seq_len) - \"disp\": Dispersion parameters (batch, seq_len) [if ZINB] - \"zero_logits\": Zero-inflation logits (batch, seq_len) [if ZINB] - \"input_cell_embs\": Cell embeddings (batch, n_classes+1, d_model) - \"input_cell_emb\": Mean cell embedding (batch, d_model) - \"output_cell_embs\": Processed cell embeddings - \"output_cell_emb\": Final cell embedding - \"cls_output_{class}\": Classification logits for each class - \"gene_embedding\": Gene embeddings [if get_gene_emb] - \"mvc_*\": MVC predictions [if do_mvc] If get_attention_layer is not None, returns (outputs_dict, attention_list) where attention_list contains QKV tensors from specified layers. Example: >>> output = model( ... gene_pos=batch[\"genes\"], ... expression=batch[\"x\"], ... req_depth=batch[\"depth\"], ... do_class=True, ... ) >>> predictions = output[\"mean\"] >>> cell_types = output[\"cls_output_cell_type_ontology_term_id\"].argmax(-1) \"\"\" cell_embs , encoding = self . _encoder ( gene_pos , expression , neighbors , neighbors_info , mask , metacell_token = metacell_token , ) # attention_bias num = ( 1 if self . use_metacell_token else 0 ) + ( ( len ( self . classes ) + 1 ) if not self . cell_transformer else 0 ) if self . attn_bias is not None : if not hasattr ( self , \"nbias_sparse\" ): bias_path = os . path . join ( self . attn_bias ) # Keep as sparse matrix - much more memory efficient self . nbias_sparse = load_npz ( bias_path ) bias = torch . zeros ( ( gene_pos . shape [ 0 ], gene_pos . shape [ 1 ] + num , gene_pos . shape [ 1 ] + num , ), device = gene_pos . device , dtype = torch . float16 , ) fade_factor = 100 # Extract only the needed values from sparse matrix batch_size = gene_pos . shape [ 0 ] # Vectorized extraction from sparse matrix for b in range ( batch_size ): indices = gene_pos [ b ] . cpu () . numpy () # Get submatrix for this batch's genes submatrix = self . nbias_sparse [ np . ix_ ( indices , indices )] bias [ b , num :, num :] = ( torch . tensor ( submatrix . toarray (), device = gene_pos . device , dtype = torch . float16 ) * fade_factor ) bias [:, num :, : num ] = - 10_000 if not self . cell_transformer : encoding = torch . cat ([ cell_embs , encoding ], dim = 1 ) if type ( self . transformer ) is FlashTransformer : transformer_output = self . transformer ( encoding , return_qkv = get_attention_layer , bias = bias if self . attn_bias is not None else None , bias_layer = list ( range ( self . nlayers - 1 )), mask_zeros = mask_zeros , ) elif type ( self . transformer ) is Performer : transformer_output = self . transformer ( encoding ) else : raise ValueError ( f \"Unknown transformer: { type ( self . transformer ) } \" ) if get_attention_layer is not None : transformer_output , qkvs = transformer_output if self . cell_transformer : cell_embs = self . cell_transformer ( cell_embs , x_kv = transformer_output ) else : cell_embs , transformer_output = transformer_output . split ( [ len ( self . classes ) + 1 , transformer_output . shape [ 1 ] - ( len ( self . classes ) + 1 ), ], dim = 1 , ) # if not provided we will mult by the current expression sum depth_mult = expression . sum ( 1 ) if depth_mult is None else depth_mult req_depth = torch . log2 ( 1 + req_depth ) res = self . _expr_decoder ( transformer_output [:, ( 1 if self . use_metacell_token else 0 ) :, :], depth_mult , req_depth , get_gene_emb , ) res . update ( self . _cell_decoder ( cell_embs , do_mvc , do_class , depth_mult , req_depth , gene_pos if do_mvc else None , ) ) return ( res , qkvs ) if get_attention_layer is not None else res","title":"forward"},{"location":"model/#scprint2.model.model.scPRINT2.log_adata","text":"log_adata will log an adata from predictions. It will log to tensorboard and wandb if available see @utils.log_adata Source code in scprint2/model/model.py 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 def log_adata ( self , gtclass = None , name = \"\" ): \"\"\" log_adata will log an adata from predictions. It will log to tensorboard and wandb if available see @utils.log_adata \"\"\" try : mdir = self . logger . save_dir if self . logger . save_dir is not None else \"/tmp\" except : mdir = \"data/\" if not os . path . exists ( mdir ): os . makedirs ( mdir ) adata , fig = utils . make_adata ( genes = self . genes , embs = self . embs , pos = self . pos if self . save_expr else None , expr_pred = self . expr_pred if self . save_expr else None , classes = self . classes , pred = self . pred if not self . keep_all_labels_pred else None , label_decoders = self . label_decoders , labels_hierarchy = self . labels_hierarchy , gtclass = gtclass , doplot = self . doplot , ) adata . write ( str ( mdir ) + \"/step_\" + str ( self . global_step ) + \"_\" + str ( self . name ) + \"_\" + str ( name ) + \"_\" + str ( self . global_rank ) + \".h5ad\" ) if self . doplot and fig is not None : logged = False try : self . logger . experiment . add_figure ( fig ) logged = True except : print ( \"couldn't log to tensorboard\" ) try : self . logger . log_image ( key = \"umaps\" , images = [ fig ], step = self . global_step ) logged = True except : print ( \"couldn't log to wandb\" ) if not logged : fig . savefig ( mdir + \"/umap_\" + self . name + \"_\" + name + \".png\" ) return adata","title":"log_adata"},{"location":"model/#scprint2.model.model.scPRINT2.on_fit_start","text":"@see pl.LightningModule Source code in scprint2/model/model.py 1599 1600 1601 1602 1603 1604 1605 def on_fit_start ( self ): \"\"\"@see pl.LightningModule\"\"\" if type ( self . transformer ) is FlashTransformer : for encoder_layers in self . transformer . blocks : encoder_layers . set_seq_parallel ( True ) for k , v in self . mat_labels_hierarchy . items (): self . mat_labels_hierarchy [ k ] = v . to ( self . device )","title":"on_fit_start"},{"location":"model/#scprint2.model.model.scPRINT2.on_load_checkpoint","text":"Handle checkpoint loading with backward compatibility. Automatically handles: - Different class configurations between checkpoint and current model - Legacy parameter names and structures - Encoder/decoder mismatches with datamodule - Gene vocabulary differences - Early stopping callback state Called automatically by PyTorch Lightning during checkpoint loading. Parameters: checkpoints ( dict ) \u2013 Checkpoint dictionary from torch.load(). Note Prints warnings when configurations differ between checkpoint and current model. These should be reviewed to ensure expected behavior. Source code in scprint2/model/model.py 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 def on_load_checkpoint ( self , checkpoints ): \"\"\" Handle checkpoint loading with backward compatibility. Automatically handles: - Different class configurations between checkpoint and current model - Legacy parameter names and structures - Encoder/decoder mismatches with datamodule - Gene vocabulary differences - Early stopping callback state Called automatically by PyTorch Lightning during checkpoint loading. Args: checkpoints (dict): Checkpoint dictionary from torch.load(). Note: Prints warnings when configurations differ between checkpoint and current model. These should be reviewed to ensure expected behavior. \"\"\" # if not the same number of labels (due to diff datasets) for name , clss in self . cls_decoders . items (): size = checkpoints [ \"state_dict\" ][ \"cls_decoders.\" + name + \".out_layer.bias\" ] . shape [ 0 ] if size != clss . out_layer . bias . shape [ 0 ]: self . cls_decoders [ name ] . out_layer = torch . nn . Linear ( clss . out_layer . weight . shape [ 1 ], size ) # from older model versions self . normalization = checkpoints [ \"hyper_parameters\" ] . get ( \"normalization\" , \"sum\" ) if ( checkpoints [ \"state_dict\" ] . get ( \"gene_encoder.0.embedding.weight\" , None ) is not None ): # replace it with the new one gene_encoder.0.embeddings.weight in the state_dict checkpoints [ \"state_dict\" ][ \"gene_encoder.0.embeddings.weight\" ] = checkpoints [ \"state_dict\" ][ \"gene_encoder.0.embedding.weight\" ] del checkpoints [ \"state_dict\" ][ \"gene_encoder.0.embedding.weight\" ] # same # when doing batch effect correction and input dataset is not the same if ( \"grad_reverse_discriminator_loss.out_layer.bias\" in checkpoints [ \"state_dict\" ] ): for k in list ( checkpoints [ \"state_dict\" ] . keys ()): if \"grad_reverse_discriminator_loss\" in k : del checkpoints [ \"state_dict\" ][ k ] print ( \"the discriminator for batch effect correction has been removed. \" \"dropping the legacy key.\" ) # same if ( checkpoints [ \"state_dict\" ] . get ( \"gene_encoder.embedding.weight\" , None ) is not None ): # replace it with the new one gene_encoder.embeddings.weight in the state_dict checkpoints [ \"state_dict\" ][ \"gene_encoder.embeddings.weight\" ] = checkpoints [ \"state_dict\" ][ \"gene_encoder.embedding.weight\" ] del checkpoints [ \"state_dict\" ][ \"gene_encoder.embedding.weight\" ] if \"classes\" in checkpoints [ \"hyper_parameters\" ]: if self . label_counts != checkpoints [ \"hyper_parameters\" ][ \"classes\" ]: if \"label_counts\" in checkpoints [ \"hyper_parameters\" ] and set ( checkpoints [ \"hyper_parameters\" ][ \"label_counts\" ] . keys () ) == set ( checkpoints [ \"hyper_parameters\" ][ \"classes\" ]): if self . classes != checkpoints [ \"hyper_parameters\" ][ \"classes\" ]: print ( \"classes have changed, be careful\" ) self . classes = checkpoints [ \"hyper_parameters\" ][ \"classes\" ] self . label_counts = checkpoints [ \"hyper_parameters\" ][ \"label_counts\" ] if self . classes == self . label_counts : raise ValueError ( \"classes and label_counts are the same, this is not allowed, please use another checkpoint\" ) else : self . label_counts = checkpoints [ \"hyper_parameters\" ][ \"classes\" ] if self . classes != list ( checkpoints [ \"hyper_parameters\" ][ \"classes\" ] . keys () ): print ( \"classes have changed, be careful\" ) self . classes = list ( checkpoints [ \"hyper_parameters\" ][ \"classes\" ] . keys () ) # else it is all good as expected else : print ( \"no classes in the checkpoint, be careful\" ) if checkpoints [ \"state_dict\" ] . get ( \"pos_encoder.pe\" ) is not None : if self . pos_encoder is None : self . pos_encoder = encoders . PositionalEncoding ( self . d_model , gene_pos_enc = [ 0 , 1 , 2 ] ) self . pos_encoder . pe = checkpoints [ \"state_dict\" ][ \"pos_encoder.pe\" ] if self . label_decoders != checkpoints [ \"hyper_parameters\" ][ \"label_decoders\" ] or self . labels_hierarchy != checkpoints [ \"hyper_parameters\" ] . get ( \"labels_hierarchy\" , {} ): print ( \"label decoders have changed, be careful\" ) self . label_decoders = checkpoints [ \"hyper_parameters\" ][ \"label_decoders\" ] self . labels_hierarchy = checkpoints [ \"hyper_parameters\" ] . get ( \"labels_hierarchy\" , {} ) for k , v in self . labels_hierarchy . items (): tens = torch . zeros (( len ( v ), self . label_counts [ k ])) for k2 , v2 in v . items (): tens [ k2 - self . label_counts [ k ], v2 ] = 1 self . mat_labels_hierarchy [ k ] = tens . to ( bool ) if ( \"gene_pos_enc\" in checkpoints [ \"hyper_parameters\" ] and checkpoints [ \"hyper_parameters\" ][ \"gene_pos_enc\" ] is not None ): if ( self . pos_encoder is None or self . pos_encoder . gene_pos_enc != checkpoints [ \"hyper_parameters\" ][ \"gene_pos_enc\" ] ): print ( \"Gene position encoding has changed in the dataloader compared to last time, trying to revert\" ) self . pos_encoder = encoders . PositionalEncoding ( self . d_model , gene_pos_enc = checkpoints [ \"hyper_parameters\" ][ \"gene_pos_enc\" ], ) checkpoints [ \"hyper_parameters\" ] . pop ( \"gene_pos_enc\" ) mencoders = {} if type ( checkpoints [ \"hyper_parameters\" ][ \"genes\" ]) is list : print ( \"converting a gene list-based model\" ) org = checkpoints [ \"hyper_parameters\" ] . get ( \"organisms\" , self . organisms ) genedf = load_genes ( org ) checkpoints [ \"hyper_parameters\" ][ \"genes\" ] = { i : genedf . index [ ( genedf . organism == i ) & genedf . index . isin ( checkpoints [ \"hyper_parameters\" ][ \"genes\" ]) ] . tolist () for i in org } if \"precpt_gene_emb\" in checkpoints [ \"hyper_parameters\" ]: checkpoints [ \"hyper_parameters\" ] . pop ( \"precpt_gene_emb\" ) if \"gene_pos_file\" in checkpoints [ \"hyper_parameters\" ]: checkpoints [ \"hyper_parameters\" ] . pop ( \"gene_pos_file\" ) if \"transformer\" in checkpoints [ \"hyper_parameters\" ]: checkpoints [ \"hyper_parameters\" ][ \"attention\" ] = checkpoints [ \"hyper_parameters\" ] . pop ( \"transformer\" ) try : if self . trainer . datamodule . decoders != self . label_decoders : print ( \"label decoders have changed, be careful\" ) # if we don't have the same decoders, we need to update the one on the datamodule side for k , v in self . label_decoders . items (): mencoders [ k ] = { va : ke for ke , va in v . items ()} self . trainer . datamodule . encoders = mencoders es = None for k in self . trainer . callbacks : if isinstance ( k , EarlyStopping ): es = k if es is not None : prev = checkpoints [ \"callbacks\" ] . get ( \"EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}\" ) if prev is not None : prev = prev [ \"patience\" ] if prev != es . patience : print ( \"updating the early stopping parameter to {} \" . format ( es . patience ) ) checkpoints [ \"callbacks\" ][ \"EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}\" ][ \"patience\" ] = es . patience if prev < es . patience : checkpoints [ \"callbacks\" ][ \"EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}\" ][ \"stopped_epoch\" ] = 0 except RuntimeError as e : if \"scPRINT2 is not attached to a `Trainer`.\" in str ( e ): print ( \"FYI: scPRINT2 is not attached to a `Trainer`.\" ) else : raise e if ( self . mvc_decoder is None and checkpoints [ \"state_dict\" ] . get ( \"mvc_decoder.gene2query.weight\" ) is not None ): for i in [ \"mvc_decoder.gene2query.weight\" , \"mvc_decoder.gene2query.bias\" , \"mvc_decoder.norm.weight\" , \"mvc_decoder.norm.bias\" , \"mvc_decoder.pred_var_zero.weight\" , ]: if i in checkpoints [ \"state_dict\" ]: del checkpoints [ \"state_dict\" ][ i ] org = checkpoints [ \"hyper_parameters\" ] . get ( \"organisms\" ) if self . organisms != org and org is not None : self . organisms = org try : self . trainer . datamodule . organisms = self . organisms except RuntimeError as e : if \"scPRINT2 is not attached to a `Trainer`.\" not in str ( e ): raise e if self . _genes != checkpoints [ \"hyper_parameters\" ][ \"genes\" ]: self . _genes = checkpoints [ \"hyper_parameters\" ][ \"genes\" ] try : self . trainer . datamodule . set_valid_genes_collator ( self . genes ) except RuntimeError as e : if \"scPRINT2 is not attached to a `Trainer`.\" not in str ( e ): raise e if not is_interactive (): self . save_hyperparameters ()","title":"on_load_checkpoint"},{"location":"model/#scprint2.model.model.scPRINT2.on_predict_epoch_end","text":"@see pl.LightningModule will Source code in scprint2/model/model.py 2634 2635 2636 2637 2638 2639 2640 def on_predict_epoch_end ( self ): \"\"\"@see pl.LightningModule will\"\"\" if self . pos . shape [ 0 ] < 100 : return if self . pred_log_adata : print ( \"adding on disk\" ) return self . log_adata ( name = \"predict_part_\" + str ( self . counter ))","title":"on_predict_epoch_end"},{"location":"model/#scprint2.model.model.scPRINT2.on_predict_epoch_start","text":"@see pl.LightningModule Source code in scprint2/model/model.py 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 def on_predict_epoch_start ( self ): \"\"\"@see pl.LightningModule\"\"\" print ( \"predict epoch start\" ) self . embs = None self . attn . data = None self . attn . attn = None self . counter = 0 if type ( self . transformer ) is FlashTransformer : for encoder_layers in self . transformer . blocks : encoder_layers . set_seq_parallel ( False )","title":"on_predict_epoch_start"},{"location":"model/#scprint2.model.model.scPRINT2.on_test_start","text":"@see pl.LightningModule Source code in scprint2/model/model.py 2324 2325 2326 2327 2328 def on_test_start ( self ): \"\"\"@see pl.LightningModule\"\"\" print ( \"test start\" ) for k , v in self . mat_labels_hierarchy . items (): self . mat_labels_hierarchy [ k ] = v . to ( self . device )","title":"on_test_start"},{"location":"model/#scprint2.model.model.scPRINT2.on_validation_epoch_end","text":"@see pl.LightningModule Source code in scprint2/model/model.py 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 def on_validation_epoch_end ( self ): \"\"\"@see pl.LightningModule\"\"\" self . pos = None self . expr_pred = None self . do_adv_cls = self . _store_adv_cls gathered_embs = self . all_gather ( self . embs ) # Merge the dictionaries from all processes for key in self . embs . keys (): self . embs [ key ] = gathered_embs [ key ] . view ( - 1 , gathered_embs [ key ] . shape [ - 1 ]) self . info = self . all_gather ( self . info ) . view ( - 1 , self . info . shape [ - 1 ]) self . pred = ( self . all_gather ( self . pred ) . view ( - 1 , self . pred . shape [ - 1 ]) if self . pred is not None else None ) # self.pos = self.all_gather(self.pos).view(-1, self.pos.shape[-1]) # self.expr_pred[0] = self.all_gather(self.expr_pred[0]).view( # -1, self.expr_pred[0].shape[-1] # ) # if len(self.expr_pred) > 1: # self.expr_pred[1] = self.all_gather(self.expr_pred[1]).view( # -1, self.expr_pred[1].shape[-1] # ) # self.expr_pred[2] = self.all_gather(self.expr_pred[2]).view( # -1, self.expr_pred[2].shape[-1] # ) if self . trainer . state . stage != \"sanity_check\" : if self . trainer . is_global_zero : print ( \"logging anndata\" ) sch = self . lr_schedulers () if sch is not None : sch . step ( self . trainer . callback_metrics [ \"val_loss\" ]) # run the test function on specific dataset if self . embs is not None : self . log_adata ( gtclass = self . info , name = \"validation_part_\" + str ( self . counter ) ) if ( self . current_epoch + 1 ) % self . test_every == 0 : self . on_test_epoch_end () # Synchronize all processes with a timeout if torch . distributed . is_initialized (): # Set a timeout that's longer than your test typically takes # Write rank to file for debugging self . trainer . strategy . barrier () self . pred = None","title":"on_validation_epoch_end"},{"location":"model/#scprint2.model.model.scPRINT2.optimizer_step","text":"@see pl.LightningModule Source code in scprint2/model/model.py 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 def optimizer_step ( self , epoch , batch_idx , optimizer , optimizer_closure ): \"\"\"@see pl.LightningModule\"\"\" # update params # manually warm up lr without a scheduler # making sure that we don't do this during lrfinder lr_scale = None prev_lr = None if ( self . trainer . global_step < self . warmup_duration + self . lrfinder_steps ) and self . lrfinder_steps <= self . trainer . global_step : for i , pg in enumerate ( optimizer . param_groups ): lr_scale = min ( 1.0 , float ( self . trainer . global_step + 1 ) / self . warmup_duration ) prev_lr = pg [ \"lr\" ] pg [ \"lr\" ] = lr_scale * self . hparams . lr for i , pg in enumerate ( optimizer . param_groups ): # if pg[\"lr\"] < 2e-5: # pg[\"lr\"] = 2e-5 self . log ( \"lr_\" + str ( i ), pg [ \"lr\" ]) if optimizer . param_groups [ 0 ][ \"lr\" ] > self . hparams . lr : if prev_lr is not None : pg [ \"lr\" ] = prev_lr else : print ( \"OPTIMIZER HAS INCREASED LR. WHYY?\" ) print ( optimizer . param_groups [ 0 ][ \"lr\" ], self . hparams . lr ) optimizer . param_groups [ 0 ][ \"lr\" ] = self . hparams . lr optimizer . step ( closure = optimizer_closure )","title":"optimizer_step"},{"location":"model/#scprint2.model.model.scPRINT2.predict_step","text":"embed given gene expression, encode the gene embedding and cell embedding. Parameters: batch ( Dict [ str , Tensor ] ) \u2013 Dictionary containing 'genes', 'x', 'depth', and optionally 'knn_cells'. batch_idx ( int ) \u2013 Index of the batch. Returns: Dict [ str , Tensor ] \u2013 Dict[str, Tensor]: Dictionary containing model predictions. Source code in scprint2/model/model.py 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 def predict_step ( self , batch : Dict [ str , Tensor ], batch_idx : int ) -> Dict [ str , Tensor ]: \"\"\" embed given gene expression, encode the gene embedding and cell embedding. Args: batch (Dict[str, Tensor]): Dictionary containing 'genes', 'x', 'depth', and optionally 'knn_cells'. batch_idx: Index of the batch. Returns: Dict[str, Tensor]: Dictionary containing model predictions. \"\"\" return self . _predict ( batch [ \"genes\" ], batch [ \"x\" ], batch [ \"depth\" ], batch . get ( \"knn_cells\" , None ), batch . get ( \"knn_cells_info\" , None ), self . predict_mode , self . pred_embedding , self . get_attention_layer , self . predict_depth_mult , )","title":"predict_step"},{"location":"model/#scprint2.model.model.scPRINT2.training_step","text":"training_step defines the train loop. It is independent of forward @see pl.LightningModule Returns: Tensor ( Tensor ) \u2013 Total loss value for the training step. Source code in scprint2/model/model.py 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 def training_step ( self , batch : Dict [ str , Tensor ], batch_idx : int , ) -> Tensor : \"\"\" training_step defines the train loop. It is independent of forward @see pl.LightningModule Returns: Tensor: Total loss value for the training step. \"\"\" total_loss , losses = self . _full_training ( batch = batch , noise = self . noise , do_next_tp = self . do_next_tp , cce_temp = self . cce_temp , do_generate = self . do_generate , run_full_forward = self . run_full_forward , mask_ratio = self . mask_ratio , ) if total_loss is None or torch . isnan ( total_loss ): raise ValueError ( \"Loss is NaN\" ) try : self . log ( \"train_loss\" , total_loss , prog_bar = True , sync_dist = True ) self . log_dict ( losses , prog_bar = True , sync_dist = True ) except Exception as e : print ( e ) print ( losses ) return total_loss","title":"training_step"},{"location":"model/#scprint2.model.model.scPRINT2.validation_step","text":"validation_step defines the validation loop. It is independent of forward @see pl.LightningModule Parameters: batch ( list [ Tensor ] ) \u2013 @see training_step Source code in scprint2/model/model.py 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 def validation_step ( self , batch , batch_idx , ): \"\"\" validation_step defines the validation loop. It is independent of forward @see pl.LightningModule Args: batch (list[Tensor]): @see training_step \"\"\" val_loss , losses = self . _full_training ( batch = batch , noise = self . noise , do_next_tp = self . do_next_tp , cce_temp = self . cce_temp , do_vae_kl = False , do_generate = self . do_generate , run_full_forward = self . run_full_forward , mask_ratio = self . mask_ratio , ) expression = batch [ \"x\" ] gene_pos = batch [ \"genes\" ] depth = batch [ \"depth\" ] metacell_token = batch . get ( \"is_meta\" , None ) knn_cells = batch . get ( \"knn_cells\" , None ) knn_cells_info = batch . get ( \"knn_cells_info\" , None ) # TODO: make this faster by only calling val loss if self . embs is not None : if self . pos . shape [ 0 ] < 100_000 / self . trainer . world_size : self . info = torch . cat ([ self . info , batch [ \"class\" ]]) self . _predict ( gene_pos , expression , depth , knn_cells = knn_cells , knn_cells_info = knn_cells_info , pred_embedding = self . pred_embedding , max_size_in_mem = 120_000 , metacell_token = metacell_token , ) else : self . info = batch [ \"class\" ] self . _predict ( gene_pos , expression , depth , knn_cells = knn_cells , knn_cells_info = knn_cells_info , pred_embedding = self . pred_embedding , max_size_in_mem = 120_000 , metacell_token = metacell_token , ) self . log ( \"val_loss\" , val_loss , sync_dist = True ) expr_loss = mean ( [ v . cpu () . item () if type ( v ) is Tensor else v for k , v in losses . items () if \"expr\" in k ] ) self . log ( \"val_loss_expr\" , expr_loss , sync_dist = True ) cls_loss = mean ( [ v . cpu () . item () if type ( v ) is Tensor else v for k , v in losses . items () if \"cls\" in k ] ) self . log ( \"val_loss_cls\" , cls_loss , sync_dist = True ) # self.log_dict(losses, sync_dist=True) return val_loss","title":"validation_step"},{"location":"model/#losses","text":"","title":"losses"},{"location":"model/#scprint2.model.loss","text":"Classes: Name Description AdversarialDiscriminatorLoss Functions: Name Description contrastive_loss Computes NT-Xent loss (InfoNCE) between two sets of vectors. criterion_neg_log_bernoulli Compute the negative log-likelihood of Bernoulli distribution ecs ecs Computes the similarity of cell embeddings based on a threshold. grad_reverse grad_reverse Reverses the gradient of the input tensor. hierarchical_classification Computes the classification loss for a given batch of predictions and ground truth labels. masked_mae Compute the masked MAE loss between input and target. masked_mse Compute the masked MSE loss between input and target. masked_nb Compute the masked negative binomial loss between input and target. masked_relative_error Compute the masked relative error between input and target. mse Compute the MSE loss between input and target. nb Computes the negative binomial (NB) loss. within_sample Compute dissimilarity between embeddings within each sample zinb Computes zero-inflated negative binomial (ZINB) loss.","title":"loss"},{"location":"model/#scprint2.model.loss.AdversarialDiscriminatorLoss","text":"Bases: Module Discriminator for the adversarial training for batch correction. Parameters: d_model ( int ) \u2013 The size of the input tensor. n_cls ( int ) \u2013 The number of classes. nlayers ( int , default: 3 ) \u2013 The number of layers in the discriminator. Defaults to 3. activation ( callable , default: LeakyReLU ) \u2013 The activation function. Defaults to nn.LeakyReLU. reverse_grad ( bool , default: True ) \u2013 Whether to reverse the gradient. Defaults Methods: Name Description forward Args: Source code in scprint2/model/loss.py 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 def __init__ ( self , d_model : int , n_cls : int , nlayers : int = 3 , activation : Callable = nn . LeakyReLU , reverse_grad : bool = True , ): \"\"\" Discriminator for the adversarial training for batch correction. Args: d_model (int): The size of the input tensor. n_cls (int): The number of classes. nlayers (int, optional): The number of layers in the discriminator. Defaults to 3. activation (callable, optional): The activation function. Defaults to nn.LeakyReLU. reverse_grad (bool, optional): Whether to reverse the gradient. Defaults \"\"\" super () . __init__ () # module list self . decoder = nn . ModuleList () for _ in range ( nlayers - 1 ): self . decoder . append ( nn . Linear ( d_model , d_model )) self . decoder . append ( nn . LayerNorm ( d_model )) self . decoder . append ( activation ()) self . out_layer = nn . Linear ( d_model , n_cls ) self . reverse_grad = reverse_grad","title":"AdversarialDiscriminatorLoss"},{"location":"model/#scprint2.model.loss.AdversarialDiscriminatorLoss.forward","text":"Parameters: x ( Tensor ) \u2013 Tensor, shape [batch_size, embsize] batch_labels ( Tensor ) \u2013 Tensor, shape [batch_size] Source code in scprint2/model/loss.py 348 349 350 351 352 353 354 355 356 357 358 359 def forward ( self , x : Tensor , batch_labels : Tensor ) -> Tensor : \"\"\" Args: x: Tensor, shape [batch_size, embsize] batch_labels: Tensor, shape [batch_size] \"\"\" if self . reverse_grad : x = grad_reverse ( x , lambd = 1.0 ) for layer in self . decoder : x = layer ( x ) x = self . out_layer ( x ) return F . cross_entropy ( x , batch_labels )","title":"forward"},{"location":"model/#scprint2.model.loss.contrastive_loss","text":"Computes NT-Xent loss (InfoNCE) between two sets of vectors. Parameters: x ( Tensor ) \u2013 Tensor of shape [batch_size, feature_dim] y ( Tensor ) \u2013 Tensor of shape [batch_size, feature_dim] temperature ( float , default: 0.1 ) \u2013 Temperature parameter to scale the similarities. Lower values make the model more confident/selective. Typical values are between 0.1 and 0.5. Returns: Tensor ( Tensor ) \u2013 NT-Xent loss value Note Assumes x[i] and y[i] are positive pairs All other combinations are considered negative pairs Uses cosine similarity scaled by temperature Source code in scprint2/model/loss.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def contrastive_loss ( x : Tensor , y : Tensor , temperature : float = 0.1 ) -> Tensor : \"\"\" Computes NT-Xent loss (InfoNCE) between two sets of vectors. Args: x: Tensor of shape [batch_size, feature_dim] y: Tensor of shape [batch_size, feature_dim] temperature: Temperature parameter to scale the similarities. Lower values make the model more confident/selective. Typical values are between 0.1 and 0.5. Returns: Tensor: NT-Xent loss value Note: - Assumes x[i] and y[i] are positive pairs - All other combinations are considered negative pairs - Uses cosine similarity scaled by temperature \"\"\" # Check input dimensions assert x . shape == y . shape , \"Input tensors must have the same shape\" batch_size = x . shape [ 0 ] # Compute cosine similarity matrix # x_unsqueeze: [batch_size, 1, feature_dim] # y_unsqueeze: [1, batch_size, feature_dim] # -> similarities: [batch_size, batch_size] similarities = ( F . cosine_similarity ( x . unsqueeze ( 1 ), y . unsqueeze ( 0 ), dim = 2 ) / temperature ) # The positive pairs are on the diagonal labels = torch . arange ( batch_size , device = x . device ) # Cross entropy loss return F . cross_entropy ( similarities , labels )","title":"contrastive_loss"},{"location":"model/#scprint2.model.loss.criterion_neg_log_bernoulli","text":"Compute the negative log-likelihood of Bernoulli distribution Source code in scprint2/model/loss.py 141 142 143 144 145 146 147 148 def criterion_neg_log_bernoulli ( input : Tensor , target : Tensor , mask : Tensor ) -> Tensor : \"\"\" Compute the negative log-likelihood of Bernoulli distribution \"\"\" mask = mask . float () bernoulli = torch . distributions . Bernoulli ( probs = input ) masked_log_probs = bernoulli . log_prob (( target > 0 ) . float ()) * mask return - masked_log_probs . sum () / mask . sum ()","title":"criterion_neg_log_bernoulli"},{"location":"model/#scprint2.model.loss.ecs","text":"ecs Computes the similarity of cell embeddings based on a threshold. Parameters: cell_emb ( Tensor ) \u2013 A tensor representing cell embeddings. ecs_threshold ( float , default: 0.5 ) \u2013 A threshold for determining similarity. Defaults to 0.5. Returns: Tensor ( Tensor ) \u2013 A tensor representing the mean of 1 minus the square of the difference between the cosine similarity and the threshold. Source code in scprint2/model/loss.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def ecs ( cell_emb : Tensor , ecs_threshold : float = 0.5 ) -> Tensor : \"\"\" ecs Computes the similarity of cell embeddings based on a threshold. Args: cell_emb (Tensor): A tensor representing cell embeddings. ecs_threshold (float, optional): A threshold for determining similarity. Defaults to 0.5. Returns: Tensor: A tensor representing the mean of 1 minus the square of the difference between the cosine similarity and the threshold. \"\"\" # Here using customized cosine similarity instead of F.cosine_similarity # to avoid the pytorch issue of similarity larger than 1.0, pytorch # 78064 # normalize the embedding cell_emb_normed = F . normalize ( cell_emb , p = 2 , dim = 1 ) cos_sim = torch . mm ( cell_emb_normed , cell_emb_normed . t ()) # mask out diagnal elements mask = torch . eye ( cos_sim . size ( 0 )) . bool () . to ( cos_sim . device ) cos_sim = cos_sim . masked_fill ( mask , 0.0 ) # only optimize positive similarities cos_sim = F . relu ( cos_sim ) return torch . mean ( 1 - ( cos_sim - ecs_threshold ) ** 2 )","title":"ecs"},{"location":"model/#scprint2.model.loss.grad_reverse","text":"grad_reverse Reverses the gradient of the input tensor. Parameters: x ( Tensor ) \u2013 The input tensor whose gradient is to be reversed. lambd ( float , default: 1.0 ) \u2013 The scaling factor for the reversed gradient. Defaults to 1.0. Returns: Tensor ( Tensor ) \u2013 The input tensor with its gradient reversed during the backward pass. Source code in scprint2/model/loss.py 373 374 375 376 377 378 379 380 381 382 383 384 def grad_reverse ( x : Tensor , lambd : float = 1.0 ) -> Tensor : \"\"\" grad_reverse Reverses the gradient of the input tensor. Args: x (Tensor): The input tensor whose gradient is to be reversed. lambd (float, optional): The scaling factor for the reversed gradient. Defaults to 1.0. Returns: Tensor: The input tensor with its gradient reversed during the backward pass. \"\"\" return GradReverse . apply ( x , lambd )","title":"grad_reverse"},{"location":"model/#scprint2.model.loss.hierarchical_classification","text":"Computes the classification loss for a given batch of predictions and ground truth labels. Parameters: pred ( Tensor ) \u2013 The predicted logits for the batch. Shape: (batch_size, n_labels) cl ( Tensor ) \u2013 The ground truth labels for the batch. Shape: (batch_size,) labels_hierarchy ( Tensor , default: None ) \u2013 The hierarchical structure of the labels. Defaults to None. A binary tensor of shape (number of parents, n_labels) if not given, will act as a regular classification loss see gist for more details of how one can compute it https://gist.github.com/jkobject/5b36bc4807edb440b86644952a49781e Raises: ValueError \u2013 If the labels_hierarchy is not found while the number of predicted labels is smaller than the number of ground truth labels. Returns: Tensor ( Tensor ) \u2013 The computed binary cross entropy loss for the given batch. Source code in scprint2/model/loss.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def hierarchical_classification ( pred : torch . Tensor , cl : torch . Tensor , labels_hierarchy : Optional [ torch . Tensor ] = None , ) -> torch . Tensor : \"\"\" Computes the classification loss for a given batch of predictions and ground truth labels. Args: pred (Tensor): The predicted logits for the batch. Shape: (batch_size, n_labels) cl (Tensor): The ground truth labels for the batch. Shape: (batch_size,) labels_hierarchy (Tensor, optional): The hierarchical structure of the labels. Defaults to None. A binary tensor of shape (number of parents, n_labels) if not given, will act as a regular classification loss see gist for more details of how one can compute it https://gist.github.com/jkobject/5b36bc4807edb440b86644952a49781e Raises: ValueError: If the labels_hierarchy is not found while the number of predicted labels is smaller than the number of ground truth labels. Returns: Tensor: The computed binary cross entropy loss for the given batch. \"\"\" maxsize = pred . shape [ 1 ] newcl = torch . zeros ( ( pred . shape [ 0 ], maxsize ), device = cl . device ) # batchsize * n_labels # if we don't know the label we set the weight to 0 else to 1 valid_indices = ( cl != - 1 ) & ( cl < maxsize ) valid_cl = cl [ valid_indices ] newcl [ valid_indices , valid_cl ] = 1 weight = torch . ones_like ( newcl , device = cl . device ) # if we don't know the label we set the weight to 0 for all labels weight [ cl == - 1 , :] = 0 # if we have non leaf values, we don't know so we don't compute grad and set weight to 0 # and add labels that won't be counted but so that we can still use them if labels_hierarchy is not None and ( cl >= maxsize ) . any (): is_parent = cl >= maxsize subset_parent_weight = weight [ is_parent ] # we set the weight of the leaf elements for pred where we don't know the leaf, to 0 # i.e. the elements where we will compute the max # in cl, parents are values past the maxsize # (if there is 10 leafs labels, the label 10,14, or 15 is a parent at position # row 0, 4, or 5 in the hierarchy matrix subset_parent_weight [ labels_hierarchy [ cl [ is_parent ] - maxsize ]] = 0 weight [ is_parent ] = subset_parent_weight # we set their lead to 1 (since the weight will be zero, not really usefull..) subset_parent_newcl = newcl [ is_parent ] subset_parent_newcl [ labels_hierarchy [ cl [ is_parent ] - maxsize ]] = 1 newcl [ is_parent ] = subset_parent_newcl # all parental nodes that have a 1 in the labels_hierarchy matrix are set to 1 # for each parent label / row in labels_hierarchy matrix, the addnewcl is # the max of the newcl values where the parent label is 1 newcl_expanded = newcl . unsqueeze ( - 1 ) . expand ( - 1 , - 1 , labels_hierarchy . shape [ 0 ]) addnewcl = torch . max ( newcl_expanded * labels_hierarchy . T , dim = 1 )[ 0 ] # for their weight, it is decreasing based on number of children they have # it is the same here as for parental labels, we don't want to compute # gradients when they are 0 meaning not parents of the true leaf label. # for now we weight related to how many labels they contain. addweight = addnewcl . clone () / ( labels_hierarchy . sum ( 1 ) ** 0.5 ) # except if it is the cl label we know about? subset_parent_weight = addweight [ is_parent ] subset_parent_weight [:, cl [ is_parent ] - maxsize ] = 1 addweight [ is_parent ] = subset_parent_weight # we apply the same mask to the pred but now we want to compute # logsumexp instead of max since we want to keep the gradients # we also set to -inf since it is a more neutral element for logsumexp pred_expanded = ( pred . clone () . unsqueeze ( - 1 ) . expand ( - 1 , - 1 , labels_hierarchy . shape [ 0 ]) ) pred_expanded = pred_expanded * labels_hierarchy . T pred_expanded [ pred_expanded == 0 ] = torch . finfo ( pred . dtype ) . min addpred = torch . logsumexp ( pred_expanded , dim = 1 ) # we add the new labels to the cl newcl = torch . cat ([ newcl , addnewcl ], dim = 1 ) weight = torch . cat ([ weight , addweight ], dim = 1 ) pred = torch . cat ([ pred , addpred ], dim = 1 ) elif labels_hierarchy is None and ( cl >= maxsize ) . any (): raise ValueError ( \"need to use labels_hierarchy for this usecase\" ) myloss = torch . nn . functional . binary_cross_entropy_with_logits ( pred , target = newcl , weight = weight ) return myloss","title":"hierarchical_classification"},{"location":"model/#scprint2.model.loss.masked_mae","text":"Compute the masked MAE loss between input and target. MAE = mean absolute error Source code in scprint2/model/loss.py 39 40 41 42 43 44 45 46 def masked_mae ( input : Tensor , target : Tensor , mask : Tensor ) -> Tensor : \"\"\" Compute the masked MAE loss between input and target. MAE = mean absolute error \"\"\" mask = mask . float () loss = F . l1_loss ( input * mask , target * mask , reduction = \"sum\" ) return loss / mask . sum ()","title":"masked_mae"},{"location":"model/#scprint2.model.loss.masked_mse","text":"Compute the masked MSE loss between input and target. Source code in scprint2/model/loss.py 13 14 15 16 17 18 19 20 21 22 23 def masked_mse ( input : Tensor , target : Tensor , mask : Tensor ) -> Tensor : \"\"\" Compute the masked MSE loss between input and target. \"\"\" mask = mask . float () input = torch . log2 ( input + 1 ) input = ( input / torch . sum ( input , dim = 1 , keepdim = True )) * 10000 target = torch . log2 ( target + 1 ) target = ( target / torch . sum ( target , dim = 1 , keepdim = True )) * 10000 loss = F . mse_loss ( input * mask , target * mask , reduction = \"sum\" ) return loss / mask . sum ()","title":"masked_mse"},{"location":"model/#scprint2.model.loss.masked_nb","text":"Compute the masked negative binomial loss between input and target. Source code in scprint2/model/loss.py 49 50 51 52 53 54 55 56 def masked_nb ( input : Tensor , target : Tensor , mask : Tensor ) -> Tensor : \"\"\" Compute the masked negative binomial loss between input and target. \"\"\" mask = mask . float () nb = torch . distributions . NegativeBinomial ( total_count = target , probs = input ) masked_log_probs = nb . log_prob ( target ) * mask return - masked_log_probs . sum () / mask . sum ()","title":"masked_nb"},{"location":"model/#scprint2.model.loss.masked_relative_error","text":"Compute the masked relative error between input and target. Source code in scprint2/model/loss.py 151 152 153 154 155 156 157 158 159 def masked_relative_error ( input : Tensor , target : Tensor , mask : torch . LongTensor ) -> Tensor : \"\"\" Compute the masked relative error between input and target. \"\"\" assert mask . any () loss = torch . abs ( input [ mask ] - target [ mask ]) / ( target [ mask ] + 1e-5 ) return loss . mean ()","title":"masked_relative_error"},{"location":"model/#scprint2.model.loss.mse","text":"Compute the MSE loss between input and target. Source code in scprint2/model/loss.py 26 27 28 29 30 31 32 33 34 35 36 def mse ( input : Tensor , target : Tensor , mask = False ) -> Tensor : \"\"\" Compute the MSE loss between input and target. \"\"\" if mask : return masked_mse ( input , target , ( target > 0 )) input = torch . log2 ( input + 1 ) input = ( input / torch . sum ( input , dim = 1 , keepdim = True )) * 10000 target = torch . log2 ( target + 1 ) target = ( target / torch . sum ( target , dim = 1 , keepdim = True )) * 10000 return F . mse_loss ( input , target , reduction = \"mean\" )","title":"mse"},{"location":"model/#scprint2.model.loss.nb","text":"Computes the negative binomial (NB) loss. This function was adapted from scvi-tools. Parameters: target ( Tensor ) \u2013 Ground truth data. mu ( Tensor ) \u2013 Means of the negative binomial distribution (must have positive support). theta ( Tensor ) \u2013 Inverse dispersion parameter (must have positive support). eps ( float , default: 0.0001 ) \u2013 Numerical stability constant. Defaults to 1e-4. Returns: Tensor ( Tensor ) \u2013 NB loss value. Source code in scprint2/model/loss.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def nb ( target : Tensor , mu : Tensor , theta : Tensor , eps = 1e-4 ) -> Tensor : \"\"\" Computes the negative binomial (NB) loss. This function was adapted from scvi-tools. Args: target (Tensor): Ground truth data. mu (Tensor): Means of the negative binomial distribution (must have positive support). theta (Tensor): Inverse dispersion parameter (must have positive support). eps (float, optional): Numerical stability constant. Defaults to 1e-4. Returns: Tensor: NB loss value. \"\"\" if theta . ndimension () == 1 : theta = theta . view ( 1 , theta . size ( 0 )) log_theta_mu_eps = torch . log ( theta + mu + eps ) res = ( theta * ( torch . log ( theta + eps ) - log_theta_mu_eps ) + target * ( torch . log ( mu + eps ) - log_theta_mu_eps ) + torch . lgamma ( target + theta ) - torch . lgamma ( theta ) - torch . lgamma ( target + 1 ) ) return - res . mean ()","title":"nb"},{"location":"model/#scprint2.model.loss.within_sample","text":"Compute dissimilarity between embeddings within each sample using a combination of cosine and L2 distance Parameters: cell_embs ( Tensor ) \u2013 tensor of shape [batch_size, num_embeddings, embedding_dim] Source code in scprint2/model/loss.py 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 def within_sample ( cell_embs : Tensor ): \"\"\" Compute dissimilarity between embeddings within each sample using a combination of cosine and L2 distance Args: cell_embs: tensor of shape [batch_size, num_embeddings, embedding_dim] \"\"\" batch_size , num_embeddings , emb_dim = cell_embs . shape # Normalize embeddings for cosine similarity cell_embs_norm = F . normalize ( cell_embs , p = 2 , dim =- 1 ) # Compute pairwise cosine similarities cos_sim = torch . bmm ( cell_embs_norm , cell_embs_norm . transpose ( 1 , 2 )) # Compute pairwise L2 distances (normalized by embedding dimension) l2_dist = torch . cdist ( cell_embs , cell_embs , p = 2 ) / np . sqrt ( emb_dim ) # Create mask for pairs (excluding self-similarity) mask = 1 - torch . eye ( num_embeddings , device = cos_sim . device ) mask = mask . unsqueeze ( 0 ) . expand ( batch_size , - 1 , - 1 ) # Combine losses: # - High cosine similarity should be penalized # - Small L2 distance should be penalized cos_loss = ( cos_sim * mask ) . pow ( 2 ) . mean () l2_loss = 1.0 / ( l2_dist * mask + 1e-3 ) . mean () return 0.5 * cos_loss + 0.5 * l2_loss","title":"within_sample"},{"location":"model/#scprint2.model.loss.zinb","text":"Computes zero-inflated negative binomial (ZINB) loss. This function was modified from scvi-tools. Parameters: target ( Tensor ) \u2013 Torch Tensor of ground truth data. mu ( Tensor ) \u2013 Torch Tensor of means of the negative binomial (must have positive support). theta ( Tensor ) \u2013 Torch Tensor of inverse dispersion parameter (must have positive support). pi ( Tensor ) \u2013 Torch Tensor of logits of the dropout parameter (real support). eps ( float , default: 0.0001 ) \u2013 Numerical stability constant. Defaults to 1e-4. Returns: Tensor ( Tensor ) \u2013 ZINB loss value. Source code in scprint2/model/loss.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def zinb ( target : Tensor , mu : Tensor , theta : Tensor , pi : Tensor , eps = 1e-4 , mask = False , ) -> Tensor : \"\"\" Computes zero-inflated negative binomial (ZINB) loss. This function was modified from scvi-tools. Args: target (Tensor): Torch Tensor of ground truth data. mu (Tensor): Torch Tensor of means of the negative binomial (must have positive support). theta (Tensor): Torch Tensor of inverse dispersion parameter (must have positive support). pi (Tensor): Torch Tensor of logits of the dropout parameter (real support). eps (float, optional): Numerical stability constant. Defaults to 1e-4. Returns: Tensor: ZINB loss value. \"\"\" # uses log(sigmoid(x)) = -softplus(-x) softplus_pi = F . softplus ( - pi ) # eps to make it positive support and taking the log log_theta_mu_eps = torch . log ( theta + mu + eps ) pi_theta_log = - pi + theta * ( torch . log ( theta + eps ) - log_theta_mu_eps ) case_zero = F . softplus ( pi_theta_log ) - softplus_pi mul_case_zero = torch . mul (( target < eps ) . type ( torch . float32 ), case_zero ) case_non_zero = ( - softplus_pi + pi_theta_log + target * ( torch . log ( mu + eps ) - log_theta_mu_eps ) + torch . lgamma ( target + theta ) - torch . lgamma ( theta ) - torch . lgamma ( target + 1 ) ) mul_case_non_zero = torch . mul (( target > eps ) . type ( torch . float32 ), case_non_zero ) res = mul_case_zero + mul_case_non_zero # we want to minize the loss but maximize the log likelyhood if mask : mask = ( target > 0 ) . float () res = res * mask return - res . sum () / mask . sum () return - res . mean ()","title":"zinb"},{"location":"model/#utils","text":"","title":"utils"},{"location":"model/#scprint2.model.utils","text":"Classes: Name Description Attention WeightedMasker Functions: Name Description downsample_profile This function downsamples the expression profile of a given single cell RNA matrix. make_adata This function creates an AnnData object from the given input parameters. simple_masker Randomly mask a batch of data. test Test the given model on the full set of benchmarks and save the results to JSON files. zinb_sample zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution.","title":"utils"},{"location":"model/#scprint2.model.utils.Attention","text":"Initialize the Attention class. Parameters: gene_dim ( int ) \u2013 The dimension of the gene. additional_tokens ( int , default: 0 ) \u2013 The number of additional tokens to add. precomp_attn ( bool , default: False ) \u2013 Whether to compute attention or it is precomputed apply_softmax ( bool , default: True ) \u2013 Whether to apply softmax to the attention. sum_heads ( bool , default: True ) \u2013 Whether to sum the heads. Methods: Name Description add_attn Aggregate the attention or data based on the precomp_attn flag. add_qk Add data to the internal storage. get Get the aggregated attention or data. Source code in scprint2/model/utils.py 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 def __init__ ( self , gene_dim : int , precomp_attn : bool = False , apply_softmax : bool = True , sum_heads : bool = True , additional_tokens : int = 0 , ): \"\"\" Initialize the Attention class. Args: gene_dim (int): The dimension of the gene. additional_tokens (int): The number of additional tokens to add. precomp_attn (bool): Whether to compute attention or it is precomputed apply_softmax (bool): Whether to apply softmax to the attention. sum_heads (bool): Whether to sum the heads. \"\"\" self . data : Optional [ Tensor ] = None self . gene_dim : int = gene_dim self . additional_tokens : int = additional_tokens self . div : Optional [ Tensor ] = None self . apply_softmax : bool = apply_softmax self . sum_heads : bool = sum_heads self . precomp_attn : bool = precomp_attn self . speciesloc : int = 0","title":"Attention"},{"location":"model/#scprint2.model.utils.Attention.add_attn","text":"Aggregate the attention or data based on the precomp_attn flag. Parameters: x ( List [ Tensor ] ) \u2013 List of tensors to aggregate. Tensor of size (batch, seq_len, 2, heads, emb) pos ( Tensor ) \u2013 Position tensor. Source code in scprint2/model/utils.py 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 def add_attn ( self , x : List [ Tensor ], pos : Tensor , expr : Optional [ Tensor ] = None ) -> None : \"\"\" Aggregate the attention or data based on the precomp_attn flag. Args: x (List[Tensor]): List of tensors to aggregate. Tensor of size (batch, seq_len, 2, heads, emb) pos (Tensor): Position tensor. \"\"\" if self . data is None : self . data = torch . zeros ( [ self . gene_dim + self . additional_tokens , self . gene_dim + self . additional_tokens , ], device = pos . device , dtype = torch . float32 , ) self . div = torch . zeros ( 1 , device = pos . device , dtype = torch . float32 ) for i , elem in enumerate ( x ): if self . apply_softmax : attn = torch . nn . functional . softmax ( elem [:, :, 0 , :, :] . permute ( 0 , 2 , 1 , 3 ) @ elem [:, :, 1 , :, :] . permute ( 0 , 2 , 3 , 1 ), dim =- 1 , ) if expr is not None : attn [:, :, self . additional_tokens :, self . additional_tokens :] = ( attn [:, :, self . additional_tokens :, self . additional_tokens :] * ( expr > 0 ) . float () . unsqueeze ( 1 ) . unsqueeze ( - 1 ) * ( expr > 0 ) . float () . unsqueeze ( 1 ) . unsqueeze ( 2 ) ) self . data += attn . sum ( 0 ) . mean ( 0 ) else : self . data [:, :] += ( ( elem [:, :, 0 , :, :] . permute ( 0 , 2 , 1 , 3 ) @ elem [:, :, 1 , :, :] . permute ( 0 , 2 , 3 , 1 ) ) . sum ( 0 ) . mean ( 0 ) ) self . div += 1","title":"add_attn"},{"location":"model/#scprint2.model.utils.Attention.add_qk","text":"Add data to the internal storage. Parameters: x ( List [ Tensor ] ) \u2013 List of tensors to add. pos ( Tensor ) \u2013 Position tensor. Source code in scprint2/model/utils.py 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 def add_qk ( self , x : List [ Tensor ], pos : Tensor , expr : Optional [ Tensor ] = None ) -> None : \"\"\" Add data to the internal storage. Args: x (List[Tensor]): List of tensors to add. pos (Tensor): Position tensor. \"\"\" # this is a debugger line if self . data is None : self . data = torch . zeros ( [ len ( x ), self . gene_dim + self . additional_tokens ] + list ( x [ 0 ] . shape [ 2 :]), device = pos . device , ) self . div = torch . zeros ( self . gene_dim + self . additional_tokens , device = pos . device ) for i in range ( x [ 0 ] . shape [ 0 ]): # batch size loc = torch . cat ( [ torch . arange ( self . additional_tokens , device = pos . device ), pos [ i ] + self . additional_tokens - self . speciesloc , ] ) . int () for j in range ( len ( x )): # number of layers * heads self . data [ j , loc , :, :, :] += x [ j ][ i ] self . div [ loc ] += 1","title":"add_qk"},{"location":"model/#scprint2.model.utils.Attention.get","text":"Get the aggregated attention or data. Returns: Optional [ ndarray ] \u2013 Optional[np.ndarray]: The aggregated attention or data. Source code in scprint2/model/utils.py 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 def get ( self ) -> Optional [ np . ndarray ]: \"\"\" Get the aggregated attention or data. Returns: Optional[np.ndarray]: The aggregated attention or data. \"\"\" if not self . precomp_attn : if self . data is None : return None # shape is (layers, genes, qkv, heads, emb) return self . data / self . div . view ( 1 , self . div . shape [ 0 ], 1 , 1 , 1 ) else : if self . data is None : return None self . data . div_ ( self . div ) return self . data","title":"get"},{"location":"model/#scprint2.model.utils.WeightedMasker","text":"Randomly mask a batch of data. Parameters: genes ( List [ str ] ) \u2013 The list of genes the model might see. TFs ( List [ str ] , default: fileToList ( FILEDIR + '/../../data/main/TFs.txt') ) \u2013 The list of TFs the model can drop. tf_weight ( float , default: 10 ) \u2013 How likely it is to drop a non TF compared to a TF. Source code in scprint2/model/utils.py 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def __init__ ( self , genes : List [ str ], TFs : List [ str ] = utils . fileToList ( FILEDIR + \"/../../data/main/TFs.txt\" ), tf_weight : float = 10 , ): \"\"\" Randomly mask a batch of data. Args: genes (List[str]): The list of genes the model might see. TFs (List[str]): The list of TFs the model can drop. tf_weight (float): How likely it is to drop a non TF compared to a TF. \"\"\" TFs = set ( TFs ) self . weights = torch . tensor ([ tf_weight if gene in TFs else 1 for gene in genes ]) self . max_to_drop = ( self . weights == tf_weight ) . sum () self . tf_weight = tf_weight","title":"WeightedMasker"},{"location":"model/#scprint2.model.utils.downsample_profile","text":"This function downsamples the expression profile of a given single cell RNA matrix. The noise is applied based on the renoise parameter, the total counts of the matrix, and the number of genes. The function first calculates the noise threshold (scaler) based on the renoise parameter. It then generates an initial matrix count by applying a Poisson distribution to a random tensor scaled by the total counts and the number of genes. The function then models the sampling zeros by applying a Poisson distribution to a random tensor scaled by the noise threshold, the total counts, and the number of genes. The function also models the technical zeros by generating a random tensor and comparing it to the noise threshold. The final matrix count is calculated by subtracting the sampling zeros from the initial matrix count and multiplying by the technical zeros. The function ensures that the final matrix count is not less than zero by taking the maximum of the final matrix count and a tensor of zeros. The function returns the final matrix count. Parameters: mat ( Tensor ) \u2013 The input matrix. dropout ( float ) \u2013 The renoise parameter. Returns: Tensor \u2013 torch.Tensor: The matrix count after applying noise. Source code in scprint2/model/utils.py 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 def downsample_profile ( mat : Tensor , dropout : float , method = \"new\" , randsamp = False ) -> Tensor : \"\"\" This function downsamples the expression profile of a given single cell RNA matrix. The noise is applied based on the renoise parameter, the total counts of the matrix, and the number of genes. The function first calculates the noise threshold (scaler) based on the renoise parameter. It then generates an initial matrix count by applying a Poisson distribution to a random tensor scaled by the total counts and the number of genes. The function then models the sampling zeros by applying a Poisson distribution to a random tensor scaled by the noise threshold, the total counts, and the number of genes. The function also models the technical zeros by generating a random tensor and comparing it to the noise threshold. The final matrix count is calculated by subtracting the sampling zeros from the initial matrix count and multiplying by the technical zeros. The function ensures that the final matrix count is not less than zero by taking the maximum of the final matrix count and a tensor of zeros. The function returns the final matrix count. Args: mat (torch.Tensor): The input matrix. dropout (float): The renoise parameter. Returns: torch.Tensor: The matrix count after applying noise. \"\"\" # Randomly drop on average N counts to each element of expression using a heavy tail Gaussian distribution # here we try to get the scale of the distribution so as to remove the right number of counts from each gene # https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02601-5#:~:text=Zero%20measurements%20in%20scRNA%2Dseq,generation%20of%20scRNA%2Dseq%20data. if randsamp : dropout = torch . rand ( mat . shape [ 0 ], device = mat . device ) * dropout dropout = ( dropout . unsqueeze ( 1 ) if len ( mat . shape ) == 2 else dropout . unsqueeze ( 1 ) . unsqueeze ( 1 ) ) if method == \"old\" : totcounts = mat . sum ( - 1 ) ngenes = mat . shape [ - 1 ] tnoise = 1 - ( 1 - dropout ) ** ( 1 / 2 ) # we model the sampling zeros (dropping 30% of the reads) res = torch . poisson ( torch . rand ( mat . shape , device = mat . device ) * (( tnoise * totcounts . unsqueeze ( - 1 )) / ( 0.5 * ngenes )) ) . int () # we model the technical zeros (dropping 50% of the genes) drop = ( torch . rand ( mat . shape , device = mat . device ) > tnoise ) . int () mat = ( mat - res ) * drop return torch . maximum ( mat , torch . zeros ( ( 1 , 1 ) if len ( mat . shape ) == 2 else ( 1 , 1 , 1 ), device = mat . device , dtype = torch . int , ), ) elif method == \"jules\" : scaler = ( 1 - dropout ) ** ( 1 / 2 ) notdrop = ( torch . rand ( mat . shape , device = mat . device , ) < scaler ) . int () notdrop [ mat == 0 ] = 0 # apply the dropout after the poisson, right? return notdrop * torch . poisson ( mat * scaler ) elif method == \"new\" : dropout = dropout * 1.1 # we model the sampling zeros (dropping 30% of the reads) res = torch . poisson (( mat * ( dropout / 2 ))) . int () # we model the technical zeros (dropping 50% of the genes) notdrop = ( torch . rand ( mat . shape , device = mat . device ) >= ( dropout / 2 )) . int () mat = ( mat - res ) * notdrop return torch . maximum ( mat , torch . zeros ( ( 1 , 1 ) if len ( mat . shape ) == 2 else ( 1 , 1 , 1 ), device = mat . device , dtype = torch . int , ), ) else : raise ValueError ( f \"method { method } not recognized\" )","title":"downsample_profile"},{"location":"model/#scprint2.model.utils.make_adata","text":"This function creates an AnnData object from the given input parameters. Parameters: genes ( list ) \u2013 List of genes that will be used as variable names. embs ( Tensor | Dict ) \u2013 Embeddings of the cells. The shape of the tensor is (n_cells, n_features). if multiple, it is a dict of name -> tensor pos ( Tensor , default: None ) \u2013 Positions of the cells. The shape of the tensor is (n_cells,). expr_pred ( List [ Tensor ] , default: None ) \u2013 Predicted expression. The shape of the tensors are (n_cells, n_genes). the first is mu, the second theta, the third pi if present classes ( list , default: None ) \u2013 List of classes, the order should be the same as in the pred and gtclass tensors. pred ( Tensor , default: None ) \u2013 Predicted labels. The shape of the tensor is (n_cells, n_classes). Default is None. label_decoders ( dict , default: None ) \u2013 Dictionary to map class codes to class names. Default is None. labels_hierarchy ( dict , default: None ) \u2013 Dictionary representing the hierarchy of labels. Default is {}. see the model for defintion. gtclass ( Tensor , default: None ) \u2013 Ground truth class values. Default is None. doplot ( bool , default: True ) \u2013 Whether to generate plots. Default is True. Returns: AnnData \u2013 anndata.AnnData: The created AnnData object. Source code in scprint2/model/utils.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def make_adata ( genes : List [ str ], embs : Union [ Tensor , Dict [ str , Tensor ]], pos : Tensor = None , expr_pred : List [ Tensor ] = None , classes : List [ str ] = None , pred : Tensor = None , label_decoders : Optional [ Dict ] = None , labels_hierarchy : Optional [ Dict ] = None , gtclass : Optional [ Tensor ] = None , doplot : bool = True , ) -> AnnData : \"\"\" This function creates an AnnData object from the given input parameters. Args: genes (list): List of genes that will be used as variable names. embs (torch.Tensor|Dict): Embeddings of the cells. The shape of the tensor is (n_cells, n_features). if multiple, it is a dict of name -> tensor pos (torch.Tensor): Positions of the cells. The shape of the tensor is (n_cells,). expr_pred (List[torch.Tensor]): Predicted expression. The shape of the tensors are (n_cells, n_genes). the first is mu, the second theta, the third pi if present classes (list): List of classes, the order should be the same as in the pred and gtclass tensors. pred (torch.Tensor, optional): Predicted labels. The shape of the tensor is (n_cells, n_classes). Default is None. label_decoders (dict, optional): Dictionary to map class codes to class names. Default is None. labels_hierarchy (dict, optional): Dictionary representing the hierarchy of labels. Default is {}. see the model for defintion. gtclass (torch.Tensor, optional): Ground truth class values. Default is None. doplot (bool, optional): Whether to generate plots. Default is True. Returns: anndata.AnnData: The created AnnData object. \"\"\" print ( \"logging the anndata\" ) colname = [ \"pred_\" + i for i in classes ] if pred is not None : obs = np . array ( pred . to ( device = \"cpu\" , dtype = torch . int32 )) # label decoders is not cls_decoders. one is a dict to map class codes (ints) # to class names the other is the module the predict the class if label_decoders is not None : obs = np . array ( [ [ label_decoders [ classes [ i ]][ n ] for n in name ] for i , name in enumerate ( obs . T ) ] ) . T if gtclass is not None : colname += classes nobs = np . array ( gtclass . to ( device = \"cpu\" , dtype = torch . int32 )) if label_decoders is not None : nobs = np . array ( [ [ label_decoders [ classes [ i ]][ n ] for n in name ] for i , name in enumerate ( nobs . T ) ] ) . T obs = np . hstack ([ obs , nobs ]) n_cells = embs [ list ( embs . keys ())[ 0 ]] . shape [ 0 ] layers = None size = len ( genes ) if pos is not None : minval = pos . min () maxval = pos . max () genes = genes [ minval : maxval + 1 ] size = len ( genes ) pos = pos - minval mu_array = np . zeros (( n_cells , size ), dtype = np . float32 ) pos = pos . cpu () . numpy () # Create empty array with same shape as expr_pred[0] # Fill array with values from expr_pred[0] for idx in range ( n_cells ): mu_array [ idx , pos [ idx ]] = expr_pred [ 0 ][ idx ] . cpu () . numpy () + 1 exist = mu_array . sum ( 0 ) != 0 mu_array = mu_array [:, exist ] mu_array [ mu_array == 1 ] = 0 layers = { \"scprint_mu\" : mu_array , # \"used_scprint\": csr_matrix(pos), } if len ( expr_pred ) > 1 : theta_array = np . zeros (( n_cells , size ), dtype = np . float32 ) # Fill array with values from expr_pred[0] for idx in range ( n_cells ): theta_array [ idx , pos [ idx ]] = expr_pred [ 1 ][ idx ] . cpu () . numpy () layers [ \"scprint_theta\" ] = theta_array [:, exist ] pi_array = np . zeros (( n_cells , size ), dtype = np . float32 ) # Fill array with values from expr_pred[0] for idx in range ( n_cells ): pi_array [ idx , pos [ idx ]] = expr_pred [ 2 ][ idx ] . cpu () . numpy () layers [ \"scprint_pi\" ] = pi_array [:, exist ] genes = [ n for i , n in enumerate ( genes ) if exist [ i ] > 0 ] else : genes = [] adata = AnnData ( X = csr_matrix (( n_cells , len ( genes ))), layers = layers , obs = ( pd . DataFrame ( obs , columns = colname , ) if pred is not None else None ), var = pd . DataFrame ( index = genes ), ) for k , v in embs . items (): adata . obsm [ \"scprint_emb_\" + k ] = v . cpu () . numpy () rep = \"scprint_emb_\" + k del embs accuracy = {} if labels_hierarchy is None : labels_hierarchy = {} if pred is not None : for clss in classes : if gtclass is not None : tr = translate ( set ( adata . obs [ clss ]), clss ) if tr is not None : adata . obs [ \"conv_\" + clss ] = adata . obs [ clss ] . replace ( tr ) tr = translate ( set ( adata . obs [ \"pred_\" + clss ]), clss ) if tr is not None : adata . obs [ \"conv_pred_\" + clss ] = adata . obs [ \"pred_\" + clss ] . replace ( tr ) res = [] if label_decoders is not None and gtclass is not None : class_topred = label_decoders [ clss ] . values () if clss in labels_hierarchy : cur_labels_hierarchy = { label_decoders [ clss ][ k ]: [ label_decoders [ clss ][ i ] for i in v ] for k , v in labels_hierarchy [ clss ] . items () } else : cur_labels_hierarchy = {} for pred , true in adata . obs [[ \"pred_\" + clss , clss ]] . values : if pred == true : res . append ( True ) continue if len ( labels_hierarchy ) > 0 : if true in cur_labels_hierarchy : res . append ( pred in cur_labels_hierarchy [ true ]) elif true not in class_topred : raise ValueError ( f \"true label { true } not in available classes\" ) elif true != \"unknown\" : res . append ( False ) elif true not in class_topred : raise ValueError ( f \"true label { true } not in available classes\" ) elif true != \"unknown\" : res . append ( False ) else : pass accuracy [ \"pred_\" + clss ] = sum ( res ) / len ( res ) if len ( res ) > 0 else 0 adata . obs = adata . obs . astype ( \"category\" ) print ( adata ) if doplot and adata . shape [ 0 ] > 100 : sc . pp . neighbors ( adata , use_rep = rep ) sc . tl . umap ( adata ) sc . tl . leiden ( adata , key_added = \"sprint_leiden\" ) if gtclass is not None : color = [ i for pair in zip ( [ \"conv_\" + i if \"conv_\" + i in adata . obs . columns else i for i in classes ], [ ( \"conv_pred_\" + i if \"conv_pred_\" + i in adata . obs . columns else \"pred_\" + i ) for i in classes ], ) for i in pair ] fig , axs = plt . subplots ( int ( len ( color ) / 2 ), 2 , figsize = ( 24 , len ( color ) * 4 ) ) plt . subplots_adjust ( wspace = 1 ) if len ( color ) > 2 : for i , col in enumerate ( color ): sc . pl . umap ( adata , color = col , ax = axs [ i // 2 , i % 2 ], show = False , ) acc = \"\" if \"pred_\" in col and col . split ( \"conv_\" )[ - 1 ] in accuracy : acc = \" (accuracy: {:.2f} )\" . format ( accuracy [ col . split ( \"conv_\" )[ - 1 ]] ) axs [ i // 2 , i % 2 ] . set_title ( col + \" UMAP\" + acc ) if \"cell_type\" in col : axs [ i // 2 , i % 2 ] . legend ( fontsize = \"x-small\" ) axs [ i // 2 , i % 2 ] . set_xlabel ( \"UMAP1\" ) axs [ i // 2 , i % 2 ] . set_ylabel ( \"UMAP2\" ) else : for i , col in enumerate ( color ): sc . pl . umap ( adata , color = col , ax = axs [ i % 2 ], show = False , ) acc = \"\" if \"pred_\" in col and col . split ( \"conv_\" )[ - 1 ] in accuracy : acc = \" (accuracy: {:.2f} )\" . format ( accuracy [ col . split ( \"conv_\" )[ - 1 ]] ) axs [ i % 2 ] . set_title ( col + \" UMAP\" + acc ) if \"cell_type\" in col : axs [ i % 2 ] . legend ( fontsize = \"x-small\" ) axs [ i % 2 ] . set_xlabel ( \"UMAP1\" ) axs [ i % 2 ] . set_ylabel ( \"UMAP2\" ) else : color = [ ( \"conv_pred_\" + i if \"conv_pred_\" + i in adata . obs . columns else \"pred_\" + i ) for i in classes ] if len ( color ) > 1 : fig , axs = plt . subplots ( len ( color ), 1 , figsize = ( 16 , len ( color ) * 8 )) for i , col in enumerate ( color ): sc . pl . umap ( adata , color = col , ax = axs [ i ], show = False , ) acc = \"\" if \"pred_\" in col and col . split ( \"conv_\" )[ - 1 ] in accuracy : acc = \" (accuracy: {:.2f} )\" . format ( accuracy [ col . split ( \"conv_\" )[ - 1 ]] ) axs [ i ] . set_title ( col + \" UMAP of \" + rep + \" embedding \" + acc ) axs [ i ] . set_xlabel ( \"UMAP1\" ) axs [ i ] . set_ylabel ( \"UMAP2\" ) else : fig = sc . pl . umap ( adata , color = color , show = False , return_fig = True , ) plt . show () else : fig = None return adata , fig","title":"make_adata"},{"location":"model/#scprint2.model.utils.simple_masker","text":"Randomly mask a batch of data. Parameters: shape ( List [ int ] ) \u2013 The shape of the data. mask_ratio ( float , default: 0.15 ) \u2013 The ratio of genes to mask, default to 0.15. Returns: Tensor \u2013 torch.Tensor: A tensor of masked data. Source code in scprint2/model/utils.py 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 def simple_masker ( shape : List [ int ], mask_ratio : float = 0.15 , ) -> torch . Tensor : \"\"\" Randomly mask a batch of data. Args: shape (List[int]): The shape of the data. mask_ratio (float): The ratio of genes to mask, default to 0.15. Returns: torch.Tensor: A tensor of masked data. \"\"\" return torch . rand ( shape ) < mask_ratio","title":"simple_masker"},{"location":"model/#scprint2.model.utils.test","text":"Test the given model on the full set of benchmarks and save the results to JSON files. Parameters: model ( Module ) \u2013 The model to be tested. filedir ( str ) \u2013 The directory where the data files are located. do_class ( bool , default: True ) \u2013 Whether to perform classification. Defaults to True. maxcells_grn ( int , default: 1024 ) \u2013 Maximum cells for GRN analysis. Defaults to 1024. Returns: None \u2013 None Source code in scprint2/model/utils.py 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 def test ( model : torch . nn . Module , filedir : str , do_class : bool = True , maxcells_grn : int = 1024 , ) -> None : \"\"\" Test the given model on the full set of benchmarks and save the results to JSON files. Args: model (torch.nn.Module): The model to be tested. filedir (str): The directory where the data files are located. do_class (bool): Whether to perform classification. Defaults to True. maxcells_grn (int): Maximum cells for GRN analysis. Defaults to 1024. Returns: None \"\"\" metrics = {} tot = {} for dataset , path in EMBEDDING_DATASETS . items (): res = embbed_task . default_benchmark ( model , dataset = path , do_class = do_class , coarse = False , ) tot [ \"embed_\" + dataset ] = res metrics . update ( { \"emb_\" + dataset + \"/scib\" : float ( res [ \"scib\" ][ \"Total\" ]), \"emb_\" + dataset + \"/scib_bio\" : float ( res [ \"scib\" ][ \"Bio conservation\" ]), \"emb_\" + dataset + \"/scib_batch\" : float ( res [ \"scib\" ][ \"Batch correction\" ]), \"emb_\" + dataset + \"/ct_class\" : float ( res [ \"classif\" ] . get ( \"cell_type_ontology_term_id\" , {}) . get ( \"macro\" , 0 ) if do_class else 0 ), \"emb_\" + dataset + \"/ct_class_macro\" : float ( res [ \"classif\" ] . get ( \"cell_type_ontology_term_id\" , {}) . get ( \"macro\" , 0 ) if do_class else 0 ), } ) print ( metrics ) gc . collect () for dataset , filepath in DENOISE_DATASETS . items (): res = denoise_task . default_benchmark ( model , dataset = filepath ) tot [ \"denoise_\" + dataset ] = res metrics . update ( { \"denoise_\" + dataset + \"/reco2full_vs_noisy2full\" : float ( res [ \"reco2full\" ] - res [ \"noisy2full\" ] ), } ) print ( metrics ) gc . collect () res = grn_task . default_benchmark ( model , \"gwps\" , batch_size = 32 if model . d_model <= 512 else 8 , maxcells = maxcells_grn , ) tot [ \"grn_gwps\" ] = res metrics . update ( { \"grn_gwps/auprc_self\" : float ( res [ \"self\" ][ \"auprc\" ]), \"grn_gwps/epr_self\" : float ( res [ \"self\" ][ \"epr\" ]), \"grn_gwps/auprc_omni\" : float ( res [ \"omni\" ][ \"auprc\" ]), \"grn_gwps/epr_omni\" : float ( res [ \"omni\" ][ \"epr\" ]), \"grn_gwps/auprc\" : float ( res [ \"mean\" ][ \"auprc\" ]), \"grn_gwps/epr\" : float ( res [ \"mean\" ][ \"epr\" ]), } ) print ( metrics ) gc . collect () for dataset , filepath in { \"old_kidney\" : \"https://datasets.cellxgene.cziscience.com/ede85b09-454b-4374-bf60-5f675e989b64.h5ad\" , # \"kidney\": \"https://datasets.cellxgene.cziscience.com/01bc7039-961f-4c24-b407-d535a2a7ba2c.h5ad\", \"lung_smart\" : \"https://datasets.cellxgene.cziscience.com/6ebba0e0-a159-406f-8095-451115673a2c.h5ad\" , # filedir + \"/../../data/yBCKp6HmXuHa0cZptMo7.h5ad\", } . items (): res = grn_task . default_benchmark ( model , filepath , # kidney dataset (2.87, 1.27) (0.00147, 0.00133) batch_size = 32 if model . d_model <= 512 else 8 , maxcells = maxcells_grn , maxgenes = 4000 , ) tot [ \"grn_omni_\" + dataset ] = res metrics . update ( { \"grn_omni_\" + dataset + \"/auprc_class\" : float ( np . mean ([ i [ \"auprc\" ] for k , i in res . items () if \"_class\" in k ]) ), \"grn_omni_\" + dataset + \"/or_class\" : float ( np . mean ([ i [ \"odd_ratio\" ] for k , i in res . items () if \"_class\" in k ]) ), \"grn_omni_\" + dataset + \"/tf_enr_class\" : float ( np . sum ( [ i . get ( \"TF_enr\" , False ) for k , i in res . items () if \"_class\" in k ] ) ), \"grn_omni_\" + dataset + \"/tf_targ_enr_class\" : float ( np . mean ( [ i [ \"significant_enriched_TFtargets\" ] for k , i in res . items () if \"_class\" in k ] ) ), \"grn_omni_\" + dataset + \"/auprc\" : float ( np . mean ([ i [ \"auprc\" ] for k , i in res . items () if \"_mean\" in k ]) ), \"grn_omni_\" + dataset + \"/epr\" : float ( np . mean ([ i [ \"epr\" ] for k , i in res . items () if \"_mean\" in k ]) ), \"grn_omni_\" + dataset + \"/or\" : float ( np . mean ([ i [ \"odd_ratio\" ] for k , i in res . items () if \"_mean\" in k ]) ), \"grn_omni_\" + dataset + \"/tf_enr\" : float ( np . sum ( [ i . get ( \"TF_enr\" , False ) for k , i in res . items () if \"_mean\" in k ] ) ), \"grn_omni_\" + dataset + \"/tf_targ_enr\" : float ( np . mean ( [ i [ \"significant_enriched_TFtargets\" ] for k , i in res . items () if \"_mean\" in k ] ) ), # 'grn_omni/ct': res['classif']['cell_type_ontology_term_id']['accuracy'], } ) print ( metrics ) gc . collect () return metrics , tot","title":"test"},{"location":"model/#scprint2.model.utils.zinb_sample","text":"zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution. Parameters: mu ( Tensor ) \u2013 The mean of the Negative Binomial (NB) distribution. theta ( Tensor ) \u2013 The dispersion parameter of the NB distribution. zi_probs ( Tensor ) \u2013 The zero-inflation probabilities. sample_shape ( Size , default: Size ([]) ) \u2013 The output shape. Defaults to torch.Size([]). Returns: Tensor \u2013 torch.Tensor: A sample from the ZINB distribution. Source code in scprint2/model/utils.py 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 def zinb_sample ( mu : torch . Tensor , theta : torch . Tensor , zi_probs : torch . Tensor , sample_shape : torch . Size = torch . Size ([]), ) -> torch . Tensor : \"\"\" zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution. Args: mu (torch.Tensor): The mean of the Negative Binomial (NB) distribution. theta (torch.Tensor): The dispersion parameter of the NB distribution. zi_probs (torch.Tensor): The zero-inflation probabilities. sample_shape (torch.Size, optional): The output shape. Defaults to torch.Size([]). Returns: torch.Tensor: A sample from the ZINB distribution. \"\"\" concentration = theta rate = theta / mu # Important remark: Gamma is parametrized by the rate = 1/scale! gamma_d = Gamma ( concentration = concentration , rate = rate ) p_means = gamma_d . sample ( sample_shape ) # Clamping as distributions objects can have buggy behaviors when # their parameters are too high l_train = torch . clamp ( p_means , max = 1e8 ) samp = Poisson ( l_train ) . sample () # Shape : (n_samples, n_cells_batch, n_vars) is_zero = torch . rand_like ( samp ) <= zi_probs samp_ = torch . where ( is_zero , torch . zeros_like ( samp ), samp ) return samp_","title":"zinb_sample"},{"location":"model/#encoder-and-decoder-modules","text":"","title":"encoder and decoder modules"},{"location":"model/#scprint2.model.encoders","text":"Classes: Name Description CategoryValueEncoder ContinuousValueEncoder DPositionalEncoding The PositionalEncoding module applies a positional encoding to a sequence of vectors. EasyExprGNN ExprBasedFT GNN GeneEncoder PositionalEncoding","title":"encoders"},{"location":"model/#scprint2.model.encoders.CategoryValueEncoder","text":"Bases: Module Encodes categorical values into a vector using an embedding layer and layer normalization. Parameters: num_embeddings ( int ) \u2013 The number of possible values. embedding_dim ( int ) \u2013 The dimension of the output vectors. padding_idx ( int , default: None ) \u2013 The index of the padding token. Defaults to None. Note: not used in the current version of scprint-2. Source code in scprint2/model/encoders.py 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 def __init__ ( self , num_embeddings : int , embedding_dim : int , padding_idx : Optional [ int ] = None , ): \"\"\" Encodes categorical values into a vector using an embedding layer and layer normalization. Args: num_embeddings (int): The number of possible values. embedding_dim (int): The dimension of the output vectors. padding_idx (int, optional): The index of the padding token. Defaults to None. Note: not used in the current version of scprint-2. \"\"\" super ( CategoryValueEncoder , self ) . __init__ () self . embedding = nn . Embedding ( num_embeddings , embedding_dim , padding_idx = padding_idx )","title":"CategoryValueEncoder"},{"location":"model/#scprint2.model.encoders.ContinuousValueEncoder","text":"Bases: Module Encode real number values to a vector using neural nets projection. Parameters: d_model ( int ) \u2013 The dimension of the input vectors. dropout ( float , default: 0.1 ) \u2013 The dropout rate to apply to the output of the positional encoding. max_value ( int , default: 100000 ) \u2013 The maximum value of the input. Defaults to 100_000. layers ( int , default: 1 ) \u2013 The number of layers in the encoder. Defaults to 1. size ( int , default: 1 ) \u2013 The size of the input. Defaults to 1. Methods: Name Description forward Args: Source code in scprint2/model/encoders.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 def __init__ ( self , d_model : int , dropout : float = 0.1 , max_value : int = 100_000 , layers : int = 1 , size : int = 1 , ): \"\"\" Encode real number values to a vector using neural nets projection. Args: d_model (int): The dimension of the input vectors. dropout (float, optional): The dropout rate to apply to the output of the positional encoding. max_value (int, optional): The maximum value of the input. Defaults to 100_000. layers (int, optional): The number of layers in the encoder. Defaults to 1. size (int, optional): The size of the input. Defaults to 1. \"\"\" super ( ContinuousValueEncoder , self ) . __init__ () self . max_value = max_value self . encoder = nn . ModuleList () self . output_dim = d_model # self.mask_value = nn.Embedding(1, d_model) self . encoder . append ( nn . Linear ( size , d_model )) for _ in range ( layers - 1 ): self . encoder . append ( nn . LayerNorm ( d_model )) self . encoder . append ( nn . ReLU ()) self . encoder . append ( nn . Dropout ( p = dropout )) self . encoder . append ( nn . Linear ( d_model , d_model ))","title":"ContinuousValueEncoder"},{"location":"model/#scprint2.model.encoders.ContinuousValueEncoder.forward","text":"Parameters: x ( Tensor ) \u2013 Tensor, shape [batch_size, seq_len] Source code in scprint2/model/encoders.py 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def forward ( self , x : Tensor , mask : Tensor = None ) -> Tensor : \"\"\" Args: x: Tensor, shape [batch_size, seq_len] \"\"\" # expand last dimension x = x . unsqueeze ( - 1 ) # use the mask embedding when x=-1 # mask = (x == -1).float() x = torch . clamp ( x , min = 0 , max = self . max_value ) for val in self . encoder : x = val ( x ) if mask is not None : x = x . masked_fill_ ( mask . unsqueeze ( - 1 ), 0 ) # x = x.masked_fill_(mask.unsqueeze(-1), self.mask_value(0)) return x","title":"forward"},{"location":"model/#scprint2.model.encoders.DPositionalEncoding","text":"Bases: Module The PositionalEncoding module applies a positional encoding to a sequence of vectors. This is necessary for the Transformer model, which does not have any inherent notion of position in a sequence. The positional encoding is added to the input embeddings and allows the model to attend to positions in the sequence. Parameters: d_model ( int ) \u2013 The dimension of the input vectors. max_len_x ( int ) \u2013 The maximum length in the x dimension. max_len_y ( int ) \u2013 The maximum length in the y dimension. maxvalue_x ( float , default: 10000.0 ) \u2013 Maximum value for x dimension scaling. Defaults to 10000.0. maxvalue_y ( float , default: 10000.0 ) \u2013 Maximum value for y dimension scaling. Defaults to 10000.0. Note: not used in the current version of scprint-2. Methods: Name Description forward Args: Source code in scprint2/model/encoders.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def __init__ ( self , d_model : int , max_len_x : int , max_len_y : int , maxvalue_x = 10000.0 , maxvalue_y = 10000.0 , ): super ( DPositionalEncoding , self ) . __init__ () position2 = torch . arange ( max_len_y ) . unsqueeze ( 1 ) position1 = torch . arange ( max_len_x ) . unsqueeze ( 1 ) half_n = d_model // 2 div_term2 = torch . exp ( torch . arange ( 0 , half_n , 2 ) * ( - math . log ( maxvalue_y ) / d_model ) ) div_term1 = torch . exp ( torch . arange ( 0 , half_n , 2 ) * ( - math . log ( maxvalue_x ) / d_model ) ) pe1 = torch . zeros ( max_len_x , 1 , d_model ) pe2 = torch . zeros ( max_len_y , 1 , d_model ) pe1 [:, 0 , 0 : half_n : 2 ] = torch . sin ( position1 * div_term1 ) pe1 [:, 0 , 1 : half_n : 2 ] = torch . cos ( position1 * div_term1 ) pe2 [:, 0 , half_n :: 2 ] = torch . sin ( position2 * div_term2 ) pe2 [:, 0 , 1 + half_n :: 2 ] = torch . cos ( position2 * div_term2 ) # https://github.com/tatp22/multidim-positional-encoding/blob/master/positional_encodings/torch_encodings.py self . register_buffer ( \"pe1\" , pe1 ) self . register_buffer ( \"pe2\" , pe2 )","title":"DPositionalEncoding"},{"location":"model/#scprint2.model.encoders.DPositionalEncoding.forward","text":"Parameters: x ( Tensor ) \u2013 Tensor, shape [seq_len, batch_size, embedding_dim] Source code in scprint2/model/encoders.py 234 235 236 237 238 239 240 241 def forward ( self , x : Tensor , pos_x : Tensor , pos_y : Tensor ) -> Tensor : \"\"\" Args: x: Tensor, shape [seq_len, batch_size, embedding_dim] \"\"\" x = x + self . pe1 [ pos_x ] x = x + self . pe2 [ pos_y ] return x","title":"forward"},{"location":"model/#scprint2.model.encoders.EasyExprGNN","text":"Bases: Module Easy Expression Graph Neural Network The main GNN used in scPRINT-2 for expression encoding. It is inspired from the DeepSets architecture to aggregate neighbor information. Parameters: self_dim ( int , default: 64 ) \u2013 Dimension of the self features output_dim ( int , default: 32 ) \u2013 Output dimension self_layers ( int , default: 2 ) \u2013 Number of layers for self features dropout ( float , default: 0.1 ) \u2013 Dropout rate shared_layers ( int , default: 2 ) \u2013 Number of shared layers neighbors_layers ( int , default: 2 ) \u2013 Number of layers for neighbors features Methods: Name Description forward Forward pass of the Easy Expression GNN Source code in scprint2/model/encoders.py 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 def __init__ ( self , self_dim : int = 64 , output_dim : int = 32 , self_layers : int = 2 , dropout : float = 0.1 , shared_layers : int = 2 , neighbors_layers : int = 2 , ): \"\"\" Easy Expression Graph Neural Network The main GNN used in scPRINT-2 for expression encoding. It is inspired from the DeepSets architecture to aggregate neighbor information. Args: self_dim (int): Dimension of the self features output_dim (int): Output dimension self_layers (int): Number of layers for self features dropout (float): Dropout rate shared_layers (int): Number of shared layers neighbors_layers (int): Number of layers for neighbors features \"\"\" super ( EasyExprGNN , self ) . __init__ () self . output_dim = output_dim self . self_dim = self_dim # neighbors self . neighbors_layers = nn . ModuleList () self . neighbors_layers . append ( nn . Linear ( 2 , self_dim // 2 )) for i in range ( neighbors_layers - 1 ): self . neighbors_layers . append ( nn . LayerNorm ( self_dim // 2 )) self . neighbors_layers . append ( nn . ReLU ()) self . neighbors_layers . append ( nn . Dropout ( p = dropout )) self . neighbors_layers . append ( nn . Linear ( self_dim // 2 , self_dim // 2 )) # self self . self_layers = nn . ModuleList () self . self_layers . append ( nn . Linear ( 1 , self_dim // 2 )) for i in range ( self_layers - 1 ): self . self_layers . append ( nn . LayerNorm ( self_dim // 2 )) self . self_layers . append ( nn . ReLU ()) self . self_layers . append ( nn . Dropout ( p = dropout )) self . self_layers . append ( nn . Linear ( self_dim // 2 , self_dim // 2 )) # shared self . shared_layers = nn . ModuleList () for i in range ( shared_layers - 1 ): self . shared_layers . append ( nn . Linear ( self_dim , self_dim )) self . shared_layers . append ( nn . LayerNorm ( self_dim )) self . shared_layers . append ( nn . ReLU ()) self . shared_layers . append ( nn . Dropout ( p = dropout )) self . shared_layers . append ( nn . Linear ( self_dim , output_dim ))","title":"EasyExprGNN"},{"location":"model/#scprint2.model.encoders.EasyExprGNN.forward","text":"Forward pass of the Easy Expression GNN Parameters: expr ( Optional [ Tensor ] , default: None ) \u2013 Tensor of shape (batch, seq_len) representing expression values neighbors ( Optional [ Tensor ] , default: None ) \u2013 Tensor of shape (batch, seq_len, n_neighbors) representing neighbor indices edge_info ( Optional [ Tensor ] , default: None ) \u2013 Tensor of shape (batch, seq_len, n_neighbors) representing edge information mask ( Optional [ Tensor ] , default: None ) \u2013 Tensor of shape (batch, seq_len) representing mask for the input Returns: Tensor \u2013 Tensor of shape (batch, seq_len, output_dim) representing the output features Source code in scprint2/model/encoders.py 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 def forward ( self , expr : Optional [ Tensor ] = None , neighbors : Optional [ Tensor ] = None , edge_info : Optional [ Tensor ] = None , mask : Optional [ Tensor ] = None , ) -> Tensor : \"\"\" Forward pass of the Easy Expression GNN Args: expr: Tensor of shape (batch, seq_len) representing expression values neighbors: Tensor of shape (batch, seq_len, n_neighbors) representing neighbor indices edge_info: Tensor of shape (batch, seq_len, n_neighbors) representing edge information mask: Tensor of shape (batch, seq_len) representing mask for the input Returns: Tensor of shape (batch, seq_len, output_dim) representing the output features \"\"\" # batch, seq_len, neighbs if neighbors is None : neighbors = torch . zeros ( ( expr . shape [ 0 ], expr . shape [ 1 ], self . self_dim // 2 ), device = expr . device ) else : neighbors = neighbors . transpose ( 1 , 2 ) neighbors = torch . cat ( [ neighbors . unsqueeze ( - 1 ), edge_info . unsqueeze ( - 1 )], dim =- 1 ) for i , layer in enumerate ( self . neighbors_layers ): # batch, seq_len, neighbs, hidden_dim neighbors = layer ( neighbors ) neighbors = neighbors . sum ( - 2 ) if expr is None : expr = torch . zeros ( ( neighbors . shape [ 0 ], neighbors . shape [ 1 ], 1 ), device = neighbors . device ) else : expr = expr . unsqueeze ( - 1 ) for i , layer in enumerate ( self . self_layers ): expr = layer ( expr ) x = torch . cat ([ expr , neighbors ], dim =- 1 ) for layer in self . shared_layers : # batch, seq_len, neighbs, hidden_dim x = layer ( x ) if mask is not None : x = x . masked_fill ( mask . unsqueeze ( - 1 ), 0 ) return x","title":"forward"},{"location":"model/#scprint2.model.encoders.ExprBasedFT","text":"Bases: Module Encode real number values to a vector using neural nets projection. Parameters: d_model ( int ) \u2013 The dimension of the input vectors. gene_encoder ( Module ) \u2013 The gene name encoder module. expr_encoder ( Module , default: Identity () ) \u2013 The expression encoder module. Defaults to nn.Identity. dropout ( float , default: 0.1 ) \u2013 The dropout rate to apply to the output of the positional encoding. layers ( int , default: 2 ) \u2013 The number of layers in the encoder. Defaults to 2. intermediary_d ( int , default: 256 + 64 ) \u2013 The dimension of the intermediary layers. Defaults to 256 + 64. Methods: Name Description forward Args: Source code in scprint2/model/encoders.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 def __init__ ( self , d_model : int , gene_encoder : nn . Module , expr_encoder : nn . Module = nn . Identity (), dropout : float = 0.1 , layers : int = 2 , intermediary_d : int = 256 + 64 , ): \"\"\" Encode real number values to a vector using neural nets projection. Args: d_model (int): The dimension of the input vectors. gene_encoder (nn.Module): The gene name encoder module. expr_encoder (nn.Module, optional): The expression encoder module. Defaults to nn.Identity. dropout (float, optional): The dropout rate to apply to the output of the positional encoding. layers (int, optional): The number of layers in the encoder. Defaults to 2. intermediary_d (int, optional): The dimension of the intermediary layers. Defaults to 256 + 64. \"\"\" super ( ExprBasedFT , self ) . __init__ () self . encoder = nn . ModuleList () # self.mask_value = nn.Embedding(1, d_model) self . add_module ( \"gene_encoder\" , gene_encoder ) self . add_module ( \"expr_encoder\" , expr_encoder ) expr_shape , gene_shape = ( self . expr_encoder . output_dim , self . gene_encoder . output_dim , ) self . encoder . append ( nn . Linear ( expr_shape + gene_shape , intermediary_d )) for i in range ( layers - 1 ): self . encoder . append ( nn . LayerNorm ( intermediary_d )) self . encoder . append ( nn . ReLU ()) self . encoder . append ( nn . Dropout ( p = dropout )) self . encoder . append ( nn . Linear ( intermediary_d , intermediary_d if i < layers - 2 else d_model ) )","title":"ExprBasedFT"},{"location":"model/#scprint2.model.encoders.ExprBasedFT.forward","text":"Parameters: gene_pos ( Tensor [ batch_size , seq_len ] ) \u2013 Gene position indices input to the gene encoder expr ( ( Tensor [ batch_size , seq_len ], Optional ) , default: None ) \u2013 Expression values input to the expression encoder mask ( ( Tensor [ batch_size , seq_len ], Optional ) , default: None ) \u2013 Mask for the input input to the expression encoder neighbors ( ( Tensor [ batch_size , seq_len , n_neighbors ], Optional ) , default: None ) \u2013 Neighbors indices input to the expression encoder when it is a GNN neighbors_info ( ( Tensor [ batch_size , seq_len , n_neighbors ], Optional ) , default: None ) \u2013 optional additional information about the neighbors input to the expression encoder when it is a GNN Source code in scprint2/model/encoders.py 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 def forward ( self , gene_pos : Tensor , expr : Optional [ Tensor ] = None , mask : Optional [ Tensor ] = None , neighbors : Optional [ Tensor ] = None , neighbors_info : Optional [ Tensor ] = None , ) -> Tensor : \"\"\" Args: gene_pos (Tensor[batch_size, seq_len]): Gene position indices input to the gene encoder expr (Tensor[batch_size, seq_len], Optional): Expression values input to the expression encoder mask (Tensor[batch_size, seq_len], Optional): Mask for the input input to the expression encoder neighbors (Tensor[batch_size, seq_len, n_neighbors], Optional): Neighbors indices input to the expression encoder when it is a GNN neighbors_info (Tensor[batch_size, seq_len, n_neighbors], Optional): optional additional information about the neighbors input to the expression encoder when it is a GNN \"\"\" # expand last dimension if neighbors is None and expr is None : expr = torch . zeros ( ( gene_pos . shape [ 0 ], gene_pos . shape [ 1 ], self . expr_encoder . output_dim ), dtype = torch . float32 , device = gene_pos . device , ) # if no expr information: consider that it is all masked else : expr = ( self . expr_encoder ( expr , mask = mask ) if neighbors is None else self . expr_encoder ( expr , neighbors , neighbors_info , mask = mask ) ) gene_pos = self . gene_encoder ( gene_pos ) x = torch . cat ([ expr , gene_pos ], dim =- 1 ) for val in self . encoder : x = val ( x ) return x","title":"forward"},{"location":"model/#scprint2.model.encoders.GNN","text":"Bases: Module Graph Neural Network model Another implementation of a GNN layer that can be used for expression encoding. Supports GCN, GAT, GraphSAGE, and DeepSets architectures. Parameters: input_dim ( int , default: 1 ) \u2013 Dimension of input node features output_dim ( int , default: 256 ) \u2013 Dimension of output node features num_layers ( int , default: 2 ) \u2013 Number of GNN layers dropout ( float , default: 0.1 ) \u2013 Dropout probability gnn_type ( str , default: 'deepset' ) \u2013 Type of GNN layer ('gcn', 'gat', 'sage', or 'deepset') Methods: Name Description forward Forward pass Source code in scprint2/model/encoders.py 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 def __init__ ( self , input_dim : int = 1 , # here, 1 or 2 merge_dim : int = 32 , output_dim : int = 256 , num_layers : int = 2 , dropout : float = 0.1 , gnn_type : str = \"deepset\" , add_connection_feature : bool = False , ): \"\"\" Graph Neural Network model Another implementation of a GNN layer that can be used for expression encoding. Supports GCN, GAT, GraphSAGE, and DeepSets architectures. Args: input_dim: Dimension of input node features output_dim: Dimension of output node features num_layers: Number of GNN layers dropout: Dropout probability gnn_type: Type of GNN layer ('gcn', 'gat', 'sage', or 'deepset') \"\"\" super () . __init__ () self . input_dim = input_dim self . output_dim = output_dim if num_layers == 1 : raise ValueError ( \"num_layers must be greater than 1\" ) self . num_layers = num_layers self . dropout = dropout self . gnn_type = gnn_type self . add_connection_feature = add_connection_feature if gnn_type == \"deepset\" : # Local MLP (phi) for processing individual nodes self . input_nn_layer = MLP ( in_channels = input_dim , hidden_channels = merge_dim , out_channels = merge_dim , num_layers = num_layers , dropout = dropout , act = \"relu\" , norm = \"layer_norm\" , ) self . input_self_layer = MLP ( in_channels = input_dim , hidden_channels = merge_dim + 2 , out_channels = merge_dim , num_layers = num_layers - 1 , dropout = dropout , act = \"relu\" , norm = \"layer_norm\" , ) # Global MLP (rho) for processing aggregated features self . output_layer = MLP ( in_channels = ( ( merge_dim * 2 ) + 1 if add_connection_feature else merge_dim * 2 ), hidden_channels = output_dim , out_channels = output_dim , num_layers = num_layers , dropout = dropout , act = \"relu\" , norm = \"layer_norm\" , ) return # Select GNN layer type for other architectures else : if gnn_type == \"gcn\" : gnn_layer = GCNConv elif gnn_type == \"gat\" : gnn_layer = GATConv elif gnn_type == \"sage\" : gnn_layer = SAGEConv else : raise ValueError ( f \"Unknown GNN type: { gnn_type } \" ) self . gnn_layer = gnn_layer ( output_dim , output_dim , add_self_loops = False , normalize = False , aggr = \"mean\" , )","title":"GNN"},{"location":"model/#scprint2.model.encoders.GNN.forward","text":"Forward pass Parameters: x ( Tensor ) \u2013 Node features [minibatch_size, ngenes] neighbors ( Tensor ) \u2013 Neighbor nodes [minibatch_size, ngenes, n_neighbors] or [minibatch_size, ngenes, n_neighbors, 2] edge_info ( Tensor , default: None ) \u2013 Graph connectivity [2, num_edges] if gnn_type != deepset, Edge features [num_edges, 1] if gnn_type == deepset, or None if gnn_type == deepset and no edge features. batch ( Tensor , default: None ) \u2013 Batch assignment vector [num_nodes] mask ( Tensor , default: None ) \u2013 Mask tensor for the nodes. Returns: Tensor ( Tensor ) \u2013 Node embeddings [num_nodes, hidden_dim] Source code in scprint2/model/encoders.py 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 def forward ( self , x : Tensor , neighbors : Tensor , edge_info : Optional [ Tensor ] = None , batch : Optional [ Tensor ] = None , mask : Optional [ Tensor ] = None , ) -> Tensor : \"\"\" Forward pass Args: x (Tensor): Node features [minibatch_size, ngenes] neighbors (Tensor): Neighbor nodes [minibatch_size, ngenes, n_neighbors] or [minibatch_size, ngenes, n_neighbors, 2] edge_info (Tensor, optional): Graph connectivity [2, num_edges] if gnn_type != deepset, Edge features [num_edges, 1] if gnn_type == deepset, or None if gnn_type == deepset and no edge features. batch (Tensor, optional): Batch assignment vector [num_nodes] mask (Tensor, optional): Mask tensor for the nodes. Returns: Tensor: Node embeddings [num_nodes, hidden_dim] \"\"\" # Standard GNN forward pass x = x . unsqueeze ( - 1 ) neighbors = neighbors . unsqueeze ( - 1 ) if self . gnn_type == \"deepset\" : neighbors = self . input_nn_layer ( neighbors ) . sum ( dim =- 3 ) x = self . input_self_layer ( x ) x = torch . cat ([ x , neighbors ], dim =- 1 ) else : x = self . gnn_layer ( x , edge_info ) neighbors = self . gnn_layer ( neighbors , edge_info ) for layer in self . layers : x = layer ( x , edge_info ) x = F . relu ( x ) x = F . dropout ( x , p = self . dropout , training = self . training ) # TODO: to finish x = self . output_layer ( x ) if mask is not None : x = x . masked_fill_ ( mask . unsqueeze ( - 1 ), 0 ) return x","title":"forward"},{"location":"model/#scprint2.model.encoders.GeneEncoder","text":"Bases: Module Encodes gene sequences into a continuous vector space using an embedding layer. Uses memory mapping for efficient access to large embedding files. Parameters: num_embeddings ( int ) \u2013 The number of possible values embedding_dim ( int ) \u2013 The dimension of the output vectors padding_idx ( int , default: None ) \u2013 The index of the padding token weights ( Tensor , default: None ) \u2013 The initial weights for the embedding layer weights_file ( str , default: None ) \u2013 Path to parquet file containing embeddings freeze ( bool , default: False ) \u2013 Whether to freeze the weights of the embedding layer Methods: Name Description __del__ Cleanup method to ensure proper handling of memory-mapped file. forward Forward pass of the encoder. Source code in scprint2/model/encoders.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __init__ ( self , num_embeddings : int , embedding_dim : int , padding_idx : Optional [ int ] = None , weights : Optional [ Tensor ] = None , weights_file : Optional [ str ] = None , freeze : bool = False , ): \"\"\" Encodes gene sequences into a continuous vector space using an embedding layer. Uses memory mapping for efficient access to large embedding files. Args: num_embeddings (int): The number of possible values embedding_dim (int): The dimension of the output vectors padding_idx (int, optional): The index of the padding token weights (Tensor, optional): The initial weights for the embedding layer weights_file (str, optional): Path to parquet file containing embeddings freeze (bool, optional): Whether to freeze the weights of the embedding layer \"\"\" super ( GeneEncoder , self ) . __init__ () self . output_dim = embedding_dim if weights_file is not None : self . memmap = True if not freeze : raise ValueError ( \"freeze must be True when using memory-mapped embeddings\" ) # Load the parquet file and create memory-mapped array import os import pandas as pd # Create memory-mapped file path self . mmap_file = f \" { weights_file } .mmap\" self . loc = None self . enc = None # Only create the memory-mapped file if it doesn't exist if not os . path . exists ( self . mmap_file ): print ( f \"Creating memory-mapped file for embeddings at { self . mmap_file } \" ) df = pd . read_parquet ( weights_file ) embeddings = torch . nn . AdaptiveAvgPool1d ( self . output_dim )( torch . tensor ( df . values ) ) # Create memory-mapped array self . embeddings = np . memmap ( self . mmap_file , dtype = \"float32\" , mode = \"w+\" , shape = embeddings . shape ) # Copy data to memory-mapped array self . embeddings [:] = embeddings . numpy () # self . embeddings . flush () # Clean up memory del df del embeddings else : print ( f \"Loading existing memory-mapped embeddings from { self . mmap_file } \" ) # Load existing memory-mapped file self . embeddings = np . memmap ( self . mmap_file , dtype = \"float32\" , mode = \"r\" , # Read-only mode since we don't need to modify shape = ( num_embeddings , embedding_dim ), ) else : self . memmap = False self . embeddings = nn . Embedding ( num_embeddings , embedding_dim , padding_idx = padding_idx , _freeze = freeze ) if weights is not None : self . embeddings . weight . data . copy_ ( torch . Tensor ( weights ))","title":"GeneEncoder"},{"location":"model/#scprint2.model.encoders.GeneEncoder.__del__","text":"Cleanup method to ensure proper handling of memory-mapped file. Source code in scprint2/model/encoders.py 116 117 118 119 120 121 122 def __del__ ( self ): \"\"\"Cleanup method to ensure proper handling of memory-mapped file.\"\"\" if hasattr ( self , \"embeddings\" ) and self . embeddings is not None : try : self . embeddings . _mmap . close () except : pass","title":"__del__"},{"location":"model/#scprint2.model.encoders.GeneEncoder.forward","text":"Forward pass of the encoder. Parameters: x ( Tensor ) \u2013 Input tensor of indices [batch_size, seq_len] Returns: Tensor ( Tensor ) \u2013 Embedded vectors [batch_size, seq_len, embedding_dim] Source code in scprint2/model/encoders.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def forward ( self , x : Tensor ) -> Tensor : \"\"\" Forward pass of the encoder. Args: x (Tensor): Input tensor of indices [batch_size, seq_len] Returns: Tensor: Embedded vectors [batch_size, seq_len, embedding_dim] \"\"\" if self . memmap : if self . loc is None or not torch . all ( x . sum ( 1 ) == self . loc ): self . enc = ( torch . from_numpy ( self . embeddings [ x . reshape ( - 1 ) . cpu () . numpy ()] . copy () ) . reshape ( x . shape + ( - 1 ,)) . to ( x . device ) ) self . loc = x . sum ( 1 ) return self . enc . clone () else : return self . embeddings ( x )","title":"forward"},{"location":"model/#scprint2.model.encoders.PositionalEncoding","text":"Bases: Module The PositionalEncoding module applies a positional encoding to a sequence of vectors. This is necessary for the Transformer model, which does not have any inherent notion of position in a sequence. The positional encoding is added to the input embeddings and allows the model to attend to positions in the sequence. Parameters: d_model ( int ) \u2013 The dimension of the input vectors. gene_pos_enc ( list [ str ] , default: [] ) \u2013 The gene position encoding to use. Note: not used in the current version of scprint-2. Methods: Name Description forward Args: Source code in scprint2/model/encoders.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def __init__ ( self , d_model : int , gene_pos_enc : list [ str ] = [], ): \"\"\" The PositionalEncoding module applies a positional encoding to a sequence of vectors. This is necessary for the Transformer model, which does not have any inherent notion of position in a sequence. The positional encoding is added to the input embeddings and allows the model to attend to positions in the sequence. Args: d_model (int): The dimension of the input vectors. gene_pos_enc (list[str], optional): The gene position encoding to use. Note: not used in the current version of scprint-2. \"\"\" super ( PositionalEncoding , self ) . __init__ () self . gene_pos_enc = gene_pos_enc max_len = max ( gene_pos_enc ) position = torch . arange ( max_len ) . unsqueeze ( 1 ) token_to_pos = { token : pos for token , pos in enumerate ( gene_pos_enc )} # Create a dictionary to convert token to position div_term = torch . exp ( torch . arange ( 0 , d_model , 2 ) * ( - math . log ( float ( 10_000 )) / d_model ) ) pe = torch . zeros ( max_len , 1 , d_model ) pe [:, 0 , 0 :: 2 ] = torch . sin ( position * div_term ) pe [:, 0 , 1 :: 2 ] = torch . cos ( position * div_term ) # we reorder them and map them to gene_id (position) arr = [] for _ , v in token_to_pos . items (): arr . append ( pe [ v - 1 ] . numpy ()) pe = torch . Tensor ( np . array ( arr )) # Remove the unnecessary middle dimension since pe should be [m, d] # pe = pe.squeeze(1) self . register_buffer ( \"pe\" , pe )","title":"PositionalEncoding"},{"location":"model/#scprint2.model.encoders.PositionalEncoding.forward","text":"Parameters: gene_pos ( Tensor ) \u2013 Gene position indices, shape [seq_len, batch_size] or [seq_len] Returns: Tensor ( Tensor ) \u2013 Positional encodings, shape [*gene_pos.shape, embedding_dim] Source code in scprint2/model/encoders.py 169 170 171 172 173 174 175 176 177 178 179 def forward ( self , gene_pos : Tensor ) -> Tensor : \"\"\" Args: gene_pos (Tensor): Gene position indices, shape [seq_len, batch_size] or [seq_len] Returns: Tensor: Positional encodings, shape [*gene_pos.shape, embedding_dim] \"\"\" return torch . index_select ( self . pe , 0 , gene_pos . reshape ( - 1 )) . reshape ( gene_pos . shape + ( - 1 ,) )","title":"forward"},{"location":"model/#scprint2.model.decoders","text":"Classes: Name Description ClsDecoder ExprDecoder GraphSDEExprDecoder MVCDecoder VAEDecoder","title":"decoders"},{"location":"model/#scprint2.model.decoders.ClsDecoder","text":"Bases: Module ClsDecoder Decoder for classification task. Parameters: d_model ( int ) \u2013 Dimension of the input. n_cls ( int ) \u2013 Number of classes. layers ( List [ int ] , default: [256, 128] ) \u2013 List of hidden layers. activation ( Callable , default: ReLU ) \u2013 Activation function. dropout ( float , default: 0.1 ) \u2013 Dropout rate. Methods: Name Description forward Args: Source code in scprint2/model/decoders.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 def __init__ ( self , d_model : int , n_cls : int , layers : List [ int ] = [ 256 , 128 ], activation : Callable = nn . ReLU , dropout : float = 0.1 , ): \"\"\" ClsDecoder Decoder for classification task. Args: d_model (int): Dimension of the input. n_cls (int): Number of classes. layers (List[int]): List of hidden layers. activation (Callable): Activation function. dropout (float): Dropout rate. \"\"\" super ( ClsDecoder , self ) . __init__ () # module List layers = [ d_model ] + layers self . decoder = nn . Sequential () self . n_cls = n_cls for i , l in enumerate ( layers [ 1 :]): self . decoder . append ( nn . Linear ( layers [ i ], l )) self . decoder . append ( nn . LayerNorm ( l )) self . decoder . append ( activation ()) self . decoder . append ( nn . Dropout ( dropout )) self . out_layer = nn . Linear ( layers [ - 1 ], n_cls )","title":"ClsDecoder"},{"location":"model/#scprint2.model.decoders.ClsDecoder.forward","text":"Parameters: x ( Tensor ) \u2013 Tensor, shape [batch_size, embsize] Source code in scprint2/model/decoders.py 259 260 261 262 263 264 265 def forward ( self , x : Tensor ) -> Tensor : \"\"\" Args: x: Tensor, shape [batch_size, embsize] \"\"\" x = self . decoder ( x ) return self . out_layer ( x )","title":"forward"},{"location":"model/#scprint2.model.decoders.ExprDecoder","text":"Bases: Module ExprDecoder Decoder for the gene expression prediction. Will output the mean, variance and zero logits, parameters of a zero inflated negative binomial distribution. Parameters: d_model ( int ) \u2013 The dimension of the model. This is the size of the input feature vector. nfirst_tokens_to_skip ( int , default: 0 ) \u2013 The number of initial labels to skip in the sequence. Defaults to 0. dropout ( float , default: 0.1 ) \u2013 The dropout rate applied during training to prevent overfitting. Defaults to 0.1. zinb ( bool , default: True ) \u2013 Whether to use a zero inflated negative binomial distribution. Defaults to True. use_depth ( bool , default: False ) \u2013 Whether to use depth as an additional feature. Defaults to False. Methods: Name Description forward x is the output of the transformer, (batch, seq_len, d_model) Source code in scprint2/model/decoders.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __init__ ( self , d_model : int , nfirst_tokens_to_skip : int = 0 , dropout : float = 0.1 , zinb : bool = True , use_depth : bool = False , ): \"\"\" ExprDecoder Decoder for the gene expression prediction. Will output the mean, variance and zero logits, parameters of a zero inflated negative binomial distribution. Args: d_model (int): The dimension of the model. This is the size of the input feature vector. nfirst_tokens_to_skip (int, optional): The number of initial labels to skip in the sequence. Defaults to 0. dropout (float, optional): The dropout rate applied during training to prevent overfitting. Defaults to 0.1. zinb (bool, optional): Whether to use a zero inflated negative binomial distribution. Defaults to True. use_depth (bool, optional): Whether to use depth as an additional feature. Defaults to False. \"\"\" super ( ExprDecoder , self ) . __init__ () self . fc = nn . Sequential ( nn . Linear ( d_model if not use_depth else d_model + 1 , d_model ), nn . LayerNorm ( d_model ), nn . LeakyReLU (), nn . Dropout ( dropout ), nn . Linear ( d_model , d_model ), nn . LayerNorm ( d_model ), nn . LeakyReLU (), ) self . pred_var_zero = nn . Linear ( d_model , 3 if zinb else 1 ) self . zinb = zinb","title":"ExprDecoder"},{"location":"model/#scprint2.model.decoders.ExprDecoder.forward","text":"x is the output of the transformer, (batch, seq_len, d_model) Source code in scprint2/model/decoders.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def forward ( self , x : Tensor , req_depth : Optional [ Tensor ] = None ) -> Dict [ str , Tensor ]: \"\"\"x is the output of the transformer, (batch, seq_len, d_model)\"\"\" # we don't do it on the labels if req_depth is not None : x = torch . cat ( [ x , req_depth . unsqueeze ( 1 ) . unsqueeze ( - 1 ) . expand ( - 1 , x . shape [ 1 ], - 1 )], dim =- 1 , ) x = self . fc ( x ) if self . zinb : pred_value , var_value , zero_logits = self . pred_var_zero ( x ) . split ( 1 , dim =- 1 ) # (batch, seq_len) # The sigmoid function is used to map the zero_logits to a probability between 0 and 1. return dict ( mean = F . softmax ( pred_value . squeeze ( - 1 ), dim =- 1 ), disp = torch . exp ( torch . clamp ( var_value . squeeze ( - 1 ), max = 15 )), zero_logits = zero_logits . squeeze ( - 1 ), ) else : pred_value = self . pred_var_zero ( x ) return dict ( mean = F . softmax ( pred_value . squeeze ( - 1 ), dim =- 1 ))","title":"forward"},{"location":"model/#scprint2.model.decoders.GraphSDEExprDecoder","text":"Bases: Module Initialize the ExprNeuralSDEDecoder module. Parameters: d_model ( int ) \u2013 The dimension of the model. drift ( Module ) \u2013 The drift component of the SDE. diffusion ( Module ) \u2013 The diffusion component of the SDE. Source code in scprint2/model/decoders.py 9 10 11 12 13 14 15 16 17 18 19 20 21 def __init__ ( self , d_model : int , drift : nn . Module , diffusion : nn . Module ): \"\"\" Initialize the ExprNeuralSDEDecoder module. Args: d_model (int): The dimension of the model. drift (nn.Module): The drift component of the SDE. diffusion (nn.Module): The diffusion component of the SDE. \"\"\" super () . __init__ () self . d_model = d_model self . drift = drift self . diffusion = diffusion","title":"GraphSDEExprDecoder"},{"location":"model/#scprint2.model.decoders.MVCDecoder","text":"Bases: Module MVCDecoder Decoder for masked value prediction of cell embeddings. Uses gene embeddings with cell embeddings to predict mean, variance, and zero logits parameters of a zero-inflated negative binomial distribution. Parameters: d_model ( int ) \u2013 Dimension of the gene embedding. arch_style ( str , default: 'inner product' ) \u2013 Architecture style of the decoder. Options: \"inner product\": Uses inner product between cell and gene embeddings, \"concat query\": Concatenates cell and gene embeddings, \"sum query\": Sums cell and gene embeddings. Defaults to \"inner product\". tot_labels ( int , default: 1 ) \u2013 Total number of labels in the input. Defaults to 1. query_activation ( Module , default: Sigmoid ) \u2013 Activation function for query vectors. Defaults to nn.Sigmoid. hidden_activation ( Module , default: PReLU ) \u2013 Activation function for hidden layers. Defaults to nn.PReLU. use_depth ( bool , default: False ) \u2013 Whether to use depth as an additional feature. Defaults to False. zinb ( bool , default: True ) \u2013 Whether to use a zero-inflated negative binomial distribution. Defaults to True. Methods: Name Description forward Args: Source code in scprint2/model/decoders.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def __init__ ( self , d_model : int , arch_style : str = \"inner product\" , tot_labels : int = 1 , query_activation : nn . Module = nn . Sigmoid , hidden_activation : nn . Module = nn . PReLU , use_depth : bool = False , zinb : bool = True , ) -> None : \"\"\" MVCDecoder Decoder for masked value prediction of cell embeddings. Uses gene embeddings with cell embeddings to predict mean, variance, and zero logits parameters of a zero-inflated negative binomial distribution. Args: d_model (int): Dimension of the gene embedding. arch_style (str, optional): Architecture style of the decoder. Options: \"inner product\": Uses inner product between cell and gene embeddings, \"concat query\": Concatenates cell and gene embeddings, \"sum query\": Sums cell and gene embeddings. Defaults to \"inner product\". tot_labels (int, optional): Total number of labels in the input. Defaults to 1. query_activation (nn.Module, optional): Activation function for query vectors. Defaults to nn.Sigmoid. hidden_activation (nn.Module, optional): Activation function for hidden layers. Defaults to nn.PReLU. use_depth (bool, optional): Whether to use depth as an additional feature. Defaults to False. zinb (bool, optional): Whether to use a zero-inflated negative binomial distribution. Defaults to True. \"\"\" super ( MVCDecoder , self ) . __init__ () if arch_style == \"inner product\" : self . gene2query = nn . Linear ( d_model if not use_depth else d_model + 1 , d_model ) self . norm = nn . LayerNorm ( d_model ) self . query_activation = query_activation () self . pred_var_zero = nn . Linear ( d_model , d_model * ( 3 if zinb else 1 ), bias = False ) elif arch_style == \"concat query\" : self . gene2query = nn . Linear ( d_model if not use_depth else d_model + 1 , d_model ) self . query_activation = query_activation () self . fc1 = nn . Linear ( d_model * ( 1 + tot_labels ), d_model // 2 ) self . hidden_activation = hidden_activation () self . fc2 = nn . Linear ( d_model // 2 , ( 3 if zinb else 1 )) elif arch_style == \"sum query\" : self . gene2query = nn . Linear ( d_model if not use_depth else d_model + 1 , d_model ) self . query_activation = query_activation () self . fc1 = nn . Linear ( d_model , 64 ) self . hidden_activation = hidden_activation () self . fc2 = nn . Linear ( 64 , ( 3 if zinb else 1 )) else : raise ValueError ( f \"Unknown arch_style: { arch_style } \" ) self . arch_style = arch_style self . do_detach = arch_style . endswith ( \"detach\" ) self . d_model = d_model self . zinb = zinb","title":"MVCDecoder"},{"location":"model/#scprint2.model.decoders.MVCDecoder.forward","text":"Parameters: cell_emb ( Tensor ) \u2013 Tensor, shape (batch, embsize=d_model) gene_embs ( Tensor ) \u2013 Tensor, shape (batch, seq_len, embsize=d_model) req_depth ( Optional [ Tensor ] , default: None ) \u2013 Tensor, shape (batch,), optional depth information. Returns: Union [ Tensor , Dict [ str , Tensor ]] \u2013 Dict[str, Tensor]: A dictionary containing the predicted mean, variance, and zero logits (if zinb is True). Source code in scprint2/model/decoders.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def forward ( self , cell_emb : Tensor , gene_embs : Tensor , req_depth : Optional [ Tensor ] = None , ) -> Union [ Tensor , Dict [ str , Tensor ]]: \"\"\" Args: cell_emb: Tensor, shape (batch, embsize=d_model) gene_embs: Tensor, shape (batch, seq_len, embsize=d_model) req_depth: Tensor, shape (batch,), optional depth information. Returns: Dict[str, Tensor]: A dictionary containing the predicted mean, variance, and zero logits (if zinb is True). \"\"\" if req_depth is not None : gene_embs = torch . cat ( [ gene_embs , req_depth . unsqueeze ( 1 ) . unsqueeze ( - 1 ) . expand ( - 1 , gene_embs . shape [ 1 ], - 1 ), ], dim =- 1 , ) if self . arch_style == \"inner product\" : query_vecs = self . query_activation ( self . norm ( self . gene2query ( gene_embs ))) if self . zinb : pred , var , zero_logits = self . pred_var_zero ( query_vecs ) . split ( self . d_model , dim =- 1 ) else : pred = self . pred_var_zero ( query_vecs ) cell_emb = cell_emb . unsqueeze ( 2 ) if self . zinb : pred , var , zero_logits = ( torch . bmm ( pred , cell_emb ) . squeeze ( 2 ), torch . bmm ( var , cell_emb ) . squeeze ( 2 ), torch . bmm ( zero_logits , cell_emb ) . squeeze ( 2 ), ) else : pred = torch . bmm ( pred , cell_emb ) . squeeze ( 2 ) # zero logits need to based on the cell_emb, because of input exprs elif self . arch_style == \"concat query\" : query_vecs = self . query_activation ( self . gene2query ( gene_embs )) # expand cell_emb to (batch, seq_len, embsize) cell_emb = cell_emb . unsqueeze ( 1 ) . expand ( - 1 , gene_embs . shape [ 1 ], - 1 ) h = self . hidden_activation ( self . fc1 ( torch . cat ([ cell_emb , query_vecs ], dim = 2 )) ) if self . zinb : pred , var , zero_logits = self . fc2 ( h ) . split ( 1 , dim =- 1 ) else : pred = self . fc2 ( h ) elif self . arch_style == \"sum query\" : query_vecs = self . query_activation ( self . gene2query ( gene_embs )) cell_emb = cell_emb . unsqueeze ( 1 ) h = self . hidden_activation ( self . fc1 ( cell_emb + query_vecs )) if self . zinb : pred , var , zero_logits = self . fc2 ( h ) . split ( 1 , dim =- 1 ) else : pred = self . fc2 ( h ) if self . zinb : return dict ( mvc_mean = F . softmax ( pred , dim =- 1 ), mvc_disp = torch . exp ( torch . clamp ( var , max = 15 )), mvc_zero_logits = zero_logits , ) else : return dict ( mvc_mean = F . softmax ( pred , dim =- 1 ))","title":"forward"},{"location":"model/#scprint2.model.decoders.VAEDecoder","text":"Bases: Module VAEDecoder for variational autoencoding of cell embeddings. Parameters: d_model ( int ) \u2013 Input dimension (original embedding size) layers ( List [ int ] , default: [64, 64] ) \u2013 List of hidden layer sizes for encoder and decoder activation ( Callable , default: ReLU ) \u2013 Activation function to use dropout ( float , default: 0.1 ) \u2013 Dropout rate return_latent ( bool , default: False ) \u2013 Whether to return the latent vectors Methods: Name Description forward Forward pass through VAE. kl_divergence Compute KL divergence between N(mu, var) and N(0, 1). reparameterize Reparameterization trick to sample from N(mu, var) from N(0,1). Source code in scprint2/model/decoders.py 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 def __init__ ( self , d_model : int , layers : List [ int ] = [ 64 , 64 ], activation : Callable = nn . ReLU , dropout : float = 0.1 , return_latent : bool = False , ): \"\"\" VAEDecoder for variational autoencoding of cell embeddings. Args: d_model (int): Input dimension (original embedding size) layers (List[int]): List of hidden layer sizes for encoder and decoder activation (Callable): Activation function to use dropout (float): Dropout rate return_latent (bool): Whether to return the latent vectors \"\"\" super ( VAEDecoder , self ) . __init__ () # Encoder layers self . return_latent = return_latent encoder_layers = [ d_model ] + layers self . encoder = nn . Sequential () for i , ( in_size , out_size ) in enumerate ( zip ( encoder_layers [: - 1 ], encoder_layers [ 1 :]) ): self . encoder . append ( nn . Linear ( in_size , out_size )) self . encoder . append ( nn . LayerNorm ( out_size )) self . encoder . append ( activation ()) self . encoder . append ( nn . Dropout ( dropout )) # VAE latent parameters self . fc_mu = nn . Linear ( encoder_layers [ - 1 ], encoder_layers [ - 1 ]) self . fc_var = nn . Linear ( encoder_layers [ - 1 ], encoder_layers [ - 1 ]) # Decoder layers decoder_layers = [ encoder_layers [ - 1 ]] + list ( reversed ( layers [: - 1 ])) + [ d_model ] self . decoder = nn . Sequential () for i , ( in_size , out_size ) in enumerate ( zip ( decoder_layers [: - 1 ], decoder_layers [ 1 :] ) # Changed to include final layer ): self . decoder . append ( nn . Linear ( in_size , out_size )) if ( i < len ( decoder_layers ) - 2 ): # Don't apply activation/norm to final layer self . decoder . append ( nn . LayerNorm ( out_size )) self . decoder . append ( activation ()) self . decoder . append ( nn . Dropout ( dropout ))","title":"VAEDecoder"},{"location":"model/#scprint2.model.decoders.VAEDecoder.forward","text":"Forward pass through VAE. Parameters: x ( Tensor ) \u2013 Input tensor of shape [batch_size, d_model] Returns: Union [ Tensor , Tuple [ Tensor , Tensor , Tensor , Tensor , Tensor ]] \u2013 If self.return_latent is True: Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]: - reconstructed_x (Tensor): Reconstructed input, shape [batch_size, d_model] - mu (Tensor): Mean of the latent Gaussian, shape [batch_size, latent_dim] - log_var (Tensor): Log variance of the latent Gaussian, shape [batch_size, latent_dim] - kl_loss (Tensor): KL divergence loss (scalar tensor) Else ( Union [ Tensor , Tuple [ Tensor , Tensor , Tensor , Tensor , Tensor ]] ) \u2013 Tensor: reconstructed_x of shape [batch_size, d_model] Source code in scprint2/model/decoders.py 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 def forward ( self , x : Tensor ) -> Union [ Tensor , Tuple [ Tensor , Tensor , Tensor , Tensor , Tensor ]]: \"\"\" Forward pass through VAE. Args: x (Tensor): Input tensor of shape [batch_size, d_model] Returns: If self.return_latent is True: Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]: - reconstructed_x (Tensor): Reconstructed input, shape [batch_size, d_model] - mu (Tensor): Mean of the latent Gaussian, shape [batch_size, latent_dim] - log_var (Tensor): Log variance of the latent Gaussian, shape [batch_size, latent_dim] - kl_loss (Tensor): KL divergence loss (scalar tensor) Else: Tensor: reconstructed_x of shape [batch_size, d_model] \"\"\" # Encode encoded = self . encoder ( x ) # Get latent parameters mu = self . fc_mu ( encoded ) log_var = self . fc_var ( encoded ) log_var = torch . clamp ( log_var , min =- 10 ) # Sample latent vector kl_loss = self . kl_divergence ( mu , log_var ) # free_bits = 2.0 # per latent dim # kl_loss = torch.clamp(kl_loss / mu.size(-1), min=free_bits) * mu.size(-1) z = self . reparameterize ( mu , log_var ) # Decode decoded = self . decoder ( z ) if self . return_latent : return decoded , mu , log_var , encoded , kl_loss return decoded , kl_loss","title":"forward"},{"location":"model/#scprint2.model.decoders.VAEDecoder.kl_divergence","text":"Compute KL divergence between N(mu, var) and N(0, 1). Parameters: mu ( Tensor ) \u2013 Mean of the latent Gaussian log_var ( Tensor ) \u2013 Log variance of the latent Gaussian Returns: Tensor ( Tensor ) \u2013 KL divergence loss Source code in scprint2/model/decoders.py 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def kl_divergence ( self , mu : Tensor , log_var : Tensor ) -> Tensor : \"\"\" Compute KL divergence between N(mu, var) and N(0, 1). Args: mu (Tensor): Mean of the latent Gaussian log_var (Tensor): Log variance of the latent Gaussian Returns: Tensor: KL divergence loss \"\"\" # KL(N(mu, var) || N(0, 1)) = -0.5 * sum(1 + log(var) - mu^2 - var) kl_loss = - 0.5 * torch . sum ( 1 + log_var - mu . pow ( 2 ) - log_var . exp (), dim = 1 ) return kl_loss . mean ()","title":"kl_divergence"},{"location":"model/#scprint2.model.decoders.VAEDecoder.reparameterize","text":"Reparameterization trick to sample from N(mu, var) from N(0,1). Parameters: mu ( Tensor ) \u2013 Mean of the latent Gaussian log_var ( Tensor ) \u2013 Log variance of the latent Gaussian Returns: Tensor ( Tensor ) \u2013 Sampled latent vector Source code in scprint2/model/decoders.py 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def reparameterize ( self , mu : Tensor , log_var : Tensor ) -> Tensor : \"\"\" Reparameterization trick to sample from N(mu, var) from N(0,1). Args: mu (Tensor): Mean of the latent Gaussian log_var (Tensor): Log variance of the latent Gaussian Returns: Tensor: Sampled latent vector \"\"\" std = torch . exp ( 0.5 * log_var ) eps = torch . randn_like ( std ) return mu + eps * std","title":"reparameterize"},{"location":"model/#scprint2.model.fsq","text":"Finite Scalar Quantization: VQ-VAE Made Simple - https://arxiv.org/abs/2309.15505 Code adapted from Jax version in Appendix A.1 Classes: Name Description FSQ Functions: Name Description round_ste Round with straight through gradients.","title":"fsq"},{"location":"model/#scprint2.model.fsq.FSQ","text":"Bases: Module Methods: Name Description bound Bound z , an array of shape (..., d). codes_to_indices Converts a code to an index in the codebook. forward einstein notation indices_to_codes Inverse of codes_to_indices . quantize Quantizes z, returns quantized zhat, same shape as z. Source code in scprint2/model/fsq.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __init__ ( self , levels : List [ int ], dim : Optional [ int ] = None , num_codebooks = 1 , keep_num_codebooks_dim : Optional [ bool ] = None , scale : Optional [ float ] = None , ): super () . __init__ () _levels = torch . tensor ( levels , dtype = int32 ) self . register_buffer ( \"_levels\" , _levels , persistent = False ) _basis = torch . cumprod ( torch . tensor ([ 1 ] + levels [: - 1 ]), dim = 0 , dtype = int32 ) self . register_buffer ( \"_basis\" , _basis , persistent = False ) self . scale = scale codebook_dim = len ( levels ) self . codebook_dim = codebook_dim effective_codebook_dim = codebook_dim * num_codebooks self . num_codebooks = num_codebooks self . effective_codebook_dim = effective_codebook_dim keep_num_codebooks_dim = default ( keep_num_codebooks_dim , num_codebooks > 1 ) assert not ( num_codebooks > 1 and not keep_num_codebooks_dim ) self . keep_num_codebooks_dim = keep_num_codebooks_dim self . dim = default ( dim , len ( _levels ) * num_codebooks ) has_projections = self . dim != effective_codebook_dim self . project_in = ( nn . Linear ( self . dim , effective_codebook_dim ) if has_projections else nn . Identity () ) self . project_out = ( nn . Linear ( effective_codebook_dim , self . dim ) if has_projections else nn . Identity () ) self . has_projections = has_projections self . codebook_size = self . _levels . prod () . item () implicit_codebook = self . indices_to_codes ( torch . arange ( self . codebook_size ), project_out = False ) self . register_buffer ( \"implicit_codebook\" , implicit_codebook , persistent = False )","title":"FSQ"},{"location":"model/#scprint2.model.fsq.FSQ.bound","text":"Bound z , an array of shape (..., d). Source code in scprint2/model/fsq.py 99 100 101 102 103 104 def bound ( self , z : Tensor , eps : float = 1e-3 ) -> Tensor : \"\"\"Bound `z`, an array of shape (..., d).\"\"\" half_l = ( self . _levels - 1 ) * ( 1 - eps ) / 2 offset = torch . where ( self . _levels % 2 == 0 , 0.5 , 0.0 ) shift = ( offset / half_l ) . tan () return ( z + shift ) . tanh () * half_l - offset","title":"bound"},{"location":"model/#scprint2.model.fsq.FSQ.codes_to_indices","text":"Converts a code to an index in the codebook. Source code in scprint2/model/fsq.py 120 121 122 123 124 def codes_to_indices ( self , zhat : Tensor ) -> Tensor : \"\"\"Converts a `code` to an index in the codebook.\"\"\" assert zhat . shape [ - 1 ] == self . codebook_dim zhat = self . _scale_and_shift ( zhat ) return ( zhat * self . _basis ) . sum ( dim =- 1 ) . to ( int32 )","title":"codes_to_indices"},{"location":"model/#scprint2.model.fsq.FSQ.forward","text":"einstein notation b - batch n - sequence (or flattened spatial dimensions) d - feature dimension, which is also log2(codebook size) c - number of codebook dim Source code in scprint2/model/fsq.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def forward ( self , z : Tensor ) -> Tensor : \"\"\" einstein notation b - batch n - sequence (or flattened spatial dimensions) d - feature dimension, which is also log2(codebook size) c - number of codebook dim \"\"\" assert z . shape [ - 1 ] == self . dim , ( f \"expected dimension of { self . dim } but found dimension of { z . shape [ - 1 ] } \" ) small = self . project_in ( z ) z = rearrange ( small , \"b (c d) -> b c d\" , c = self . num_codebooks ) codes = self . quantize ( z ) indices = self . codes_to_indices ( codes ) codes = rearrange ( codes , \"b c d -> b (c d)\" ) out = self . project_out ( codes ) if not self . keep_num_codebooks_dim : indices = rearrange ( indices , \"... 1 -> ...\" ) return out , indices , small","title":"forward"},{"location":"model/#scprint2.model.fsq.FSQ.indices_to_codes","text":"Inverse of codes_to_indices . Source code in scprint2/model/fsq.py 126 127 128 129 130 131 132 133 134 135 136 137 def indices_to_codes ( self , indices : Tensor , project_out = True ) -> Tensor : \"\"\"Inverse of `codes_to_indices`.\"\"\" indices = rearrange ( indices , \"... -> ... 1\" ) codes_non_centered = ( indices // self . _basis ) % self . _levels codes = self . _scale_and_shift_inverse ( codes_non_centered ) if self . keep_num_codebooks_dim : codes = rearrange ( codes , \"... c d -> ... (c d)\" ) if project_out : codes = self . project_out ( codes ) return codes","title":"indices_to_codes"},{"location":"model/#scprint2.model.fsq.FSQ.quantize","text":"Quantizes z, returns quantized zhat, same shape as z. Source code in scprint2/model/fsq.py 106 107 108 109 110 def quantize ( self , z : Tensor ) -> Tensor : \"\"\"Quantizes z, returns quantized zhat, same shape as z.\"\"\" quantized = round_ste ( self . bound ( z )) half_width = self . _levels // 2 # Renormalize to [-1, 1]. return quantized / half_width","title":"quantize"},{"location":"model/#scprint2.model.fsq.round_ste","text":"Round with straight through gradients. Source code in scprint2/model/fsq.py 39 40 41 42 def round_ste ( z : Tensor ) -> Tensor : \"\"\"Round with straight through gradients.\"\"\" zhat = z . round () return z + ( zhat - z ) . detach ()","title":"round_ste"},{"location":"pretrain/","text":"Pre-training scPRINT-2 scPRINT-2 is a large model that can be pre-trained on a large dataset of single cell data. This pre-training is quite efficient for scPRINT-2 and smaller models can be pretrained on any hardware with a 20GB NVIDIA GPU. Setup of the database To perform pretraining you will need a large dataset. We recommend using the laminDB to assemble such a large database of dataset and to use our scdataloader package to perform the data loading to the model. In addition, you will need to preprocess your datasets. To make sure that the fields are all here, the genes are in the right format, the raw counts are used, etc... We recommend using the Preprocessor class of the scdataloader package. Moreover scdataloader works with a set of ontologies. To install these, use the function populate_my_ontologies from the scdataloader package. If you do not have your own database of anndatas, we recommend the cellxgene database and our associated helper function to download and preprocess all of cellxgene in a single command with scdataloader . Finally you might want to generate gene embeddings to use with scPRINT-2 instead of learning these tokens from scratch. For this you can use the gene_embedders module of scPRINT-2, which usage is detailed in the notebooks/generate_gene_embeddings.ipynb notebook and also gene locations using the additional notebook notebooks/genelocs.ipynb Pre-training to pretrain scPRINT-2 we strongly recommend using command line as it can take multiple days (and using some HPC plateform like slurm or others). If on your own machine, use something like screen at least \ud83d\ude09. Most of the pre-training usage follows from pytorch lightning with scprint-2 fit you will launch a training run. It will populate both the datamodule (see scdataloader ), the model (see model.py ), the trainer (see pytorch lightning ) and the various callbacks. But you might want to use additional parameters. For this, you can use the config folder and the yaml files in it. These files are used to store the main hyperparameters of the model and the training scheme. More hyperparameters are given to the scPRINT-2 model via a Trainer callback I created (see trainer/trainer.py ). This is used to specify parameters to scPRINT-2 that are used solely during training and are not part of the model definition itself, like lr, schedulers, optimizers, etc.. I use a callback as it is how pytorch lightning requires us to send training parameters to the model. Thus a full command line to train scPRINT-2 on a slurm cluster might look like this: conda activate scprint2 ### slurm level stuff module load cuda/12.2 sbatch -p gpu #gpu partition -q gpu #gpu queue --gres=gpu:A40:4,gmem:40G #gpu type (4 A40 with 40GB of GPU mem) --cpus-per-task 16 --mem-per-gpu 90G #RAM per GPU --ntasks-per-node=1 #### # actuall scprint-2 command slurm/submit.sh 'fit --config config/base_v1.yml #base config file (see below) --config config/pretrain_medium.yml #the differences when training a large model --model.nhead 8 # changing this parameter from the large model directly in command line (cannot do 4 heads of 128dim with A40 GPUs...) --scprint_training.name o2uniqsx #an id for the model (not needed but useful) ' with the base yaml file containing: # general params project: scprint_scale #project name for saving data and wandb seed_everything: 42 ckpt_path: null #we don't have a checkpoint weights as we train from scratch set_float32_matmul_precision: True wandblog: all #we use wandb here log_freq: 200 log_graph: True trainer: #training level params precision: 16-mixed #we use mixed precision 16bit for training gradient_clip_val: 100 #needed log_every_n_steps: 100 .... logger: #we can add multiple loggers (see below) - class_path: lightning.pytorch.loggers.WandbLogger callbacks: #you can create your own callback and add it here or use lightning's callbacks - class_path: lightning.pytorch.callbacks.StochasticWeightAveraging init_args: swa_lrs: 0.03 ... model: # model params dropout: 0.1 transformer: normal #flashattention is used ... data: #datamodule params organisms: #we will use these 2 species - NCBITaxon:9606 - NCBITaxon:10090 gene_position_tolerance: 10_000 #gene location: if genes are closer than 10kb, they are considered as the same location gene_embeddings: ./data/main/gene_embeddings/ #the embeddings of genes (see above ) collection_name: all no zhang13M # the name of the laminDB collection we will use how: random expr # how we collate the expression data (here random expressed genes) max_len: 2200 #how many genes we use in the model context during training weight_scaler: 50 #how do we scale the weighted random sampling procedure (see our manuscript) ... We use wanDB in our case, however scPRINT-2 and pytorch lightning support a breadth of logging tools: see loggers . We use slurm in our usecase here but scPRINT-2 and pytorch lightning has been made to work in a breadth of environments e.g. .","title":"pre-training"},{"location":"pretrain/#pre-training-scprint-2","text":"scPRINT-2 is a large model that can be pre-trained on a large dataset of single cell data. This pre-training is quite efficient for scPRINT-2 and smaller models can be pretrained on any hardware with a 20GB NVIDIA GPU.","title":"Pre-training scPRINT-2"},{"location":"pretrain/#setup-of-the-database","text":"To perform pretraining you will need a large dataset. We recommend using the laminDB to assemble such a large database of dataset and to use our scdataloader package to perform the data loading to the model. In addition, you will need to preprocess your datasets. To make sure that the fields are all here, the genes are in the right format, the raw counts are used, etc... We recommend using the Preprocessor class of the scdataloader package. Moreover scdataloader works with a set of ontologies. To install these, use the function populate_my_ontologies from the scdataloader package. If you do not have your own database of anndatas, we recommend the cellxgene database and our associated helper function to download and preprocess all of cellxgene in a single command with scdataloader . Finally you might want to generate gene embeddings to use with scPRINT-2 instead of learning these tokens from scratch. For this you can use the gene_embedders module of scPRINT-2, which usage is detailed in the notebooks/generate_gene_embeddings.ipynb notebook and also gene locations using the additional notebook notebooks/genelocs.ipynb","title":"Setup of the database"},{"location":"pretrain/#pre-training","text":"to pretrain scPRINT-2 we strongly recommend using command line as it can take multiple days (and using some HPC plateform like slurm or others). If on your own machine, use something like screen at least \ud83d\ude09. Most of the pre-training usage follows from pytorch lightning with scprint-2 fit you will launch a training run. It will populate both the datamodule (see scdataloader ), the model (see model.py ), the trainer (see pytorch lightning ) and the various callbacks. But you might want to use additional parameters. For this, you can use the config folder and the yaml files in it. These files are used to store the main hyperparameters of the model and the training scheme. More hyperparameters are given to the scPRINT-2 model via a Trainer callback I created (see trainer/trainer.py ). This is used to specify parameters to scPRINT-2 that are used solely during training and are not part of the model definition itself, like lr, schedulers, optimizers, etc.. I use a callback as it is how pytorch lightning requires us to send training parameters to the model. Thus a full command line to train scPRINT-2 on a slurm cluster might look like this: conda activate scprint2 ### slurm level stuff module load cuda/12.2 sbatch -p gpu #gpu partition -q gpu #gpu queue --gres=gpu:A40:4,gmem:40G #gpu type (4 A40 with 40GB of GPU mem) --cpus-per-task 16 --mem-per-gpu 90G #RAM per GPU --ntasks-per-node=1 #### # actuall scprint-2 command slurm/submit.sh 'fit --config config/base_v1.yml #base config file (see below) --config config/pretrain_medium.yml #the differences when training a large model --model.nhead 8 # changing this parameter from the large model directly in command line (cannot do 4 heads of 128dim with A40 GPUs...) --scprint_training.name o2uniqsx #an id for the model (not needed but useful) ' with the base yaml file containing: # general params project: scprint_scale #project name for saving data and wandb seed_everything: 42 ckpt_path: null #we don't have a checkpoint weights as we train from scratch set_float32_matmul_precision: True wandblog: all #we use wandb here log_freq: 200 log_graph: True trainer: #training level params precision: 16-mixed #we use mixed precision 16bit for training gradient_clip_val: 100 #needed log_every_n_steps: 100 .... logger: #we can add multiple loggers (see below) - class_path: lightning.pytorch.loggers.WandbLogger callbacks: #you can create your own callback and add it here or use lightning's callbacks - class_path: lightning.pytorch.callbacks.StochasticWeightAveraging init_args: swa_lrs: 0.03 ... model: # model params dropout: 0.1 transformer: normal #flashattention is used ... data: #datamodule params organisms: #we will use these 2 species - NCBITaxon:9606 - NCBITaxon:10090 gene_position_tolerance: 10_000 #gene location: if genes are closer than 10kb, they are considered as the same location gene_embeddings: ./data/main/gene_embeddings/ #the embeddings of genes (see above ) collection_name: all no zhang13M # the name of the laminDB collection we will use how: random expr # how we collate the expression data (here random expressed genes) max_len: 2200 #how many genes we use in the model context during training weight_scaler: 50 #how do we scale the weighted random sampling procedure (see our manuscript) ... We use wanDB in our case, however scPRINT-2 and pytorch lightning support a breadth of logging tools: see loggers . We use slurm in our usecase here but scPRINT-2 and pytorch lightning has been made to work in a breadth of environments e.g. .","title":"Pre-training"},{"location":"structure/","text":"structure gene tokenizers Function to get tokens from a set of genes, given their ensembl ids. For now use 2 different models: RNABert : for non coding genes ESM3 : for protein coding genes given ids, a fasta file, will use the models to compute an embedding of each gene. This could be potentially applied to genes with mutations and from different species. data_loaders From scDataloader. (see more in the available readmes and website https://jkobject.com/scDataLoader) For now can work with either one to many AnnData's or a laminDB Collection of AnnDatas allows you to preprocess your anndatas too. They can be stored locally or remotely stores them in a Dataset class. Creates the DataLoaders from a Datamodule Class. Collates the results using a Collator function. model Extends from lightning data module to implement all the necessary functions to do: training validation testing prediction (inference) is subdivided into multiple parts: encoder transformer decoder the fsq module trainer & cli the model uses lightning's training toolkit and CLI tools. to use CLI, just call scprint2 ... (will call the __main__.py function). Additional, training-specific informations are passed to the model using the trainer.py function. specific training schemes are available under the config folder as yaml files. Moreover the model can be trained on multiple compute types. SLURM scripts are available under the slurm folder. tasks Implement different tasks that a pretrained model would perform. for now: GRN prediction: given a single cell dataset and a group (cell type, cluster, ...) will output a GRnnData completed with a predicted GRN from the attention of the model. denoising: from a single cell dataset, will modify the count matrices to predict what it would have looked like if it had been sequenced deeper, according to the model. embedding: from a single cell dataset, will create embeddings (low dimensional representations) of each cells, as well as prediction of the cell labels the model has been trained on (cell type, disease, ethnicity, sex...). It will also output a umap and predicted expression from the zinb, post bottleneck (similar to a VAE decoder prediction) generation: from a set of conditions (cell type, disease state, ethnicity, sex...) will generate new cell profile matching these conditions. imputation: from a single cell dataset with missing values (genes), will impute the missing genes according to the model. finetuning: from a pretrained model and a new single cell dataset, will finetune the model to better fit the new dataset and potentially correct for batch effects if provided with batch labels. gene embedding: from a list of genes, and an expression profile will output embeddings for gene.","title":"structure"},{"location":"structure/#structure","text":"","title":"structure"},{"location":"structure/#gene-tokenizers","text":"Function to get tokens from a set of genes, given their ensembl ids. For now use 2 different models: RNABert : for non coding genes ESM3 : for protein coding genes given ids, a fasta file, will use the models to compute an embedding of each gene. This could be potentially applied to genes with mutations and from different species.","title":"gene tokenizers"},{"location":"structure/#data_loaders","text":"From scDataloader. (see more in the available readmes and website https://jkobject.com/scDataLoader) For now can work with either one to many AnnData's or a laminDB Collection of AnnDatas allows you to preprocess your anndatas too. They can be stored locally or remotely stores them in a Dataset class. Creates the DataLoaders from a Datamodule Class. Collates the results using a Collator function.","title":"data_loaders"},{"location":"structure/#model","text":"Extends from lightning data module to implement all the necessary functions to do: training validation testing prediction (inference) is subdivided into multiple parts: encoder transformer decoder the fsq module","title":"model"},{"location":"structure/#trainer-cli","text":"the model uses lightning's training toolkit and CLI tools. to use CLI, just call scprint2 ... (will call the __main__.py function). Additional, training-specific informations are passed to the model using the trainer.py function. specific training schemes are available under the config folder as yaml files. Moreover the model can be trained on multiple compute types. SLURM scripts are available under the slurm folder.","title":"trainer &amp; cli"},{"location":"structure/#tasks","text":"Implement different tasks that a pretrained model would perform. for now: GRN prediction: given a single cell dataset and a group (cell type, cluster, ...) will output a GRnnData completed with a predicted GRN from the attention of the model. denoising: from a single cell dataset, will modify the count matrices to predict what it would have looked like if it had been sequenced deeper, according to the model. embedding: from a single cell dataset, will create embeddings (low dimensional representations) of each cells, as well as prediction of the cell labels the model has been trained on (cell type, disease, ethnicity, sex...). It will also output a umap and predicted expression from the zinb, post bottleneck (similar to a VAE decoder prediction) generation: from a set of conditions (cell type, disease state, ethnicity, sex...) will generate new cell profile matching these conditions. imputation: from a single cell dataset with missing values (genes), will impute the missing genes according to the model. finetuning: from a pretrained model and a new single cell dataset, will finetune the model to better fit the new dataset and potentially correct for batch effects if provided with batch labels. gene embedding: from a list of genes, and an expression profile will output embeddings for gene.","title":"tasks"},{"location":"tasks/","text":"Documentation for the tasks scprint2.tasks.cell_emb Classes: Name Description Embedder Functions: Name Description compute_classification Compute classification metrics for the given annotated data. compute_corr Compute the correlation between the output and target matrices. default_benchmark Run the default benchmark for embedding and annotation using the scPRINT model. display_confusion_matrix Display the confusion matrix for true vs predicted cell types. Embedder Embedder a class to embed and annotate cells using a model Parameters: batch_size ( int , default: 64 ) \u2013 The size of the batches to be used in the DataLoader. Defaults to 64. num_workers ( int , default: 8 ) \u2013 The number of worker processes to use for data loading. Defaults to 8. how ( str , default: 'random expr' ) \u2013 The method to be used for selecting valid genes. Defaults to \"random expr\". - \"random expr\": random expression - \"most var\": highly variable genes in the dataset - \"some\": specific genes (from genelist) - \"most expr\": most expressed genes in the cell max_len ( int , default: 2000 ) \u2013 The maximum length of the gene sequence given to the model. Defaults to 1000. doclass ( bool , default: True ) \u2013 Whether to perform classification. Defaults to True. pred_embedding ( List [ str ] , default: ['all'] ) \u2013 The list of labels to be used for plotting embeddings. Defaults to [ \"cell_type_ontology_term_id\", \"disease_ontology_term_id\", \"self_reported_ethnicity_ontology_term_id\", \"sex_ontology_term_id\", ]. doplot ( bool , default: True ) \u2013 Whether to generate plots. Defaults to True. keep_all_labels_pred ( bool , default: False ) \u2013 Whether to keep all class predictions. Defaults to False, will only keep the most likely class. genelist ( List [ str ] , default: None ) \u2013 The list of genes to be used for embedding. Defaults to []: In this case, \"how\" needs to be \"most var\" or \"random expr\". save_every ( int , default: 40000 ) \u2013 The number of cells to save at a time. Defaults to 100_000. This is important to avoid memory issues. unknown_label ( str , default: 'unknown' ) \u2013 The label to be used for unknown cell types. Defaults to \"unknown\". use_knn ( bool , default: True ) \u2013 Whether to use k-nearest neighbors information. Defaults to True. Methods: Name Description __call__ call function to call the embedding Source code in scprint2/tasks/cell_emb.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , batch_size : int = 64 , num_workers : int = 8 , how : str = \"random expr\" , max_len : int = 2000 , doclass : bool = True , pred_embedding : List [ str ] = [ \"all\" , ], doplot : bool = True , keep_all_labels_pred : bool = False , genelist : Optional [ List [ str ]] = None , save_every : int = 40_000 , unknown_label : str = \"unknown\" , use_knn : bool = True , ): \"\"\" Embedder a class to embed and annotate cells using a model Args: batch_size (int, optional): The size of the batches to be used in the DataLoader. Defaults to 64. num_workers (int, optional): The number of worker processes to use for data loading. Defaults to 8. how (str, optional): The method to be used for selecting valid genes. Defaults to \"random expr\". - \"random expr\": random expression - \"most var\": highly variable genes in the dataset - \"some\": specific genes (from genelist) - \"most expr\": most expressed genes in the cell max_len (int, optional): The maximum length of the gene sequence given to the model. Defaults to 1000. doclass (bool, optional): Whether to perform classification. Defaults to True. pred_embedding (List[str], optional): The list of labels to be used for plotting embeddings. Defaults to [ \"cell_type_ontology_term_id\", \"disease_ontology_term_id\", \"self_reported_ethnicity_ontology_term_id\", \"sex_ontology_term_id\", ]. doplot (bool, optional): Whether to generate plots. Defaults to True. keep_all_labels_pred (bool, optional): Whether to keep all class predictions. Defaults to False, will only keep the most likely class. genelist (List[str], optional): The list of genes to be used for embedding. Defaults to []: In this case, \"how\" needs to be \"most var\" or \"random expr\". save_every (int, optional): The number of cells to save at a time. Defaults to 100_000. This is important to avoid memory issues. unknown_label (str, optional): The label to be used for unknown cell types. Defaults to \"unknown\". use_knn (bool, optional): Whether to use k-nearest neighbors information. Defaults to True. \"\"\" self . batch_size = batch_size self . num_workers = num_workers self . how = how self . max_len = max_len self . pred_embedding = pred_embedding self . keep_all_labels_pred = keep_all_labels_pred self . doplot = doplot self . doclass = doclass self . genelist = genelist if genelist is not None else [] self . save_every = save_every self . pred = None self . unknown_label = unknown_label self . use_knn = use_knn __call__ call function to call the embedding Parameters: model ( Module ) \u2013 The scPRINT model to be used for embedding and annotation. adata ( AnnData ) \u2013 The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Raises: ValueError \u2013 If the model does not have a logger attribute. ValueError \u2013 If the model does not have a global_step attribute. Returns: AnnData ( AnnData ) \u2013 The annotated data matrix with embedded cell representations. dict ( dict ) \u2013 classification metrics results when some ground truth information was available in the anndata. Source code in scprint2/tasks/cell_emb.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 def __call__ ( self , model : torch . nn . Module , adata : AnnData ) -> tuple [ AnnData , dict ]: \"\"\" __call__ function to call the embedding Args: model (torch.nn.Module): The scPRINT model to be used for embedding and annotation. adata (AnnData): The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Raises: ValueError: If the model does not have a logger attribute. ValueError: If the model does not have a global_step attribute. Returns: AnnData: The annotated data matrix with embedded cell representations. dict: classification metrics results when some ground truth information was available in the anndata. \"\"\" # one of \"all\" \"sample\" \"none\" model . predict_mode = \"none\" self . pred = None prevkeep = model . keep_all_labels_pred model . keep_all_labels_pred = self . keep_all_labels_pred # Add at least the organism you are working with if self . how == \"most var\" : sc . pp . highly_variable_genes ( adata , flavor = \"seurat_v3\" , n_top_genes = self . max_len ) self . genelist = adata . var . index [ adata . var . highly_variable ] adataset = SimpleAnnDataset ( adata , obs_to_output = [ \"organism_ontology_term_id\" ], get_knn_cells = model . expr_emb_style == \"metacell\" and self . use_knn , ) col = Collator ( organisms = model . organisms , valid_genes = model . genes , how = self . how if self . how != \"most var\" else \"some\" , max_len = self . max_len , add_zero_genes = 0 , genelist = self . genelist if self . how in [ \"most var\" , \"some\" ] else [], n_bins = model . n_input_bins if model . expr_emb_style == \"binned\" else 0 , ) dataloader = DataLoader ( adataset , collate_fn = col , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , ) model . eval () model . on_predict_epoch_start () device = model . device . type prevplot = model . doplot model . pred_log_adata = True model . doplot = self . doplot and not self . keep_all_labels_pred model . save_expr = False rand = random_str () dtype = ( torch . float16 if isinstance ( model . transformer , FlashTransformer ) else model . dtype ) with ( torch . no_grad (), torch . autocast ( device_type = device , dtype = dtype ), ): for batch in tqdm ( dataloader ): gene_pos , expression , depth = ( batch [ \"genes\" ] . to ( device ), batch [ \"x\" ] . to ( device ), batch [ \"depth\" ] . to ( device ), ) pred = model . _predict ( gene_pos , expression , depth , knn_cells = ( batch [ \"knn_cells\" ] . to ( device ) if model . expr_emb_style == \"metacell\" and self . use_knn else None ), knn_cells_info = ( batch [ \"knn_cells_info\" ] . to ( device ) if model . expr_emb_style == \"metacell\" and self . use_knn else None ), pred_embedding = self . pred_embedding , max_size_in_mem = self . save_every , name = \"embed_\" + rand + \"_\" , ) torch . cuda . empty_cache () if self . keep_all_labels_pred : if pred is not None : self . pred = ( pred if self . pred is None else torch . cat ([ self . pred , pred ]) ) model . log_adata ( name = \"embed_\" + rand + \"_\" + str ( model . counter )) model . pos = None model . expr_pred = None model . embs = None if self . keep_all_labels_pred : self . pred = ( model . pred if self . pred is None else torch . cat ([ self . pred , model . pred ]) ) model . pred = None model . save_expr = True try : mdir = ( model . logger . save_dir if model . logger . save_dir is not None else \"data\" ) except : mdir = \"data\" pred_adata = [] del adataset , dataloader for i in range ( model . counter + 1 ): file = ( mdir + \"/step_\" + str ( model . global_step ) + \"_\" + model . name + \"_embed_\" + rand + \"_\" + str ( i ) + \"_\" + str ( model . global_rank ) + \".h5ad\" ) pred_adata . append ( sc . read_h5ad ( file )) os . remove ( file ) pred_adata = concat ( pred_adata ) pred_adata . obs . index = adata . obs . index try : adata . obsm [ \"X_scprint_umap\" ] = pred_adata . obsm [ \"X_umap\" ] del pred_adata . obsm [ \"X_umap\" ] except : print ( \"too few cells to embed into a umap\" ) try : adata . obs [ \"scprint_leiden\" ] = pred_adata . obs [ \"scprint_leiden\" ] except : print ( \"too few cells to compute a clustering\" ) if self . pred_embedding == [ \"all\" ]: pred_embedding = [ \"other\" ] + model . classes else : pred_embedding = self . pred_embedding if len ( pred_embedding ) == 1 : adata . obsm [ \"scprint_emb\" ] = pred_adata . obsm [ \"scprint_emb_\" + pred_embedding [ 0 ] ] . astype ( np . float32 ) else : adata . obsm [ \"scprint_emb\" ] = np . zeros ( pred_adata . obsm [ \"scprint_emb_\" + pred_embedding [ 0 ]] . shape , dtype = np . float32 , ) i = 0 for k , v in pred_adata . obsm . items (): adata . obsm [ k ] = v . astype ( np . float32 ) if model . compressor is not None : if i == 0 : adata . obsm [ \"scprint_emb\" ] = v . astype ( np . float32 ) else : adata . obsm [ \"scprint_emb\" ] = np . hstack ( [ adata . obsm [ \"scprint_emb\" ], v . astype ( np . float32 )] ) else : adata . obsm [ \"scprint_emb\" ] += v . astype ( np . float32 ) i += 1 if model . compressor is None : adata . obsm [ \"scprint_emb\" ] = adata . obsm [ \"scprint_emb\" ] / i for key , value in pred_adata . uns . items (): adata . uns [ key ] = value pred_adata . obs . index = adata . obs . index model . keep_all_labels_pred = prevkeep model . doplot = prevplot adata . obs = pd . concat ([ adata . obs , pred_adata . obs ], axis = 1 ) del pred_adata if self . keep_all_labels_pred : allclspred = self . pred . to ( device = \"cpu\" ) . numpy () columns = [] for cl in model . classes : n = model . label_counts [ cl ] columns += [ model . label_decoders [ cl ][ i ] for i in range ( n )] allclspred = pd . DataFrame ( allclspred , columns = columns , index = adata . obs . index ) adata . obs = pd . concat ([ adata . obs , allclspred ], axis = 1 ) metrics = {} if self . doclass and not self . keep_all_labels_pred : for cl in model . classes : res = [] if cl not in adata . obs . columns : continue class_topred = model . label_decoders [ cl ] . values () if cl in model . labels_hierarchy : # class_groupings = { # k: [ # i.ontology_id # for i in bt.CellType.filter(k).first().children.all() # ] # for k in set(adata.obs[cl].unique()) - set(class_topred) # } cur_labels_hierarchy = { model . label_decoders [ cl ][ k ]: [ model . label_decoders [ cl ][ i ] for i in v ] for k , v in model . labels_hierarchy [ cl ] . items () } else : cur_labels_hierarchy = {} for pred , true in adata . obs [[ \"pred_\" + cl , cl ]] . values : if pred == true : res . append ( True ) continue if len ( cur_labels_hierarchy ) > 0 : if true in cur_labels_hierarchy : res . append ( pred in cur_labels_hierarchy [ true ]) continue elif true != self . unknown_label : res . append ( False ) elif true not in class_topred : print ( f \"true label { true } not in available classes\" ) return adata , metrics elif true not in class_topred : print ( f \"true label { true } not in available classes\" ) return adata , metrics elif true != self . unknown_label : res . append ( False ) # else true is unknown # else we pass if len ( res ) == 0 : # true was always unknown res = [ 1 ] if self . doplot : print ( \" \" , cl ) print ( \" accuracy:\" , sum ( res ) / len ( res )) print ( \" \" ) metrics . update ({ cl + \"_accuracy\" : sum ( res ) / len ( res )}) self . pred = None return adata , metrics compute_classification Compute classification metrics for the given annotated data. Parameters: adata ( AnnData ) \u2013 The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. classes ( List [ str ] ) \u2013 List of class labels to be used for classification. label_decoders ( Dict [ str , Any ] ) \u2013 Dictionary of label decoders for each class. labels_hierarchy ( Dict [ str , Any ] ) \u2013 Dictionary representing the hierarchy of labels. metric_type ( List [ str ] , default: ['macro', 'micro', 'weighted'] ) \u2013 List of metric types to compute. Defaults to [\"macro\", \"micro\", \"weighted\"]. Returns: Dict [ str , Dict [ str , float ]] \u2013 Dict[str, Dict[str, float]]: A dictionary containing classification metrics for each class. Source code in scprint2/tasks/cell_emb.py 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 def compute_classification ( adata : AnnData , classes : List [ str ], label_decoders : Dict [ str , Any ], labels_hierarchy : Dict [ str , Any ], metric_type : List [ str ] = [ \"macro\" , \"micro\" , \"weighted\" ], use_unknown : bool = False , ) -> Dict [ str , Dict [ str , float ]]: \"\"\" Compute classification metrics for the given annotated data. Args: adata (AnnData): The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. classes (List[str]): List of class labels to be used for classification. label_decoders (Dict[str, Any]): Dictionary of label decoders for each class. labels_hierarchy (Dict[str, Any]): Dictionary representing the hierarchy of labels. metric_type (List[str], optional): List of metric types to compute. Defaults to [\"macro\", \"micro\", \"weighted\"]. Returns: Dict[str, Dict[str, float]]: A dictionary containing classification metrics for each class. \"\"\" metrics = {} for clss in classes : res = [] if clss not in adata . obs . columns : print ( \"not in columns\" ) continue labels_topred = label_decoders [ clss ] . values () if clss in labels_hierarchy : parentdf = ( bt . CellType . filter () . to_dataframe ( include = [ \"parents__ontology_id\" , \"ontology_id\" ], limit = None ) . set_index ( \"ontology_id\" )[[ \"parents__ontology_id\" ]] ) parentdf . parents__ontology_id = parentdf . parents__ontology_id . astype ( str ) class_groupings = { k : get_descendants ( k , parentdf ) for k in set ( adata . obs [ clss ] . unique ()) } tokeep = np . array ([ True ] * adata . shape [ 0 ]) for i , ( pred , true ) in enumerate ( adata . obs [[ \"pred_\" + clss , clss ]] . values ): if pred == true : res . append ( true ) continue if true == \"unknown\" : tokeep [ i ] = False if clss in labels_hierarchy : if true in class_groupings : if pred == \"unknown\" and not use_unknown : tokeep [ i ] = False res . append ( true if pred in class_groupings [ true ] else \"\" ) continue elif true not in labels_topred : raise ValueError ( f \"true label { true } not in available classes\" ) elif true not in labels_topred : raise ValueError ( f \"true label { true } not in available classes\" ) res . append ( \"\" ) metrics [ clss ] = {} metrics [ clss ][ \"accuracy\" ] = np . mean ( np . array ( res )[ tokeep ] == adata . obs [ clss ] . values [ tokeep ] ) for x in metric_type : metrics [ clss ][ x ] = f1_score ( np . array ( res )[ tokeep ], adata . obs [ clss ] . values [ tokeep ], average = x ) return metrics compute_corr Compute the correlation between the output and target matrices. Parameters: out ( ndarray ) \u2013 The output matrix. to ( ndarray ) \u2013 The target matrix. doplot ( bool , default: True ) \u2013 Whether to generate a plot of the correlation coefficients. Defaults to True. compute_mean_regress ( bool , default: False ) \u2013 Whether to compute mean regression. Defaults to False. plot_corr_size ( int , default: 64 ) \u2013 The size of the plot for correlation. Defaults to 64. Returns: dict ( dict ) \u2013 A dictionary containing the computed metrics. Source code in scprint2/tasks/cell_emb.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 def compute_corr ( out : np . ndarray , to : np . ndarray , doplot : bool = True , compute_mean_regress : bool = False , plot_corr_size : int = 64 , ) -> dict : \"\"\" Compute the correlation between the output and target matrices. Args: out (np.ndarray): The output matrix. to (np.ndarray): The target matrix. doplot (bool, optional): Whether to generate a plot of the correlation coefficients. Defaults to True. compute_mean_regress (bool, optional): Whether to compute mean regression. Defaults to False. plot_corr_size (int, optional): The size of the plot for correlation. Defaults to 64. Returns: dict: A dictionary containing the computed metrics. \"\"\" metrics = {} corr_coef , p_value = spearmanr ( out , to . T , ) corr_coef [ p_value > 0.05 ] = 0 # corr_coef[] # only on non zero values, # compare a1-b1 corr with a1-b(n) corr. should be higher # Plot correlation coefficient val = plot_corr_size + 2 if compute_mean_regress else plot_corr_size metrics . update ( { \"recons_corr\" : np . mean ( corr_coef [ val :, : plot_corr_size ] . diagonal ())} ) if compute_mean_regress : metrics . update ( { \"mean_regress\" : np . mean ( corr_coef [ plot_corr_size : plot_corr_size + 2 , : plot_corr_size , ] . flatten () ) } ) if doplot : plt . figure ( figsize = ( 10 , 5 )) plt . imshow ( corr_coef , cmap = \"coolwarm\" , interpolation = \"none\" , vmin =- 1 , vmax = 1 ) plt . colorbar () plt . title ( 'Correlation Coefficient of expr and i[\"x\"]' ) plt . show () return metrics default_benchmark Run the default benchmark for embedding and annotation using the scPRINT model. Parameters: model ( Module ) \u2013 The scPRINT model to be used for embedding and annotation. folder_dir ( str , default: FILE_LOC + '/../../data/' ) \u2013 The directory containing data files. dataset ( str , default: FILE_LOC + '/../../data/gNNpgpo6gATjuxTE7CCp.h5ad' ) \u2013 The dataset to use for benchmarking. Can be a path or URL. do_class ( bool , default: True ) \u2013 Whether to perform classification. Defaults to True. coarse ( bool , default: False ) \u2013 Whether to use coarse cell type annotations. Defaults to False. Returns: dict ( dict ) \u2013 A dictionary containing the benchmark metrics. Source code in scprint2/tasks/cell_emb.py 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 def default_benchmark ( model : torch . nn . Module , folder_dir : str = FILE_LOC + \"/../../data/\" , dataset : str = FILE_LOC + \"/../../data/gNNpgpo6gATjuxTE7CCp.h5ad\" , do_class : bool = True , coarse : bool = False , ) -> dict : \"\"\" Run the default benchmark for embedding and annotation using the scPRINT model. Args: model (torch.nn.Module): The scPRINT model to be used for embedding and annotation. folder_dir (str, optional): The directory containing data files. dataset (str, optional): The dataset to use for benchmarking. Can be a path or URL. do_class (bool, optional): Whether to perform classification. Defaults to True. coarse (bool, optional): Whether to use coarse cell type annotations. Defaults to False. Returns: dict: A dictionary containing the benchmark metrics. \"\"\" if dataset . startswith ( \"https://\" ): adata = sc . read ( folder_dir + dataset . split ( \"/\" )[ - 1 ] + ( \".h5ad\" if not dataset . endswith ( \".h5ad\" ) else \"\" ), backup_url = dataset , ) else : adata = sc . read_h5ad ( dataset ) if adata . shape [ 0 ] > 100_000 : adata = adata [ adata . obs_names [ np . random . choice ( adata . shape [ 0 ], 100_000 , replace = False )] ] max_len = 4000 if adata . X . sum ( 1 ) . mean () < 50_000 else 8000 batch_size = 64 if adata . X . sum ( 1 ) . mean () < 50_000 else 32 log_every = 10_000 if dataset . split ( \"/\" )[ - 1 ] in [ \"24539942\" , \"24539828\" ]: # lung and pancreas adata . obs [ \"organism_ontology_term_id\" ] = \"NCBITaxon:9606\" use_layer = \"counts\" is_symbol = True batch_key = \"tech\" if dataset . split ( \"/\" )[ - 1 ] == \"24539828\" else \"batch\" label_key = \"celltype\" if dataset . split ( \"/\" )[ - 1 ] == \"24539828\" else \"cell_type\" adata . obs [ \"cell_type_ontology_term_id\" ] = adata . obs [ label_key ] . replace ( COARSE if coarse else FINE ) adata . obs [ \"assay_ontology_term_id\" ] = adata . obs [ batch_key ] . replace ( COARSE if coarse else FINE ) else : use_layer = None is_symbol = False batch_key = ( \"batch\" if dataset . split ( \"/\" )[ - 1 ] == \"661d5ec2-ca57-413c-8374-f49b0054ddba.h5ad\" else \"assay_ontology_term_id\" ) label_key = \"cell_type_ontology_term_id\" preprocessor = Preprocessor ( use_layer = use_layer , is_symbol = is_symbol , force_preprocess = True , skip_validate = True , do_postp = model . expr_emb_style == \"metacell\" , drop_non_primary = False , ) adata = preprocessor ( adata . copy ()) if model . expr_emb_style == \"metacell\" : sc . pp . neighbors ( adata , use_rep = \"X_pca\" ) embedder = Embedder ( pred_embedding = ( model . pred_embedding if model . pred_embedding is not None else [ \"all\" ] ), doclass = do_class , max_len = max_len , doplot = False , keep_all_labels_pred = False , save_every = log_every , batch_size = batch_size , how = \"random expr\" , ) adata , metrics = embedder ( model , adata ) bm = Benchmarker ( adata , batch_key = batch_key , label_key = label_key , embedding_obsm_keys = [ \"scprint_emb\" ], ) bm . benchmark () metrics . update ( { \"scib\" : bm . get_results ( min_max_scale = False ) . T . to_dict ()[ \"scprint_emb\" ]} ) if model . class_scale > 0 : metrics [ \"classif\" ] = compute_classification ( adata , model . classes , model . label_decoders , model . labels_hierarchy ) return metrics display_confusion_matrix Display the confusion matrix for true vs predicted cell types. Parameters: nadata ( AnnData ) \u2013 Annotated data object containing predictions and ground truth. pred ( str , default: 'conv_pred_cell_type_ontology_term_id' ) \u2013 Column name for predictions. Defaults to \"conv_pred_cell_type_ontology_term_id\". true ( str , default: 'cell_type' ) \u2013 Column name for ground truth. Defaults to \"cell_type\". Source code in scprint2/tasks/cell_emb.py 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 def display_confusion_matrix ( nadata , pred = \"conv_pred_cell_type_ontology_term_id\" , true = \"cell_type\" ): \"\"\" Display the confusion matrix for true vs predicted cell types. Args: nadata (AnnData): Annotated data object containing predictions and ground truth. pred (str): Column name for predictions. Defaults to \"conv_pred_cell_type_ontology_term_id\". true (str): Column name for ground truth. Defaults to \"cell_type\". \"\"\" counts = None for k , v in nadata . obs [ true ] . value_counts () . items (): name = k + \" - \" + str ( v ) if counts is None : counts = pd . DataFrame ( nadata . obs . loc [ nadata . obs [ true ] == k , pred , ] . value_counts () ) . rename ( columns = { \"count\" : name }) else : counts = pd . concat ( [ counts , pd . DataFrame ( nadata . obs . loc [ nadata . obs [ true ] == k , pred , ] . value_counts (), ) . rename ( columns = { \"count\" : name }), ], axis = 1 , ) counts = counts . T # Fill NaN values with 0 for visualization counts_filled = counts . fillna ( 0 ) # Create the heatmap plt . figure ( figsize = ( 12 , 10 )) # Convert to percentages (row-wise normalization) counts_percentage = counts_filled . div ( counts_filled . sum ( axis = 1 ), axis = 0 ) * 100 counts_percentage = counts_percentage . iloc [:, counts_percentage . values . max ( 0 ) > 5 ] ax = sns . heatmap ( counts_percentage , cmap = \"Blues\" , cbar_kws = { \"label\" : \"Percentage (%)\" }, linewidths = 0.5 , square = True , ) # place the x-label on top ax . xaxis . set_label_position ( \"top\" ) ax . xaxis . tick_top () plt . title ( \"Confusion Matrix: \" + true + \" vs \" + pred + \" (Percentage)\" , fontsize = 16 , pad = 20 , ) ax . set_xlabel ( pred , fontsize = 12 ) ax . set_ylabel ( true + \" (with counts)\" , fontsize = 12 ) ax . set_xticklabels ( ax . get_xticklabels (), rotation = 45 , ha = \"left\" , fontsize = 12 ) ax . set_yticklabels ( ax . get_yticklabels (), rotation = 0 , fontsize = 14 ) plt . tight_layout () plt . show () scprint2.tasks.grn Classes: Name Description GNInfer Functions: Name Description default_benchmark default_benchmark function to run the default scPRINT GRN benchmark GNInfer GNInfer a class to infer gene regulatory networks from a dataset using a scPRINT model. Parameters: batch_size ( int , default: 64 ) \u2013 Batch size for processing. Defaults to 64. num_workers ( int , default: 8 ) \u2013 Number of workers for data loading. Defaults to 8. drop_unexpressed ( bool , default: True ) \u2013 Whether to drop unexpressed genes. Defaults to True. In this context, genes that have no expression in the dataset are dropped. num_genes ( int , default: 3000 ) \u2013 Number of genes to consider. Defaults to 3000. max_cells ( int , default: 0 ) \u2013 Maximum number of cells to consider. Defaults to 0. if less than total number of cells, only the top max_cells cells with the most counts will be considered. cell_type_col ( str , default: 'cell_type' ) \u2013 Column name for cell type information. Defaults to \"cell_type\". how ( str , default: 'most var within' ) \u2013 Method to select genes. Options are \"most var\", \"random expr\", \"some\". Defaults to \"most var\". - \"most var across\": select the most variable genes across all cell types - \"most var within\": select the most variable genes within a cell type - \"random expr\": select random expressed genes - \"some\": select a subset of genes defined in genelist - \"most expr\": select the most expressed genes in the cell type genelist ( list , default: None ) \u2013 List of genes to consider. Defaults to an empty list. layer ( Optional [ List [ int ]] , default: None ) \u2013 List of layers to use for the inference. Defaults to None. preprocess ( str , default: 'softmax' ) \u2013 Preprocessing method. Options are \"softmax\", \"sinkhorn\", \"none\". Defaults to \"softmax\". head_agg ( str , default: 'mean' ) \u2013 Aggregation method for heads. Options are \"mean_full\", \"mean\", \"sum\", \"none\". Defaults to \"mean\". filtration ( str , default: 'thresh' ) \u2013 Filtration method for the adjacency matrix. Options are \"thresh\", \"top-k\", \"mst\", \"known\", \"none\". Defaults to \"thresh\". k ( int , default: 10 ) \u2013 Number of top connections to keep if filtration is \"top-k\". Defaults to 10. known_grn ( optional , default: None ) \u2013 Known gene regulatory network to use as a reference. Defaults to None. - We will only keep the genes that are present in the known GRN. precomp_attn ( bool , default: True ) \u2013 Whether to let the model precompute attn or do it at the end. This takes more memory but the model can compute mean over the attention matrices instead of over the qs and ks then taking the product. It is required for mean_full head_agg. Defaults to False. symmetrize ( bool , default: False ) \u2013 Whether to GRN. Defaults to False. loc ( str , default: './' ) \u2013 Location to save results. Defaults to \"./\". use_knn ( bool , default: True ) \u2013 Whether to use k-nearest neighbors information. Defaults to True. Methods: Name Description __call__ call runs the method aggregate part to aggregate the qks and compute the attns filter part to filter the attn matrix given user inputs predict part to predict the qks or attns matrices from the adata with the model Source code in scprint2/tasks/grn.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __init__ ( self , batch_size : int = 64 , num_workers : int = 8 , drop_unexpressed : bool = True , num_genes : int = 3000 , max_cells : int = 0 , cell_type_col : str = \"cell_type\" , how : str = \"most var within\" , # random expr, most var within, most var across, some genelist : Optional [ List [ str ]] = None , layer : Optional [ List [ int ]] = None , preprocess : str = \"softmax\" , # sinkhorn, softmax, none head_agg : str = \"mean\" , # mean, sum, none, mean_full filtration : str = \"thresh\" , # thresh, top-k, mst, known, none k : int = 10 , known_grn : Optional [ Any ] = None , precomp_attn : bool = True , symmetrize : bool = False , loc : str = \"./\" , use_knn : bool = True , ): \"\"\" GNInfer a class to infer gene regulatory networks from a dataset using a scPRINT model. Args: batch_size (int, optional): Batch size for processing. Defaults to 64. num_workers (int, optional): Number of workers for data loading. Defaults to 8. drop_unexpressed (bool, optional): Whether to drop unexpressed genes. Defaults to True. In this context, genes that have no expression in the dataset are dropped. num_genes (int, optional): Number of genes to consider. Defaults to 3000. max_cells (int, optional): Maximum number of cells to consider. Defaults to 0. if less than total number of cells, only the top `max_cells` cells with the most counts will be considered. cell_type_col (str, optional): Column name for cell type information. Defaults to \"cell_type\". how (str, optional): Method to select genes. Options are \"most var\", \"random expr\", \"some\". Defaults to \"most var\". - \"most var across\": select the most variable genes across all cell types - \"most var within\": select the most variable genes within a cell type - \"random expr\": select random expressed genes - \"some\": select a subset of genes defined in genelist - \"most expr\": select the most expressed genes in the cell type genelist (list, optional): List of genes to consider. Defaults to an empty list. layer (Optional[List[int]], optional): List of layers to use for the inference. Defaults to None. preprocess (str, optional): Preprocessing method. Options are \"softmax\", \"sinkhorn\", \"none\". Defaults to \"softmax\". head_agg (str, optional): Aggregation method for heads. Options are \"mean_full\", \"mean\", \"sum\", \"none\". Defaults to \"mean\". filtration (str, optional): Filtration method for the adjacency matrix. Options are \"thresh\", \"top-k\", \"mst\", \"known\", \"none\". Defaults to \"thresh\". k (int, optional): Number of top connections to keep if filtration is \"top-k\". Defaults to 10. known_grn (optional): Known gene regulatory network to use as a reference. Defaults to None. - We will only keep the genes that are present in the known GRN. precomp_attn (bool, optional): Whether to let the model precompute attn or do it at the end. This takes more memory but the model can compute mean over the attention matrices instead of over the qs and ks then taking the product. It is required for mean_full head_agg. Defaults to False. symmetrize (bool, optional): Whether to GRN. Defaults to False. loc (str, optional): Location to save results. Defaults to \"./\". use_knn (bool, optional): Whether to use k-nearest neighbors information. Defaults to True. \"\"\" self . batch_size = batch_size self . num_workers = num_workers self . layer = layer self . loc = loc self . how = how assert self . how in [ \"most var within\" , \"most var across\" , \"random expr\" , \"some\" , \"most expr\" , ], \"how must be one of 'most var within', 'most var across', 'random expr', 'some', 'most expr'\" self . num_genes = num_genes if self . how != \"some\" else len ( self . genelist ) self . preprocess = preprocess self . cell_type_col = cell_type_col self . filtration = filtration self . genelist = genelist if genelist is not None else [] self . k = k self . symmetrize = symmetrize self . known_grn = known_grn self . head_agg = head_agg self . max_cells = max_cells self . curr_genes = None self . drop_unexpressed = drop_unexpressed self . use_knn = use_knn if self . filtration != \"none\" and self . head_agg == \"none\" : raise ValueError ( \"filtration must be 'none' when head_agg is 'none'\" ) __call__ call runs the method Parameters: model ( Module ) \u2013 The model to be used for generating the network adata ( AnnData ) \u2013 Annotated data matrix of shape n_obs \u00d7 n_vars . n_obs is the number of cells and n_vars is the number of genes. cell_type ( str , default: None ) \u2013 Specific cell type to filter the data. Defaults to None. Returns: AnnData ( AnnData ) \u2013 Annotated data matrix with predictions and annotations. ndarray \u2013 np.ndarray: Filtered adjacency matrix. Source code in scprint2/tasks/grn.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def __call__ ( self , model : torch . nn . Module , adata : AnnData , cell_type = None ) -> tuple [ AnnData , np . ndarray ]: \"\"\" __call__ runs the method Args: model (torch.nn.Module): The model to be used for generating the network adata (AnnData): Annotated data matrix of shape `n_obs` \u00d7 `n_vars`. `n_obs` is the number of cells and `n_vars` is the number of genes. cell_type (str, optional): Specific cell type to filter the data. Defaults to None. Returns: AnnData: Annotated data matrix with predictions and annotations. np.ndarray: Filtered adjacency matrix. \"\"\" # Add at least the organism you are working with if self . layer is None : self . layer = list ( range ( model . nlayers )) self . n_cell_embs = model . attn . additional_tokens subadata = self . predict ( model , adata , self . layer , cell_type ) adjacencies = self . aggregate ( model ) model . attn . data = None if self . head_agg == \"none\" : return self . save ( adjacencies [ self . n_cell_embs :, self . n_cell_embs :, :], subadata , ) else : return self . save ( self . filter ( adjacencies )[ self . n_cell_embs :, self . n_cell_embs :], subadata , loc = self . loc , ) aggregate part to aggregate the qks and compute the attns or to aggregate the attns or do nothing if already done Source code in scprint2/tasks/grn.py 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 def aggregate ( self , model ): \"\"\" part to aggregate the qks and compute the attns or to aggregate the attns or do nothing if already done \"\"\" attn , genes = model . attn . get (), model . genes if model . attn . precomp_attn : self . curr_genes = [ i for i in genes if i in self . curr_genes ] return attn . detach () . cpu () . numpy () if self . how == \"random expr\" and self . drop_unexpressed : keep = np . array ( [ 1 ] * self . n_cell_embs + [ i in self . curr_genes for i in genes ], dtype = bool , ) attn = attn [:, keep , :, :, :] badloc = torch . isnan ( attn . sum (( 0 , 2 , 3 , 4 ))) attn = attn [:, ~ badloc , :, :, :] badloc = badloc . detach () . cpu () . numpy () self . curr_genes = ( np . array ( self . curr_genes )[ ~ badloc [ self . n_cell_embs :]] if self . how == \"random expr\" else [ i for i in genes if i in self . curr_genes ] ) # attn = attn[:, :, 0, :, :].permute(0, 2, 1, 3) @ attn[:, :, 1, :, :].permute( # 0, 2, 3, 1 # ) attns = None Qs = ( attn [:, :, 0 , :, :] . permute ( 0 , 2 , 1 , 3 ) . reshape ( - 1 , attn . shape [ 1 ], attn . shape [ - 1 ]) ) Ks = ( attn [:, :, 1 , :, :] . permute ( 0 , 2 , 1 , 3 ) . reshape ( - 1 , attn . shape [ 1 ], attn . shape [ - 1 ]) ) for i in range ( Qs . shape [ 0 ]): attn = Qs [ i ] @ Ks [ i ] . T # return attn if self . preprocess == \"sinkhorn\" : scale = Qs . shape [ - 1 ] ** - 0.5 attn = attn * scale if attn . numel () > 100_000_000 : raise ValueError ( \"you can't sinkhorn such a large matrix\" ) sink = SinkhornDistance ( 0.1 , max_iter = 200 ) attn = sink ( attn )[ 0 ] attn = attn * Qs . shape [ - 1 ] elif self . preprocess == \"softmax\" : scale = Qs . shape [ - 1 ] ** - 0.5 attn = attn * scale attn = torch . nn . functional . softmax ( attn , dim =- 1 ) elif self . preprocess == \"softpick\" : attn = softpick ( attn ) elif self . preprocess == \"none\" : pass else : raise ValueError ( \"preprocess must be one of 'sinkhorn', 'softmax', 'none'\" ) if self . symmetrize : attn = ( attn + attn . T ) / 2 if self . head_agg == \"mean\" : attns = attn + ( attns if attns is not None else 0 ) elif self . head_agg == \"max\" : attns = torch . max ( attn , attns ) if attns is not None else attn elif self . head_agg == \"none\" : attn = attn . reshape ( attn . shape [ 0 ], attn . shape [ 1 ], 1 ) if attns is not None : attns = torch . cat (( attns , attn . detach () . cpu ()), dim = 2 ) else : attns = attn . detach () . cpu () else : raise ValueError ( \"head_agg must be one of 'mean', 'mean_full', 'max' or 'none'\" ) if self . head_agg == \"mean\" : attns = attns / Qs . shape [ 0 ] return ( attns . detach () . cpu () . numpy () if self . head_agg != \"none\" else attns . numpy () ) filter part to filter the attn matrix given user inputs Source code in scprint2/tasks/grn.py 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 def filter ( self , adj , gt = None ): \"\"\" part to filter the attn matrix given user inputs \"\"\" if self . filtration == \"thresh\" : adj [ adj < ( 1 / adj . shape [ - 1 ])] = 0 res = ( adj != 0 ) . sum () if res / adj . shape [ 0 ] ** 2 < 0.01 : adj = scipy . sparse . csr_matrix ( adj ) elif self . filtration == \"none\" : pass elif self . filtration == \"top-k\" : args = np . argsort ( adj ) adj [ np . arange ( adj . shape [ 0 ])[:, None ], args [:, : - self . k ]] = 0 adj = scipy . sparse . csr_matrix ( adj ) elif self . filtration == \"known\" and gt is not None : gt = gt . reindex ( sorted ( gt . columns ), axis = 1 ) gt = gt . reindex ( sorted ( gt . columns ), axis = 0 ) gt = gt [ gt . index . isin ( self . curr_genes )] . iloc [ :, gt . columns . isin ( self . curr_genes ) ] loc = np . isin ( self . curr_genes , gt . index ) self . curr_genes = np . array ( self . curr_genes )[ loc ] adj = adj [ self . n_cell_embs :, self . n_cell_embs :][ loc ][:, loc ] adj [ gt . values != 1 ] = 0 adj = scipy . sparse . csr_matrix ( adj ) elif self . filtration == \"tmfg\" : adj = nx . to_scipy_sparse_array ( tmfg ( adj )) elif self . filtration == \"mst\" : pass else : raise ValueError ( \"filtration must be one of 'thresh', 'none' or 'top-k'\" ) res = ( adj != 0 ) . sum () if self . filtration != \"none\" else adj . shape [ 0 ] ** 2 print ( f \"avg link count: { res } , sparsity: { res / adj . shape [ 0 ] ** 2 } \" ) return adj predict part to predict the qks or attns matrices from the adata with the model Source code in scprint2/tasks/grn.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 def predict ( self , model , adata , layer , cell_type = None ): \"\"\" part to predict the qks or attns matrices from the adata with the model \"\"\" self . curr_genes = None model . pred_log_adata = False if cell_type is not None : subadata = adata [ adata . obs [ self . cell_type_col ] == cell_type ] . copy () else : subadata = adata . copy () if self . how == \"most var within\" : try : sc . pp . highly_variable_genes ( subadata , flavor = \"seurat_v3\" , n_top_genes = self . num_genes ) except ValueError : sc . pp . highly_variable_genes ( subadata , flavor = \"seurat_v3\" , n_top_genes = self . num_genes , span = 0.6 , ) self . curr_genes = ( subadata . var . index [ subadata . var . highly_variable ] . tolist () + self . genelist ) print ( \"number of expressed genes in this cell type: \" + str (( subadata . X . sum ( 0 ) > 1 ) . sum ()) ) elif self . how == \"most var across\" and cell_type is not None : adata . raw = adata sc . tl . rank_genes_groups ( adata , mask_var = adata . var . index . isin ( model . genes ), groupby = self . cell_type_col , groups = [ cell_type ], ) diff_expr_genes = adata . uns [ \"rank_genes_groups\" ][ \"names\" ][ cell_type ] diff_expr_genes = [ gene for gene in diff_expr_genes if gene in model . genes ] self . curr_genes = diff_expr_genes [: self . num_genes ] + self . genelist self . curr_genes . sort () elif self . how == \"random expr\" : self . curr_genes = model . genes # raise ValueError(\"cannot do it yet\") pass elif self . how == \"some\" and len ( self . genelist ) > 0 : self . curr_genes = self . genelist elif self . how == \"most expr\" : self . curr_genes = adata . var . index [ adata . X . sum ( 0 ) . A1 . argsort ()[:: - 1 ] ] . tolist ()[: self . num_genes ] else : raise ValueError ( \"something wrong with your inputs\" ) if self . drop_unexpressed : expr = subadata . var [( subadata . X . sum ( 0 ) > 0 ) . tolist ()[ 0 ]] . index . tolist () self . curr_genes = [ i for i in self . curr_genes if i in expr ] # Order cells by total count cell_sums = subadata . X . sum ( axis = 1 ) order = np . argsort ( - cell_sums . A1 if scipy . sparse . issparse ( subadata . X ) else - cell_sums ) subadata = subadata [ order ] . copy () subadata = subadata [: self . max_cells ] if self . max_cells else subadata if len ( subadata ) == 0 : raise ValueError ( \"no cells in the dataset\" ) adataset = SimpleAnnDataset ( subadata , obs_to_output = [ \"organism_ontology_term_id\" ], get_knn_cells = model . expr_emb_style == \"metacell\" and self . use_knn , ) col = Collator ( organisms = model . organisms , valid_genes = model . genes , max_len = self . num_genes if self . how == \"random expr\" else 0 , how = \"some\" if self . how != \"random expr\" else \"random expr\" , genelist = self . curr_genes if self . how != \"random expr\" else [], n_bins = model . n_input_bins if model . expr_emb_style == \"binned\" else 0 , ) dataloader = DataLoader ( adataset , collate_fn = col , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , ) model . attn . precomp_attn = self . head_agg == \"mean_full\" if self . num_genes > 10_000 and model . attn . precomp_attn : raise ValueError ( \"need less genes for a non-shared-qk version\" ) prevplot = model . doplot model . doplot = False model . on_predict_epoch_start () model . eval () model . attn . data = None # reparametrize the attn process if model . transformer . attn_type == \"hyper\" : self . curr_genes = [ i for i in model . genes if i in self . curr_genes ] num = ( 1 if model . use_metacell_token else 0 ) + ( ( len ( model . classes ) + 1 ) if not model . cell_transformer else 0 ) if ( len ( self . curr_genes ) + num ) % 128 != 0 : self . curr_genes = self . curr_genes [ : ( len ( self . curr_genes ) // 128 * 128 ) - num ] if self . how != \"random expr\" : if model . attn . precomp_attn : model . attn . gene_dim = len ( set ( self . curr_genes ) & set ( model . genes )) model . attn . apply_softmax = self . preprocess == \"softmax\" else : if subadata . obs [ \"organism_ontology_term_id\" ] . unique () . shape [ 0 ] > 1 : raise ValueError ( \"only one organism at a time is supported for precomp_attn\" ) n = False for i , k in col . start_idx . items (): if n : model . attn . gene_dim = k - model . attn . speciesloc break if i == subadata . obs [ \"organism_ontology_term_id\" ] . unique ()[ 0 ]: model . attn . speciesloc = k n = True elif not model . attn . precomp_attn : raise ValueError ( \"full attention (i.e. precomp_attn=True) is not supported for random expr\" ) device = model . device . type dtype = ( torch . float16 if isinstance ( model . transformer , FlashTransformer ) else model . dtype ) with torch . no_grad (), torch . autocast ( device_type = device , dtype = dtype ): for batch in tqdm ( dataloader ): gene_pos , expression , depth = ( batch [ \"genes\" ] . to ( device ), batch [ \"x\" ] . to ( device ), batch [ \"depth\" ] . to ( device ), ) model . _predict ( gene_pos , expression , depth , knn_cells = ( batch [ \"knn_cells\" ] . to ( device ) if model . expr_emb_style == \"metacell\" and self . use_knn else None ), knn_cells_info = ( batch [ \"knn_cells_info\" ] . to ( device ) if model . expr_emb_style == \"metacell\" and self . use_knn else None ), keep_output = False , get_attention_layer = layer if type ( layer ) is list else [ layer ], ) torch . cuda . empty_cache () model . doplot = prevplot return subadata default_benchmark default_benchmark function to run the default scPRINT GRN benchmark Parameters: model ( Any ) \u2013 The scPRINT model to be used for the benchmark. default_dataset ( str , default: 'sroy' ) \u2013 The default dataset to use for benchmarking. Defaults to \"sroy\". cell_types ( List [ str ] , default: [] ) \u2013 List of cell types to include in the benchmark. Defaults to []. maxlayers ( int , default: 16 ) \u2013 Maximum number of layers to use from the model. Defaults to 16. maxgenes ( int , default: 5000 ) \u2013 Maximum number of genes to consider. Defaults to 5000. batch_size ( int , default: 32 ) \u2013 Batch size for processing. Defaults to 32. maxcells ( int , default: 1024 ) \u2013 Maximum number of cells to consider. Defaults to 1024. Returns: dict ( dict ) \u2013 A dictionary containing the benchmark metrics. Source code in scprint2/tasks/grn.py 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 def default_benchmark ( model : Any , default_dataset : str = \"sroy\" , cell_types : List [ str ] = [], maxlayers : int = 16 , maxgenes : int = 5000 , batch_size : int = 32 , maxcells : int = 1024 , ) -> dict : \"\"\" default_benchmark function to run the default scPRINT GRN benchmark Args: model (Any): The scPRINT model to be used for the benchmark. default_dataset (str, optional): The default dataset to use for benchmarking. Defaults to \"sroy\". cell_types (List[str], optional): List of cell types to include in the benchmark. Defaults to []. maxlayers (int, optional): Maximum number of layers to use from the model. Defaults to 16. maxgenes (int, optional): Maximum number of genes to consider. Defaults to 5000. batch_size (int, optional): Batch size for processing. Defaults to 32. maxcells (int, optional): Maximum number of cells to consider. Defaults to 1024. Returns: dict: A dictionary containing the benchmark metrics. \"\"\" metrics = {} layers = list ( range ( model . nlayers ))[ max ( 0 , model . nlayers - maxlayers ) :] clf_omni = None if default_dataset == \"sroy\" : preprocessor = Preprocessor ( is_symbol = True , force_preprocess = True , skip_validate = True , do_postp = model . expr_emb_style == \"metacell\" , min_valid_genes_id = 5000 , min_dataset_size = 64 , keepdata = True , ) clf_self = None todo = [ ( \"han\" , \"human\" , \"full\" ), ( \"mine\" , \"human\" , \"full\" ), ( \"han\" , \"human\" , \"chip\" ), ( \"han\" , \"human\" , \"ko\" ), ( \"tran\" , \"mouse\" , \"full\" ), ( \"zhao\" , \"mouse\" , \"full\" ), ( \"tran\" , \"mouse\" , \"chip\" ), ( \"tran\" , \"mouse\" , \"ko\" ), ] for da , spe , gt in todo : if gt != \"full\" : continue if \"NCBITaxon:10090\" not in model . organisms and spe == \"mouse\" : continue print ( da + \"_\" + gt ) preadata = get_sroy_gt ( get = da , species = spe , gt = gt ) adata = preprocessor ( preadata . copy ()) if model . expr_emb_style == \"metacell\" : sc . pp . neighbors ( adata , use_rep = \"X_pca\" ) grn_inferer = GNInfer ( layer = layers , how = \"most var within\" , preprocess = ( \"softpick\" if model . attention in [ \"softpick\" , \"softpick-flash\" ] else \"softmax\" ), head_agg = \"none\" , filtration = \"none\" , num_genes = maxgenes , num_workers = 8 , max_cells = maxcells , batch_size = batch_size , ) grn = grn_inferer ( model , adata ) grn . varp [ \"all\" ] = grn . varp [ \"GRN\" ] grn . var [ \"ensembl_id\" ] = grn . var . index grn . var [ \"symbol\" ] = make_index_unique ( grn . var [ \"symbol\" ] . astype ( str )) grn . var . index = grn . var [ \"symbol\" ] grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ] . mean ( - 1 ) . T metrics [ \"mean_\" + da + \"_\" + gt ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) grn . varp [ \"GRN\" ] = grn . varp [ \"GRN\" ] . T if spe == \"human\" : metrics [ \"mean_\" + da + \"_\" + gt + \"_base\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . scprint_benchmark () ## OMNI if clf_omni is None : grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ] _ , m , clf_omni = train_classifier ( grn , C = 1 , train_size = 0.9 , class_weight = { 1 : 800 , 0 : 1 }, shuffle = True , return_full = False , ) joblib . dump ( clf_omni , \"clf_omni.pkl\" ) metrics [ \"omni_classifier\" ] = m coef = clf_omni . coef_ [ 0 ] if clf_omni . coef_ . shape [ 0 ] == 1 else clf_omni . coef_ grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) if spe == \"human\" : metrics [ \"omni_\" + da + \"_\" + gt + \"_base\" ] = BenGRN ( grn , do_auc = True , doplot = True ) . scprint_benchmark () grn . varp [ \"GRN\" ] = grn . varp [ \"GRN\" ] . T metrics [ \"omni_\" + da + \"_\" + gt ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) ## SELF if clf_self is None : grn . varp [ \"GRN\" ] = np . transpose ( grn . varp [ \"all\" ], ( 1 , 0 , 2 )) _ , m , clf_self = train_classifier ( grn , other = preadata , C = 1 , train_size = 0.5 , class_weight = { 1 : 40 , 0 : 1 }, shuffle = False , return_full = False , ) metrics [ \"self_classifier\" ] = m coef = clf_self . coef_ [ 0 ] if clf_self . coef_ . shape [ 0 ] == 1 else clf_self . coef_ grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) . T metrics [ \"self_\" + da + \"_\" + gt ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) if spe == \"human\" : grn . varp [ \"GRN\" ] = grn . varp [ \"GRN\" ] . T metrics [ \"self_\" + da + \"_\" + gt + \"_base\" ] = BenGRN ( grn , do_auc = True , doplot = True ) . scprint_benchmark () ## chip / ko if ( da , spe , \"chip\" ) in todo : preadata = get_sroy_gt ( get = da , species = spe , gt = \"chip\" ) grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ] . mean ( - 1 ) . T metrics [ \"mean_\" + da + \"_\" + \"chip\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) . T metrics [ \"omni_\" + da + \"_\" + \"chip\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) . T metrics [ \"self_\" + da + \"_\" + \"chip\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) if ( da , spe , \"ko\" ) in todo : preadata = get_sroy_gt ( get = da , species = spe , gt = \"ko\" ) grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ] . mean ( - 1 ) . T metrics [ \"mean_\" + da + \"_\" + \"ko\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) . T metrics [ \"omni_\" + da + \"_\" + \"ko\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) . T metrics [ \"self_\" + da + \"_\" + \"ko\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) del grn elif default_dataset == \"gwps\" : adata = get_perturb_gt () preprocessor = Preprocessor ( force_preprocess = True , keepdata = True , skip_validate = True , do_postp = model . expr_emb_style == \"metacell\" , min_valid_genes_id = maxgenes , min_dataset_size = 64 , ) nadata = preprocessor ( adata . copy ()) if model . expr_emb_style == \"metacell\" : sc . pp . neighbors ( nadata , use_rep = \"X_pca\" ) nadata . var [ \"isTF\" ] = False nadata . var . loc [ nadata . var . gene_name . isin ( grnutils . TF ), \"isTF\" ] = True nadata . var [ \"isTF\" ] . sum () grn_inferer = GNInfer ( layer = layers , how = \"most var within\" , preprocess = ( \"softpick\" if model . attention in [ \"softpick\" , \"softpick-flash\" ] else \"softmax\" ), head_agg = \"none\" , filtration = \"none\" , num_genes = maxgenes , max_cells = maxcells , num_workers = 8 , batch_size = batch_size , ) grn = grn_inferer ( model , nadata ) del nadata grn . varp [ \"all\" ] = grn . varp [ \"GRN\" ] grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ] . mean ( - 1 ) . T metrics [ \"mean\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = adata ) grn . var [ \"ensembl_id\" ] = grn . var . index grn . var . index = grn . var [ \"symbol\" ] grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ] . mean ( - 1 ) metrics [ \"mean_base\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . scprint_benchmark () grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ] grn . var . index = grn . var [ \"ensembl_id\" ] _ , m , clf_omni = train_classifier ( grn , C = 1 , train_size = 0.9 , class_weight = { 1 : 800 , 0 : 1 }, shuffle = True , doplot = False , return_full = False , use_col = \"gene_name\" , ) coef = clf_omni . coef_ [ 0 ] if clf_omni . coef_ . shape [ 0 ] == 1 else clf_omni . coef_ grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) . T metrics [ \"omni\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = adata ) metrics [ \"omni_classifier\" ] = m grn . var . index = grn . var [ \"symbol\" ] grn . varp [ \"GRN\" ] = grn . varp [ \"GRN\" ] . T metrics [ \"omni_base\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . scprint_benchmark () grn . varp [ \"GRN\" ] = np . transpose ( grn . varp [ \"all\" ], ( 1 , 0 , 2 )) grn . var . index = grn . var [ \"ensembl_id\" ] _ , m , clf_self = train_classifier ( grn , other = adata , C = 1 , train_size = 0.5 , class_weight = { 1 : 40 , 0 : 1 }, doplot = False , shuffle = False , return_full = False , use_col = \"ensembl_id\" , ) coef = clf_self . coef_ [ 0 ] if clf_self . coef_ . shape [ 0 ] == 1 else clf_self . coef_ grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) . T metrics [ \"self\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = adata ) metrics [ \"self_classifier\" ] = m grn . var . index = grn . var [ \"symbol\" ] grn . varp [ \"GRN\" ] = grn . varp [ \"GRN\" ] . T metrics [ \"self_base\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . scprint_benchmark () elif default_dataset == \"genernib\" : raise ValueError ( \"Not implemented\" ) # for adata in [NORMAN, OP, ADAMSON]: # adata = sc.read_h5ad(adata) # adata.obs[\"organism_ontology_term_id\"] = \"NCBITaxon:9606\" # preprocessor = Preprocessor( # force_preprocess=False, # skip_validate=True, # drop_non_primary=False, # do_postp=False, # min_valid_genes_id=1000, # min_dataset_size=64, # keepdata=True, # is_symbol=True, # use_raw=False, # ) # adata = preprocessor(adata.copy()) # run_gene_rnib( # adata=adata, # model=model, # layer=layers, # how=\"most var within\", # preprocess=\"softmax\", # ) # grn_inferer = GNInfer( # how=\"most var across\", # preprocess=\"softmax\", # head_agg=\"mean\", # filtration=\"none\", # forward_mode=\"none\", # num_genes=3_000, # max_cells=3000, # batch_size=10, # cell_type_col=\"perturbation\", # layer=list(range(model.nlayers))[:], # ) # grn = grn_inferer(model, adata, cell_type=\"ctrl\") # grn.var.index = make_index_unique(grn.var[\"symbol\"].astype(str)) else : # max_genes=4000 if default_dataset . startswith ( \"https://\" ): adata = sc . read ( FILEDIR + \"/../../data/\" + default_dataset . split ( \"/\" )[ - 1 ], backup_url = default_dataset , ) else : adata = sc . read_h5ad ( default_dataset ) if default_dataset . split ( \"/\" )[ - 1 ] in [ \"yBCKp6HmXuHa0cZptMo7.h5ad\" ]: use_layer = \"counts\" is_symbol = True else : use_layer = None is_symbol = False preprocessor = Preprocessor ( use_layer = use_layer , is_symbol = is_symbol , force_preprocess = True , skip_validate = True , do_postp = model . expr_emb_style == \"metacell\" , drop_non_primary = False , ) adata = preprocessor ( adata . copy ()) adata . var [ \"isTF\" ] = False adata . var . loc [ adata . var . symbol . isin ( grnutils . TF ), \"isTF\" ] = True if model . expr_emb_style == \"metacell\" : if \"X_pca\" not in adata . obsm : sc . pp . pca ( adata , n_comps = 50 ) sc . pp . neighbors ( adata , use_rep = \"X_pca\" ) for celltype in list ( adata . obs [ \"cell_type\" ] . unique ())[: 14 ]: # print(celltype) # grn_inferer = GNInfer( # layer=layers, # how=\"random expr\", # preprocess=\"softmax\", # head_agg=\"max\", # filtration=\"none\", # num_workers=8, # num_genes=2200, # max_cells=maxcells, # batch_size=batch_size, # ) # # grn = grn_inferer(model, adata[adata.X.sum(1) > 500], cell_type=celltype) # grn.var.index = make_index_unique(grn.var[\"symbol\"].astype(str)) # metrics[celltype + \"_scprint\"] = BenGRN( # grn, doplot=False # ).scprint_benchmark() # del grn # gc.collect() grn_inferer = GNInfer ( layer = layers , how = \"most var across\" , preprocess = ( \"softpick\" if model . attention in [ \"softpick\" , \"softpick-flash\" ] else \"softmax\" ), head_agg = \"none\" , filtration = \"none\" , num_workers = 8 , num_genes = maxgenes , max_cells = maxcells , batch_size = batch_size , ) grn = grn_inferer ( model , adata [ adata . X . sum ( 1 ) > 500 ], cell_type = celltype ) grn . var . index = make_index_unique ( grn . var [ \"symbol\" ] . astype ( str )) grn . varp [ \"all\" ] = grn . varp [ \"GRN\" ] grn . varp [ \"GRN\" ] = grn . varp [ \"GRN\" ] . mean ( - 1 ) metrics [ celltype + \"_scprint_mean\" ] = BenGRN ( grn , doplot = False ) . scprint_benchmark () if clf_omni is None : grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ] _ , m , clf_omni = train_classifier ( grn , C = 1 , train_size = 0.6 , max_iter = 300 , class_weight = { 1 : 800 , 0 : 1 }, return_full = False , shuffle = True , doplot = False , ) joblib . dump ( clf_omni , \"clf_omni.pkl\" ) metrics [ \"classifier\" ] = m coef = clf_omni . coef_ [ 0 ] if clf_omni . coef_ . shape [ 0 ] == 1 else clf_omni . coef_ grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) metrics [ celltype + \"_scprint_class\" ] = BenGRN ( grn , doplot = False ) . scprint_benchmark () del grn gc . collect () return metrics scprint2.tasks.denoise Classes: Name Description Denoiser Functions: Name Description default_benchmark default_benchmark function used to run the default denoising benchmark of scPRINT split_molecules Splits molecules into two (potentially overlapping) groups. Denoiser Denoiser class for denoising scRNA-seq data using a scPRINT model Parameters: batch_size ( int , default: 10 ) \u2013 Batch size for processing. Defaults to 10. num_workers ( int , default: 1 ) \u2013 Number of workers for data loading. Defaults to 1. max_len ( int , default: 5000 ) \u2013 Maximum number of genes to consider. Defaults to 5000. how ( str , default: 'most var' ) \u2013 Method to select genes. Options are \"most var\", \"random expr\", \"some\". Defaults to \"most var\". - \"most var\": select the most variable genes - \"random expr\": select random expressed genes - \"some\": select a subset of genes defined in genelist max_cells ( int , default: 500000 ) \u2013 Number of cells to use for plotting correlation. Defaults to 10000. doplot ( bool , default: False ) \u2013 Whether to generate plots of the similarity between the denoised and true expression data. Defaults to False. Only works when downsample_expr is not None and max_cells < 100. predict_depth_mult ( int , default: 4 ) \u2013 Multiplier for prediction depth. Defaults to 4. This will artificially increase the sequencing depth (or number of counts) to 4 times the original depth. downsample_expr ( Optional [ float ] , default: None ) \u2013 Fraction of expression data to downsample. Defaults to None. This is usefull to test the ability of the model to denoise the dataset. only to use the input data as a benchmark dataset. When this option is on, the denoiser will output benchmark metrics genelist ( List [ str ] , default: None ) \u2013 The list of genes to be used for embedding. Defaults to []: In this case, \"how\" needs to be \"most var\" or \"random expr\". save_every ( int , default: 100000 ) \u2013 The number of cells to save at a time. Defaults to 100_000. This is important to avoid memory issues. pred_embedding ( List [ str ] , default: ['cell_type_ontology_term_id'] ) \u2013 The embedding type to be used as the denoising will also predict the cell embeddings. additional_info ( bool , default: False ) \u2013 Whether to print additional benchmark information during denoising. Defaults to False. only useful when downsampling is used. apply_zero_pred ( bool , default: False ) \u2013 Whether to apply zero inflation to the output value during denoising, else uses only the predicted mean. applying zero inflation might give results closer to the specific biases of sequencing technologies but less biological truthful. use_knn ( bool , default: True ) \u2013 Whether to use knn cells for denoising when the model uses metacell expression embedding. Defaults to True. Methods: Name Description __call__ call calling the function Source code in scprint2/tasks/denoise.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __init__ ( self , batch_size : int = 10 , num_workers : int = 1 , max_len : int = 5_000 , how : str = \"most var\" , max_cells : int = 500_000 , doplot : bool = False , predict_depth_mult : int = 4 , downsample_expr : Optional [ float ] = None , genelist : Optional [ List [ str ]] = None , save_every : int = 100_000 , pred_embedding : List [ str ] = [ \"cell_type_ontology_term_id\" ], additional_info : bool = False , apply_zero_pred : bool = False , use_knn : bool = True , ): \"\"\" Denoiser class for denoising scRNA-seq data using a scPRINT model Args: batch_size (int, optional): Batch size for processing. Defaults to 10. num_workers (int, optional): Number of workers for data loading. Defaults to 1. max_len (int, optional): Maximum number of genes to consider. Defaults to 5000. how (str, optional): Method to select genes. Options are \"most var\", \"random expr\", \"some\". Defaults to \"most var\". - \"most var\": select the most variable genes - \"random expr\": select random expressed genes - \"some\": select a subset of genes defined in genelist max_cells (int, optional): Number of cells to use for plotting correlation. Defaults to 10000. doplot (bool, optional): Whether to generate plots of the similarity between the denoised and true expression data. Defaults to False. Only works when downsample_expr is not None and max_cells < 100. predict_depth_mult (int, optional): Multiplier for prediction depth. Defaults to 4. This will artificially increase the sequencing depth (or number of counts) to 4 times the original depth. downsample_expr (Optional[float], optional): Fraction of expression data to downsample. Defaults to None. This is usefull to test the ability of the model to denoise the dataset. only to use the input data as a benchmark dataset. When this option is on, the denoiser will output benchmark metrics genelist (List[str], optional): The list of genes to be used for embedding. Defaults to []: In this case, \"how\" needs to be \"most var\" or \"random expr\". save_every (int, optional): The number of cells to save at a time. Defaults to 100_000. This is important to avoid memory issues. pred_embedding (List[str], optional): The embedding type to be used as the denoising will also predict the cell embeddings. additional_info (bool, optional): Whether to print additional benchmark information during denoising. Defaults to False. only useful when downsampling is used. apply_zero_pred (bool, optional): Whether to apply zero inflation to the output value during denoising, else uses only the predicted mean. applying zero inflation might give results closer to the specific biases of sequencing technologies but less biological truthful. use_knn (bool, optional): Whether to use knn cells for denoising when the model uses metacell expression embedding. Defaults to True. \"\"\" self . batch_size = batch_size self . num_workers = num_workers self . max_len = max_len self . max_cells = max_cells self . doplot = doplot self . predict_depth_mult = predict_depth_mult self . how = how self . downsample_expr = downsample_expr self . genelist = genelist self . save_every = save_every self . pred_embedding = pred_embedding self . additional_info = additional_info self . apply_zero_pred = apply_zero_pred self . use_knn = use_knn __call__ call calling the function Parameters: model ( Module ) \u2013 The scPRINT model to be used for denoising. adata ( AnnData ) \u2013 The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Returns: dict ( dict ) \u2013 The benchmark metrics if downsampling is used. Optional [ ndarray ] \u2013 Optional[np.ndarray]: The random set of cells used if max_cells < adata.shape[0]. AnnData ( AnnData ) \u2013 The denoised annotated data matrix. Source code in scprint2/tasks/denoise.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 def __call__ ( self , model : torch . nn . Module , adata : AnnData ) -> tuple [ dict , Optional [ np . ndarray ], AnnData ]: \"\"\" __call__ calling the function Args: model (torch.nn.Module): The scPRINT model to be used for denoising. adata (AnnData): The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Returns: dict: The benchmark metrics if downsampling is used. Optional[np.ndarray]: The random set of cells used if max_cells < adata.shape[0]. AnnData: The denoised annotated data matrix. \"\"\" # Select random number random_indices = None if self . max_cells < adata . shape [ 0 ]: random_indices = np . random . randint ( low = 0 , high = adata . shape [ 0 ], size = self . max_cells ) adataset = SimpleAnnDataset ( adata [ random_indices ], obs_to_output = [ \"organism_ontology_term_id\" ], get_knn_cells = model . expr_emb_style == \"metacell\" and self . use_knn , ) else : adataset = SimpleAnnDataset ( adata , obs_to_output = [ \"organism_ontology_term_id\" ], get_knn_cells = model . expr_emb_style == \"metacell\" and self . use_knn , ) if self . how == \"most var\" : sc . pp . highly_variable_genes ( adata , flavor = \"seurat_v3\" , n_top_genes = self . max_len , span = 0.99 ) self . genelist = adata . var . index [ adata . var . highly_variable ] else : self . genelist = adata . var . index self . genelist = [ i for i in model . genes if i in self . genelist ] print ( f \"working on { len ( self . genelist ) } accepted genes\" ) col = Collator ( organisms = model . organisms , valid_genes = model . genes , max_len = self . max_len , how = \"some\" if self . how == \"most var\" else self . how , genelist = self . genelist if self . how != \"random expr\" else [], n_bins = model . n_input_bins if model . expr_emb_style == \"binned\" else 0 , ) dataloader = DataLoader ( adataset , collate_fn = col , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , ) prevplot = model . doplot model . doplot = self . doplot model . on_predict_epoch_start () model . eval () device = model . device . type model . pred_log_adata = True stored_noisy = None rand = random_str () dtype = ( torch . float16 if type ( model . transformer ) is FlashTransformer else model . dtype ) torch . cuda . empty_cache () save_expr = model . save_expr model . save_expr = True with torch . no_grad (), torch . autocast ( device_type = device , dtype = dtype ): for batch in tqdm ( dataloader ): gene_pos , expression , depth = ( batch [ \"genes\" ] . to ( device ), batch [ \"x\" ] . to ( device ), batch [ \"depth\" ] . to ( device ), ) knn_cells = ( batch [ \"knn_cells\" ] . to ( device ) if model . expr_emb_style == \"metacell\" and self . use_knn else None ) if self . downsample_expr is not None : expression = utils . downsample_profile ( expression , self . downsample_expr ) if knn_cells is not None : for i in range ( knn_cells . shape [ 1 ]): knn_cells [:, i ] = utils . downsample_profile ( knn_cells [:, i ], self . downsample_expr ) if stored_noisy is None : stored_noisy = expression . cpu () . numpy () else : stored_noisy = np . concatenate ( [ stored_noisy , expression . cpu () . numpy ()], axis = 0 ) model . _predict ( gene_pos , expression , depth , knn_cells = ( batch [ \"knn_cells\" ] . to ( device ) if model . expr_emb_style == \"metacell\" and self . use_knn else None ), knn_cells_info = ( batch [ \"knn_cells_info\" ] . to ( device ) if model . expr_emb_style == \"metacell\" and self . use_knn else None ), do_generate = False , depth_mult = self . predict_depth_mult , pred_embedding = self . pred_embedding , max_size_in_mem = self . save_every , name = \"denoise_\" + rand + \"_\" , ) torch . cuda . empty_cache () model . log_adata ( name = \"denoise_\" + rand + \"_\" + str ( model . counter )) try : mdir = ( model . logger . save_dir if model . logger . save_dir is not None else \"data\" ) except : mdir = \"data\" pred_adata = [] for i in range ( model . counter + 1 ): file = ( mdir + \"/step_\" + str ( model . global_step ) + \"_\" + model . name + \"_denoise_\" + rand + \"_\" + str ( i ) + \"_\" + str ( model . global_rank ) + \".h5ad\" ) pred_adata . append ( sc . read_h5ad ( file )) os . remove ( file ) pred_adata = concat ( pred_adata ) if model . transformer . attn_type == \"hyper\" : # seq len must be a multiple of 128 num = ( 1 if model . use_metacell_token else 0 ) + ( ( len ( model . classes ) + 1 ) if not model . cell_transformer else 0 ) if ( stored_noisy . shape [ 1 ] + num ) % 128 != 0 : stored_noisy = stored_noisy [ :, : (( stored_noisy . shape [ 1 ]) // 128 * 128 ) - num ] pred_adata . X = stored_noisy metrics = None model . doplot = prevplot model . save_expr = save_expr if self . downsample_expr is not None : reco = np . array ( pred_adata . layers [ \"scprint_mu\" ] . data ) . reshape ( pred_adata . shape [ 0 ], - 1 ) # reco = reco * F.sigmoid( # torch.Tensor(np.array(pred_adata.layers[\"scprint_pi\"].data).reshape(pred_adata.shape[0], -1)) < 0.5 # ).numpy() adata = ( adata [ random_indices , adata . var . index . isin ( pred_adata . var . index )] if random_indices is not None else adata [:, adata . var . index . isin ( pred_adata . var . index )] ) true = adata [ :, pred_adata . var . index [ pred_adata . var . index . isin ( adata . var . index ) ] . to_list (), ] . X . toarray () if self . apply_zero_pred : reco = ( reco * ( 1 - F . sigmoid ( torch . Tensor ( np . array ( pred_adata . layers [ \"scprint_pi\" ] . data ) . reshape ( pred_adata . shape [ 0 ], - 1 ) ) ) ) . numpy () ) corr_coef , p_value = spearmanr ( np . vstack ([ reco [ true != 0 ], stored_noisy [ true != 0 ], true [ true != 0 ]]) . T ) metrics = { \"reco2noisy\" : corr_coef [ 0 , 1 ], \"reco2full\" : corr_coef [ 0 , 2 ], \"noisy2full\" : corr_coef [ 1 , 2 ], } if self . additional_info : # Sample only 3000 elements for correlation calculation if reco . shape [ 0 ] > 3000 : indices = np . random . choice ( reco . shape [ 0 ], 3000 , replace = False ) reco = reco [ indices ] stored_noisy = stored_noisy [ indices ] true = true [ indices ] corr , p_value = spearmanr ( np . vstack ( [ reco . flatten (), stored_noisy . flatten (), true . flatten (), ] ) . T ) m = { \"reco2full\" : corr [ 0 , 2 ], \"noisy2full\" : corr [ 1 , 2 ], } print ( \"corr with zeros: \" ) print ( m ) cell_wise = np . array ( [ spearmanr ( reco [ i ][ true [ i ] != 0 ], true [ i ][ true [ i ] != 0 ])[ 0 ] for i in range ( reco . shape [ 0 ]) ] ) torm = np . array ( [ spearmanr ( stored_noisy [ i ][ true [ i ] != 0 ], true [ i ][ true [ i ] != 0 ])[ 0 ] for i in range ( reco . shape [ 0 ]) ] ) cell_wise -= torm cell_wise_zero = np . mean ( [ spearmanr ( reco [ i ], true [ i ])[ 0 ] for i in range ( reco . shape [ 0 ])] ) print ( \"cell_wise self corr (reco, noisy, true)\" ) print ( { \"cell_wise_w_zero\" : cell_wise_zero , \"cell_wise_to_noisy\" : np . mean ( cell_wise ), } ) print ( \"depth-wise plot\" ) plot_cell_depth_wise_corr_improvement ( cell_wise , ( true > 0 ) . sum ( 1 )) if self . doplot and self . max_cells < 100 : corr_coef [ p_value > 0.05 ] = 0 plt . figure ( figsize = ( 10 , 5 )) plt . imshow ( corr_coef , cmap = \"coolwarm\" , interpolation = \"none\" , vmin =- 1 , vmax = 1 ) plt . colorbar () plt . title ( \"Expression Correlation Coefficient\" ) plt . show () return metrics , random_indices , pred_adata default_benchmark default_benchmark function used to run the default denoising benchmark of scPRINT Parameters: model ( Any ) \u2013 The scPRINT model to be used for the benchmark. folder_dir ( str , default: FILE_DIR + '/../../data/' ) \u2013 Directory containing data files. dataset ( str , default: FILE_DIR + '/../../data/gNNpgpo6gATjuxTE7CCp.h5ad' ) \u2013 Path to the dataset to use for benchmarking. Returns: dict ( dict ) \u2013 A dictionary containing the benchmark metrics. Source code in scprint2/tasks/denoise.py 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def default_benchmark ( model : Any , folder_dir : str = FILE_DIR + \"/../../data/\" , dataset : str = FILE_DIR + \"/../../data/gNNpgpo6gATjuxTE7CCp.h5ad\" , # r4iCehg3Tw5IbCLiCIbl ) -> dict : \"\"\" default_benchmark function used to run the default denoising benchmark of scPRINT Args: model (Any): The scPRINT model to be used for the benchmark. folder_dir (str, optional): Directory containing data files. dataset (str, optional): Path to the dataset to use for benchmarking. Returns: dict: A dictionary containing the benchmark metrics. \"\"\" if dataset . startswith ( \"https://\" ): adata = sc . read ( folder_dir + dataset . split ( \"/\" )[ - 1 ], backup_url = dataset , ) else : adata = sc . read_h5ad ( dataset ) if dataset . split ( \"/\" )[ - 1 ] == \"gNNpgpo6gATjuxTE7CCp.h5ad\" : use_layer = \"counts\" is_symbol = True else : use_layer = None is_symbol = False max_len = 4000 if adata . X . sum ( 1 ) . mean () < 150_000 else 8000 preprocessor = Preprocessor ( use_layer = use_layer , is_symbol = is_symbol , force_preprocess = True , skip_validate = True , do_postp = model . expr_emb_style == \"metacell\" , drop_non_primary = False , ) adata = preprocessor ( adata . copy ()) if model . expr_emb_style == \"metacell\" : if \"X_pca\" not in adata . obsm : sc . pp . pca ( adata , n_comps = 50 ) sc . pp . neighbors ( adata , use_rep = \"X_pca\" ) denoise = Denoiser ( batch_size = 40 if model . expr_emb_style != \"metacell\" else 20 , max_len = max_len , max_cells = 10_000 , doplot = False , num_workers = 8 , predict_depth_mult = 5 , downsample_expr = 0.7 , pred_embedding = model . pred_embedding , ) return denoise ( model , adata )[ 0 ] split_molecules Splits molecules into two (potentially overlapping) groups. :param umis: Array of molecules to split :param data_split: Proportion of molecules to assign to the first group :param overlap_factor: Overlap correction factor, if desired :param random_state: For reproducible sampling :return: umis_X and umis_Y, representing split and ~(1 - split) counts sampled from the input array Source code in scprint2/tasks/denoise.py 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def split_molecules ( umis : np . ndarray , data_split : float , overlap_factor : float = 0.0 , random_state : np . random . RandomState = None , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Splits molecules into two (potentially overlapping) groups. :param umis: Array of molecules to split :param data_split: Proportion of molecules to assign to the first group :param overlap_factor: Overlap correction factor, if desired :param random_state: For reproducible sampling :return: umis_X and umis_Y, representing ``split`` and ``~(1 - split)`` counts sampled from the input array \"\"\" if random_state is None : random_state = np . random . RandomState () umis_X_disjoint = random_state . binomial ( umis , data_split - overlap_factor ) umis_Y_disjoint = random_state . binomial ( umis - umis_X_disjoint , ( 1 - data_split ) / ( 1 - data_split + overlap_factor ) ) overlap_factor = umis - umis_X_disjoint - umis_Y_disjoint umis_X = umis_X_disjoint + overlap_factor umis_Y = umis_Y_disjoint + overlap_factor return umis_X , umis_Y scprint2.tasks.gene_emb Classes: Name Description GeneEmbeddingExtractor GeneEmbeddingExtractor Parameters: genelist ( list [ str ] ) \u2013 List of genes to restrict to. batch_size ( int , default: 64 ) \u2013 Batch size for the DataLoader. Defaults to 64. num_workers ( int , default: 8 ) \u2013 Number of workers for DataLoader. Defaults to 8. save_every ( int , default: 4000 ) \u2013 Save embeddings every save_every batches. Defaults to 4000. average ( bool , default: False ) \u2013 Whether to average embeddings across all cells. Defaults to False. save_dir ( str , default: None ) \u2013 Directory to save embeddings. If None, embeddings are not saved. Defaults to None. use_knn ( bool , default: False ) \u2013 Whether to use k-nearest neighbors information. Defaults to False. Source code in scprint2/tasks/gene_emb.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , genelist : list [ str ], batch_size : int = 64 , num_workers : int = 8 , save_every : int = 4_000 , average : bool = False , save_dir : str = None , use_knn : bool = False , ): \"\"\" Args: genelist (list[str]): List of genes to restrict to. batch_size (int): Batch size for the DataLoader. Defaults to 64. num_workers (int): Number of workers for DataLoader. Defaults to 8. save_every (int): Save embeddings every `save_every` batches. Defaults to 4000. average (bool): Whether to average embeddings across all cells. Defaults to False. save_dir (str): Directory to save embeddings. If None, embeddings are not saved. Defaults to None. use_knn (bool): Whether to use k-nearest neighbors information. Defaults to False. \"\"\" self . genelist = genelist self . batch_size = batch_size self . num_workers = num_workers self . save_every = save_every self . average = average self . save_dir = save_dir self . use_knn = use_knn scprint2.tasks.generate Classes: Name Description Generate Generate Embedder a class to embed and annotate cells using a model Parameters: genelist ( List [ str ] ) \u2013 The list of genes for which to generate expression data. batch_size ( int , default: 64 ) \u2013 The size of the batches to be used in the DataLoader. Defaults to 64. embedding_to_use ( List [ str ] , default: ['all'] ) \u2013 The list of embeddings to be used for generating expression. Defaults to [\"all\"]. Methods: Name Description __call__ call function to call the embedding Source code in scprint2/tasks/generate.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , genelist : List [ str ], batch_size : int = 64 , embedding_to_use : List [ str ] = [ \"all\" , ], ): \"\"\" Embedder a class to embed and annotate cells using a model Args: genelist (List[str]): The list of genes for which to generate expression data. batch_size (int, optional): The size of the batches to be used in the DataLoader. Defaults to 64. embedding_to_use (List[str], optional): The list of embeddings to be used for generating expression. Defaults to [\"all\"]. \"\"\" self . batch_size = batch_size self . embedding_to_use = embedding_to_use self . genelist = genelist if genelist is not None else [] __call__ call function to call the embedding Parameters: model ( Module ) \u2013 The scPRINT model to be used for embedding and annotation. adata ( AnnData ) \u2013 The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Raises: ValueError \u2013 If the model does not have a logger attribute. ValueError \u2013 If the model does not have a global_step attribute. Returns: AnnData ( AnnData ) \u2013 The annotated data matrix with embedded cell representations. List [ str ] \u2013 List[str]: List of gene names used in the embedding. ndarray \u2013 np.ndarray: The predicted expression values if sample\"none\". dict ( dict ) \u2013 Additional metrics and information from the embedding process. Source code in scprint2/tasks/generate.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def __call__ ( self , model : torch . nn . Module , adata : AnnData ) -> tuple [ AnnData , List [ str ], np . ndarray , dict ]: \"\"\" __call__ function to call the embedding Args: model (torch.nn.Module): The scPRINT model to be used for embedding and annotation. adata (AnnData): The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Raises: ValueError: If the model does not have a logger attribute. ValueError: If the model does not have a global_step attribute. Returns: AnnData: The annotated data matrix with embedded cell representations. List[str]: List of gene names used in the embedding. np.ndarray: The predicted expression values if sample\"none\". dict: Additional metrics and information from the embedding process. \"\"\" # one of \"all\" \"sample\" \"none\" model . predict_mode = \"none\" model . eval () model . on_predict_epoch_start () device = model . device . type dtype = ( torch . float16 if isinstance ( model . transformer , FlashTransformer ) else model . dtype ) if self . embedding_to_use == [ \"all\" ]: use = [ i for i in adata . obsm . keys () if i . startswith ( \"scprint_emb_\" ) and i != \"scprint_emb_other\" ] else : use = self . embedding_to_use res = [] with ( torch . no_grad (), torch . autocast ( device_type = device , dtype = dtype ), ): gene_pos = torch . tensor ( [ model . genes . index ( g ) for g in self . genelist ], ) . to ( device = device ) gene_pos = gene_pos . unsqueeze ( 0 ) . repeat_interleave ( self . batch_size , 0 ) req_depth = torch . tensor ( adata . X . sum ( 1 )) . squeeze ( - 1 ) . to ( device = device ) for batch in tqdm ( range ( adata . shape [ 0 ] // self . batch_size + 1 )): embeddings = [] start = batch * self . batch_size end = min (( batch + 1 ) * self . batch_size , adata . shape [ 0 ]) for emb in use : embeddings . append ( torch . tensor ( adata . obsm [ emb ][ start : end ]) . unsqueeze ( 1 ) ) embeddings = torch . concat ( embeddings , dim = 1 ) . to ( device = device ) output = model . _generate ( gene_pos = gene_pos [ 0 : end - start , :], cell_embs = embeddings , depth_mult = req_depth [ start : end ], req_depth = req_depth [ start : end ], metacell_token = None , ) res . append ( torch . concat ( [ output [ \"mean\" ] . detach () . cpu () . unsqueeze ( 0 ), output [ \"disp\" ] . detach () . cpu () . unsqueeze ( 0 ), output [ \"zero_logits\" ] . detach () . cpu () . unsqueeze ( 0 ), ] ) if \"disp\" in output else output [ \"mean\" ] . detach () . cpu () . unsqueeze ( 0 ) ) torch . cuda . empty_cache () res = torch . concat ( res , dim = 1 ) pred_adata = AnnData ( X = res [ 0 , :, :] . numpy (), obs = adata . obs . copy (), var = pd . DataFrame ( index = pd . Index ( self . genelist ), ), layers = None if res . shape [ 1 ] == 1 else { \"disp\" : res [ 1 , :, :] . numpy (), \"zero_logits\" : res [ 2 , :, :] . numpy (), }, ) return pred_adata scprint2.tasks.impute Classes: Name Description Imputer Imputer Imputer class for imputing missing values in scRNA-seq data using a scPRINT model Parameters: batch_size ( int , default: 10 ) \u2013 Batch size for processing. Defaults to 10. num_workers ( int , default: 1 ) \u2013 Number of workers for data loading. Defaults to 1. max_cells ( int , default: 500000 ) \u2013 Number of cells to use for plotting correlation. Defaults to 10000. doplot ( bool , default: False ) \u2013 Whether to generate plots of the similarity between the denoised and true expression data. Defaults to False. Only works when downsample_expr is not None and max_cells < 100. method ( str , default: 'generative' ) \u2013 Imputation method, either 'masking' or 'generative'. Defaults to 'generative'. predict_depth_mult ( int , default: 4 ) \u2013 Multiplier for prediction depth. Defaults to 4. This will artificially increase the sequencing depth (or number of counts) to 4 times the original depth. genes_to_use ( List [ str ] ) \u2013 List of genes to use for imputation. Defaults to None. genes_to_impute ( List [ str ] ) \u2013 List of genes to impute. Defaults to None. save_every ( int , default: 100000 ) \u2013 The number of cells to save at a time. Defaults to 100_000. This is important to avoid memory issues. apply_zero_pred ( bool , default: True ) \u2013 Whether to apply zero prediction adjustment. Defaults to True. applying zero inflation might give results closer to the specific biases of sequencing technologies but less biological truthful. use_knn ( bool , default: True ) \u2013 Whether to use k-nearest neighbors information. Defaults to True. Methods: Name Description __call__ call calling the function Source code in scprint2/tasks/impute.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , genes_to_use : List [ str ], genes_to_impute : List [ str ], batch_size : int = 10 , num_workers : int = 1 , max_cells : int = 500_000 , doplot : bool = False , method : str = \"generative\" , predict_depth_mult : int = 4 , save_every : int = 100_000 , apply_zero_pred : bool = True , use_knn : bool = True , ): \"\"\" Imputer class for imputing missing values in scRNA-seq data using a scPRINT model Args: batch_size (int, optional): Batch size for processing. Defaults to 10. num_workers (int, optional): Number of workers for data loading. Defaults to 1. max_cells (int, optional): Number of cells to use for plotting correlation. Defaults to 10000. doplot (bool, optional): Whether to generate plots of the similarity between the denoised and true expression data. Defaults to False. Only works when downsample_expr is not None and max_cells < 100. method (str, optional): Imputation method, either 'masking' or 'generative'. Defaults to 'generative'. predict_depth_mult (int, optional): Multiplier for prediction depth. Defaults to 4. This will artificially increase the sequencing depth (or number of counts) to 4 times the original depth. genes_to_use (List[str]): List of genes to use for imputation. Defaults to None. genes_to_impute (List[str]): List of genes to impute. Defaults to None. save_every (int, optional): The number of cells to save at a time. Defaults to 100_000. This is important to avoid memory issues. apply_zero_pred (bool, optional): Whether to apply zero prediction adjustment. Defaults to True. applying zero inflation might give results closer to the specific biases of sequencing technologies but less biological truthful. use_knn (bool, optional): Whether to use k-nearest neighbors information. Defaults to True. \"\"\" self . batch_size = batch_size self . num_workers = num_workers self . max_cells = max_cells self . doplot = doplot self . predict_depth_mult = predict_depth_mult self . save_every = save_every self . genes_to_use = genes_to_use self . genes_to_impute = genes_to_impute self . method = method self . apply_zero_pred = apply_zero_pred self . use_knn = use_knn __call__ call calling the function Parameters: model ( Module ) \u2013 The scPRINT model to be used for denoising. adata ( AnnData ) \u2013 The anndata of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Returns: Optional [ ndarray ] \u2013 Optional[np.ndarray]: The random indices of the cells used when max_cells < adata.shape[0]. AnnData ( AnnData ) \u2013 The imputed anndata. Source code in scprint2/tasks/impute.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 def __call__ ( self , model : torch . nn . Module , adata : AnnData ) -> tuple [ Optional [ np . ndarray ], AnnData ]: \"\"\" __call__ calling the function Args: model (torch.nn.Module): The scPRINT model to be used for denoising. adata (AnnData): The anndata of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Returns: Optional[np.ndarray]: The random indices of the cells used when max_cells < adata.shape[0]. AnnData: The imputed anndata. \"\"\" # Select random number random_indices = None if self . max_cells < adata . shape [ 0 ]: random_indices = np . random . randint ( low = 0 , high = adata . shape [ 0 ], size = self . max_cells ) adataset = SimpleAnnDataset ( adata [ random_indices ], obs_to_output = [ \"organism_ontology_term_id\" ], get_knn_cells = model . expr_emb_style == \"metacell\" and self . use_knn , ) else : adataset = SimpleAnnDataset ( adata , obs_to_output = [ \"organism_ontology_term_id\" ], get_knn_cells = model . expr_emb_style == \"metacell\" and self . use_knn , ) genes_to_use = set ( model . genes ) & set ( self . genes_to_use ) print ( f \" { 100 * len ( genes_to_use ) / len ( self . genes_to_use ) } % of genes to use are available in the model\" ) genes_to_impute = set ( model . genes ) & set ( self . genes_to_impute ) print ( f \" { 100 * len ( genes_to_impute ) / len ( self . genes_to_impute ) } % of genes to impute are available in the model\" ) tot = genes_to_use | genes_to_impute tot = sorted ( tot ) col = Collator ( organisms = model . organisms , valid_genes = model . genes , how = \"some\" , genelist = list ( genes_to_use ) + ( list ( genes_to_impute ) if self . method == \"masking\" else []), n_bins = model . n_input_bins if model . expr_emb_style == \"binned\" else 0 , ) dataloader = DataLoader ( adataset , collate_fn = col , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , ) mask = None generate_on = None if self . method == \"masking\" : mask = torch . Tensor ( [ i in genes_to_use for i in tot ], ) . to ( device = model . device , dtype = torch . bool ) elif self . method == \"generative\" : generate_on = ( torch . Tensor ([ model . genes . index ( i ) for i in genes_to_impute ]) . to ( device = model . device ) . long () . unsqueeze ( 0 ) . repeat ( self . batch_size , 1 ) ) else : raise ValueError ( \"need to be one of generative or masking\" ) prevplot = model . doplot model . doplot = self . doplot model . on_predict_epoch_start () model . eval () device = model . device . type rand = random_str () dtype = ( torch . float16 if type ( model . transformer ) is FlashTransformer else model . dtype ) save_expr = model . save_expr model . save_expr = True torch . cuda . empty_cache () with torch . no_grad (), torch . autocast ( device_type = device , dtype = dtype ): for batch in tqdm ( dataloader ): gene_pos , expression , depth = ( batch [ \"genes\" ] . to ( device ), batch [ \"x\" ] . to ( device ), batch [ \"depth\" ] . to ( device ), ) model . _predict ( gene_pos , expression , depth , knn_cells = ( batch [ \"knn_cells\" ] . to ( device ) if model . expr_emb_style == \"metacell\" and self . use_knn else None ), do_generate = self . method == \"generative\" , depth_mult = self . predict_depth_mult , max_size_in_mem = self . save_every , name = \"impute\" + rand + \"_\" , mask = mask , generate_on = generate_on , ) torch . cuda . empty_cache () model . log_adata ( name = \"impute\" + rand + \"_\" + str ( model . counter )) try : mdir = ( model . logger . save_dir if model . logger . save_dir is not None else \"data\" ) except : mdir = \"data\" pred_adata = [] for i in range ( model . counter + 1 ): file = ( mdir + \"/step_\" + str ( model . global_step ) + \"_\" + model . name + \"_impute\" + rand + \"_\" + str ( i ) + \"_\" + str ( model . global_rank ) + \".h5ad\" ) pred_adata . append ( sc . read_h5ad ( file )) os . remove ( file ) pred_adata = concat ( pred_adata ) model . doplot = prevplot model . save_expr = save_expr # pred_adata.X = adata.X if random_indices is None else adata.X[random_indices] true_imp = pred_adata . X [:, pred_adata . var . index . isin ( genes_to_impute )] . toarray () if true_imp . sum () > 0 : # we had some gt pred_imp = pred_adata . layers [ \"scprint_mu\" ][ :, pred_adata . var . index . isin ( genes_to_impute ) ] . toarray () pred_known = pred_adata . layers [ \"scprint_mu\" ][ :, pred_adata . var . index . isin ( genes_to_use ) ] . toarray () true_known = pred_adata . X [ :, pred_adata . var . index . isin ( genes_to_use ) ] . toarray () if self . apply_zero_pred : pred_imp = ( pred_imp * ( 1 - F . sigmoid ( torch . Tensor ( pred_adata . layers [ \"scprint_pi\" ][ :, pred_adata . var . index . isin ( genes_to_impute ) ] . toarray () ) ) ) . numpy () ) pred_known = ( pred_known * ( 1 - F . sigmoid ( torch . Tensor ( pred_adata . layers [ \"scprint_pi\" ][ :, pred_adata . var . index . isin ( genes_to_use ) ] . toarray () ) ) ) . numpy () ) cell_wise_pred = np . array ( [ spearmanr ( pred_imp [ i ], true_imp [ i ])[ 0 ] for i in range ( pred_imp . shape [ 0 ]) ] ) cell_wise_known = np . array ( [ spearmanr ( pred_known [ i ], true_known [ i ])[ 0 ] for i in range ( pred_known . shape [ 0 ]) ] ) print ( { \"cell_wise_known\" : np . mean ( cell_wise_known ), \"cell_wise_pred\" : np . mean ( cell_wise_pred ), } ) if self . doplot : print ( \"depth-wise plot\" ) plot_cell_depth_wise_corr_improvement ( cell_wise_known , cell_wise_pred ) return random_indices , pred_adata scprint2.tasks.finetune Classes: Name Description FinetuneBatchClass Functions: Name Description mmd_loss Compute Maximum Mean Discrepancy (MMD) loss between two 2D embedding matrices. FinetuneBatchClass Embedder a class to embed and annotate cells using a model Parameters: batch_key ( str , default: 'batch' ) \u2013 The key in adata.obs that indicates the batch information. Defaults to \"batch\". learn_batches_on ( str , default: None ) \u2013 The key in adata.obs to learn batch embeddings on. Defaults to None. if none, will not learn the batch embeddings. the goal is e.g. when having a new species, to learn an embedding for it during finetuning and replace the \"learn_batches_on\" embedding in the model with it, in this case it should be \"organism_ontology_term_id\". batch correction might indeed be better learnt with this additional argument in some cases. do_mmd_on ( str , default: None ) \u2013 The key in adata.obs to learn batch embeddings on. Defaults to None. this embedding should have less batch information in it, after finetuning. predict_keys ( List [ str ] , default: ['cell_type_ontology_term_id'] ) \u2013 List of keys in adata.obs to predict during fine-tuning. Defaults to [\"cell_type_ontology_term_id\"]. batch_size ( int , default: 16 ) \u2013 The size of the batches to be used in the DataLoader. Defaults to 64. num_workers ( int , default: 8 ) \u2013 The number of worker processes to use for data loading. Defaults to 8. max_len ( int , default: 5000 ) \u2013 The maximum length of the sequences to be processed. Defaults to 5000. lr ( float , default: 0.0002 ) \u2013 The learning rate for the optimizer. Defaults to 0.0002. num_epochs ( int , default: 8 ) \u2013 The number of epochs to train the model. Defaults to 8. ft_mode ( str , default: 'xpressor' ) \u2013 The fine-tuning mode, either \"xpressor\" or \"full\". Defaults to \"xpressor\". frac_train ( float , default: 0.8 ) \u2013 The fraction of data to be used for training. Defaults to 0.8. loss_scalers ( dict , default: {} ) \u2013 A dictionary specifying the scaling factors for different loss components. Defaults to {}. expr, class, mmd, kl, and any of the predict_keys can be specified. use_knn ( bool , default: True ) \u2013 Whether to use k-nearest neighbors information. Defaults to True. Methods: Name Description __call__ call function to call the embedding Source code in scprint2/tasks/finetune.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __init__ ( self , batch_key : str = \"batch\" , predict_keys : List [ str ] = [ \"cell_type_ontology_term_id\" ], max_len : int = 5000 , learn_batches_on : Optional [ str ] = None , num_workers : int = 8 , batch_size : int = 16 , num_epochs : int = 8 , do_mmd_on : Optional [ str ] = None , lr : float = 0.0002 , ft_mode : str = \"xpressor\" , frac_train : float = 0.8 , loss_scalers : dict = {}, use_knn : bool = True , ): \"\"\" Embedder a class to embed and annotate cells using a model Args: batch_key (str, optional): The key in adata.obs that indicates the batch information. Defaults to \"batch\". learn_batches_on (str, optional): The key in adata.obs to learn batch embeddings on. Defaults to None. if none, will not learn the batch embeddings. the goal is e.g. when having a new species, to learn an embedding for it during finetuning and replace the \"learn_batches_on\" embedding in the model with it, in this case it should be \"organism_ontology_term_id\". batch correction might indeed be better learnt with this additional argument in some cases. do_mmd_on (str, optional):The key in adata.obs to learn batch embeddings on. Defaults to None. this embedding should have less batch information in it, after finetuning. predict_keys (List[str], optional): List of keys in adata.obs to predict during fine-tuning. Defaults to [\"cell_type_ontology_term_id\"]. batch_size (int, optional): The size of the batches to be used in the DataLoader. Defaults to 64. num_workers (int, optional): The number of worker processes to use for data loading. Defaults to 8. max_len (int, optional): The maximum length of the sequences to be processed. Defaults to 5000. lr (float, optional): The learning rate for the optimizer. Defaults to 0.0002. num_epochs (int, optional): The number of epochs to train the model. Defaults to 8. ft_mode (str, optional): The fine-tuning mode, either \"xpressor\" or \"full\". Defaults to \"xpressor\". frac_train (float, optional): The fraction of data to be used for training. Defaults to 0.8. loss_scalers (dict, optional): A dictionary specifying the scaling factors for different loss components. Defaults to {}. expr, class, mmd, kl, and any of the predict_keys can be specified. use_knn (bool, optional): Whether to use k-nearest neighbors information. Defaults to True. \"\"\" self . batch_size = batch_size self . num_workers = num_workers self . batch_key = batch_key self . learn_batches_on = learn_batches_on self . predict_keys = predict_keys self . max_len = max_len self . lr = lr self . num_epochs = num_epochs self . ft_mode = ft_mode self . frac_train = frac_train self . batch_emb = None self . batch_encoder = {} self . do_mmd_on = do_mmd_on self . loss_scalers = loss_scalers self . use_knn = use_knn __call__ call function to call the embedding Parameters: model ( Module ) \u2013 The scPRINT model to be used for embedding and annotation. adata ( AnnData , default: None ) \u2013 The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Defaults to None. if provided, it will be split into training and validation sets. train_data ( AnnData , default: None ) \u2013 The training data. Defaults to None. if adata is provided, this will be ignored. val_data ( AnnData , default: None ) \u2013 The validation data. Defaults to None. if adata is provided, this will be ignored. Raises: ValueError \u2013 If the model does not have a logger attribute. ValueError \u2013 If the model does not have a global_step attribute. Returns: Module \u2013 torch.nn.Module: the fine-tuned model Source code in scprint2/tasks/finetune.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 def __call__ ( self , model : torch . nn . Module , adata : AnnData = None , train_data : AnnData = None , val_data : AnnData = None , ) -> torch . nn . Module : \"\"\" __call__ function to call the embedding Args: model (torch.nn.Module): The scPRINT model to be used for embedding and annotation. adata (AnnData): The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Defaults to None. if provided, it will be split into training and validation sets. train_data (AnnData, optional): The training data. Defaults to None. if adata is provided, this will be ignored. val_data (AnnData, optional): The validation data. Defaults to None. if adata is provided, this will be ignored. Raises: ValueError: If the model does not have a logger attribute. ValueError: If the model does not have a global_step attribute. Returns: torch.nn.Module: the fine-tuned model \"\"\" # one of \"all\" \"sample\" \"none\" model . predict_mode = \"none\" if self . ft_mode == \"xpressor\" : for val in model . parameters (): val . requires_grad = False # setting all to TRUE for val in model . cell_transformer . parameters (): val . requires_grad = True for val in model . transformer . blocks [ - 1 ] . parameters (): val . requires_grad = True for i in model . transformer . blocks : i . cross_attn . requires_grad = True for val in model . compressor . parameters (): val . requires_grad = True for val in self . predict_keys : for val in model . cls_decoders [ val ] . parameters (): val . requires_grad = True elif self . ft_mode == \"full\" : for val in model . parameters (): val . requires_grad = True else : raise ValueError ( \"ft_mode must be one of 'xpressor' or 'full'\" ) # PREPARING THE DATA if adata is not None : n_train = int ( self . frac_train * len ( adata )) train_idx = np . random . choice ( len ( adata ), n_train , replace = False ) val_idx = np . setdiff1d ( np . arange ( len ( adata )), train_idx ) train_data = adata [ train_idx ] . copy () val_data = adata [ val_idx ] . copy () print ( f \"Training data: { train_data . shape } \" ) print ( f \"Validation data: { val_data . shape } \" ) mencoders = {} for k , v in model . label_decoders . items (): mencoders [ k ] = { va : ke for ke , va in v . items ()} # this needs to remain its original name as it is expect like that by collator, otherwise need to send org_to_id as params for i in self . predict_keys : if len ( set ( train_data . obs [ i ]) - set ( mencoders [ i ] . keys ())) > 0 : print ( \"missing labels for \" , i ) train_data . obs [ i ] = train_data . obs [ i ] . apply ( lambda x : x if x in mencoders [ i ] else \"unknown\" ) if \"organism_ontology_term_id\" not in self . predict_keys : self . predict_keys . append ( \"organism_ontology_term_id\" ) # create datasets self . batch_encoder = { i : n for n , i in enumerate ( train_data . obs [ self . batch_key ] . astype ( \"category\" ) . cat . categories ) } mencoders [ self . batch_key ] = self . batch_encoder train_dataset = SimpleAnnDataset ( train_data , obs_to_output = self . predict_keys + [ self . batch_key ], get_knn_cells = model . expr_emb_style == \"metacell\" and self . use_knn , encoder = mencoders , ) if val_data is not None : for i in self . predict_keys : if i != \"organism_ontology_term_id\" : if len ( set ( val_data . obs [ i ]) - set ( mencoders [ i ] . keys ())) > 0 : val_data . obs [ i ] = val_data . obs [ i ] . apply ( lambda x : x if x in mencoders [ i ] else \"unknown\" ) self . batch_encoder . update ( { i : n + len ( self . batch_encoder ) for n , i in enumerate ( val_data . obs [ self . batch_key ] . astype ( \"category\" ) . cat . categories ) if i not in self . batch_encoder } ) mencoders [ self . batch_key ] = self . batch_encoder val_dataset = SimpleAnnDataset ( val_data , obs_to_output = self . predict_keys + [ self . batch_key ], get_knn_cells = model . expr_emb_style == \"metacell\" and self . use_knn , encoder = mencoders , ) # Create collator collator = Collator ( organisms = model . organisms , valid_genes = model . genes , class_names = self . predict_keys + [ self . batch_key ], how = \"random expr\" , # or \"all expr\" for full expression max_len = self . max_len , org_to_id = mencoders . get ( \"organism_ontology_term_id\" , {}), ) # Create data loaders train_loader = DataLoader ( train_dataset , collate_fn = collator , batch_size = self . batch_size , # Adjust based on GPU memory num_workers = self . num_workers , shuffle = True , ) if val_data is not None : val_loader = DataLoader ( val_dataset , collate_fn = collator , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , ) if self . learn_batches_on is not None : if val_data is not None : print ( \"all batch key values in val_data should also be present in train_adata!!!\" ) self . batch_emb = torch . nn . Embedding ( num_embeddings = train_data . obs [ self . batch_key ] . nunique (), embedding_dim = ( model . compressor [ self . learn_batches_on ] . fc_mu . weight . shape [ 0 ] if hasattr ( model , \"compressor\" ) else model . d_model ), ) ## PREPARING THE OPTIM all_params = ( list ( model . parameters ()) # + list(batch_cls.parameters()) + ( list ( self . batch_emb . parameters ()) if self . learn_batches_on is not None else [] ) ) # Setup optimizer optimizer = torch . optim . AdamW ( all_params , lr = self . lr , weight_decay = 0.01 , betas = ( 0.9 , 0.999 ), eps = 1e-8 , ) # Setup scheduler scheduler = torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = \"min\" , factor = 0.5 , patience = 2 ) # Setup automatic mixed precision scaler = torch . cuda . amp . GradScaler () if torch . cuda . is_available () else None for k , i in model . mat_labels_hierarchy . items (): model . mat_labels_hierarchy [ k ] = i . to ( model . device ) ## train for epoch in range ( self . num_epochs ): print ( f \" \\n Epoch { epoch + 1 } / { self . num_epochs } \" ) print ( f \"Current learning rate: { optimizer . param_groups [ 0 ][ 'lr' ] : .2e } \" ) # Training phase train_loss = 0.0 train_steps = 0 avg_expr = 0 avg_cls = 0 avg_mmd = 0 pbar = tqdm ( train_loader , desc = \"Training\" ) model . train () for batch_idx , batch in enumerate ( pbar ): optimizer . zero_grad () total_loss , cls_loss , mmd , loss_expr = self . batch_corr_pass ( batch , model ) # Backward pass scaler . scale ( total_loss ) . backward () scaler . unscale_ ( optimizer ) torch . nn . utils . clip_grad_norm_ ( model . parameters (), max_norm = 1.0 ) scaler . step ( optimizer ) scaler . update () train_loss += total_loss . item () train_steps += 1 avg_cls += cls_loss . item () avg_expr += loss_expr . item () avg_mmd += mmd # Update progress bar pbar . set_postfix ( { \"loss\" : f \" { total_loss . item () : .4f } \" , \"avg_loss\" : f \" { train_loss / train_steps : .4f } \" , \"cls_loss\" : f \" { cls_loss . item () : .4f } \" , \"mmd_loss\" : f \" { mmd : .4f } \" , \"expr_loss\" : f \" { loss_expr . item () : .4f } \" , } ) # Validation phase if val_data is not None : model . eval () val_loss = 0.0 val_steps = 0 val_loss_expr = 0.0 val_mmd = 0.0 val_cls = 0.0 val_loss_to_prt = 0.0 with torch . no_grad (): for batch in val_loader : # tqdm(val_loader, desc=\"Validation\"): loss_val , cls_loss , mmd , loss_expr = self . batch_corr_pass ( batch , model ) val_loss_to_prt += loss_val . item () val_loss += loss_val . item () val_steps += 1 val_loss_expr += loss_expr . item () val_mmd += mmd val_cls += cls_loss . item () try : avg_val_loss = val_loss_to_prt / val_steps avg_train_loss = train_loss / train_steps except ZeroDivisionError : print ( \"Error: Division by zero occurred while calculating average losses.\" ) avg_train_loss = 0 print ( \"cls_loss: {:.4f} , mmd_loss: {:.4f} , expr_loss: {:.4f} \" . format ( val_cls / val_steps , val_mmd / val_steps , val_loss_expr / val_steps , ) ) print ( f \"Train Loss: { avg_train_loss : .4f } , Val Loss: { avg_val_loss : .4f } \" ) # Store LR before scheduler step for comparison lr_before = optimizer . param_groups [ 0 ][ \"lr\" ] # Update learning rate scheduler . step ( avg_val_loss ) # Check if LR was reduced lr_after = optimizer . param_groups [ 0 ][ \"lr\" ] if lr_after < lr_before : print ( f \"\ud83d\udd3b Learning rate reduced from { lr_before : .2e } to { lr_after : .2e } (factor: { lr_after / lr_before : .3f } )\" ) else : print ( f \"\u2705 Learning rate unchanged: { lr_after : .2e } \" ) # Early stopping check (simple implementation) if epoch > 3 and val_loss / val_steps > 1.3 * avg_train_loss : print ( \"Early stopping due to overfitting\" ) break print ( \"Manual fine-tuning completed!\" ) model . eval () return model mmd_loss Compute Maximum Mean Discrepancy (MMD) loss between two 2D embedding matrices. Parameters: X ( Tensor ) \u2013 Tensor of shape (n1, emb_dim) - first set of embeddings Y ( Tensor ) \u2013 Tensor of shape (n2, emb_dim) - second set of embeddings Returns: Tensor \u2013 torch.Tensor: MMD loss value (negative to encourage dissimilarity) Source code in scprint2/tasks/finetune.py 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 def mmd_loss ( X : torch . Tensor , Y : torch . Tensor ) -> torch . Tensor : \"\"\" Compute Maximum Mean Discrepancy (MMD) loss between two 2D embedding matrices. Args: X (torch.Tensor): Tensor of shape (n1, emb_dim) - first set of embeddings Y (torch.Tensor): Tensor of shape (n2, emb_dim) - second set of embeddings Returns: torch.Tensor: MMD loss value (negative to encourage dissimilarity) \"\"\" def rbf_kernel ( x , y , sigma ): \"\"\"Compute RBF kernel between two sets of vectors\"\"\" distance = torch . cdist ( x , y , p = 2 ) ** 2 return torch . exp ( - distance / ( 2 * sigma ** 2 )) def energy_kernel ( x , y ): \"\"\"Compute Energy kernel between two sets of vectors\"\"\" distance = torch . cdist ( x , y , p = 2 ) return - distance # Use multiple kernel bandwidths for better performance sigmas = [ 0 ] # [0.1, 1.0, 10.0] mmd_loss = 0.0 for sigma in sigmas : # K(X, X) - kernel matrix within first group (n1 x n1) # k_xx = rbf_kernel(X, X, sigma) k_xx = energy_kernel ( X , X ) # K(Y, Y) - kernel matrix within second group (n2 x n2) # k_yy = rbf_kernel(Y, Y, sigma) k_yy = energy_kernel ( Y , Y ) # K(X, Y) - kernel matrix between groups (n1 x n2) # k_xy = rbf_kernel(X, Y, sigma) k_xy = energy_kernel ( X , Y ) # Unbiased MMD estimation n1 = X . shape [ 0 ] n2 = Y . shape [ 0 ] # Remove diagonal elements for unbiased estimation of K(X,X) and K(Y,Y) # For K(X,X): exclude diagonal if n1 > 1 : mask_xx = 1 - torch . eye ( n1 , device = X . device ) k_xx_term = ( k_xx * mask_xx ) . sum () / ( n1 * ( n1 - 1 )) else : k_xx_term = 0.0 # For K(Y,Y): exclude diagonal if n2 > 1 : mask_yy = 1 - torch . eye ( n2 , device = Y . device ) k_yy_term = ( k_yy * mask_yy ) . sum () / ( n2 * ( n2 - 1 )) else : k_yy_term = 0.0 # For K(X,Y): use all elements (no diagonal to exclude) k_xy_term = k_xy . mean () # MMD^2 = E[K(X,X)] + E[K(Y,Y)] - 2*E[K(X,Y)] mmd_squared = k_xx_term + k_yy_term - 2 * k_xy_term mmd_loss += mmd_squared # Return negative MMD to encourage dissimilarity (higher MMD = more different) return mmd_loss / len ( sigmas )","title":"tasks"},{"location":"tasks/#documentation-for-the-tasks","text":"","title":"Documentation for the tasks"},{"location":"tasks/#scprint2.tasks.cell_emb","text":"Classes: Name Description Embedder Functions: Name Description compute_classification Compute classification metrics for the given annotated data. compute_corr Compute the correlation between the output and target matrices. default_benchmark Run the default benchmark for embedding and annotation using the scPRINT model. display_confusion_matrix Display the confusion matrix for true vs predicted cell types.","title":"cell_emb"},{"location":"tasks/#scprint2.tasks.cell_emb.Embedder","text":"Embedder a class to embed and annotate cells using a model Parameters: batch_size ( int , default: 64 ) \u2013 The size of the batches to be used in the DataLoader. Defaults to 64. num_workers ( int , default: 8 ) \u2013 The number of worker processes to use for data loading. Defaults to 8. how ( str , default: 'random expr' ) \u2013 The method to be used for selecting valid genes. Defaults to \"random expr\". - \"random expr\": random expression - \"most var\": highly variable genes in the dataset - \"some\": specific genes (from genelist) - \"most expr\": most expressed genes in the cell max_len ( int , default: 2000 ) \u2013 The maximum length of the gene sequence given to the model. Defaults to 1000. doclass ( bool , default: True ) \u2013 Whether to perform classification. Defaults to True. pred_embedding ( List [ str ] , default: ['all'] ) \u2013 The list of labels to be used for plotting embeddings. Defaults to [ \"cell_type_ontology_term_id\", \"disease_ontology_term_id\", \"self_reported_ethnicity_ontology_term_id\", \"sex_ontology_term_id\", ]. doplot ( bool , default: True ) \u2013 Whether to generate plots. Defaults to True. keep_all_labels_pred ( bool , default: False ) \u2013 Whether to keep all class predictions. Defaults to False, will only keep the most likely class. genelist ( List [ str ] , default: None ) \u2013 The list of genes to be used for embedding. Defaults to []: In this case, \"how\" needs to be \"most var\" or \"random expr\". save_every ( int , default: 40000 ) \u2013 The number of cells to save at a time. Defaults to 100_000. This is important to avoid memory issues. unknown_label ( str , default: 'unknown' ) \u2013 The label to be used for unknown cell types. Defaults to \"unknown\". use_knn ( bool , default: True ) \u2013 Whether to use k-nearest neighbors information. Defaults to True. Methods: Name Description __call__ call function to call the embedding Source code in scprint2/tasks/cell_emb.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , batch_size : int = 64 , num_workers : int = 8 , how : str = \"random expr\" , max_len : int = 2000 , doclass : bool = True , pred_embedding : List [ str ] = [ \"all\" , ], doplot : bool = True , keep_all_labels_pred : bool = False , genelist : Optional [ List [ str ]] = None , save_every : int = 40_000 , unknown_label : str = \"unknown\" , use_knn : bool = True , ): \"\"\" Embedder a class to embed and annotate cells using a model Args: batch_size (int, optional): The size of the batches to be used in the DataLoader. Defaults to 64. num_workers (int, optional): The number of worker processes to use for data loading. Defaults to 8. how (str, optional): The method to be used for selecting valid genes. Defaults to \"random expr\". - \"random expr\": random expression - \"most var\": highly variable genes in the dataset - \"some\": specific genes (from genelist) - \"most expr\": most expressed genes in the cell max_len (int, optional): The maximum length of the gene sequence given to the model. Defaults to 1000. doclass (bool, optional): Whether to perform classification. Defaults to True. pred_embedding (List[str], optional): The list of labels to be used for plotting embeddings. Defaults to [ \"cell_type_ontology_term_id\", \"disease_ontology_term_id\", \"self_reported_ethnicity_ontology_term_id\", \"sex_ontology_term_id\", ]. doplot (bool, optional): Whether to generate plots. Defaults to True. keep_all_labels_pred (bool, optional): Whether to keep all class predictions. Defaults to False, will only keep the most likely class. genelist (List[str], optional): The list of genes to be used for embedding. Defaults to []: In this case, \"how\" needs to be \"most var\" or \"random expr\". save_every (int, optional): The number of cells to save at a time. Defaults to 100_000. This is important to avoid memory issues. unknown_label (str, optional): The label to be used for unknown cell types. Defaults to \"unknown\". use_knn (bool, optional): Whether to use k-nearest neighbors information. Defaults to True. \"\"\" self . batch_size = batch_size self . num_workers = num_workers self . how = how self . max_len = max_len self . pred_embedding = pred_embedding self . keep_all_labels_pred = keep_all_labels_pred self . doplot = doplot self . doclass = doclass self . genelist = genelist if genelist is not None else [] self . save_every = save_every self . pred = None self . unknown_label = unknown_label self . use_knn = use_knn","title":"Embedder"},{"location":"tasks/#scprint2.tasks.cell_emb.Embedder.__call__","text":"call function to call the embedding Parameters: model ( Module ) \u2013 The scPRINT model to be used for embedding and annotation. adata ( AnnData ) \u2013 The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Raises: ValueError \u2013 If the model does not have a logger attribute. ValueError \u2013 If the model does not have a global_step attribute. Returns: AnnData ( AnnData ) \u2013 The annotated data matrix with embedded cell representations. dict ( dict ) \u2013 classification metrics results when some ground truth information was available in the anndata. Source code in scprint2/tasks/cell_emb.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 def __call__ ( self , model : torch . nn . Module , adata : AnnData ) -> tuple [ AnnData , dict ]: \"\"\" __call__ function to call the embedding Args: model (torch.nn.Module): The scPRINT model to be used for embedding and annotation. adata (AnnData): The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Raises: ValueError: If the model does not have a logger attribute. ValueError: If the model does not have a global_step attribute. Returns: AnnData: The annotated data matrix with embedded cell representations. dict: classification metrics results when some ground truth information was available in the anndata. \"\"\" # one of \"all\" \"sample\" \"none\" model . predict_mode = \"none\" self . pred = None prevkeep = model . keep_all_labels_pred model . keep_all_labels_pred = self . keep_all_labels_pred # Add at least the organism you are working with if self . how == \"most var\" : sc . pp . highly_variable_genes ( adata , flavor = \"seurat_v3\" , n_top_genes = self . max_len ) self . genelist = adata . var . index [ adata . var . highly_variable ] adataset = SimpleAnnDataset ( adata , obs_to_output = [ \"organism_ontology_term_id\" ], get_knn_cells = model . expr_emb_style == \"metacell\" and self . use_knn , ) col = Collator ( organisms = model . organisms , valid_genes = model . genes , how = self . how if self . how != \"most var\" else \"some\" , max_len = self . max_len , add_zero_genes = 0 , genelist = self . genelist if self . how in [ \"most var\" , \"some\" ] else [], n_bins = model . n_input_bins if model . expr_emb_style == \"binned\" else 0 , ) dataloader = DataLoader ( adataset , collate_fn = col , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , ) model . eval () model . on_predict_epoch_start () device = model . device . type prevplot = model . doplot model . pred_log_adata = True model . doplot = self . doplot and not self . keep_all_labels_pred model . save_expr = False rand = random_str () dtype = ( torch . float16 if isinstance ( model . transformer , FlashTransformer ) else model . dtype ) with ( torch . no_grad (), torch . autocast ( device_type = device , dtype = dtype ), ): for batch in tqdm ( dataloader ): gene_pos , expression , depth = ( batch [ \"genes\" ] . to ( device ), batch [ \"x\" ] . to ( device ), batch [ \"depth\" ] . to ( device ), ) pred = model . _predict ( gene_pos , expression , depth , knn_cells = ( batch [ \"knn_cells\" ] . to ( device ) if model . expr_emb_style == \"metacell\" and self . use_knn else None ), knn_cells_info = ( batch [ \"knn_cells_info\" ] . to ( device ) if model . expr_emb_style == \"metacell\" and self . use_knn else None ), pred_embedding = self . pred_embedding , max_size_in_mem = self . save_every , name = \"embed_\" + rand + \"_\" , ) torch . cuda . empty_cache () if self . keep_all_labels_pred : if pred is not None : self . pred = ( pred if self . pred is None else torch . cat ([ self . pred , pred ]) ) model . log_adata ( name = \"embed_\" + rand + \"_\" + str ( model . counter )) model . pos = None model . expr_pred = None model . embs = None if self . keep_all_labels_pred : self . pred = ( model . pred if self . pred is None else torch . cat ([ self . pred , model . pred ]) ) model . pred = None model . save_expr = True try : mdir = ( model . logger . save_dir if model . logger . save_dir is not None else \"data\" ) except : mdir = \"data\" pred_adata = [] del adataset , dataloader for i in range ( model . counter + 1 ): file = ( mdir + \"/step_\" + str ( model . global_step ) + \"_\" + model . name + \"_embed_\" + rand + \"_\" + str ( i ) + \"_\" + str ( model . global_rank ) + \".h5ad\" ) pred_adata . append ( sc . read_h5ad ( file )) os . remove ( file ) pred_adata = concat ( pred_adata ) pred_adata . obs . index = adata . obs . index try : adata . obsm [ \"X_scprint_umap\" ] = pred_adata . obsm [ \"X_umap\" ] del pred_adata . obsm [ \"X_umap\" ] except : print ( \"too few cells to embed into a umap\" ) try : adata . obs [ \"scprint_leiden\" ] = pred_adata . obs [ \"scprint_leiden\" ] except : print ( \"too few cells to compute a clustering\" ) if self . pred_embedding == [ \"all\" ]: pred_embedding = [ \"other\" ] + model . classes else : pred_embedding = self . pred_embedding if len ( pred_embedding ) == 1 : adata . obsm [ \"scprint_emb\" ] = pred_adata . obsm [ \"scprint_emb_\" + pred_embedding [ 0 ] ] . astype ( np . float32 ) else : adata . obsm [ \"scprint_emb\" ] = np . zeros ( pred_adata . obsm [ \"scprint_emb_\" + pred_embedding [ 0 ]] . shape , dtype = np . float32 , ) i = 0 for k , v in pred_adata . obsm . items (): adata . obsm [ k ] = v . astype ( np . float32 ) if model . compressor is not None : if i == 0 : adata . obsm [ \"scprint_emb\" ] = v . astype ( np . float32 ) else : adata . obsm [ \"scprint_emb\" ] = np . hstack ( [ adata . obsm [ \"scprint_emb\" ], v . astype ( np . float32 )] ) else : adata . obsm [ \"scprint_emb\" ] += v . astype ( np . float32 ) i += 1 if model . compressor is None : adata . obsm [ \"scprint_emb\" ] = adata . obsm [ \"scprint_emb\" ] / i for key , value in pred_adata . uns . items (): adata . uns [ key ] = value pred_adata . obs . index = adata . obs . index model . keep_all_labels_pred = prevkeep model . doplot = prevplot adata . obs = pd . concat ([ adata . obs , pred_adata . obs ], axis = 1 ) del pred_adata if self . keep_all_labels_pred : allclspred = self . pred . to ( device = \"cpu\" ) . numpy () columns = [] for cl in model . classes : n = model . label_counts [ cl ] columns += [ model . label_decoders [ cl ][ i ] for i in range ( n )] allclspred = pd . DataFrame ( allclspred , columns = columns , index = adata . obs . index ) adata . obs = pd . concat ([ adata . obs , allclspred ], axis = 1 ) metrics = {} if self . doclass and not self . keep_all_labels_pred : for cl in model . classes : res = [] if cl not in adata . obs . columns : continue class_topred = model . label_decoders [ cl ] . values () if cl in model . labels_hierarchy : # class_groupings = { # k: [ # i.ontology_id # for i in bt.CellType.filter(k).first().children.all() # ] # for k in set(adata.obs[cl].unique()) - set(class_topred) # } cur_labels_hierarchy = { model . label_decoders [ cl ][ k ]: [ model . label_decoders [ cl ][ i ] for i in v ] for k , v in model . labels_hierarchy [ cl ] . items () } else : cur_labels_hierarchy = {} for pred , true in adata . obs [[ \"pred_\" + cl , cl ]] . values : if pred == true : res . append ( True ) continue if len ( cur_labels_hierarchy ) > 0 : if true in cur_labels_hierarchy : res . append ( pred in cur_labels_hierarchy [ true ]) continue elif true != self . unknown_label : res . append ( False ) elif true not in class_topred : print ( f \"true label { true } not in available classes\" ) return adata , metrics elif true not in class_topred : print ( f \"true label { true } not in available classes\" ) return adata , metrics elif true != self . unknown_label : res . append ( False ) # else true is unknown # else we pass if len ( res ) == 0 : # true was always unknown res = [ 1 ] if self . doplot : print ( \" \" , cl ) print ( \" accuracy:\" , sum ( res ) / len ( res )) print ( \" \" ) metrics . update ({ cl + \"_accuracy\" : sum ( res ) / len ( res )}) self . pred = None return adata , metrics","title":"__call__"},{"location":"tasks/#scprint2.tasks.cell_emb.compute_classification","text":"Compute classification metrics for the given annotated data. Parameters: adata ( AnnData ) \u2013 The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. classes ( List [ str ] ) \u2013 List of class labels to be used for classification. label_decoders ( Dict [ str , Any ] ) \u2013 Dictionary of label decoders for each class. labels_hierarchy ( Dict [ str , Any ] ) \u2013 Dictionary representing the hierarchy of labels. metric_type ( List [ str ] , default: ['macro', 'micro', 'weighted'] ) \u2013 List of metric types to compute. Defaults to [\"macro\", \"micro\", \"weighted\"]. Returns: Dict [ str , Dict [ str , float ]] \u2013 Dict[str, Dict[str, float]]: A dictionary containing classification metrics for each class. Source code in scprint2/tasks/cell_emb.py 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 def compute_classification ( adata : AnnData , classes : List [ str ], label_decoders : Dict [ str , Any ], labels_hierarchy : Dict [ str , Any ], metric_type : List [ str ] = [ \"macro\" , \"micro\" , \"weighted\" ], use_unknown : bool = False , ) -> Dict [ str , Dict [ str , float ]]: \"\"\" Compute classification metrics for the given annotated data. Args: adata (AnnData): The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. classes (List[str]): List of class labels to be used for classification. label_decoders (Dict[str, Any]): Dictionary of label decoders for each class. labels_hierarchy (Dict[str, Any]): Dictionary representing the hierarchy of labels. metric_type (List[str], optional): List of metric types to compute. Defaults to [\"macro\", \"micro\", \"weighted\"]. Returns: Dict[str, Dict[str, float]]: A dictionary containing classification metrics for each class. \"\"\" metrics = {} for clss in classes : res = [] if clss not in adata . obs . columns : print ( \"not in columns\" ) continue labels_topred = label_decoders [ clss ] . values () if clss in labels_hierarchy : parentdf = ( bt . CellType . filter () . to_dataframe ( include = [ \"parents__ontology_id\" , \"ontology_id\" ], limit = None ) . set_index ( \"ontology_id\" )[[ \"parents__ontology_id\" ]] ) parentdf . parents__ontology_id = parentdf . parents__ontology_id . astype ( str ) class_groupings = { k : get_descendants ( k , parentdf ) for k in set ( adata . obs [ clss ] . unique ()) } tokeep = np . array ([ True ] * adata . shape [ 0 ]) for i , ( pred , true ) in enumerate ( adata . obs [[ \"pred_\" + clss , clss ]] . values ): if pred == true : res . append ( true ) continue if true == \"unknown\" : tokeep [ i ] = False if clss in labels_hierarchy : if true in class_groupings : if pred == \"unknown\" and not use_unknown : tokeep [ i ] = False res . append ( true if pred in class_groupings [ true ] else \"\" ) continue elif true not in labels_topred : raise ValueError ( f \"true label { true } not in available classes\" ) elif true not in labels_topred : raise ValueError ( f \"true label { true } not in available classes\" ) res . append ( \"\" ) metrics [ clss ] = {} metrics [ clss ][ \"accuracy\" ] = np . mean ( np . array ( res )[ tokeep ] == adata . obs [ clss ] . values [ tokeep ] ) for x in metric_type : metrics [ clss ][ x ] = f1_score ( np . array ( res )[ tokeep ], adata . obs [ clss ] . values [ tokeep ], average = x ) return metrics","title":"compute_classification"},{"location":"tasks/#scprint2.tasks.cell_emb.compute_corr","text":"Compute the correlation between the output and target matrices. Parameters: out ( ndarray ) \u2013 The output matrix. to ( ndarray ) \u2013 The target matrix. doplot ( bool , default: True ) \u2013 Whether to generate a plot of the correlation coefficients. Defaults to True. compute_mean_regress ( bool , default: False ) \u2013 Whether to compute mean regression. Defaults to False. plot_corr_size ( int , default: 64 ) \u2013 The size of the plot for correlation. Defaults to 64. Returns: dict ( dict ) \u2013 A dictionary containing the computed metrics. Source code in scprint2/tasks/cell_emb.py 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 def compute_corr ( out : np . ndarray , to : np . ndarray , doplot : bool = True , compute_mean_regress : bool = False , plot_corr_size : int = 64 , ) -> dict : \"\"\" Compute the correlation between the output and target matrices. Args: out (np.ndarray): The output matrix. to (np.ndarray): The target matrix. doplot (bool, optional): Whether to generate a plot of the correlation coefficients. Defaults to True. compute_mean_regress (bool, optional): Whether to compute mean regression. Defaults to False. plot_corr_size (int, optional): The size of the plot for correlation. Defaults to 64. Returns: dict: A dictionary containing the computed metrics. \"\"\" metrics = {} corr_coef , p_value = spearmanr ( out , to . T , ) corr_coef [ p_value > 0.05 ] = 0 # corr_coef[] # only on non zero values, # compare a1-b1 corr with a1-b(n) corr. should be higher # Plot correlation coefficient val = plot_corr_size + 2 if compute_mean_regress else plot_corr_size metrics . update ( { \"recons_corr\" : np . mean ( corr_coef [ val :, : plot_corr_size ] . diagonal ())} ) if compute_mean_regress : metrics . update ( { \"mean_regress\" : np . mean ( corr_coef [ plot_corr_size : plot_corr_size + 2 , : plot_corr_size , ] . flatten () ) } ) if doplot : plt . figure ( figsize = ( 10 , 5 )) plt . imshow ( corr_coef , cmap = \"coolwarm\" , interpolation = \"none\" , vmin =- 1 , vmax = 1 ) plt . colorbar () plt . title ( 'Correlation Coefficient of expr and i[\"x\"]' ) plt . show () return metrics","title":"compute_corr"},{"location":"tasks/#scprint2.tasks.cell_emb.default_benchmark","text":"Run the default benchmark for embedding and annotation using the scPRINT model. Parameters: model ( Module ) \u2013 The scPRINT model to be used for embedding and annotation. folder_dir ( str , default: FILE_LOC + '/../../data/' ) \u2013 The directory containing data files. dataset ( str , default: FILE_LOC + '/../../data/gNNpgpo6gATjuxTE7CCp.h5ad' ) \u2013 The dataset to use for benchmarking. Can be a path or URL. do_class ( bool , default: True ) \u2013 Whether to perform classification. Defaults to True. coarse ( bool , default: False ) \u2013 Whether to use coarse cell type annotations. Defaults to False. Returns: dict ( dict ) \u2013 A dictionary containing the benchmark metrics. Source code in scprint2/tasks/cell_emb.py 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 def default_benchmark ( model : torch . nn . Module , folder_dir : str = FILE_LOC + \"/../../data/\" , dataset : str = FILE_LOC + \"/../../data/gNNpgpo6gATjuxTE7CCp.h5ad\" , do_class : bool = True , coarse : bool = False , ) -> dict : \"\"\" Run the default benchmark for embedding and annotation using the scPRINT model. Args: model (torch.nn.Module): The scPRINT model to be used for embedding and annotation. folder_dir (str, optional): The directory containing data files. dataset (str, optional): The dataset to use for benchmarking. Can be a path or URL. do_class (bool, optional): Whether to perform classification. Defaults to True. coarse (bool, optional): Whether to use coarse cell type annotations. Defaults to False. Returns: dict: A dictionary containing the benchmark metrics. \"\"\" if dataset . startswith ( \"https://\" ): adata = sc . read ( folder_dir + dataset . split ( \"/\" )[ - 1 ] + ( \".h5ad\" if not dataset . endswith ( \".h5ad\" ) else \"\" ), backup_url = dataset , ) else : adata = sc . read_h5ad ( dataset ) if adata . shape [ 0 ] > 100_000 : adata = adata [ adata . obs_names [ np . random . choice ( adata . shape [ 0 ], 100_000 , replace = False )] ] max_len = 4000 if adata . X . sum ( 1 ) . mean () < 50_000 else 8000 batch_size = 64 if adata . X . sum ( 1 ) . mean () < 50_000 else 32 log_every = 10_000 if dataset . split ( \"/\" )[ - 1 ] in [ \"24539942\" , \"24539828\" ]: # lung and pancreas adata . obs [ \"organism_ontology_term_id\" ] = \"NCBITaxon:9606\" use_layer = \"counts\" is_symbol = True batch_key = \"tech\" if dataset . split ( \"/\" )[ - 1 ] == \"24539828\" else \"batch\" label_key = \"celltype\" if dataset . split ( \"/\" )[ - 1 ] == \"24539828\" else \"cell_type\" adata . obs [ \"cell_type_ontology_term_id\" ] = adata . obs [ label_key ] . replace ( COARSE if coarse else FINE ) adata . obs [ \"assay_ontology_term_id\" ] = adata . obs [ batch_key ] . replace ( COARSE if coarse else FINE ) else : use_layer = None is_symbol = False batch_key = ( \"batch\" if dataset . split ( \"/\" )[ - 1 ] == \"661d5ec2-ca57-413c-8374-f49b0054ddba.h5ad\" else \"assay_ontology_term_id\" ) label_key = \"cell_type_ontology_term_id\" preprocessor = Preprocessor ( use_layer = use_layer , is_symbol = is_symbol , force_preprocess = True , skip_validate = True , do_postp = model . expr_emb_style == \"metacell\" , drop_non_primary = False , ) adata = preprocessor ( adata . copy ()) if model . expr_emb_style == \"metacell\" : sc . pp . neighbors ( adata , use_rep = \"X_pca\" ) embedder = Embedder ( pred_embedding = ( model . pred_embedding if model . pred_embedding is not None else [ \"all\" ] ), doclass = do_class , max_len = max_len , doplot = False , keep_all_labels_pred = False , save_every = log_every , batch_size = batch_size , how = \"random expr\" , ) adata , metrics = embedder ( model , adata ) bm = Benchmarker ( adata , batch_key = batch_key , label_key = label_key , embedding_obsm_keys = [ \"scprint_emb\" ], ) bm . benchmark () metrics . update ( { \"scib\" : bm . get_results ( min_max_scale = False ) . T . to_dict ()[ \"scprint_emb\" ]} ) if model . class_scale > 0 : metrics [ \"classif\" ] = compute_classification ( adata , model . classes , model . label_decoders , model . labels_hierarchy ) return metrics","title":"default_benchmark"},{"location":"tasks/#scprint2.tasks.cell_emb.display_confusion_matrix","text":"Display the confusion matrix for true vs predicted cell types. Parameters: nadata ( AnnData ) \u2013 Annotated data object containing predictions and ground truth. pred ( str , default: 'conv_pred_cell_type_ontology_term_id' ) \u2013 Column name for predictions. Defaults to \"conv_pred_cell_type_ontology_term_id\". true ( str , default: 'cell_type' ) \u2013 Column name for ground truth. Defaults to \"cell_type\". Source code in scprint2/tasks/cell_emb.py 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 def display_confusion_matrix ( nadata , pred = \"conv_pred_cell_type_ontology_term_id\" , true = \"cell_type\" ): \"\"\" Display the confusion matrix for true vs predicted cell types. Args: nadata (AnnData): Annotated data object containing predictions and ground truth. pred (str): Column name for predictions. Defaults to \"conv_pred_cell_type_ontology_term_id\". true (str): Column name for ground truth. Defaults to \"cell_type\". \"\"\" counts = None for k , v in nadata . obs [ true ] . value_counts () . items (): name = k + \" - \" + str ( v ) if counts is None : counts = pd . DataFrame ( nadata . obs . loc [ nadata . obs [ true ] == k , pred , ] . value_counts () ) . rename ( columns = { \"count\" : name }) else : counts = pd . concat ( [ counts , pd . DataFrame ( nadata . obs . loc [ nadata . obs [ true ] == k , pred , ] . value_counts (), ) . rename ( columns = { \"count\" : name }), ], axis = 1 , ) counts = counts . T # Fill NaN values with 0 for visualization counts_filled = counts . fillna ( 0 ) # Create the heatmap plt . figure ( figsize = ( 12 , 10 )) # Convert to percentages (row-wise normalization) counts_percentage = counts_filled . div ( counts_filled . sum ( axis = 1 ), axis = 0 ) * 100 counts_percentage = counts_percentage . iloc [:, counts_percentage . values . max ( 0 ) > 5 ] ax = sns . heatmap ( counts_percentage , cmap = \"Blues\" , cbar_kws = { \"label\" : \"Percentage (%)\" }, linewidths = 0.5 , square = True , ) # place the x-label on top ax . xaxis . set_label_position ( \"top\" ) ax . xaxis . tick_top () plt . title ( \"Confusion Matrix: \" + true + \" vs \" + pred + \" (Percentage)\" , fontsize = 16 , pad = 20 , ) ax . set_xlabel ( pred , fontsize = 12 ) ax . set_ylabel ( true + \" (with counts)\" , fontsize = 12 ) ax . set_xticklabels ( ax . get_xticklabels (), rotation = 45 , ha = \"left\" , fontsize = 12 ) ax . set_yticklabels ( ax . get_yticklabels (), rotation = 0 , fontsize = 14 ) plt . tight_layout () plt . show ()","title":"display_confusion_matrix"},{"location":"tasks/#scprint2.tasks.grn","text":"Classes: Name Description GNInfer Functions: Name Description default_benchmark default_benchmark function to run the default scPRINT GRN benchmark","title":"grn"},{"location":"tasks/#scprint2.tasks.grn.GNInfer","text":"GNInfer a class to infer gene regulatory networks from a dataset using a scPRINT model. Parameters: batch_size ( int , default: 64 ) \u2013 Batch size for processing. Defaults to 64. num_workers ( int , default: 8 ) \u2013 Number of workers for data loading. Defaults to 8. drop_unexpressed ( bool , default: True ) \u2013 Whether to drop unexpressed genes. Defaults to True. In this context, genes that have no expression in the dataset are dropped. num_genes ( int , default: 3000 ) \u2013 Number of genes to consider. Defaults to 3000. max_cells ( int , default: 0 ) \u2013 Maximum number of cells to consider. Defaults to 0. if less than total number of cells, only the top max_cells cells with the most counts will be considered. cell_type_col ( str , default: 'cell_type' ) \u2013 Column name for cell type information. Defaults to \"cell_type\". how ( str , default: 'most var within' ) \u2013 Method to select genes. Options are \"most var\", \"random expr\", \"some\". Defaults to \"most var\". - \"most var across\": select the most variable genes across all cell types - \"most var within\": select the most variable genes within a cell type - \"random expr\": select random expressed genes - \"some\": select a subset of genes defined in genelist - \"most expr\": select the most expressed genes in the cell type genelist ( list , default: None ) \u2013 List of genes to consider. Defaults to an empty list. layer ( Optional [ List [ int ]] , default: None ) \u2013 List of layers to use for the inference. Defaults to None. preprocess ( str , default: 'softmax' ) \u2013 Preprocessing method. Options are \"softmax\", \"sinkhorn\", \"none\". Defaults to \"softmax\". head_agg ( str , default: 'mean' ) \u2013 Aggregation method for heads. Options are \"mean_full\", \"mean\", \"sum\", \"none\". Defaults to \"mean\". filtration ( str , default: 'thresh' ) \u2013 Filtration method for the adjacency matrix. Options are \"thresh\", \"top-k\", \"mst\", \"known\", \"none\". Defaults to \"thresh\". k ( int , default: 10 ) \u2013 Number of top connections to keep if filtration is \"top-k\". Defaults to 10. known_grn ( optional , default: None ) \u2013 Known gene regulatory network to use as a reference. Defaults to None. - We will only keep the genes that are present in the known GRN. precomp_attn ( bool , default: True ) \u2013 Whether to let the model precompute attn or do it at the end. This takes more memory but the model can compute mean over the attention matrices instead of over the qs and ks then taking the product. It is required for mean_full head_agg. Defaults to False. symmetrize ( bool , default: False ) \u2013 Whether to GRN. Defaults to False. loc ( str , default: './' ) \u2013 Location to save results. Defaults to \"./\". use_knn ( bool , default: True ) \u2013 Whether to use k-nearest neighbors information. Defaults to True. Methods: Name Description __call__ call runs the method aggregate part to aggregate the qks and compute the attns filter part to filter the attn matrix given user inputs predict part to predict the qks or attns matrices from the adata with the model Source code in scprint2/tasks/grn.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __init__ ( self , batch_size : int = 64 , num_workers : int = 8 , drop_unexpressed : bool = True , num_genes : int = 3000 , max_cells : int = 0 , cell_type_col : str = \"cell_type\" , how : str = \"most var within\" , # random expr, most var within, most var across, some genelist : Optional [ List [ str ]] = None , layer : Optional [ List [ int ]] = None , preprocess : str = \"softmax\" , # sinkhorn, softmax, none head_agg : str = \"mean\" , # mean, sum, none, mean_full filtration : str = \"thresh\" , # thresh, top-k, mst, known, none k : int = 10 , known_grn : Optional [ Any ] = None , precomp_attn : bool = True , symmetrize : bool = False , loc : str = \"./\" , use_knn : bool = True , ): \"\"\" GNInfer a class to infer gene regulatory networks from a dataset using a scPRINT model. Args: batch_size (int, optional): Batch size for processing. Defaults to 64. num_workers (int, optional): Number of workers for data loading. Defaults to 8. drop_unexpressed (bool, optional): Whether to drop unexpressed genes. Defaults to True. In this context, genes that have no expression in the dataset are dropped. num_genes (int, optional): Number of genes to consider. Defaults to 3000. max_cells (int, optional): Maximum number of cells to consider. Defaults to 0. if less than total number of cells, only the top `max_cells` cells with the most counts will be considered. cell_type_col (str, optional): Column name for cell type information. Defaults to \"cell_type\". how (str, optional): Method to select genes. Options are \"most var\", \"random expr\", \"some\". Defaults to \"most var\". - \"most var across\": select the most variable genes across all cell types - \"most var within\": select the most variable genes within a cell type - \"random expr\": select random expressed genes - \"some\": select a subset of genes defined in genelist - \"most expr\": select the most expressed genes in the cell type genelist (list, optional): List of genes to consider. Defaults to an empty list. layer (Optional[List[int]], optional): List of layers to use for the inference. Defaults to None. preprocess (str, optional): Preprocessing method. Options are \"softmax\", \"sinkhorn\", \"none\". Defaults to \"softmax\". head_agg (str, optional): Aggregation method for heads. Options are \"mean_full\", \"mean\", \"sum\", \"none\". Defaults to \"mean\". filtration (str, optional): Filtration method for the adjacency matrix. Options are \"thresh\", \"top-k\", \"mst\", \"known\", \"none\". Defaults to \"thresh\". k (int, optional): Number of top connections to keep if filtration is \"top-k\". Defaults to 10. known_grn (optional): Known gene regulatory network to use as a reference. Defaults to None. - We will only keep the genes that are present in the known GRN. precomp_attn (bool, optional): Whether to let the model precompute attn or do it at the end. This takes more memory but the model can compute mean over the attention matrices instead of over the qs and ks then taking the product. It is required for mean_full head_agg. Defaults to False. symmetrize (bool, optional): Whether to GRN. Defaults to False. loc (str, optional): Location to save results. Defaults to \"./\". use_knn (bool, optional): Whether to use k-nearest neighbors information. Defaults to True. \"\"\" self . batch_size = batch_size self . num_workers = num_workers self . layer = layer self . loc = loc self . how = how assert self . how in [ \"most var within\" , \"most var across\" , \"random expr\" , \"some\" , \"most expr\" , ], \"how must be one of 'most var within', 'most var across', 'random expr', 'some', 'most expr'\" self . num_genes = num_genes if self . how != \"some\" else len ( self . genelist ) self . preprocess = preprocess self . cell_type_col = cell_type_col self . filtration = filtration self . genelist = genelist if genelist is not None else [] self . k = k self . symmetrize = symmetrize self . known_grn = known_grn self . head_agg = head_agg self . max_cells = max_cells self . curr_genes = None self . drop_unexpressed = drop_unexpressed self . use_knn = use_knn if self . filtration != \"none\" and self . head_agg == \"none\" : raise ValueError ( \"filtration must be 'none' when head_agg is 'none'\" )","title":"GNInfer"},{"location":"tasks/#scprint2.tasks.grn.GNInfer.__call__","text":"call runs the method Parameters: model ( Module ) \u2013 The model to be used for generating the network adata ( AnnData ) \u2013 Annotated data matrix of shape n_obs \u00d7 n_vars . n_obs is the number of cells and n_vars is the number of genes. cell_type ( str , default: None ) \u2013 Specific cell type to filter the data. Defaults to None. Returns: AnnData ( AnnData ) \u2013 Annotated data matrix with predictions and annotations. ndarray \u2013 np.ndarray: Filtered adjacency matrix. Source code in scprint2/tasks/grn.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def __call__ ( self , model : torch . nn . Module , adata : AnnData , cell_type = None ) -> tuple [ AnnData , np . ndarray ]: \"\"\" __call__ runs the method Args: model (torch.nn.Module): The model to be used for generating the network adata (AnnData): Annotated data matrix of shape `n_obs` \u00d7 `n_vars`. `n_obs` is the number of cells and `n_vars` is the number of genes. cell_type (str, optional): Specific cell type to filter the data. Defaults to None. Returns: AnnData: Annotated data matrix with predictions and annotations. np.ndarray: Filtered adjacency matrix. \"\"\" # Add at least the organism you are working with if self . layer is None : self . layer = list ( range ( model . nlayers )) self . n_cell_embs = model . attn . additional_tokens subadata = self . predict ( model , adata , self . layer , cell_type ) adjacencies = self . aggregate ( model ) model . attn . data = None if self . head_agg == \"none\" : return self . save ( adjacencies [ self . n_cell_embs :, self . n_cell_embs :, :], subadata , ) else : return self . save ( self . filter ( adjacencies )[ self . n_cell_embs :, self . n_cell_embs :], subadata , loc = self . loc , )","title":"__call__"},{"location":"tasks/#scprint2.tasks.grn.GNInfer.aggregate","text":"part to aggregate the qks and compute the attns or to aggregate the attns or do nothing if already done Source code in scprint2/tasks/grn.py 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 def aggregate ( self , model ): \"\"\" part to aggregate the qks and compute the attns or to aggregate the attns or do nothing if already done \"\"\" attn , genes = model . attn . get (), model . genes if model . attn . precomp_attn : self . curr_genes = [ i for i in genes if i in self . curr_genes ] return attn . detach () . cpu () . numpy () if self . how == \"random expr\" and self . drop_unexpressed : keep = np . array ( [ 1 ] * self . n_cell_embs + [ i in self . curr_genes for i in genes ], dtype = bool , ) attn = attn [:, keep , :, :, :] badloc = torch . isnan ( attn . sum (( 0 , 2 , 3 , 4 ))) attn = attn [:, ~ badloc , :, :, :] badloc = badloc . detach () . cpu () . numpy () self . curr_genes = ( np . array ( self . curr_genes )[ ~ badloc [ self . n_cell_embs :]] if self . how == \"random expr\" else [ i for i in genes if i in self . curr_genes ] ) # attn = attn[:, :, 0, :, :].permute(0, 2, 1, 3) @ attn[:, :, 1, :, :].permute( # 0, 2, 3, 1 # ) attns = None Qs = ( attn [:, :, 0 , :, :] . permute ( 0 , 2 , 1 , 3 ) . reshape ( - 1 , attn . shape [ 1 ], attn . shape [ - 1 ]) ) Ks = ( attn [:, :, 1 , :, :] . permute ( 0 , 2 , 1 , 3 ) . reshape ( - 1 , attn . shape [ 1 ], attn . shape [ - 1 ]) ) for i in range ( Qs . shape [ 0 ]): attn = Qs [ i ] @ Ks [ i ] . T # return attn if self . preprocess == \"sinkhorn\" : scale = Qs . shape [ - 1 ] ** - 0.5 attn = attn * scale if attn . numel () > 100_000_000 : raise ValueError ( \"you can't sinkhorn such a large matrix\" ) sink = SinkhornDistance ( 0.1 , max_iter = 200 ) attn = sink ( attn )[ 0 ] attn = attn * Qs . shape [ - 1 ] elif self . preprocess == \"softmax\" : scale = Qs . shape [ - 1 ] ** - 0.5 attn = attn * scale attn = torch . nn . functional . softmax ( attn , dim =- 1 ) elif self . preprocess == \"softpick\" : attn = softpick ( attn ) elif self . preprocess == \"none\" : pass else : raise ValueError ( \"preprocess must be one of 'sinkhorn', 'softmax', 'none'\" ) if self . symmetrize : attn = ( attn + attn . T ) / 2 if self . head_agg == \"mean\" : attns = attn + ( attns if attns is not None else 0 ) elif self . head_agg == \"max\" : attns = torch . max ( attn , attns ) if attns is not None else attn elif self . head_agg == \"none\" : attn = attn . reshape ( attn . shape [ 0 ], attn . shape [ 1 ], 1 ) if attns is not None : attns = torch . cat (( attns , attn . detach () . cpu ()), dim = 2 ) else : attns = attn . detach () . cpu () else : raise ValueError ( \"head_agg must be one of 'mean', 'mean_full', 'max' or 'none'\" ) if self . head_agg == \"mean\" : attns = attns / Qs . shape [ 0 ] return ( attns . detach () . cpu () . numpy () if self . head_agg != \"none\" else attns . numpy () )","title":"aggregate"},{"location":"tasks/#scprint2.tasks.grn.GNInfer.filter","text":"part to filter the attn matrix given user inputs Source code in scprint2/tasks/grn.py 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 def filter ( self , adj , gt = None ): \"\"\" part to filter the attn matrix given user inputs \"\"\" if self . filtration == \"thresh\" : adj [ adj < ( 1 / adj . shape [ - 1 ])] = 0 res = ( adj != 0 ) . sum () if res / adj . shape [ 0 ] ** 2 < 0.01 : adj = scipy . sparse . csr_matrix ( adj ) elif self . filtration == \"none\" : pass elif self . filtration == \"top-k\" : args = np . argsort ( adj ) adj [ np . arange ( adj . shape [ 0 ])[:, None ], args [:, : - self . k ]] = 0 adj = scipy . sparse . csr_matrix ( adj ) elif self . filtration == \"known\" and gt is not None : gt = gt . reindex ( sorted ( gt . columns ), axis = 1 ) gt = gt . reindex ( sorted ( gt . columns ), axis = 0 ) gt = gt [ gt . index . isin ( self . curr_genes )] . iloc [ :, gt . columns . isin ( self . curr_genes ) ] loc = np . isin ( self . curr_genes , gt . index ) self . curr_genes = np . array ( self . curr_genes )[ loc ] adj = adj [ self . n_cell_embs :, self . n_cell_embs :][ loc ][:, loc ] adj [ gt . values != 1 ] = 0 adj = scipy . sparse . csr_matrix ( adj ) elif self . filtration == \"tmfg\" : adj = nx . to_scipy_sparse_array ( tmfg ( adj )) elif self . filtration == \"mst\" : pass else : raise ValueError ( \"filtration must be one of 'thresh', 'none' or 'top-k'\" ) res = ( adj != 0 ) . sum () if self . filtration != \"none\" else adj . shape [ 0 ] ** 2 print ( f \"avg link count: { res } , sparsity: { res / adj . shape [ 0 ] ** 2 } \" ) return adj","title":"filter"},{"location":"tasks/#scprint2.tasks.grn.GNInfer.predict","text":"part to predict the qks or attns matrices from the adata with the model Source code in scprint2/tasks/grn.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 def predict ( self , model , adata , layer , cell_type = None ): \"\"\" part to predict the qks or attns matrices from the adata with the model \"\"\" self . curr_genes = None model . pred_log_adata = False if cell_type is not None : subadata = adata [ adata . obs [ self . cell_type_col ] == cell_type ] . copy () else : subadata = adata . copy () if self . how == \"most var within\" : try : sc . pp . highly_variable_genes ( subadata , flavor = \"seurat_v3\" , n_top_genes = self . num_genes ) except ValueError : sc . pp . highly_variable_genes ( subadata , flavor = \"seurat_v3\" , n_top_genes = self . num_genes , span = 0.6 , ) self . curr_genes = ( subadata . var . index [ subadata . var . highly_variable ] . tolist () + self . genelist ) print ( \"number of expressed genes in this cell type: \" + str (( subadata . X . sum ( 0 ) > 1 ) . sum ()) ) elif self . how == \"most var across\" and cell_type is not None : adata . raw = adata sc . tl . rank_genes_groups ( adata , mask_var = adata . var . index . isin ( model . genes ), groupby = self . cell_type_col , groups = [ cell_type ], ) diff_expr_genes = adata . uns [ \"rank_genes_groups\" ][ \"names\" ][ cell_type ] diff_expr_genes = [ gene for gene in diff_expr_genes if gene in model . genes ] self . curr_genes = diff_expr_genes [: self . num_genes ] + self . genelist self . curr_genes . sort () elif self . how == \"random expr\" : self . curr_genes = model . genes # raise ValueError(\"cannot do it yet\") pass elif self . how == \"some\" and len ( self . genelist ) > 0 : self . curr_genes = self . genelist elif self . how == \"most expr\" : self . curr_genes = adata . var . index [ adata . X . sum ( 0 ) . A1 . argsort ()[:: - 1 ] ] . tolist ()[: self . num_genes ] else : raise ValueError ( \"something wrong with your inputs\" ) if self . drop_unexpressed : expr = subadata . var [( subadata . X . sum ( 0 ) > 0 ) . tolist ()[ 0 ]] . index . tolist () self . curr_genes = [ i for i in self . curr_genes if i in expr ] # Order cells by total count cell_sums = subadata . X . sum ( axis = 1 ) order = np . argsort ( - cell_sums . A1 if scipy . sparse . issparse ( subadata . X ) else - cell_sums ) subadata = subadata [ order ] . copy () subadata = subadata [: self . max_cells ] if self . max_cells else subadata if len ( subadata ) == 0 : raise ValueError ( \"no cells in the dataset\" ) adataset = SimpleAnnDataset ( subadata , obs_to_output = [ \"organism_ontology_term_id\" ], get_knn_cells = model . expr_emb_style == \"metacell\" and self . use_knn , ) col = Collator ( organisms = model . organisms , valid_genes = model . genes , max_len = self . num_genes if self . how == \"random expr\" else 0 , how = \"some\" if self . how != \"random expr\" else \"random expr\" , genelist = self . curr_genes if self . how != \"random expr\" else [], n_bins = model . n_input_bins if model . expr_emb_style == \"binned\" else 0 , ) dataloader = DataLoader ( adataset , collate_fn = col , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , ) model . attn . precomp_attn = self . head_agg == \"mean_full\" if self . num_genes > 10_000 and model . attn . precomp_attn : raise ValueError ( \"need less genes for a non-shared-qk version\" ) prevplot = model . doplot model . doplot = False model . on_predict_epoch_start () model . eval () model . attn . data = None # reparametrize the attn process if model . transformer . attn_type == \"hyper\" : self . curr_genes = [ i for i in model . genes if i in self . curr_genes ] num = ( 1 if model . use_metacell_token else 0 ) + ( ( len ( model . classes ) + 1 ) if not model . cell_transformer else 0 ) if ( len ( self . curr_genes ) + num ) % 128 != 0 : self . curr_genes = self . curr_genes [ : ( len ( self . curr_genes ) // 128 * 128 ) - num ] if self . how != \"random expr\" : if model . attn . precomp_attn : model . attn . gene_dim = len ( set ( self . curr_genes ) & set ( model . genes )) model . attn . apply_softmax = self . preprocess == \"softmax\" else : if subadata . obs [ \"organism_ontology_term_id\" ] . unique () . shape [ 0 ] > 1 : raise ValueError ( \"only one organism at a time is supported for precomp_attn\" ) n = False for i , k in col . start_idx . items (): if n : model . attn . gene_dim = k - model . attn . speciesloc break if i == subadata . obs [ \"organism_ontology_term_id\" ] . unique ()[ 0 ]: model . attn . speciesloc = k n = True elif not model . attn . precomp_attn : raise ValueError ( \"full attention (i.e. precomp_attn=True) is not supported for random expr\" ) device = model . device . type dtype = ( torch . float16 if isinstance ( model . transformer , FlashTransformer ) else model . dtype ) with torch . no_grad (), torch . autocast ( device_type = device , dtype = dtype ): for batch in tqdm ( dataloader ): gene_pos , expression , depth = ( batch [ \"genes\" ] . to ( device ), batch [ \"x\" ] . to ( device ), batch [ \"depth\" ] . to ( device ), ) model . _predict ( gene_pos , expression , depth , knn_cells = ( batch [ \"knn_cells\" ] . to ( device ) if model . expr_emb_style == \"metacell\" and self . use_knn else None ), knn_cells_info = ( batch [ \"knn_cells_info\" ] . to ( device ) if model . expr_emb_style == \"metacell\" and self . use_knn else None ), keep_output = False , get_attention_layer = layer if type ( layer ) is list else [ layer ], ) torch . cuda . empty_cache () model . doplot = prevplot return subadata","title":"predict"},{"location":"tasks/#scprint2.tasks.grn.default_benchmark","text":"default_benchmark function to run the default scPRINT GRN benchmark Parameters: model ( Any ) \u2013 The scPRINT model to be used for the benchmark. default_dataset ( str , default: 'sroy' ) \u2013 The default dataset to use for benchmarking. Defaults to \"sroy\". cell_types ( List [ str ] , default: [] ) \u2013 List of cell types to include in the benchmark. Defaults to []. maxlayers ( int , default: 16 ) \u2013 Maximum number of layers to use from the model. Defaults to 16. maxgenes ( int , default: 5000 ) \u2013 Maximum number of genes to consider. Defaults to 5000. batch_size ( int , default: 32 ) \u2013 Batch size for processing. Defaults to 32. maxcells ( int , default: 1024 ) \u2013 Maximum number of cells to consider. Defaults to 1024. Returns: dict ( dict ) \u2013 A dictionary containing the benchmark metrics. Source code in scprint2/tasks/grn.py 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 def default_benchmark ( model : Any , default_dataset : str = \"sroy\" , cell_types : List [ str ] = [], maxlayers : int = 16 , maxgenes : int = 5000 , batch_size : int = 32 , maxcells : int = 1024 , ) -> dict : \"\"\" default_benchmark function to run the default scPRINT GRN benchmark Args: model (Any): The scPRINT model to be used for the benchmark. default_dataset (str, optional): The default dataset to use for benchmarking. Defaults to \"sroy\". cell_types (List[str], optional): List of cell types to include in the benchmark. Defaults to []. maxlayers (int, optional): Maximum number of layers to use from the model. Defaults to 16. maxgenes (int, optional): Maximum number of genes to consider. Defaults to 5000. batch_size (int, optional): Batch size for processing. Defaults to 32. maxcells (int, optional): Maximum number of cells to consider. Defaults to 1024. Returns: dict: A dictionary containing the benchmark metrics. \"\"\" metrics = {} layers = list ( range ( model . nlayers ))[ max ( 0 , model . nlayers - maxlayers ) :] clf_omni = None if default_dataset == \"sroy\" : preprocessor = Preprocessor ( is_symbol = True , force_preprocess = True , skip_validate = True , do_postp = model . expr_emb_style == \"metacell\" , min_valid_genes_id = 5000 , min_dataset_size = 64 , keepdata = True , ) clf_self = None todo = [ ( \"han\" , \"human\" , \"full\" ), ( \"mine\" , \"human\" , \"full\" ), ( \"han\" , \"human\" , \"chip\" ), ( \"han\" , \"human\" , \"ko\" ), ( \"tran\" , \"mouse\" , \"full\" ), ( \"zhao\" , \"mouse\" , \"full\" ), ( \"tran\" , \"mouse\" , \"chip\" ), ( \"tran\" , \"mouse\" , \"ko\" ), ] for da , spe , gt in todo : if gt != \"full\" : continue if \"NCBITaxon:10090\" not in model . organisms and spe == \"mouse\" : continue print ( da + \"_\" + gt ) preadata = get_sroy_gt ( get = da , species = spe , gt = gt ) adata = preprocessor ( preadata . copy ()) if model . expr_emb_style == \"metacell\" : sc . pp . neighbors ( adata , use_rep = \"X_pca\" ) grn_inferer = GNInfer ( layer = layers , how = \"most var within\" , preprocess = ( \"softpick\" if model . attention in [ \"softpick\" , \"softpick-flash\" ] else \"softmax\" ), head_agg = \"none\" , filtration = \"none\" , num_genes = maxgenes , num_workers = 8 , max_cells = maxcells , batch_size = batch_size , ) grn = grn_inferer ( model , adata ) grn . varp [ \"all\" ] = grn . varp [ \"GRN\" ] grn . var [ \"ensembl_id\" ] = grn . var . index grn . var [ \"symbol\" ] = make_index_unique ( grn . var [ \"symbol\" ] . astype ( str )) grn . var . index = grn . var [ \"symbol\" ] grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ] . mean ( - 1 ) . T metrics [ \"mean_\" + da + \"_\" + gt ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) grn . varp [ \"GRN\" ] = grn . varp [ \"GRN\" ] . T if spe == \"human\" : metrics [ \"mean_\" + da + \"_\" + gt + \"_base\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . scprint_benchmark () ## OMNI if clf_omni is None : grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ] _ , m , clf_omni = train_classifier ( grn , C = 1 , train_size = 0.9 , class_weight = { 1 : 800 , 0 : 1 }, shuffle = True , return_full = False , ) joblib . dump ( clf_omni , \"clf_omni.pkl\" ) metrics [ \"omni_classifier\" ] = m coef = clf_omni . coef_ [ 0 ] if clf_omni . coef_ . shape [ 0 ] == 1 else clf_omni . coef_ grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) if spe == \"human\" : metrics [ \"omni_\" + da + \"_\" + gt + \"_base\" ] = BenGRN ( grn , do_auc = True , doplot = True ) . scprint_benchmark () grn . varp [ \"GRN\" ] = grn . varp [ \"GRN\" ] . T metrics [ \"omni_\" + da + \"_\" + gt ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) ## SELF if clf_self is None : grn . varp [ \"GRN\" ] = np . transpose ( grn . varp [ \"all\" ], ( 1 , 0 , 2 )) _ , m , clf_self = train_classifier ( grn , other = preadata , C = 1 , train_size = 0.5 , class_weight = { 1 : 40 , 0 : 1 }, shuffle = False , return_full = False , ) metrics [ \"self_classifier\" ] = m coef = clf_self . coef_ [ 0 ] if clf_self . coef_ . shape [ 0 ] == 1 else clf_self . coef_ grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) . T metrics [ \"self_\" + da + \"_\" + gt ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) if spe == \"human\" : grn . varp [ \"GRN\" ] = grn . varp [ \"GRN\" ] . T metrics [ \"self_\" + da + \"_\" + gt + \"_base\" ] = BenGRN ( grn , do_auc = True , doplot = True ) . scprint_benchmark () ## chip / ko if ( da , spe , \"chip\" ) in todo : preadata = get_sroy_gt ( get = da , species = spe , gt = \"chip\" ) grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ] . mean ( - 1 ) . T metrics [ \"mean_\" + da + \"_\" + \"chip\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) . T metrics [ \"omni_\" + da + \"_\" + \"chip\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) . T metrics [ \"self_\" + da + \"_\" + \"chip\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) if ( da , spe , \"ko\" ) in todo : preadata = get_sroy_gt ( get = da , species = spe , gt = \"ko\" ) grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ] . mean ( - 1 ) . T metrics [ \"mean_\" + da + \"_\" + \"ko\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) . T metrics [ \"omni_\" + da + \"_\" + \"ko\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) . T metrics [ \"self_\" + da + \"_\" + \"ko\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = preadata ) del grn elif default_dataset == \"gwps\" : adata = get_perturb_gt () preprocessor = Preprocessor ( force_preprocess = True , keepdata = True , skip_validate = True , do_postp = model . expr_emb_style == \"metacell\" , min_valid_genes_id = maxgenes , min_dataset_size = 64 , ) nadata = preprocessor ( adata . copy ()) if model . expr_emb_style == \"metacell\" : sc . pp . neighbors ( nadata , use_rep = \"X_pca\" ) nadata . var [ \"isTF\" ] = False nadata . var . loc [ nadata . var . gene_name . isin ( grnutils . TF ), \"isTF\" ] = True nadata . var [ \"isTF\" ] . sum () grn_inferer = GNInfer ( layer = layers , how = \"most var within\" , preprocess = ( \"softpick\" if model . attention in [ \"softpick\" , \"softpick-flash\" ] else \"softmax\" ), head_agg = \"none\" , filtration = \"none\" , num_genes = maxgenes , max_cells = maxcells , num_workers = 8 , batch_size = batch_size , ) grn = grn_inferer ( model , nadata ) del nadata grn . varp [ \"all\" ] = grn . varp [ \"GRN\" ] grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ] . mean ( - 1 ) . T metrics [ \"mean\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = adata ) grn . var [ \"ensembl_id\" ] = grn . var . index grn . var . index = grn . var [ \"symbol\" ] grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ] . mean ( - 1 ) metrics [ \"mean_base\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . scprint_benchmark () grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ] grn . var . index = grn . var [ \"ensembl_id\" ] _ , m , clf_omni = train_classifier ( grn , C = 1 , train_size = 0.9 , class_weight = { 1 : 800 , 0 : 1 }, shuffle = True , doplot = False , return_full = False , use_col = \"gene_name\" , ) coef = clf_omni . coef_ [ 0 ] if clf_omni . coef_ . shape [ 0 ] == 1 else clf_omni . coef_ grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) . T metrics [ \"omni\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = adata ) metrics [ \"omni_classifier\" ] = m grn . var . index = grn . var [ \"symbol\" ] grn . varp [ \"GRN\" ] = grn . varp [ \"GRN\" ] . T metrics [ \"omni_base\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . scprint_benchmark () grn . varp [ \"GRN\" ] = np . transpose ( grn . varp [ \"all\" ], ( 1 , 0 , 2 )) grn . var . index = grn . var [ \"ensembl_id\" ] _ , m , clf_self = train_classifier ( grn , other = adata , C = 1 , train_size = 0.5 , class_weight = { 1 : 40 , 0 : 1 }, doplot = False , shuffle = False , return_full = False , use_col = \"ensembl_id\" , ) coef = clf_self . coef_ [ 0 ] if clf_self . coef_ . shape [ 0 ] == 1 else clf_self . coef_ grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) . T metrics [ \"self\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . compare_to ( other = adata ) metrics [ \"self_classifier\" ] = m grn . var . index = grn . var [ \"symbol\" ] grn . varp [ \"GRN\" ] = grn . varp [ \"GRN\" ] . T metrics [ \"self_base\" ] = BenGRN ( grn , do_auc = True , doplot = False ) . scprint_benchmark () elif default_dataset == \"genernib\" : raise ValueError ( \"Not implemented\" ) # for adata in [NORMAN, OP, ADAMSON]: # adata = sc.read_h5ad(adata) # adata.obs[\"organism_ontology_term_id\"] = \"NCBITaxon:9606\" # preprocessor = Preprocessor( # force_preprocess=False, # skip_validate=True, # drop_non_primary=False, # do_postp=False, # min_valid_genes_id=1000, # min_dataset_size=64, # keepdata=True, # is_symbol=True, # use_raw=False, # ) # adata = preprocessor(adata.copy()) # run_gene_rnib( # adata=adata, # model=model, # layer=layers, # how=\"most var within\", # preprocess=\"softmax\", # ) # grn_inferer = GNInfer( # how=\"most var across\", # preprocess=\"softmax\", # head_agg=\"mean\", # filtration=\"none\", # forward_mode=\"none\", # num_genes=3_000, # max_cells=3000, # batch_size=10, # cell_type_col=\"perturbation\", # layer=list(range(model.nlayers))[:], # ) # grn = grn_inferer(model, adata, cell_type=\"ctrl\") # grn.var.index = make_index_unique(grn.var[\"symbol\"].astype(str)) else : # max_genes=4000 if default_dataset . startswith ( \"https://\" ): adata = sc . read ( FILEDIR + \"/../../data/\" + default_dataset . split ( \"/\" )[ - 1 ], backup_url = default_dataset , ) else : adata = sc . read_h5ad ( default_dataset ) if default_dataset . split ( \"/\" )[ - 1 ] in [ \"yBCKp6HmXuHa0cZptMo7.h5ad\" ]: use_layer = \"counts\" is_symbol = True else : use_layer = None is_symbol = False preprocessor = Preprocessor ( use_layer = use_layer , is_symbol = is_symbol , force_preprocess = True , skip_validate = True , do_postp = model . expr_emb_style == \"metacell\" , drop_non_primary = False , ) adata = preprocessor ( adata . copy ()) adata . var [ \"isTF\" ] = False adata . var . loc [ adata . var . symbol . isin ( grnutils . TF ), \"isTF\" ] = True if model . expr_emb_style == \"metacell\" : if \"X_pca\" not in adata . obsm : sc . pp . pca ( adata , n_comps = 50 ) sc . pp . neighbors ( adata , use_rep = \"X_pca\" ) for celltype in list ( adata . obs [ \"cell_type\" ] . unique ())[: 14 ]: # print(celltype) # grn_inferer = GNInfer( # layer=layers, # how=\"random expr\", # preprocess=\"softmax\", # head_agg=\"max\", # filtration=\"none\", # num_workers=8, # num_genes=2200, # max_cells=maxcells, # batch_size=batch_size, # ) # # grn = grn_inferer(model, adata[adata.X.sum(1) > 500], cell_type=celltype) # grn.var.index = make_index_unique(grn.var[\"symbol\"].astype(str)) # metrics[celltype + \"_scprint\"] = BenGRN( # grn, doplot=False # ).scprint_benchmark() # del grn # gc.collect() grn_inferer = GNInfer ( layer = layers , how = \"most var across\" , preprocess = ( \"softpick\" if model . attention in [ \"softpick\" , \"softpick-flash\" ] else \"softmax\" ), head_agg = \"none\" , filtration = \"none\" , num_workers = 8 , num_genes = maxgenes , max_cells = maxcells , batch_size = batch_size , ) grn = grn_inferer ( model , adata [ adata . X . sum ( 1 ) > 500 ], cell_type = celltype ) grn . var . index = make_index_unique ( grn . var [ \"symbol\" ] . astype ( str )) grn . varp [ \"all\" ] = grn . varp [ \"GRN\" ] grn . varp [ \"GRN\" ] = grn . varp [ \"GRN\" ] . mean ( - 1 ) metrics [ celltype + \"_scprint_mean\" ] = BenGRN ( grn , doplot = False ) . scprint_benchmark () if clf_omni is None : grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ] _ , m , clf_omni = train_classifier ( grn , C = 1 , train_size = 0.6 , max_iter = 300 , class_weight = { 1 : 800 , 0 : 1 }, return_full = False , shuffle = True , doplot = False , ) joblib . dump ( clf_omni , \"clf_omni.pkl\" ) metrics [ \"classifier\" ] = m coef = clf_omni . coef_ [ 0 ] if clf_omni . coef_ . shape [ 0 ] == 1 else clf_omni . coef_ grn . varp [ \"GRN\" ] = grn . varp [ \"all\" ][:, :, coef > 0 ] . mean ( - 1 ) metrics [ celltype + \"_scprint_class\" ] = BenGRN ( grn , doplot = False ) . scprint_benchmark () del grn gc . collect () return metrics","title":"default_benchmark"},{"location":"tasks/#scprint2.tasks.denoise","text":"Classes: Name Description Denoiser Functions: Name Description default_benchmark default_benchmark function used to run the default denoising benchmark of scPRINT split_molecules Splits molecules into two (potentially overlapping) groups.","title":"denoise"},{"location":"tasks/#scprint2.tasks.denoise.Denoiser","text":"Denoiser class for denoising scRNA-seq data using a scPRINT model Parameters: batch_size ( int , default: 10 ) \u2013 Batch size for processing. Defaults to 10. num_workers ( int , default: 1 ) \u2013 Number of workers for data loading. Defaults to 1. max_len ( int , default: 5000 ) \u2013 Maximum number of genes to consider. Defaults to 5000. how ( str , default: 'most var' ) \u2013 Method to select genes. Options are \"most var\", \"random expr\", \"some\". Defaults to \"most var\". - \"most var\": select the most variable genes - \"random expr\": select random expressed genes - \"some\": select a subset of genes defined in genelist max_cells ( int , default: 500000 ) \u2013 Number of cells to use for plotting correlation. Defaults to 10000. doplot ( bool , default: False ) \u2013 Whether to generate plots of the similarity between the denoised and true expression data. Defaults to False. Only works when downsample_expr is not None and max_cells < 100. predict_depth_mult ( int , default: 4 ) \u2013 Multiplier for prediction depth. Defaults to 4. This will artificially increase the sequencing depth (or number of counts) to 4 times the original depth. downsample_expr ( Optional [ float ] , default: None ) \u2013 Fraction of expression data to downsample. Defaults to None. This is usefull to test the ability of the model to denoise the dataset. only to use the input data as a benchmark dataset. When this option is on, the denoiser will output benchmark metrics genelist ( List [ str ] , default: None ) \u2013 The list of genes to be used for embedding. Defaults to []: In this case, \"how\" needs to be \"most var\" or \"random expr\". save_every ( int , default: 100000 ) \u2013 The number of cells to save at a time. Defaults to 100_000. This is important to avoid memory issues. pred_embedding ( List [ str ] , default: ['cell_type_ontology_term_id'] ) \u2013 The embedding type to be used as the denoising will also predict the cell embeddings. additional_info ( bool , default: False ) \u2013 Whether to print additional benchmark information during denoising. Defaults to False. only useful when downsampling is used. apply_zero_pred ( bool , default: False ) \u2013 Whether to apply zero inflation to the output value during denoising, else uses only the predicted mean. applying zero inflation might give results closer to the specific biases of sequencing technologies but less biological truthful. use_knn ( bool , default: True ) \u2013 Whether to use knn cells for denoising when the model uses metacell expression embedding. Defaults to True. Methods: Name Description __call__ call calling the function Source code in scprint2/tasks/denoise.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __init__ ( self , batch_size : int = 10 , num_workers : int = 1 , max_len : int = 5_000 , how : str = \"most var\" , max_cells : int = 500_000 , doplot : bool = False , predict_depth_mult : int = 4 , downsample_expr : Optional [ float ] = None , genelist : Optional [ List [ str ]] = None , save_every : int = 100_000 , pred_embedding : List [ str ] = [ \"cell_type_ontology_term_id\" ], additional_info : bool = False , apply_zero_pred : bool = False , use_knn : bool = True , ): \"\"\" Denoiser class for denoising scRNA-seq data using a scPRINT model Args: batch_size (int, optional): Batch size for processing. Defaults to 10. num_workers (int, optional): Number of workers for data loading. Defaults to 1. max_len (int, optional): Maximum number of genes to consider. Defaults to 5000. how (str, optional): Method to select genes. Options are \"most var\", \"random expr\", \"some\". Defaults to \"most var\". - \"most var\": select the most variable genes - \"random expr\": select random expressed genes - \"some\": select a subset of genes defined in genelist max_cells (int, optional): Number of cells to use for plotting correlation. Defaults to 10000. doplot (bool, optional): Whether to generate plots of the similarity between the denoised and true expression data. Defaults to False. Only works when downsample_expr is not None and max_cells < 100. predict_depth_mult (int, optional): Multiplier for prediction depth. Defaults to 4. This will artificially increase the sequencing depth (or number of counts) to 4 times the original depth. downsample_expr (Optional[float], optional): Fraction of expression data to downsample. Defaults to None. This is usefull to test the ability of the model to denoise the dataset. only to use the input data as a benchmark dataset. When this option is on, the denoiser will output benchmark metrics genelist (List[str], optional): The list of genes to be used for embedding. Defaults to []: In this case, \"how\" needs to be \"most var\" or \"random expr\". save_every (int, optional): The number of cells to save at a time. Defaults to 100_000. This is important to avoid memory issues. pred_embedding (List[str], optional): The embedding type to be used as the denoising will also predict the cell embeddings. additional_info (bool, optional): Whether to print additional benchmark information during denoising. Defaults to False. only useful when downsampling is used. apply_zero_pred (bool, optional): Whether to apply zero inflation to the output value during denoising, else uses only the predicted mean. applying zero inflation might give results closer to the specific biases of sequencing technologies but less biological truthful. use_knn (bool, optional): Whether to use knn cells for denoising when the model uses metacell expression embedding. Defaults to True. \"\"\" self . batch_size = batch_size self . num_workers = num_workers self . max_len = max_len self . max_cells = max_cells self . doplot = doplot self . predict_depth_mult = predict_depth_mult self . how = how self . downsample_expr = downsample_expr self . genelist = genelist self . save_every = save_every self . pred_embedding = pred_embedding self . additional_info = additional_info self . apply_zero_pred = apply_zero_pred self . use_knn = use_knn","title":"Denoiser"},{"location":"tasks/#scprint2.tasks.denoise.Denoiser.__call__","text":"call calling the function Parameters: model ( Module ) \u2013 The scPRINT model to be used for denoising. adata ( AnnData ) \u2013 The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Returns: dict ( dict ) \u2013 The benchmark metrics if downsampling is used. Optional [ ndarray ] \u2013 Optional[np.ndarray]: The random set of cells used if max_cells < adata.shape[0]. AnnData ( AnnData ) \u2013 The denoised annotated data matrix. Source code in scprint2/tasks/denoise.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 def __call__ ( self , model : torch . nn . Module , adata : AnnData ) -> tuple [ dict , Optional [ np . ndarray ], AnnData ]: \"\"\" __call__ calling the function Args: model (torch.nn.Module): The scPRINT model to be used for denoising. adata (AnnData): The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Returns: dict: The benchmark metrics if downsampling is used. Optional[np.ndarray]: The random set of cells used if max_cells < adata.shape[0]. AnnData: The denoised annotated data matrix. \"\"\" # Select random number random_indices = None if self . max_cells < adata . shape [ 0 ]: random_indices = np . random . randint ( low = 0 , high = adata . shape [ 0 ], size = self . max_cells ) adataset = SimpleAnnDataset ( adata [ random_indices ], obs_to_output = [ \"organism_ontology_term_id\" ], get_knn_cells = model . expr_emb_style == \"metacell\" and self . use_knn , ) else : adataset = SimpleAnnDataset ( adata , obs_to_output = [ \"organism_ontology_term_id\" ], get_knn_cells = model . expr_emb_style == \"metacell\" and self . use_knn , ) if self . how == \"most var\" : sc . pp . highly_variable_genes ( adata , flavor = \"seurat_v3\" , n_top_genes = self . max_len , span = 0.99 ) self . genelist = adata . var . index [ adata . var . highly_variable ] else : self . genelist = adata . var . index self . genelist = [ i for i in model . genes if i in self . genelist ] print ( f \"working on { len ( self . genelist ) } accepted genes\" ) col = Collator ( organisms = model . organisms , valid_genes = model . genes , max_len = self . max_len , how = \"some\" if self . how == \"most var\" else self . how , genelist = self . genelist if self . how != \"random expr\" else [], n_bins = model . n_input_bins if model . expr_emb_style == \"binned\" else 0 , ) dataloader = DataLoader ( adataset , collate_fn = col , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , ) prevplot = model . doplot model . doplot = self . doplot model . on_predict_epoch_start () model . eval () device = model . device . type model . pred_log_adata = True stored_noisy = None rand = random_str () dtype = ( torch . float16 if type ( model . transformer ) is FlashTransformer else model . dtype ) torch . cuda . empty_cache () save_expr = model . save_expr model . save_expr = True with torch . no_grad (), torch . autocast ( device_type = device , dtype = dtype ): for batch in tqdm ( dataloader ): gene_pos , expression , depth = ( batch [ \"genes\" ] . to ( device ), batch [ \"x\" ] . to ( device ), batch [ \"depth\" ] . to ( device ), ) knn_cells = ( batch [ \"knn_cells\" ] . to ( device ) if model . expr_emb_style == \"metacell\" and self . use_knn else None ) if self . downsample_expr is not None : expression = utils . downsample_profile ( expression , self . downsample_expr ) if knn_cells is not None : for i in range ( knn_cells . shape [ 1 ]): knn_cells [:, i ] = utils . downsample_profile ( knn_cells [:, i ], self . downsample_expr ) if stored_noisy is None : stored_noisy = expression . cpu () . numpy () else : stored_noisy = np . concatenate ( [ stored_noisy , expression . cpu () . numpy ()], axis = 0 ) model . _predict ( gene_pos , expression , depth , knn_cells = ( batch [ \"knn_cells\" ] . to ( device ) if model . expr_emb_style == \"metacell\" and self . use_knn else None ), knn_cells_info = ( batch [ \"knn_cells_info\" ] . to ( device ) if model . expr_emb_style == \"metacell\" and self . use_knn else None ), do_generate = False , depth_mult = self . predict_depth_mult , pred_embedding = self . pred_embedding , max_size_in_mem = self . save_every , name = \"denoise_\" + rand + \"_\" , ) torch . cuda . empty_cache () model . log_adata ( name = \"denoise_\" + rand + \"_\" + str ( model . counter )) try : mdir = ( model . logger . save_dir if model . logger . save_dir is not None else \"data\" ) except : mdir = \"data\" pred_adata = [] for i in range ( model . counter + 1 ): file = ( mdir + \"/step_\" + str ( model . global_step ) + \"_\" + model . name + \"_denoise_\" + rand + \"_\" + str ( i ) + \"_\" + str ( model . global_rank ) + \".h5ad\" ) pred_adata . append ( sc . read_h5ad ( file )) os . remove ( file ) pred_adata = concat ( pred_adata ) if model . transformer . attn_type == \"hyper\" : # seq len must be a multiple of 128 num = ( 1 if model . use_metacell_token else 0 ) + ( ( len ( model . classes ) + 1 ) if not model . cell_transformer else 0 ) if ( stored_noisy . shape [ 1 ] + num ) % 128 != 0 : stored_noisy = stored_noisy [ :, : (( stored_noisy . shape [ 1 ]) // 128 * 128 ) - num ] pred_adata . X = stored_noisy metrics = None model . doplot = prevplot model . save_expr = save_expr if self . downsample_expr is not None : reco = np . array ( pred_adata . layers [ \"scprint_mu\" ] . data ) . reshape ( pred_adata . shape [ 0 ], - 1 ) # reco = reco * F.sigmoid( # torch.Tensor(np.array(pred_adata.layers[\"scprint_pi\"].data).reshape(pred_adata.shape[0], -1)) < 0.5 # ).numpy() adata = ( adata [ random_indices , adata . var . index . isin ( pred_adata . var . index )] if random_indices is not None else adata [:, adata . var . index . isin ( pred_adata . var . index )] ) true = adata [ :, pred_adata . var . index [ pred_adata . var . index . isin ( adata . var . index ) ] . to_list (), ] . X . toarray () if self . apply_zero_pred : reco = ( reco * ( 1 - F . sigmoid ( torch . Tensor ( np . array ( pred_adata . layers [ \"scprint_pi\" ] . data ) . reshape ( pred_adata . shape [ 0 ], - 1 ) ) ) ) . numpy () ) corr_coef , p_value = spearmanr ( np . vstack ([ reco [ true != 0 ], stored_noisy [ true != 0 ], true [ true != 0 ]]) . T ) metrics = { \"reco2noisy\" : corr_coef [ 0 , 1 ], \"reco2full\" : corr_coef [ 0 , 2 ], \"noisy2full\" : corr_coef [ 1 , 2 ], } if self . additional_info : # Sample only 3000 elements for correlation calculation if reco . shape [ 0 ] > 3000 : indices = np . random . choice ( reco . shape [ 0 ], 3000 , replace = False ) reco = reco [ indices ] stored_noisy = stored_noisy [ indices ] true = true [ indices ] corr , p_value = spearmanr ( np . vstack ( [ reco . flatten (), stored_noisy . flatten (), true . flatten (), ] ) . T ) m = { \"reco2full\" : corr [ 0 , 2 ], \"noisy2full\" : corr [ 1 , 2 ], } print ( \"corr with zeros: \" ) print ( m ) cell_wise = np . array ( [ spearmanr ( reco [ i ][ true [ i ] != 0 ], true [ i ][ true [ i ] != 0 ])[ 0 ] for i in range ( reco . shape [ 0 ]) ] ) torm = np . array ( [ spearmanr ( stored_noisy [ i ][ true [ i ] != 0 ], true [ i ][ true [ i ] != 0 ])[ 0 ] for i in range ( reco . shape [ 0 ]) ] ) cell_wise -= torm cell_wise_zero = np . mean ( [ spearmanr ( reco [ i ], true [ i ])[ 0 ] for i in range ( reco . shape [ 0 ])] ) print ( \"cell_wise self corr (reco, noisy, true)\" ) print ( { \"cell_wise_w_zero\" : cell_wise_zero , \"cell_wise_to_noisy\" : np . mean ( cell_wise ), } ) print ( \"depth-wise plot\" ) plot_cell_depth_wise_corr_improvement ( cell_wise , ( true > 0 ) . sum ( 1 )) if self . doplot and self . max_cells < 100 : corr_coef [ p_value > 0.05 ] = 0 plt . figure ( figsize = ( 10 , 5 )) plt . imshow ( corr_coef , cmap = \"coolwarm\" , interpolation = \"none\" , vmin =- 1 , vmax = 1 ) plt . colorbar () plt . title ( \"Expression Correlation Coefficient\" ) plt . show () return metrics , random_indices , pred_adata","title":"__call__"},{"location":"tasks/#scprint2.tasks.denoise.default_benchmark","text":"default_benchmark function used to run the default denoising benchmark of scPRINT Parameters: model ( Any ) \u2013 The scPRINT model to be used for the benchmark. folder_dir ( str , default: FILE_DIR + '/../../data/' ) \u2013 Directory containing data files. dataset ( str , default: FILE_DIR + '/../../data/gNNpgpo6gATjuxTE7CCp.h5ad' ) \u2013 Path to the dataset to use for benchmarking. Returns: dict ( dict ) \u2013 A dictionary containing the benchmark metrics. Source code in scprint2/tasks/denoise.py 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def default_benchmark ( model : Any , folder_dir : str = FILE_DIR + \"/../../data/\" , dataset : str = FILE_DIR + \"/../../data/gNNpgpo6gATjuxTE7CCp.h5ad\" , # r4iCehg3Tw5IbCLiCIbl ) -> dict : \"\"\" default_benchmark function used to run the default denoising benchmark of scPRINT Args: model (Any): The scPRINT model to be used for the benchmark. folder_dir (str, optional): Directory containing data files. dataset (str, optional): Path to the dataset to use for benchmarking. Returns: dict: A dictionary containing the benchmark metrics. \"\"\" if dataset . startswith ( \"https://\" ): adata = sc . read ( folder_dir + dataset . split ( \"/\" )[ - 1 ], backup_url = dataset , ) else : adata = sc . read_h5ad ( dataset ) if dataset . split ( \"/\" )[ - 1 ] == \"gNNpgpo6gATjuxTE7CCp.h5ad\" : use_layer = \"counts\" is_symbol = True else : use_layer = None is_symbol = False max_len = 4000 if adata . X . sum ( 1 ) . mean () < 150_000 else 8000 preprocessor = Preprocessor ( use_layer = use_layer , is_symbol = is_symbol , force_preprocess = True , skip_validate = True , do_postp = model . expr_emb_style == \"metacell\" , drop_non_primary = False , ) adata = preprocessor ( adata . copy ()) if model . expr_emb_style == \"metacell\" : if \"X_pca\" not in adata . obsm : sc . pp . pca ( adata , n_comps = 50 ) sc . pp . neighbors ( adata , use_rep = \"X_pca\" ) denoise = Denoiser ( batch_size = 40 if model . expr_emb_style != \"metacell\" else 20 , max_len = max_len , max_cells = 10_000 , doplot = False , num_workers = 8 , predict_depth_mult = 5 , downsample_expr = 0.7 , pred_embedding = model . pred_embedding , ) return denoise ( model , adata )[ 0 ]","title":"default_benchmark"},{"location":"tasks/#scprint2.tasks.denoise.split_molecules","text":"Splits molecules into two (potentially overlapping) groups. :param umis: Array of molecules to split :param data_split: Proportion of molecules to assign to the first group :param overlap_factor: Overlap correction factor, if desired :param random_state: For reproducible sampling :return: umis_X and umis_Y, representing split and ~(1 - split) counts sampled from the input array Source code in scprint2/tasks/denoise.py 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def split_molecules ( umis : np . ndarray , data_split : float , overlap_factor : float = 0.0 , random_state : np . random . RandomState = None , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Splits molecules into two (potentially overlapping) groups. :param umis: Array of molecules to split :param data_split: Proportion of molecules to assign to the first group :param overlap_factor: Overlap correction factor, if desired :param random_state: For reproducible sampling :return: umis_X and umis_Y, representing ``split`` and ``~(1 - split)`` counts sampled from the input array \"\"\" if random_state is None : random_state = np . random . RandomState () umis_X_disjoint = random_state . binomial ( umis , data_split - overlap_factor ) umis_Y_disjoint = random_state . binomial ( umis - umis_X_disjoint , ( 1 - data_split ) / ( 1 - data_split + overlap_factor ) ) overlap_factor = umis - umis_X_disjoint - umis_Y_disjoint umis_X = umis_X_disjoint + overlap_factor umis_Y = umis_Y_disjoint + overlap_factor return umis_X , umis_Y","title":"split_molecules"},{"location":"tasks/#scprint2.tasks.gene_emb","text":"Classes: Name Description GeneEmbeddingExtractor","title":"gene_emb"},{"location":"tasks/#scprint2.tasks.gene_emb.GeneEmbeddingExtractor","text":"Parameters: genelist ( list [ str ] ) \u2013 List of genes to restrict to. batch_size ( int , default: 64 ) \u2013 Batch size for the DataLoader. Defaults to 64. num_workers ( int , default: 8 ) \u2013 Number of workers for DataLoader. Defaults to 8. save_every ( int , default: 4000 ) \u2013 Save embeddings every save_every batches. Defaults to 4000. average ( bool , default: False ) \u2013 Whether to average embeddings across all cells. Defaults to False. save_dir ( str , default: None ) \u2013 Directory to save embeddings. If None, embeddings are not saved. Defaults to None. use_knn ( bool , default: False ) \u2013 Whether to use k-nearest neighbors information. Defaults to False. Source code in scprint2/tasks/gene_emb.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , genelist : list [ str ], batch_size : int = 64 , num_workers : int = 8 , save_every : int = 4_000 , average : bool = False , save_dir : str = None , use_knn : bool = False , ): \"\"\" Args: genelist (list[str]): List of genes to restrict to. batch_size (int): Batch size for the DataLoader. Defaults to 64. num_workers (int): Number of workers for DataLoader. Defaults to 8. save_every (int): Save embeddings every `save_every` batches. Defaults to 4000. average (bool): Whether to average embeddings across all cells. Defaults to False. save_dir (str): Directory to save embeddings. If None, embeddings are not saved. Defaults to None. use_knn (bool): Whether to use k-nearest neighbors information. Defaults to False. \"\"\" self . genelist = genelist self . batch_size = batch_size self . num_workers = num_workers self . save_every = save_every self . average = average self . save_dir = save_dir self . use_knn = use_knn","title":"GeneEmbeddingExtractor"},{"location":"tasks/#scprint2.tasks.generate","text":"Classes: Name Description Generate","title":"generate"},{"location":"tasks/#scprint2.tasks.generate.Generate","text":"Embedder a class to embed and annotate cells using a model Parameters: genelist ( List [ str ] ) \u2013 The list of genes for which to generate expression data. batch_size ( int , default: 64 ) \u2013 The size of the batches to be used in the DataLoader. Defaults to 64. embedding_to_use ( List [ str ] , default: ['all'] ) \u2013 The list of embeddings to be used for generating expression. Defaults to [\"all\"]. Methods: Name Description __call__ call function to call the embedding Source code in scprint2/tasks/generate.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , genelist : List [ str ], batch_size : int = 64 , embedding_to_use : List [ str ] = [ \"all\" , ], ): \"\"\" Embedder a class to embed and annotate cells using a model Args: genelist (List[str]): The list of genes for which to generate expression data. batch_size (int, optional): The size of the batches to be used in the DataLoader. Defaults to 64. embedding_to_use (List[str], optional): The list of embeddings to be used for generating expression. Defaults to [\"all\"]. \"\"\" self . batch_size = batch_size self . embedding_to_use = embedding_to_use self . genelist = genelist if genelist is not None else []","title":"Generate"},{"location":"tasks/#scprint2.tasks.generate.Generate.__call__","text":"call function to call the embedding Parameters: model ( Module ) \u2013 The scPRINT model to be used for embedding and annotation. adata ( AnnData ) \u2013 The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Raises: ValueError \u2013 If the model does not have a logger attribute. ValueError \u2013 If the model does not have a global_step attribute. Returns: AnnData ( AnnData ) \u2013 The annotated data matrix with embedded cell representations. List [ str ] \u2013 List[str]: List of gene names used in the embedding. ndarray \u2013 np.ndarray: The predicted expression values if sample\"none\". dict ( dict ) \u2013 Additional metrics and information from the embedding process. Source code in scprint2/tasks/generate.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def __call__ ( self , model : torch . nn . Module , adata : AnnData ) -> tuple [ AnnData , List [ str ], np . ndarray , dict ]: \"\"\" __call__ function to call the embedding Args: model (torch.nn.Module): The scPRINT model to be used for embedding and annotation. adata (AnnData): The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Raises: ValueError: If the model does not have a logger attribute. ValueError: If the model does not have a global_step attribute. Returns: AnnData: The annotated data matrix with embedded cell representations. List[str]: List of gene names used in the embedding. np.ndarray: The predicted expression values if sample\"none\". dict: Additional metrics and information from the embedding process. \"\"\" # one of \"all\" \"sample\" \"none\" model . predict_mode = \"none\" model . eval () model . on_predict_epoch_start () device = model . device . type dtype = ( torch . float16 if isinstance ( model . transformer , FlashTransformer ) else model . dtype ) if self . embedding_to_use == [ \"all\" ]: use = [ i for i in adata . obsm . keys () if i . startswith ( \"scprint_emb_\" ) and i != \"scprint_emb_other\" ] else : use = self . embedding_to_use res = [] with ( torch . no_grad (), torch . autocast ( device_type = device , dtype = dtype ), ): gene_pos = torch . tensor ( [ model . genes . index ( g ) for g in self . genelist ], ) . to ( device = device ) gene_pos = gene_pos . unsqueeze ( 0 ) . repeat_interleave ( self . batch_size , 0 ) req_depth = torch . tensor ( adata . X . sum ( 1 )) . squeeze ( - 1 ) . to ( device = device ) for batch in tqdm ( range ( adata . shape [ 0 ] // self . batch_size + 1 )): embeddings = [] start = batch * self . batch_size end = min (( batch + 1 ) * self . batch_size , adata . shape [ 0 ]) for emb in use : embeddings . append ( torch . tensor ( adata . obsm [ emb ][ start : end ]) . unsqueeze ( 1 ) ) embeddings = torch . concat ( embeddings , dim = 1 ) . to ( device = device ) output = model . _generate ( gene_pos = gene_pos [ 0 : end - start , :], cell_embs = embeddings , depth_mult = req_depth [ start : end ], req_depth = req_depth [ start : end ], metacell_token = None , ) res . append ( torch . concat ( [ output [ \"mean\" ] . detach () . cpu () . unsqueeze ( 0 ), output [ \"disp\" ] . detach () . cpu () . unsqueeze ( 0 ), output [ \"zero_logits\" ] . detach () . cpu () . unsqueeze ( 0 ), ] ) if \"disp\" in output else output [ \"mean\" ] . detach () . cpu () . unsqueeze ( 0 ) ) torch . cuda . empty_cache () res = torch . concat ( res , dim = 1 ) pred_adata = AnnData ( X = res [ 0 , :, :] . numpy (), obs = adata . obs . copy (), var = pd . DataFrame ( index = pd . Index ( self . genelist ), ), layers = None if res . shape [ 1 ] == 1 else { \"disp\" : res [ 1 , :, :] . numpy (), \"zero_logits\" : res [ 2 , :, :] . numpy (), }, ) return pred_adata","title":"__call__"},{"location":"tasks/#scprint2.tasks.impute","text":"Classes: Name Description Imputer","title":"impute"},{"location":"tasks/#scprint2.tasks.impute.Imputer","text":"Imputer class for imputing missing values in scRNA-seq data using a scPRINT model Parameters: batch_size ( int , default: 10 ) \u2013 Batch size for processing. Defaults to 10. num_workers ( int , default: 1 ) \u2013 Number of workers for data loading. Defaults to 1. max_cells ( int , default: 500000 ) \u2013 Number of cells to use for plotting correlation. Defaults to 10000. doplot ( bool , default: False ) \u2013 Whether to generate plots of the similarity between the denoised and true expression data. Defaults to False. Only works when downsample_expr is not None and max_cells < 100. method ( str , default: 'generative' ) \u2013 Imputation method, either 'masking' or 'generative'. Defaults to 'generative'. predict_depth_mult ( int , default: 4 ) \u2013 Multiplier for prediction depth. Defaults to 4. This will artificially increase the sequencing depth (or number of counts) to 4 times the original depth. genes_to_use ( List [ str ] ) \u2013 List of genes to use for imputation. Defaults to None. genes_to_impute ( List [ str ] ) \u2013 List of genes to impute. Defaults to None. save_every ( int , default: 100000 ) \u2013 The number of cells to save at a time. Defaults to 100_000. This is important to avoid memory issues. apply_zero_pred ( bool , default: True ) \u2013 Whether to apply zero prediction adjustment. Defaults to True. applying zero inflation might give results closer to the specific biases of sequencing technologies but less biological truthful. use_knn ( bool , default: True ) \u2013 Whether to use k-nearest neighbors information. Defaults to True. Methods: Name Description __call__ call calling the function Source code in scprint2/tasks/impute.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , genes_to_use : List [ str ], genes_to_impute : List [ str ], batch_size : int = 10 , num_workers : int = 1 , max_cells : int = 500_000 , doplot : bool = False , method : str = \"generative\" , predict_depth_mult : int = 4 , save_every : int = 100_000 , apply_zero_pred : bool = True , use_knn : bool = True , ): \"\"\" Imputer class for imputing missing values in scRNA-seq data using a scPRINT model Args: batch_size (int, optional): Batch size for processing. Defaults to 10. num_workers (int, optional): Number of workers for data loading. Defaults to 1. max_cells (int, optional): Number of cells to use for plotting correlation. Defaults to 10000. doplot (bool, optional): Whether to generate plots of the similarity between the denoised and true expression data. Defaults to False. Only works when downsample_expr is not None and max_cells < 100. method (str, optional): Imputation method, either 'masking' or 'generative'. Defaults to 'generative'. predict_depth_mult (int, optional): Multiplier for prediction depth. Defaults to 4. This will artificially increase the sequencing depth (or number of counts) to 4 times the original depth. genes_to_use (List[str]): List of genes to use for imputation. Defaults to None. genes_to_impute (List[str]): List of genes to impute. Defaults to None. save_every (int, optional): The number of cells to save at a time. Defaults to 100_000. This is important to avoid memory issues. apply_zero_pred (bool, optional): Whether to apply zero prediction adjustment. Defaults to True. applying zero inflation might give results closer to the specific biases of sequencing technologies but less biological truthful. use_knn (bool, optional): Whether to use k-nearest neighbors information. Defaults to True. \"\"\" self . batch_size = batch_size self . num_workers = num_workers self . max_cells = max_cells self . doplot = doplot self . predict_depth_mult = predict_depth_mult self . save_every = save_every self . genes_to_use = genes_to_use self . genes_to_impute = genes_to_impute self . method = method self . apply_zero_pred = apply_zero_pred self . use_knn = use_knn","title":"Imputer"},{"location":"tasks/#scprint2.tasks.impute.Imputer.__call__","text":"call calling the function Parameters: model ( Module ) \u2013 The scPRINT model to be used for denoising. adata ( AnnData ) \u2013 The anndata of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Returns: Optional [ ndarray ] \u2013 Optional[np.ndarray]: The random indices of the cells used when max_cells < adata.shape[0]. AnnData ( AnnData ) \u2013 The imputed anndata. Source code in scprint2/tasks/impute.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 def __call__ ( self , model : torch . nn . Module , adata : AnnData ) -> tuple [ Optional [ np . ndarray ], AnnData ]: \"\"\" __call__ calling the function Args: model (torch.nn.Module): The scPRINT model to be used for denoising. adata (AnnData): The anndata of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Returns: Optional[np.ndarray]: The random indices of the cells used when max_cells < adata.shape[0]. AnnData: The imputed anndata. \"\"\" # Select random number random_indices = None if self . max_cells < adata . shape [ 0 ]: random_indices = np . random . randint ( low = 0 , high = adata . shape [ 0 ], size = self . max_cells ) adataset = SimpleAnnDataset ( adata [ random_indices ], obs_to_output = [ \"organism_ontology_term_id\" ], get_knn_cells = model . expr_emb_style == \"metacell\" and self . use_knn , ) else : adataset = SimpleAnnDataset ( adata , obs_to_output = [ \"organism_ontology_term_id\" ], get_knn_cells = model . expr_emb_style == \"metacell\" and self . use_knn , ) genes_to_use = set ( model . genes ) & set ( self . genes_to_use ) print ( f \" { 100 * len ( genes_to_use ) / len ( self . genes_to_use ) } % of genes to use are available in the model\" ) genes_to_impute = set ( model . genes ) & set ( self . genes_to_impute ) print ( f \" { 100 * len ( genes_to_impute ) / len ( self . genes_to_impute ) } % of genes to impute are available in the model\" ) tot = genes_to_use | genes_to_impute tot = sorted ( tot ) col = Collator ( organisms = model . organisms , valid_genes = model . genes , how = \"some\" , genelist = list ( genes_to_use ) + ( list ( genes_to_impute ) if self . method == \"masking\" else []), n_bins = model . n_input_bins if model . expr_emb_style == \"binned\" else 0 , ) dataloader = DataLoader ( adataset , collate_fn = col , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , ) mask = None generate_on = None if self . method == \"masking\" : mask = torch . Tensor ( [ i in genes_to_use for i in tot ], ) . to ( device = model . device , dtype = torch . bool ) elif self . method == \"generative\" : generate_on = ( torch . Tensor ([ model . genes . index ( i ) for i in genes_to_impute ]) . to ( device = model . device ) . long () . unsqueeze ( 0 ) . repeat ( self . batch_size , 1 ) ) else : raise ValueError ( \"need to be one of generative or masking\" ) prevplot = model . doplot model . doplot = self . doplot model . on_predict_epoch_start () model . eval () device = model . device . type rand = random_str () dtype = ( torch . float16 if type ( model . transformer ) is FlashTransformer else model . dtype ) save_expr = model . save_expr model . save_expr = True torch . cuda . empty_cache () with torch . no_grad (), torch . autocast ( device_type = device , dtype = dtype ): for batch in tqdm ( dataloader ): gene_pos , expression , depth = ( batch [ \"genes\" ] . to ( device ), batch [ \"x\" ] . to ( device ), batch [ \"depth\" ] . to ( device ), ) model . _predict ( gene_pos , expression , depth , knn_cells = ( batch [ \"knn_cells\" ] . to ( device ) if model . expr_emb_style == \"metacell\" and self . use_knn else None ), do_generate = self . method == \"generative\" , depth_mult = self . predict_depth_mult , max_size_in_mem = self . save_every , name = \"impute\" + rand + \"_\" , mask = mask , generate_on = generate_on , ) torch . cuda . empty_cache () model . log_adata ( name = \"impute\" + rand + \"_\" + str ( model . counter )) try : mdir = ( model . logger . save_dir if model . logger . save_dir is not None else \"data\" ) except : mdir = \"data\" pred_adata = [] for i in range ( model . counter + 1 ): file = ( mdir + \"/step_\" + str ( model . global_step ) + \"_\" + model . name + \"_impute\" + rand + \"_\" + str ( i ) + \"_\" + str ( model . global_rank ) + \".h5ad\" ) pred_adata . append ( sc . read_h5ad ( file )) os . remove ( file ) pred_adata = concat ( pred_adata ) model . doplot = prevplot model . save_expr = save_expr # pred_adata.X = adata.X if random_indices is None else adata.X[random_indices] true_imp = pred_adata . X [:, pred_adata . var . index . isin ( genes_to_impute )] . toarray () if true_imp . sum () > 0 : # we had some gt pred_imp = pred_adata . layers [ \"scprint_mu\" ][ :, pred_adata . var . index . isin ( genes_to_impute ) ] . toarray () pred_known = pred_adata . layers [ \"scprint_mu\" ][ :, pred_adata . var . index . isin ( genes_to_use ) ] . toarray () true_known = pred_adata . X [ :, pred_adata . var . index . isin ( genes_to_use ) ] . toarray () if self . apply_zero_pred : pred_imp = ( pred_imp * ( 1 - F . sigmoid ( torch . Tensor ( pred_adata . layers [ \"scprint_pi\" ][ :, pred_adata . var . index . isin ( genes_to_impute ) ] . toarray () ) ) ) . numpy () ) pred_known = ( pred_known * ( 1 - F . sigmoid ( torch . Tensor ( pred_adata . layers [ \"scprint_pi\" ][ :, pred_adata . var . index . isin ( genes_to_use ) ] . toarray () ) ) ) . numpy () ) cell_wise_pred = np . array ( [ spearmanr ( pred_imp [ i ], true_imp [ i ])[ 0 ] for i in range ( pred_imp . shape [ 0 ]) ] ) cell_wise_known = np . array ( [ spearmanr ( pred_known [ i ], true_known [ i ])[ 0 ] for i in range ( pred_known . shape [ 0 ]) ] ) print ( { \"cell_wise_known\" : np . mean ( cell_wise_known ), \"cell_wise_pred\" : np . mean ( cell_wise_pred ), } ) if self . doplot : print ( \"depth-wise plot\" ) plot_cell_depth_wise_corr_improvement ( cell_wise_known , cell_wise_pred ) return random_indices , pred_adata","title":"__call__"},{"location":"tasks/#scprint2.tasks.finetune","text":"Classes: Name Description FinetuneBatchClass Functions: Name Description mmd_loss Compute Maximum Mean Discrepancy (MMD) loss between two 2D embedding matrices.","title":"finetune"},{"location":"tasks/#scprint2.tasks.finetune.FinetuneBatchClass","text":"Embedder a class to embed and annotate cells using a model Parameters: batch_key ( str , default: 'batch' ) \u2013 The key in adata.obs that indicates the batch information. Defaults to \"batch\". learn_batches_on ( str , default: None ) \u2013 The key in adata.obs to learn batch embeddings on. Defaults to None. if none, will not learn the batch embeddings. the goal is e.g. when having a new species, to learn an embedding for it during finetuning and replace the \"learn_batches_on\" embedding in the model with it, in this case it should be \"organism_ontology_term_id\". batch correction might indeed be better learnt with this additional argument in some cases. do_mmd_on ( str , default: None ) \u2013 The key in adata.obs to learn batch embeddings on. Defaults to None. this embedding should have less batch information in it, after finetuning. predict_keys ( List [ str ] , default: ['cell_type_ontology_term_id'] ) \u2013 List of keys in adata.obs to predict during fine-tuning. Defaults to [\"cell_type_ontology_term_id\"]. batch_size ( int , default: 16 ) \u2013 The size of the batches to be used in the DataLoader. Defaults to 64. num_workers ( int , default: 8 ) \u2013 The number of worker processes to use for data loading. Defaults to 8. max_len ( int , default: 5000 ) \u2013 The maximum length of the sequences to be processed. Defaults to 5000. lr ( float , default: 0.0002 ) \u2013 The learning rate for the optimizer. Defaults to 0.0002. num_epochs ( int , default: 8 ) \u2013 The number of epochs to train the model. Defaults to 8. ft_mode ( str , default: 'xpressor' ) \u2013 The fine-tuning mode, either \"xpressor\" or \"full\". Defaults to \"xpressor\". frac_train ( float , default: 0.8 ) \u2013 The fraction of data to be used for training. Defaults to 0.8. loss_scalers ( dict , default: {} ) \u2013 A dictionary specifying the scaling factors for different loss components. Defaults to {}. expr, class, mmd, kl, and any of the predict_keys can be specified. use_knn ( bool , default: True ) \u2013 Whether to use k-nearest neighbors information. Defaults to True. Methods: Name Description __call__ call function to call the embedding Source code in scprint2/tasks/finetune.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __init__ ( self , batch_key : str = \"batch\" , predict_keys : List [ str ] = [ \"cell_type_ontology_term_id\" ], max_len : int = 5000 , learn_batches_on : Optional [ str ] = None , num_workers : int = 8 , batch_size : int = 16 , num_epochs : int = 8 , do_mmd_on : Optional [ str ] = None , lr : float = 0.0002 , ft_mode : str = \"xpressor\" , frac_train : float = 0.8 , loss_scalers : dict = {}, use_knn : bool = True , ): \"\"\" Embedder a class to embed and annotate cells using a model Args: batch_key (str, optional): The key in adata.obs that indicates the batch information. Defaults to \"batch\". learn_batches_on (str, optional): The key in adata.obs to learn batch embeddings on. Defaults to None. if none, will not learn the batch embeddings. the goal is e.g. when having a new species, to learn an embedding for it during finetuning and replace the \"learn_batches_on\" embedding in the model with it, in this case it should be \"organism_ontology_term_id\". batch correction might indeed be better learnt with this additional argument in some cases. do_mmd_on (str, optional):The key in adata.obs to learn batch embeddings on. Defaults to None. this embedding should have less batch information in it, after finetuning. predict_keys (List[str], optional): List of keys in adata.obs to predict during fine-tuning. Defaults to [\"cell_type_ontology_term_id\"]. batch_size (int, optional): The size of the batches to be used in the DataLoader. Defaults to 64. num_workers (int, optional): The number of worker processes to use for data loading. Defaults to 8. max_len (int, optional): The maximum length of the sequences to be processed. Defaults to 5000. lr (float, optional): The learning rate for the optimizer. Defaults to 0.0002. num_epochs (int, optional): The number of epochs to train the model. Defaults to 8. ft_mode (str, optional): The fine-tuning mode, either \"xpressor\" or \"full\". Defaults to \"xpressor\". frac_train (float, optional): The fraction of data to be used for training. Defaults to 0.8. loss_scalers (dict, optional): A dictionary specifying the scaling factors for different loss components. Defaults to {}. expr, class, mmd, kl, and any of the predict_keys can be specified. use_knn (bool, optional): Whether to use k-nearest neighbors information. Defaults to True. \"\"\" self . batch_size = batch_size self . num_workers = num_workers self . batch_key = batch_key self . learn_batches_on = learn_batches_on self . predict_keys = predict_keys self . max_len = max_len self . lr = lr self . num_epochs = num_epochs self . ft_mode = ft_mode self . frac_train = frac_train self . batch_emb = None self . batch_encoder = {} self . do_mmd_on = do_mmd_on self . loss_scalers = loss_scalers self . use_knn = use_knn","title":"FinetuneBatchClass"},{"location":"tasks/#scprint2.tasks.finetune.FinetuneBatchClass.__call__","text":"call function to call the embedding Parameters: model ( Module ) \u2013 The scPRINT model to be used for embedding and annotation. adata ( AnnData , default: None ) \u2013 The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Defaults to None. if provided, it will be split into training and validation sets. train_data ( AnnData , default: None ) \u2013 The training data. Defaults to None. if adata is provided, this will be ignored. val_data ( AnnData , default: None ) \u2013 The validation data. Defaults to None. if adata is provided, this will be ignored. Raises: ValueError \u2013 If the model does not have a logger attribute. ValueError \u2013 If the model does not have a global_step attribute. Returns: Module \u2013 torch.nn.Module: the fine-tuned model Source code in scprint2/tasks/finetune.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 def __call__ ( self , model : torch . nn . Module , adata : AnnData = None , train_data : AnnData = None , val_data : AnnData = None , ) -> torch . nn . Module : \"\"\" __call__ function to call the embedding Args: model (torch.nn.Module): The scPRINT model to be used for embedding and annotation. adata (AnnData): The annotated data matrix of shape n_obs x n_vars. Rows correspond to cells and columns to genes. Defaults to None. if provided, it will be split into training and validation sets. train_data (AnnData, optional): The training data. Defaults to None. if adata is provided, this will be ignored. val_data (AnnData, optional): The validation data. Defaults to None. if adata is provided, this will be ignored. Raises: ValueError: If the model does not have a logger attribute. ValueError: If the model does not have a global_step attribute. Returns: torch.nn.Module: the fine-tuned model \"\"\" # one of \"all\" \"sample\" \"none\" model . predict_mode = \"none\" if self . ft_mode == \"xpressor\" : for val in model . parameters (): val . requires_grad = False # setting all to TRUE for val in model . cell_transformer . parameters (): val . requires_grad = True for val in model . transformer . blocks [ - 1 ] . parameters (): val . requires_grad = True for i in model . transformer . blocks : i . cross_attn . requires_grad = True for val in model . compressor . parameters (): val . requires_grad = True for val in self . predict_keys : for val in model . cls_decoders [ val ] . parameters (): val . requires_grad = True elif self . ft_mode == \"full\" : for val in model . parameters (): val . requires_grad = True else : raise ValueError ( \"ft_mode must be one of 'xpressor' or 'full'\" ) # PREPARING THE DATA if adata is not None : n_train = int ( self . frac_train * len ( adata )) train_idx = np . random . choice ( len ( adata ), n_train , replace = False ) val_idx = np . setdiff1d ( np . arange ( len ( adata )), train_idx ) train_data = adata [ train_idx ] . copy () val_data = adata [ val_idx ] . copy () print ( f \"Training data: { train_data . shape } \" ) print ( f \"Validation data: { val_data . shape } \" ) mencoders = {} for k , v in model . label_decoders . items (): mencoders [ k ] = { va : ke for ke , va in v . items ()} # this needs to remain its original name as it is expect like that by collator, otherwise need to send org_to_id as params for i in self . predict_keys : if len ( set ( train_data . obs [ i ]) - set ( mencoders [ i ] . keys ())) > 0 : print ( \"missing labels for \" , i ) train_data . obs [ i ] = train_data . obs [ i ] . apply ( lambda x : x if x in mencoders [ i ] else \"unknown\" ) if \"organism_ontology_term_id\" not in self . predict_keys : self . predict_keys . append ( \"organism_ontology_term_id\" ) # create datasets self . batch_encoder = { i : n for n , i in enumerate ( train_data . obs [ self . batch_key ] . astype ( \"category\" ) . cat . categories ) } mencoders [ self . batch_key ] = self . batch_encoder train_dataset = SimpleAnnDataset ( train_data , obs_to_output = self . predict_keys + [ self . batch_key ], get_knn_cells = model . expr_emb_style == \"metacell\" and self . use_knn , encoder = mencoders , ) if val_data is not None : for i in self . predict_keys : if i != \"organism_ontology_term_id\" : if len ( set ( val_data . obs [ i ]) - set ( mencoders [ i ] . keys ())) > 0 : val_data . obs [ i ] = val_data . obs [ i ] . apply ( lambda x : x if x in mencoders [ i ] else \"unknown\" ) self . batch_encoder . update ( { i : n + len ( self . batch_encoder ) for n , i in enumerate ( val_data . obs [ self . batch_key ] . astype ( \"category\" ) . cat . categories ) if i not in self . batch_encoder } ) mencoders [ self . batch_key ] = self . batch_encoder val_dataset = SimpleAnnDataset ( val_data , obs_to_output = self . predict_keys + [ self . batch_key ], get_knn_cells = model . expr_emb_style == \"metacell\" and self . use_knn , encoder = mencoders , ) # Create collator collator = Collator ( organisms = model . organisms , valid_genes = model . genes , class_names = self . predict_keys + [ self . batch_key ], how = \"random expr\" , # or \"all expr\" for full expression max_len = self . max_len , org_to_id = mencoders . get ( \"organism_ontology_term_id\" , {}), ) # Create data loaders train_loader = DataLoader ( train_dataset , collate_fn = collator , batch_size = self . batch_size , # Adjust based on GPU memory num_workers = self . num_workers , shuffle = True , ) if val_data is not None : val_loader = DataLoader ( val_dataset , collate_fn = collator , batch_size = self . batch_size , num_workers = self . num_workers , shuffle = False , ) if self . learn_batches_on is not None : if val_data is not None : print ( \"all batch key values in val_data should also be present in train_adata!!!\" ) self . batch_emb = torch . nn . Embedding ( num_embeddings = train_data . obs [ self . batch_key ] . nunique (), embedding_dim = ( model . compressor [ self . learn_batches_on ] . fc_mu . weight . shape [ 0 ] if hasattr ( model , \"compressor\" ) else model . d_model ), ) ## PREPARING THE OPTIM all_params = ( list ( model . parameters ()) # + list(batch_cls.parameters()) + ( list ( self . batch_emb . parameters ()) if self . learn_batches_on is not None else [] ) ) # Setup optimizer optimizer = torch . optim . AdamW ( all_params , lr = self . lr , weight_decay = 0.01 , betas = ( 0.9 , 0.999 ), eps = 1e-8 , ) # Setup scheduler scheduler = torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = \"min\" , factor = 0.5 , patience = 2 ) # Setup automatic mixed precision scaler = torch . cuda . amp . GradScaler () if torch . cuda . is_available () else None for k , i in model . mat_labels_hierarchy . items (): model . mat_labels_hierarchy [ k ] = i . to ( model . device ) ## train for epoch in range ( self . num_epochs ): print ( f \" \\n Epoch { epoch + 1 } / { self . num_epochs } \" ) print ( f \"Current learning rate: { optimizer . param_groups [ 0 ][ 'lr' ] : .2e } \" ) # Training phase train_loss = 0.0 train_steps = 0 avg_expr = 0 avg_cls = 0 avg_mmd = 0 pbar = tqdm ( train_loader , desc = \"Training\" ) model . train () for batch_idx , batch in enumerate ( pbar ): optimizer . zero_grad () total_loss , cls_loss , mmd , loss_expr = self . batch_corr_pass ( batch , model ) # Backward pass scaler . scale ( total_loss ) . backward () scaler . unscale_ ( optimizer ) torch . nn . utils . clip_grad_norm_ ( model . parameters (), max_norm = 1.0 ) scaler . step ( optimizer ) scaler . update () train_loss += total_loss . item () train_steps += 1 avg_cls += cls_loss . item () avg_expr += loss_expr . item () avg_mmd += mmd # Update progress bar pbar . set_postfix ( { \"loss\" : f \" { total_loss . item () : .4f } \" , \"avg_loss\" : f \" { train_loss / train_steps : .4f } \" , \"cls_loss\" : f \" { cls_loss . item () : .4f } \" , \"mmd_loss\" : f \" { mmd : .4f } \" , \"expr_loss\" : f \" { loss_expr . item () : .4f } \" , } ) # Validation phase if val_data is not None : model . eval () val_loss = 0.0 val_steps = 0 val_loss_expr = 0.0 val_mmd = 0.0 val_cls = 0.0 val_loss_to_prt = 0.0 with torch . no_grad (): for batch in val_loader : # tqdm(val_loader, desc=\"Validation\"): loss_val , cls_loss , mmd , loss_expr = self . batch_corr_pass ( batch , model ) val_loss_to_prt += loss_val . item () val_loss += loss_val . item () val_steps += 1 val_loss_expr += loss_expr . item () val_mmd += mmd val_cls += cls_loss . item () try : avg_val_loss = val_loss_to_prt / val_steps avg_train_loss = train_loss / train_steps except ZeroDivisionError : print ( \"Error: Division by zero occurred while calculating average losses.\" ) avg_train_loss = 0 print ( \"cls_loss: {:.4f} , mmd_loss: {:.4f} , expr_loss: {:.4f} \" . format ( val_cls / val_steps , val_mmd / val_steps , val_loss_expr / val_steps , ) ) print ( f \"Train Loss: { avg_train_loss : .4f } , Val Loss: { avg_val_loss : .4f } \" ) # Store LR before scheduler step for comparison lr_before = optimizer . param_groups [ 0 ][ \"lr\" ] # Update learning rate scheduler . step ( avg_val_loss ) # Check if LR was reduced lr_after = optimizer . param_groups [ 0 ][ \"lr\" ] if lr_after < lr_before : print ( f \"\ud83d\udd3b Learning rate reduced from { lr_before : .2e } to { lr_after : .2e } (factor: { lr_after / lr_before : .3f } )\" ) else : print ( f \"\u2705 Learning rate unchanged: { lr_after : .2e } \" ) # Early stopping check (simple implementation) if epoch > 3 and val_loss / val_steps > 1.3 * avg_train_loss : print ( \"Early stopping due to overfitting\" ) break print ( \"Manual fine-tuning completed!\" ) model . eval () return model","title":"__call__"},{"location":"tasks/#scprint2.tasks.finetune.mmd_loss","text":"Compute Maximum Mean Discrepancy (MMD) loss between two 2D embedding matrices. Parameters: X ( Tensor ) \u2013 Tensor of shape (n1, emb_dim) - first set of embeddings Y ( Tensor ) \u2013 Tensor of shape (n2, emb_dim) - second set of embeddings Returns: Tensor \u2013 torch.Tensor: MMD loss value (negative to encourage dissimilarity) Source code in scprint2/tasks/finetune.py 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 def mmd_loss ( X : torch . Tensor , Y : torch . Tensor ) -> torch . Tensor : \"\"\" Compute Maximum Mean Discrepancy (MMD) loss between two 2D embedding matrices. Args: X (torch.Tensor): Tensor of shape (n1, emb_dim) - first set of embeddings Y (torch.Tensor): Tensor of shape (n2, emb_dim) - second set of embeddings Returns: torch.Tensor: MMD loss value (negative to encourage dissimilarity) \"\"\" def rbf_kernel ( x , y , sigma ): \"\"\"Compute RBF kernel between two sets of vectors\"\"\" distance = torch . cdist ( x , y , p = 2 ) ** 2 return torch . exp ( - distance / ( 2 * sigma ** 2 )) def energy_kernel ( x , y ): \"\"\"Compute Energy kernel between two sets of vectors\"\"\" distance = torch . cdist ( x , y , p = 2 ) return - distance # Use multiple kernel bandwidths for better performance sigmas = [ 0 ] # [0.1, 1.0, 10.0] mmd_loss = 0.0 for sigma in sigmas : # K(X, X) - kernel matrix within first group (n1 x n1) # k_xx = rbf_kernel(X, X, sigma) k_xx = energy_kernel ( X , X ) # K(Y, Y) - kernel matrix within second group (n2 x n2) # k_yy = rbf_kernel(Y, Y, sigma) k_yy = energy_kernel ( Y , Y ) # K(X, Y) - kernel matrix between groups (n1 x n2) # k_xy = rbf_kernel(X, Y, sigma) k_xy = energy_kernel ( X , Y ) # Unbiased MMD estimation n1 = X . shape [ 0 ] n2 = Y . shape [ 0 ] # Remove diagonal elements for unbiased estimation of K(X,X) and K(Y,Y) # For K(X,X): exclude diagonal if n1 > 1 : mask_xx = 1 - torch . eye ( n1 , device = X . device ) k_xx_term = ( k_xx * mask_xx ) . sum () / ( n1 * ( n1 - 1 )) else : k_xx_term = 0.0 # For K(Y,Y): exclude diagonal if n2 > 1 : mask_yy = 1 - torch . eye ( n2 , device = Y . device ) k_yy_term = ( k_yy * mask_yy ) . sum () / ( n2 * ( n2 - 1 )) else : k_yy_term = 0.0 # For K(X,Y): use all elements (no diagonal to exclude) k_xy_term = k_xy . mean () # MMD^2 = E[K(X,X)] + E[K(Y,Y)] - 2*E[K(X,Y)] mmd_squared = k_xx_term + k_yy_term - 2 * k_xy_term mmd_loss += mmd_squared # Return negative MMD to encourage dissimilarity (higher MMD = more different) return mmd_loss / len ( sigmas )","title":"mmd_loss"},{"location":"tokenizers/","text":"Documentation for the tokenizers module scprint2.tokenizers.protein_embedder Classes: Name Description ESM2 ESM2 ESM2 a ghost class to call protein LLMs to encode protein sequences. Parameters: config ( str , default: 'esm-extract' ) \u2013 The configuration for the model. Defaults to \"esm-extract\". pretrained_model ( str , default: 'esm2_t33_650M_UR50D' ) \u2013 The pretrained model to be used. Defaults to \"esm2_t33_650M_UR50D\". Methods: Name Description __call__ Call the ESM2 model on the input file. read_results Read multiple .pt files in a folder and convert them into a DataFrame. Source code in scprint2/tokenizers/protein_embedder.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , config : str = \"esm-extract\" , pretrained_model : str = \"esm2_t33_650M_UR50D\" , ): \"\"\" ESM2 a ghost class to call protein LLMs to encode protein sequences. Args: config (str, optional): The configuration for the model. Defaults to \"esm-extract\". pretrained_model (str, optional): The pretrained model to be used. Defaults to \"esm2_t33_650M_UR50D\". \"\"\" self . config = config self . pretrained_model = pretrained_model __call__ Call the ESM2 model on the input file. Parameters: input_file ( str ) \u2013 The input file to be processed. output_folder ( str , default: '/tmp/esm_out/' ) \u2013 The folder where the output will be stored. Defaults to \"/tmp/esm_out/\". cache ( bool , default: True ) \u2013 If True, use cached data if available. Defaults to True. Returns: DataFrame \u2013 pd.DataFrame: The results of the model as a DataFrame. Source code in scprint2/tokenizers/protein_embedder.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __call__ ( self , input_file : str , output_folder : str = \"/tmp/esm_out/\" , cache : bool = True ) -> pd . DataFrame : \"\"\" Call the ESM2 model on the input file. Args: input_file (str): The input file to be processed. output_folder (str, optional): The folder where the output will be stored. Defaults to \"/tmp/esm_out/\". cache (bool, optional): If True, use cached data if available. Defaults to True. Returns: pd.DataFrame: The results of the model as a DataFrame. \"\"\" if not os . path . exists ( output_folder ) or not cache : os . makedirs ( output_folder , exist_ok = True ) print ( \"running ESM2\" ) cmd = ( self . config + \" \" + self . pretrained_model + \" \" + input_file + \" \" + output_folder + \" --include mean\" ) try : run_command ( cmd , shell = True ) except Exception as e : raise RuntimeError ( \"An error occurred while running the esm-extract command: \" + str ( e ) ) return self . read_results ( output_folder ) read_results Read multiple .pt files in a folder and convert them into a DataFrame. Parameters: output_folder ( str ) \u2013 The folder where the .pt files are stored. Returns: DataFrame \u2013 pd.DataFrame: The results of the model as a DataFrame. Source code in scprint2/tokenizers/protein_embedder.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def read_results ( self , output_folder : str ) -> pd . DataFrame : \"\"\" Read multiple .pt files in a folder and convert them into a DataFrame. Args: output_folder (str): The folder where the .pt files are stored. Returns: pd.DataFrame: The results of the model as a DataFrame. \"\"\" files = os . listdir ( output_folder ) files = [ i for i in files if i . endswith ( \".pt\" )] results = [] for file in files : results . append ( load ( output_folder + file )[ \"mean_representations\" ][ 33 ] . numpy () . tolist () ) return pd . DataFrame ( data = results , index = [ file . split ( \".\" )[ 0 ] for file in files ]) scprint2.tokenizers.embedder Functions: Name Description protein_embeddings_generator protein_embeddings_generator embed a set of genes using fasta file and LLMs protein_embeddings_generator protein_embeddings_generator embed a set of genes using fasta file and LLMs Parameters: genedf ( DataFrame , default: None ) \u2013 A DataFrame containing gene information. organism ( str , default: 'homo_sapiens' ) \u2013 The organism to which the genes belong. Defaults to \"homo_sapiens\". cache ( bool , default: True ) \u2013 If True, the function will use cached data if available. Defaults to True. fasta_path ( str , default: '/tmp/data/fasta/' ) \u2013 The path to the directory where the fasta files are stored. Defaults to \"/tmp/data/fasta/\". embedding_size ( int , default: 512 ) \u2013 The size of the embeddings to be generated. Defaults to 512. Returns: pd.DataFrame: Returns a DataFrame containing the protein embeddings. pd.DataFrame: Returns the naming dataframe. Source code in scprint2/tokenizers/embedder.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def protein_embeddings_generator ( genedf : pd . DataFrame = None , organism : str = \"homo_sapiens\" , # mus_musculus, cache : bool = True , fasta_path : str = \"/tmp/data/fasta/\" , embedding_size : int = 512 , embedder : str = \"esm3\" , # or glm2 cuda : bool = True , ): \"\"\" protein_embeddings_generator embed a set of genes using fasta file and LLMs Args: genedf (pd.DataFrame): A DataFrame containing gene information. organism (str, optional): The organism to which the genes belong. Defaults to \"homo_sapiens\". cache (bool, optional): If True, the function will use cached data if available. Defaults to True. fasta_path (str, optional): The path to the directory where the fasta files are stored. Defaults to \"/tmp/data/fasta/\". embedding_size (int, optional): The size of the embeddings to be generated. Defaults to 512. Returns: pd.DataFrame: Returns a DataFrame containing the protein embeddings. pd.DataFrame: Returns the naming dataframe. \"\"\" # given a gene file and organism # load the organism fasta if not already done fasta_path_pep , fasta_path_ncrna = utils . load_fasta_species ( species = organism , output_path = fasta_path , cache = cache ) # subset the fasta fasta_name = fasta_path_pep . split ( \"/\" )[ - 1 ] utils . utils . run_command ([ \"gunzip\" , fasta_path_pep ]) protgenedf = ( genedf [ genedf [ \"biotype\" ] == \"protein_coding\" ] if genedf is not None else None ) found , naming_df = utils . subset_fasta ( protgenedf . index . tolist () if protgenedf is not None else None , subfasta_path = fasta_path + \"subset.fa\" , fasta_path = fasta_path + fasta_name [: - 3 ], drop_unknown_seq = True , ) if embedder == \"esm2\" : prot_embedder = ESM2 () prot_embeddings = prot_embedder ( fasta_path + \"subset.fa\" , output_folder = fasta_path + \"esm_out/\" , cache = cache ) elif embedder == \"esm3\" : from Bio import SeqIO from esm.models.esmc import ESMC from esm.sdk.api import ESMProtein , LogitsConfig prot_embeddings = [] names = [] client = ESMC . from_pretrained ( \"esmc_600m\" ) . to ( \"cuda\" if cuda else \"cpu\" ) conf = LogitsConfig ( sequence = True , return_embeddings = True ) with ( open ( fasta_path + \"subset.fa\" , \"r\" ) as fasta ,): for record in tqdm ( SeqIO . parse ( fasta , \"fasta\" )): protein = ESMProtein ( sequence = str ( record . seq )) protein_tensor = client . encode ( protein ) logits_output = client . logits ( protein_tensor , conf ) prot_embeddings . append ( logits_output . embeddings [ 0 ] . mean ( 0 ) . cpu () . numpy () . tolist () ) names . append ( record . id ) else : raise ValueError ( f \"Embedder { embedder } not supported\" ) # load the data and erase / zip the rest # utils.utils.run_command([\"gzip\", fasta_path + fasta_name[:-3]]) # return the embedding and gene file # TODO: to redebug # do the same for RNA # rnagenedf = genedf[genedf[\"biotype\"] != \"protein_coding\"] # fasta_file = next( # file for file in os.listdir(fasta_path) if file.endswith(\".ncrna.fa.gz\") # ) # utils.utils.run_command([\"gunzip\", fasta_path + fasta_file]) # utils.subset_fasta( # rnagenedf[\"ensembl_gene_id\"].tolist(), # subfasta_path=fasta_path + \"subset.ncrna.fa\", # fasta_path=fasta_path + fasta_file[:-3], # drop_unknown_seq=True, # ) # rna_embedder = RNABERT() # rna_embeddings = rna_embedder(fasta_path + \"subset.ncrna.fa\") ## Check if the sizes of the cembeddings are not the same # utils.utils.run_command([\"gzip\", fasta_path + fasta_file[:-3]]) # m = AdaptiveAvgPool1d ( embedding_size ) prot_embeddings = pd . DataFrame ( data = m ( torch . tensor ( np . array ( prot_embeddings ))), index = names ) # rna_embeddings = pd.DataFrame( # data=m(torch.tensor(rna_embeddings.values)), index=rna_embeddings.index # ) # Concatenate the embeddings return prot_embeddings , naming_df # pd.concat([prot_embeddings, rna_embeddings])","title":"tokenizers"},{"location":"tokenizers/#documentation-for-the-tokenizers-module","text":"","title":"Documentation for the tokenizers module"},{"location":"tokenizers/#scprint2.tokenizers.protein_embedder","text":"Classes: Name Description ESM2","title":"protein_embedder"},{"location":"tokenizers/#scprint2.tokenizers.protein_embedder.ESM2","text":"ESM2 a ghost class to call protein LLMs to encode protein sequences. Parameters: config ( str , default: 'esm-extract' ) \u2013 The configuration for the model. Defaults to \"esm-extract\". pretrained_model ( str , default: 'esm2_t33_650M_UR50D' ) \u2013 The pretrained model to be used. Defaults to \"esm2_t33_650M_UR50D\". Methods: Name Description __call__ Call the ESM2 model on the input file. read_results Read multiple .pt files in a folder and convert them into a DataFrame. Source code in scprint2/tokenizers/protein_embedder.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def __init__ ( self , config : str = \"esm-extract\" , pretrained_model : str = \"esm2_t33_650M_UR50D\" , ): \"\"\" ESM2 a ghost class to call protein LLMs to encode protein sequences. Args: config (str, optional): The configuration for the model. Defaults to \"esm-extract\". pretrained_model (str, optional): The pretrained model to be used. Defaults to \"esm2_t33_650M_UR50D\". \"\"\" self . config = config self . pretrained_model = pretrained_model","title":"ESM2"},{"location":"tokenizers/#scprint2.tokenizers.protein_embedder.ESM2.__call__","text":"Call the ESM2 model on the input file. Parameters: input_file ( str ) \u2013 The input file to be processed. output_folder ( str , default: '/tmp/esm_out/' ) \u2013 The folder where the output will be stored. Defaults to \"/tmp/esm_out/\". cache ( bool , default: True ) \u2013 If True, use cached data if available. Defaults to True. Returns: DataFrame \u2013 pd.DataFrame: The results of the model as a DataFrame. Source code in scprint2/tokenizers/protein_embedder.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __call__ ( self , input_file : str , output_folder : str = \"/tmp/esm_out/\" , cache : bool = True ) -> pd . DataFrame : \"\"\" Call the ESM2 model on the input file. Args: input_file (str): The input file to be processed. output_folder (str, optional): The folder where the output will be stored. Defaults to \"/tmp/esm_out/\". cache (bool, optional): If True, use cached data if available. Defaults to True. Returns: pd.DataFrame: The results of the model as a DataFrame. \"\"\" if not os . path . exists ( output_folder ) or not cache : os . makedirs ( output_folder , exist_ok = True ) print ( \"running ESM2\" ) cmd = ( self . config + \" \" + self . pretrained_model + \" \" + input_file + \" \" + output_folder + \" --include mean\" ) try : run_command ( cmd , shell = True ) except Exception as e : raise RuntimeError ( \"An error occurred while running the esm-extract command: \" + str ( e ) ) return self . read_results ( output_folder )","title":"__call__"},{"location":"tokenizers/#scprint2.tokenizers.protein_embedder.ESM2.read_results","text":"Read multiple .pt files in a folder and convert them into a DataFrame. Parameters: output_folder ( str ) \u2013 The folder where the .pt files are stored. Returns: DataFrame \u2013 pd.DataFrame: The results of the model as a DataFrame. Source code in scprint2/tokenizers/protein_embedder.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def read_results ( self , output_folder : str ) -> pd . DataFrame : \"\"\" Read multiple .pt files in a folder and convert them into a DataFrame. Args: output_folder (str): The folder where the .pt files are stored. Returns: pd.DataFrame: The results of the model as a DataFrame. \"\"\" files = os . listdir ( output_folder ) files = [ i for i in files if i . endswith ( \".pt\" )] results = [] for file in files : results . append ( load ( output_folder + file )[ \"mean_representations\" ][ 33 ] . numpy () . tolist () ) return pd . DataFrame ( data = results , index = [ file . split ( \".\" )[ 0 ] for file in files ])","title":"read_results"},{"location":"tokenizers/#scprint2.tokenizers.embedder","text":"Functions: Name Description protein_embeddings_generator protein_embeddings_generator embed a set of genes using fasta file and LLMs","title":"embedder"},{"location":"tokenizers/#scprint2.tokenizers.embedder.protein_embeddings_generator","text":"protein_embeddings_generator embed a set of genes using fasta file and LLMs Parameters: genedf ( DataFrame , default: None ) \u2013 A DataFrame containing gene information. organism ( str , default: 'homo_sapiens' ) \u2013 The organism to which the genes belong. Defaults to \"homo_sapiens\". cache ( bool , default: True ) \u2013 If True, the function will use cached data if available. Defaults to True. fasta_path ( str , default: '/tmp/data/fasta/' ) \u2013 The path to the directory where the fasta files are stored. Defaults to \"/tmp/data/fasta/\". embedding_size ( int , default: 512 ) \u2013 The size of the embeddings to be generated. Defaults to 512. Returns: pd.DataFrame: Returns a DataFrame containing the protein embeddings. pd.DataFrame: Returns the naming dataframe. Source code in scprint2/tokenizers/embedder.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def protein_embeddings_generator ( genedf : pd . DataFrame = None , organism : str = \"homo_sapiens\" , # mus_musculus, cache : bool = True , fasta_path : str = \"/tmp/data/fasta/\" , embedding_size : int = 512 , embedder : str = \"esm3\" , # or glm2 cuda : bool = True , ): \"\"\" protein_embeddings_generator embed a set of genes using fasta file and LLMs Args: genedf (pd.DataFrame): A DataFrame containing gene information. organism (str, optional): The organism to which the genes belong. Defaults to \"homo_sapiens\". cache (bool, optional): If True, the function will use cached data if available. Defaults to True. fasta_path (str, optional): The path to the directory where the fasta files are stored. Defaults to \"/tmp/data/fasta/\". embedding_size (int, optional): The size of the embeddings to be generated. Defaults to 512. Returns: pd.DataFrame: Returns a DataFrame containing the protein embeddings. pd.DataFrame: Returns the naming dataframe. \"\"\" # given a gene file and organism # load the organism fasta if not already done fasta_path_pep , fasta_path_ncrna = utils . load_fasta_species ( species = organism , output_path = fasta_path , cache = cache ) # subset the fasta fasta_name = fasta_path_pep . split ( \"/\" )[ - 1 ] utils . utils . run_command ([ \"gunzip\" , fasta_path_pep ]) protgenedf = ( genedf [ genedf [ \"biotype\" ] == \"protein_coding\" ] if genedf is not None else None ) found , naming_df = utils . subset_fasta ( protgenedf . index . tolist () if protgenedf is not None else None , subfasta_path = fasta_path + \"subset.fa\" , fasta_path = fasta_path + fasta_name [: - 3 ], drop_unknown_seq = True , ) if embedder == \"esm2\" : prot_embedder = ESM2 () prot_embeddings = prot_embedder ( fasta_path + \"subset.fa\" , output_folder = fasta_path + \"esm_out/\" , cache = cache ) elif embedder == \"esm3\" : from Bio import SeqIO from esm.models.esmc import ESMC from esm.sdk.api import ESMProtein , LogitsConfig prot_embeddings = [] names = [] client = ESMC . from_pretrained ( \"esmc_600m\" ) . to ( \"cuda\" if cuda else \"cpu\" ) conf = LogitsConfig ( sequence = True , return_embeddings = True ) with ( open ( fasta_path + \"subset.fa\" , \"r\" ) as fasta ,): for record in tqdm ( SeqIO . parse ( fasta , \"fasta\" )): protein = ESMProtein ( sequence = str ( record . seq )) protein_tensor = client . encode ( protein ) logits_output = client . logits ( protein_tensor , conf ) prot_embeddings . append ( logits_output . embeddings [ 0 ] . mean ( 0 ) . cpu () . numpy () . tolist () ) names . append ( record . id ) else : raise ValueError ( f \"Embedder { embedder } not supported\" ) # load the data and erase / zip the rest # utils.utils.run_command([\"gzip\", fasta_path + fasta_name[:-3]]) # return the embedding and gene file # TODO: to redebug # do the same for RNA # rnagenedf = genedf[genedf[\"biotype\"] != \"protein_coding\"] # fasta_file = next( # file for file in os.listdir(fasta_path) if file.endswith(\".ncrna.fa.gz\") # ) # utils.utils.run_command([\"gunzip\", fasta_path + fasta_file]) # utils.subset_fasta( # rnagenedf[\"ensembl_gene_id\"].tolist(), # subfasta_path=fasta_path + \"subset.ncrna.fa\", # fasta_path=fasta_path + fasta_file[:-3], # drop_unknown_seq=True, # ) # rna_embedder = RNABERT() # rna_embeddings = rna_embedder(fasta_path + \"subset.ncrna.fa\") ## Check if the sizes of the cembeddings are not the same # utils.utils.run_command([\"gzip\", fasta_path + fasta_file[:-3]]) # m = AdaptiveAvgPool1d ( embedding_size ) prot_embeddings = pd . DataFrame ( data = m ( torch . tensor ( np . array ( prot_embeddings ))), index = names ) # rna_embeddings = pd.DataFrame( # data=m(torch.tensor(rna_embeddings.values)), index=rna_embeddings.index # ) # Concatenate the embeddings return prot_embeddings , naming_df # pd.concat([prot_embeddings, rna_embeddings])","title":"protein_embeddings_generator"},{"location":"utils/","text":"Documentation for the utils modules scprint2.utils.sinkhorn Classes: Name Description SinkhornDistance SinkhornDistance Bases: Module SinkhornDistance Initialize the SinkhornDistance class Parameters: eps ( float , default: 0.01 ) \u2013 Regularization parameter. Defaults to 1e-2. max_iter ( int , default: 100 ) \u2013 Maximum number of Sinkhorn iterations. Defaults to 100. reduction ( str , default: 'none' ) \u2013 Specifies the reduction to apply to the output. Defaults to \"none\". Methods: Name Description M Modified cost for logarithmic updates ave Barycenter subroutine, used by kinetic acceleration through extrapolation. forward forward Compute the Sinkhorn distance between two measures with cost matrix c Source code in scprint2/utils/sinkhorn.py 8 9 10 11 12 13 14 15 16 17 18 19 20 def __init__ ( self , eps : float = 1e-2 , max_iter : int = 100 , reduction : str = \"none\" ): \"\"\" SinkhornDistance Initialize the SinkhornDistance class Args: eps (float, optional): Regularization parameter. Defaults to 1e-2. max_iter (int, optional): Maximum number of Sinkhorn iterations. Defaults to 100. reduction (str, optional): Specifies the reduction to apply to the output. Defaults to \"none\". \"\"\" super ( SinkhornDistance , self ) . __init__ () self . eps = eps self . max_iter = max_iter self . reduction = reduction M Modified cost for logarithmic updates Source code in scprint2/utils/sinkhorn.py 99 100 101 102 def M ( self , C , u , v ): \"Modified cost for logarithmic updates\" \"\"\"$M_{ij} = (-c_{ij} + u_i + v_j) / epsilon$\"\"\" return ( - C + u . unsqueeze ( - 1 ) + v . unsqueeze ( 1 )) / self . eps ave staticmethod Barycenter subroutine, used by kinetic acceleration through extrapolation. Source code in scprint2/utils/sinkhorn.py 104 105 106 107 @staticmethod def ave ( u , u1 , tau ): \"Barycenter subroutine, used by kinetic acceleration through extrapolation.\" return tau * u + ( 1 - tau ) * u1 forward forward Compute the Sinkhorn distance between two measures with cost matrix c Parameters: c ( Tensor ) \u2013 The cost matrix between the two measures. Returns: Tensor \u2013 torch.Tensor: The computed Sinkhorn distance. Source code in scprint2/utils/sinkhorn.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def forward ( self , c : torch . Tensor ) -> torch . Tensor : \"\"\" forward Compute the Sinkhorn distance between two measures with cost matrix c Args: c (torch.Tensor): The cost matrix between the two measures. Returns: torch.Tensor: The computed Sinkhorn distance. \"\"\" C = - c x_points = C . shape [ - 2 ] batch_size = C . shape [ 0 ] # both marginals are fixed with equal weights mu = ( torch . empty ( batch_size , x_points , dtype = C . dtype , requires_grad = False , device = C . device , ) . fill_ ( 1.0 / x_points ) . squeeze () ) nu = ( torch . empty ( batch_size , x_points , dtype = C . dtype , requires_grad = False , device = C . device , ) . fill_ ( 1.0 / x_points ) . squeeze () ) u = torch . zeros_like ( mu ) v = torch . zeros_like ( nu ) # Stopping criterion thresh = 1e-12 # Sinkhorn iterations for i in range ( self . max_iter ): if i % 2 == 0 : u1 = u # useful to check the update u = ( self . eps * ( torch . log ( mu ) - torch . logsumexp ( self . M ( C , u , v ), dim =- 1 )) + u ) err = ( u - u1 ) . abs () . sum ( - 1 ) . mean () else : v = ( self . eps * ( torch . log ( nu ) - torch . logsumexp ( self . M ( C , u , v ) . transpose ( - 2 , - 1 ), dim =- 1 ) ) + v ) v = v . detach () . requires_grad_ ( False ) v [ v > 9 * 1e8 ] = 0.0 v = v . detach () . requires_grad_ ( True ) if err . item () < thresh : break U , V = u , v # Transport plan pi = diag(a)*K*diag(b) pi = torch . exp ( self . M ( C , U , V )) # Sinkhorn distance return pi , C , U , V scprint2.utils.utils Functions: Name Description add_points parts of volcano plot category_str2int category_str2int converts a list of category strings to a list of category integers. correlationMatrix Make an interactive correlation matrix from an array using bokeh createFoldersFor will recursively create folders if needed until having all the folders required to save the file in this filepath fileToList loads an input file with a\\n b\\n.. into a list [a,b,..] get_free_gpu get_free_gpu finds the GPU with the most free memory using nvidia-smi. get_git_commit get_git_commit gets the current git commit hash. heatmap Make an interactive heatmap from a dataframe using bokeh inf_loop wrapper function for endless data loader. isnotebook check whether excuting in jupyter notebook. listToFile listToFile loads a list with [a,b,..] into an input file a\\n b\\n.. prepare_device setup GPU device if available. get gpu device indices which are used for DataParallel run_command run_command runs a command in the shell and prints the output. selector Part of Volcano plot: A function to separate tfs from everything else set_seed set random seed. subset_h5ad_by_format Create new anndata object according to slot info specifications. volcano Make an interactive volcano plot from Differential Expression analysis tools outputs add_points parts of volcano plot Source code in scprint2/utils/utils.py 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 def add_points ( p , df1 , x , y , color = \"blue\" , alpha = 0.2 , outline = False , maxvalue = 100 ): \"\"\"parts of volcano plot\"\"\" # Define colors in a dictionary to access them with # the key from the pandas groupby funciton. df = df1 . copy () transformed_q = - df [ y ] . apply ( np . log10 ) . values transformed_q [ transformed_q == np . inf ] = maxvalue transformed_q [ transformed_q > maxvalue ] = maxvalue df [ \"transformed_q\" ] = transformed_q df [ \"color\" ] = color df [ \"alpha\" ] = alpha df [ \"size\" ] = 7 source1 = ColumnDataSource ( df ) # Specify data source p . scatter ( x = x , y = \"transformed_q\" , size = \"size\" , alpha = \"alpha\" , source = source1 , color = \"color\" , name = \"circles\" , ) if outline : p . scatter ( x = x , y = \"transformed_q\" , size = 7 , alpha = 1 , source = source1 , color = \"black\" , fill_color = None , name = \"outlines\" , ) # prettify p . background_fill_color = \"#DFDFE5\" p . background_fill_alpha = 0.5 return p , source1 category_str2int category_str2int converts a list of category strings to a list of category integers. Parameters: category_strs ( List [ str ] ) \u2013 A list of category strings to be converted. Returns: List [ int ] \u2013 List[int]: A list of integers corresponding to the input category strings. Source code in scprint2/utils/utils.py 105 106 107 108 109 110 111 112 113 114 115 116 117 def category_str2int ( category_strs : List [ str ]) -> List [ int ]: \"\"\" category_str2int converts a list of category strings to a list of category integers. Args: category_strs (List[str]): A list of category strings to be converted. Returns: List[int]: A list of integers corresponding to the input category strings. \"\"\" set_category_strs = set ( category_strs ) name2id = { name : i for i , name in enumerate ( set_category_strs )} return [ name2id [ name ] for name in category_strs ] correlationMatrix Make an interactive correlation matrix from an array using bokeh data: arrayLike of int / float/ bool of size(names val) or (names names) names: list[str] of names for each rows colors: list[int] of size(names) a color for each names (good to display clusters) pvals: arraylike of int / float/ bool of size(names val) or (names names) with the corresponding pvalues maxokpval: float threshold when pvalue is considered good. otherwise lowers the size of the square until 10 -3 when it disappears other: arrayLike of int / float/ bool of size(names val) or (names names), an additional information matrix that you want ot display with opacity whereas correlations willl be displayed with title: str the plot title dataIsCorr: bool if not true, we will compute the corrcoef of the data array invert: bool whether or not to invert the matrix before running corrcoef size: int the plot size folder: str of folder location where to save the plot, won't save if empty interactive: bool whether or not to make the plot interactive (else will use matplotlib) maxval: float clamping coloring up to maxval minval: float clamping coloring down to minval the bokeh object if interactive else None Source code in scprint2/utils/utils.py 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 def correlationMatrix ( data , names , colors = None , pvals = None , maxokpval = 10 **- 9 , other = None , title = \"correlation Matrix\" , dataIsCorr = False , invert = False , size = 40 , folder = \"\" , interactive = False , maxval = None , minval = None , ): \"\"\" Make an interactive correlation matrix from an array using bokeh Args: ----- data: arrayLike of int / float/ bool of size(names*val) or (names*names) names: list[str] of names for each rows colors: list[int] of size(names) a color for each names (good to display clusters) pvals: arraylike of int / float/ bool of size(names*val) or (names*names) with the corresponding pvalues maxokpval: float threshold when pvalue is considered good. otherwise lowers the size of the square until 10**-3 when it disappears other: arrayLike of int / float/ bool of size(names*val) or (names*names), an additional information matrix that you want ot display with opacity whereas correlations willl be displayed with title: str the plot title dataIsCorr: bool if not true, we will compute the corrcoef of the data array invert: bool whether or not to invert the matrix before running corrcoef size: int the plot size folder: str of folder location where to save the plot, won't save if empty interactive: bool whether or not to make the plot interactive (else will use matplotlib) maxval: float clamping coloring up to maxval minval: float clamping coloring down to minval Returns: ------- the bokeh object if interactive else None \"\"\" if not dataIsCorr : print ( \"computing correlations\" ) data = np . corrcoef ( np . array ( data ) if not invert else np . array ( data ) . T ) else : data = np . array ( data ) regdata = data . copy () if minval is not None : data [ data < minval ] = minval if maxval is not None : data [ data > maxval ] = maxval data = data / data . max () TOOLS = ( \"hover,crosshair,pan,wheel_zoom,zoom_in,zoom_out,box_zoom,undo,redo,reset,save\" ) xname = [] yname = [] color = [] alpha = [] height = [] width = [] if type ( colors ) is list : print ( \"we are assuming you want to display clusters with colors\" ) elif other is not None : print ( \"we are assuming you want to display the other of your correlation with opacity\" ) if pvals is not None : print ( \"we are assuming you want to display the pvals of your correlation with size\" ) regpvals = pvals . copy () u = pvals < maxokpval pvals [ ~ u ] = np . log10 ( 1 / pvals [ ~ u ]) pvals = pvals / pvals . max () pvals [ u ] = 1 if interactive : xname = [] yname = [] color = [] for i , name1 in enumerate ( names ): for j , name2 in enumerate ( names ): xname . append ( name1 ) yname . append ( name2 ) if pvals is not None : height . append ( max ( 0.1 , min ( 0.9 , pvals [ i , j ]))) color . append ( cc . coolwarm [ int (( data [ i , j ] * 127 ) + 127 )]) alpha . append ( min ( abs ( data [ i , j ]), 0.9 )) elif other is not None : color . append ( cc . coolwarm [ int (( data [ i , j ] * 127 ) + 127 )]) alpha . append ( max ( min ( other [ i , j ], 0.9 ), 0.1 ) if other [ i , j ] != 0 else 0 ) else : alpha . append ( min ( abs ( data [ i , j ]), 0.9 )) if colors is not None : if type ( colors ) is list : if colors [ i ] == colors [ j ]: color . append ( Category10 [ 10 ][ colors [ i ]]) else : color . append ( \"lightgrey\" ) elif pvals is None and other is None : color . append ( \"grey\" if data [ i , j ] > 0 else Category20 [ 3 ][ 2 ]) print ( regdata . max ()) if pvals is not None : width = height . copy () data = dict ( xname = xname , yname = yname , colors = color , alphas = alpha , data = regdata . ravel (), pvals = regpvals . ravel (), width = width , height = height , ) else : data = dict ( xname = xname , yname = yname , colors = color , alphas = alpha , data = data . ravel () ) tt = [( \"names\" , \"@yname, @xname\" ), ( \"value\" , \"@data\" )] if pvals is not None : tt . append (( \"pvals\" , \"@pvals\" )) p = figure ( title = title if title is not None else \"Correlation Matrix\" , x_axis_location = \"above\" , tools = TOOLS , x_range = list ( reversed ( names )), y_range = names , tooltips = tt , ) p . width = 800 p . height = 800 p . grid . grid_line_color = None p . axis . axis_line_color = None p . axis . major_tick_line_color = None p . axis . major_label_text_font_size = \"5pt\" p . axis . major_label_standoff = 0 p . xaxis . major_label_orientation = np . pi / 3 p . output_backend = \"svg\" p . rect ( \"xname\" , \"yname\" , width = 0.9 if not width else \"width\" , height = 0.9 if not height else \"height\" , source = data , color = \"colors\" , alpha = \"alphas\" , line_color = None , hover_line_color = \"black\" , hover_color = \"colors\" , ) save ( p , folder + title . replace ( \" \" , \"_\" ) + \"_correlation.html\" ) try : p . output_backend = \"svg\" export_svg ( p , filename = folder + title . replace ( \" \" , \"_\" ) + \"_correlation.svg\" ) except ( RuntimeError , Exception ) as e : print ( f \"Could not save SVG: { e } \" ) try : show ( p ) except Exception as e : print ( f \"Could not show plot: { e } \" ) return p # show the plot else : plt . figure ( figsize = ( size , 200 )) plt . title ( \"the correlation matrix\" ) plt . imshow ( data ) plt . savefig ( title + \"_correlation.pdf\" ) plt . show () createFoldersFor will recursively create folders if needed until having all the folders required to save the file in this filepath Source code in scprint2/utils/utils.py 94 95 96 97 98 99 100 101 102 def createFoldersFor ( filepath ): \"\"\" will recursively create folders if needed until having all the folders required to save the file in this filepath \"\"\" prevval = \"\" for val in os . path . expanduser ( filepath ) . split ( \"/\" )[: - 1 ]: prevval += val + \"/\" if not os . path . exists ( prevval ): os . mkdir ( prevval ) fileToList loads an input file with a\\n b\\n.. into a list [a,b,..] Parameters: filename ( str ) \u2013 The path to the file to be loaded. strconv ( callable , default: lambda x: x ) \u2013 Function to convert each line. Defaults to identity function. Returns: list ( list ) \u2013 The list of converted values from the file. Source code in scprint2/utils/utils.py 49 50 51 52 53 54 55 56 57 58 59 60 61 def fileToList ( filename : str , strconv : callable = lambda x : x ) -> list : \"\"\" loads an input file with a\\\\n b\\\\n.. into a list [a,b,..] Args: filename (str): The path to the file to be loaded. strconv (callable): Function to convert each line. Defaults to identity function. Returns: list: The list of converted values from the file. \"\"\" with open ( filename ) as f : return [ strconv ( val [: - 1 ]) for val in f . readlines ()] get_free_gpu get_free_gpu finds the GPU with the most free memory using nvidia-smi. Returns: int ( int ) \u2013 The index of the GPU with the most free memory. Source code in scprint2/utils/utils.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def get_free_gpu () -> int : \"\"\" get_free_gpu finds the GPU with the most free memory using nvidia-smi. Returns: int: The index of the GPU with the most free memory. \"\"\" import subprocess import sys from io import StringIO gpu_stats = subprocess . check_output ( [ \"nvidia-smi\" , \"--format=csv\" , \"--query-gpu=memory.used,memory.free\" , ] ) . decode ( \"utf-8\" ) gpu_df = pd . read_csv ( StringIO ( gpu_stats ), names = [ \"memory.used\" , \"memory.free\" ], skiprows = 1 ) print ( \"GPU usage: \\n {} \" . format ( gpu_df )) gpu_df [ \"memory.free\" ] = gpu_df [ \"memory.free\" ] . map ( lambda x : int ( x . rstrip ( \" [MiB]\" ))) idx = gpu_df [ \"memory.free\" ] . idxmax () print ( \"Find free GPU {} with {} free MiB\" . format ( idx , gpu_df . iloc [ idx ][ \"memory.free\" ]) ) return idx get_git_commit get_git_commit gets the current git commit hash. Returns: str ( str ) \u2013 The current git commit Source code in scprint2/utils/utils.py 165 166 167 168 169 170 171 172 def get_git_commit () -> str : \"\"\" get_git_commit gets the current git commit hash. Returns: str: The current git commit \"\"\" return subprocess . check_output ([ \"git\" , \"rev-parse\" , \"HEAD\" ]) . decode ( \"utf-8\" ) . strip () heatmap Make an interactive heatmap from a dataframe using bokeh data: dataframe of int / float/ bool of size(names1 names2) colors: list[int] of size(names) a color for each names (good to display clusters) pvals: arraylike of int / float/ bool of size(names val) or (names names) with the corresponding pvalues maxokpval: float threshold when pvalue is considered good. otherwise lowers the size of the square until 10 *-3 when it disappears title: str the plot title size: int the plot size folder: str of folder location where to save the plot, won't save if empty interactive: bool whether or not to make the plot interactive (else will use matplotlib) maxval: float clamping coloring up to maxval minval: float clamping coloring down to minval the bokeh object if interactive else None Source code in scprint2/utils/utils.py 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 def heatmap ( data , colors = None , title = \"correlation Matrix\" , size = 40 , other = None , folder = \"\" , interactive = False , pvals = None , maxokpval = 10 **- 9 , maxval = None , minval = None , ): \"\"\" Make an interactive heatmap from a dataframe using bokeh Args: ----- data: dataframe of int / float/ bool of size(names1*names2) colors: list[int] of size(names) a color for each names (good to display clusters) pvals: arraylike of int / float/ bool of size(names*val) or (names*names) with the corresponding pvalues maxokpval: float threshold when pvalue is considered good. otherwise lowers the size of the square until 10**-3 when it disappears title: str the plot title size: int the plot size folder: str of folder location where to save the plot, won't save if empty interactive: bool whether or not to make the plot interactive (else will use matplotlib) maxval: float clamping coloring up to maxval minval: float clamping coloring down to minval Returns: ------- the bokeh object if interactive else None \"\"\" regdata = data . copy () if minval is not None : data [ data < minval ] = minval if maxval is not None : data [ data > maxval ] = maxval data = data / data . max () data = data . values TOOLS = ( \"hover,crosshair,pan,wheel_zoom,zoom_in,zoom_out,box_zoom,undo,redo,reset,save\" ) xname = [] yname = [] color = [] alpha = [] height = [] width = [] if pvals is not None : print ( \"we are assuming you want to display the pvals of your correlation with size\" ) regpvals = pvals . copy () u = pvals < maxokpval pvals [ ~ u ] = np . log10 ( 1 / pvals [ ~ u ]) pvals = pvals / pvals . max () pvals [ u ] = 1 if interactive : xname = [] yname = [] color = [] for i , name1 in enumerate ( regdata . index ): for j , name2 in enumerate ( regdata . columns ): xname . append ( name2 ) yname . append ( name1 ) if pvals is not None : # import pdb;pdb.set_trace() height . append ( max ( 0.1 , min ( 0.9 , pvals . loc [ name1 ][ name2 ]))) color . append ( cc . coolwarm [ int (( data [ i , j ] * 128 ) + 127 )]) alpha . append ( min ( abs ( data [ i , j ]), 0.9 )) elif other is not None : color . append ( cc . coolwarm [ int (( data [ i , j ] * 128 ) + 127 )]) alpha . append ( max ( min ( other [ i , j ], 0.9 ), 0.1 ) if other [ i , j ] != 0 else 0 ) else : alpha . append ( min ( abs ( data [ i , j ]), 0.9 )) if colors is not None : if type ( colors ) is list : if colors [ i ] == colors [ j ]: color . append ( Category10 [ 10 ][ colors [ i ]]) else : color . append ( \"lightgrey\" ) elif pvals is None and other is None : color . append ( \"grey\" if data [ i , j ] > 0 else Category20 [ 3 ][ 2 ]) if pvals is not None : width = height . copy () data = dict ( xname = xname , yname = yname , colors = color , alphas = alpha , data = regdata . values . ravel (), pvals = regpvals . values . ravel (), width = width , height = height , ) else : data = dict ( xname = xname , yname = yname , colors = color , alphas = alpha , data = data . ravel () ) tt = [( \"names\" , \"@yname, @xname\" ), ( \"value\" , \"@data\" )] if pvals is not None : tt . append (( \"pvals\" , \"@pvals\" )) p = figure ( title = title if title is not None else \"Heatmap\" , x_axis_location = \"above\" , tools = TOOLS , x_range = list ( reversed ( regdata . columns . astype ( str ) . tolist ())), y_range = regdata . index . tolist (), tooltips = tt , ) p . width = 800 p . height = 800 p . grid . grid_line_color = None p . axis . axis_line_color = None p . axis . major_tick_line_color = None p . axis . major_label_text_font_size = \"5pt\" p . axis . major_label_standoff = 0 p . xaxis . major_label_orientation = np . pi / 3 p . output_backend = \"svg\" p . rect ( \"xname\" , \"yname\" , width = 0.9 if not width else \"width\" , height = 0.9 if not height else \"height\" , source = data , color = \"colors\" , alpha = \"alphas\" , line_color = None , hover_line_color = \"black\" , hover_color = \"colors\" , ) save ( p , folder + title . replace ( \" \" , \"_\" ) + \"_heatmap.html\" ) try : p . output_backend = \"svg\" export_svg ( p , filename = folder + title . replace ( \" \" , \"_\" ) + \"_correlation.svg\" ) except ( RuntimeError , Exception ) as e : print ( f \"Could not save SVG: { e } \" ) try : show ( p ) except Exception as e : print ( f \"Could not show plot: { e } \" ) return p # show the plot else : plt . figure ( figsize = size ) plt . title ( \"the correlation matrix\" ) plt . imshow ( data ) plt . savefig ( title + \"_correlation.pdf\" ) plt . show () inf_loop wrapper function for endless data loader. Source code in scprint2/utils/utils.py 193 194 195 196 def inf_loop ( data_loader ): \"\"\"wrapper function for endless data loader.\"\"\" for loader in repeat ( data_loader ): yield from loader isnotebook check whether excuting in jupyter notebook. Source code in scprint2/utils/utils.py 120 121 122 123 124 125 126 127 128 129 130 131 def isnotebook () -> bool : \"\"\"check whether excuting in jupyter notebook.\"\"\" try : shell = get_ipython () . __class__ . __name__ if shell == \"ZMQInteractiveShell\" : return True # Jupyter notebook or qtconsole elif shell == \"TerminalInteractiveShell\" : return True # Terminal running IPython else : return False # Other type (?) except NameError : return False # Probably standard Python interpreter listToFile listToFile loads a list with [a,b,..] into an input file a\\n b\\n.. Parameters: li ( list ) \u2013 The list of elements to be written to the file. filename ( str ) \u2013 The name of the file where the list will be written. strconv ( callable , default: lambda x: str ( x ) ) \u2013 A function to convert each element of the list to a string. Defaults to str. Returns: None \u2013 None Source code in scprint2/utils/utils.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def listToFile ( li : List [ str ], filename : str , strconv : callable = lambda x : str ( x ) ) -> None : \"\"\" listToFile loads a list with [a,b,..] into an input file a\\\\n b\\\\n.. Args: li (list): The list of elements to be written to the file. filename (str): The name of the file where the list will be written. strconv (callable, optional): A function to convert each element of the list to a string. Defaults to str. Returns: None \"\"\" with open ( filename , \"w\" ) as f : for item in li : f . write ( \" %s \\n \" % strconv ( item )) prepare_device setup GPU device if available. get gpu device indices which are used for DataParallel Source code in scprint2/utils/utils.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def prepare_device ( n_gpu_use ): \"\"\" setup GPU device if available. get gpu device indices which are used for DataParallel \"\"\" n_gpu = torch . cuda . device_count () if n_gpu_use > 0 and n_gpu == 0 : print ( \"Warning: There's no GPU available on this machine,\" \"training will be performed on CPU.\" ) n_gpu_use = 0 if n_gpu_use > n_gpu : print ( f \"Warning: The number of GPU's configured to use is { n_gpu_use } , but only { n_gpu } are \" \"available on this machine.\" ) n_gpu_use = n_gpu device = torch . device ( \"cuda:0\" if n_gpu_use > 0 else \"cpu\" ) list_ids = list ( range ( n_gpu_use )) return device , list_ids run_command run_command runs a command in the shell and prints the output. Parameters: command ( str ) \u2013 The command to be executed in the shell. Returns: int ( int ) \u2013 The return code of the command executed. Source code in scprint2/utils/utils.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def run_command ( command : str , ** kwargs ) -> int : \"\"\" run_command runs a command in the shell and prints the output. Args: command (str): The command to be executed in the shell. Returns: int: The return code of the command executed. \"\"\" process = subprocess . Popen ( command , stdout = subprocess . PIPE , ** kwargs ) while True : if process . poll () is not None : break output = process . stdout . readline () if output : print ( output . strip ()) rc = process . poll () return rc selector Part of Volcano plot: A function to separate tfs from everything else Source code in scprint2/utils/utils.py 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 def selector ( df , valtoextract = [], logfoldtohighlight = 0.15 , pvaltohighlight = 0.1 , minlogfold = 0.15 , minpval = 0.1 , ): \"\"\"Part of Volcano plot: A function to separate tfs from everything else\"\"\" toshow = ( df . pvalue < minpval ) & ( abs ( df . log2FoldChange ) > minlogfold ) df = df [ toshow ] sig = ( df . pvalue < pvaltohighlight ) & ( abs ( df . log2FoldChange ) > logfoldtohighlight ) if valtoextract : not_tf = ~ df . gene_id . isin ( valtoextract ) is_tf = df . gene_id . isin ( valtoextract ) to_plot_not = df [ ~ sig | not_tf ] to_plot_yes = df [ sig & is_tf ] else : to_plot_not = df [ ~ sig ] to_plot_yes = df [ sig ] return to_plot_not , to_plot_yes set_seed set random seed. Source code in scprint2/utils/utils.py 83 84 85 86 87 88 89 def set_seed ( seed : int = 42 ): \"\"\"set random seed.\"\"\" random . seed ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False subset_h5ad_by_format Create new anndata object according to slot info specifications. Arguments: adata -- An AnnData object to subset (required) config -- A Viash config object as read by openproblems.project.read_viash_config (required) arg_name -- The name of the argument in the config file that specifies the output format (required) field_rename_dict -- A mapping between the slots of the source h5ad and the slots of the destination h5ad. Example of slot_mapping: ``` slot_mapping = { \"layers\": { \"counts\": par[\"layer_counts\"], }, \"obs\": { \"cell_type\": par[\"obs_cell_type\"], \"batch\": par[\"obs_batch\"], } } Source code in scprint2/utils/utils.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def subset_h5ad_by_format ( adata , config , arg_name , field_rename_dict = {}): \"\"\"Create new anndata object according to slot info specifications. Arguments: adata -- An AnnData object to subset (required) config -- A Viash config object as read by openproblems.project.read_viash_config (required) arg_name -- The name of the argument in the config file that specifies the output format (required) field_rename_dict -- A mapping between the slots of the source h5ad and the slots of the destination h5ad. Example of slot_mapping: ``` slot_mapping = { \"layers\": { \"counts\": par[\"layer_counts\"], }, \"obs\": { \"cell_type\": par[\"obs_cell_type\"], \"batch\": par[\"obs_batch\"], } } \"\"\" import anndata as ad import pandas as pd assert isinstance ( adata , ad . AnnData ), \"adata must be an AnnData object\" assert isinstance ( config , dict ), \"config must be a dictionary\" # find argument arg = next ( ( x for x in config [ \"all_arguments\" ] if x [ \"clean_name\" ] == arg_name ), None ) assert arg , f \"Argument ' { arg_name } ' not found in config\" # find file format file_format = ( arg . get ( \"info\" ) or {}) . get ( \"format\" ) assert file_format , f \"Argument ' { arg_name } ' has no .info.format\" # find file format type file_format_type = file_format . get ( \"type\" ) assert file_format_type == \"h5ad\" , \"format must be a h5ad type\" structs = [ \"layers\" , \"obs\" , \"var\" , \"uns\" , \"obsp\" , \"obsm\" , \"varp\" , \"varm\" ] kwargs = {} for struct in structs : struct_format = file_format . get ( struct , {}) struct_rename = field_rename_dict . get ( struct , {}) # fetch data from adata data = {} for field_format in struct_format : dest_name = field_format [ \"name\" ] # where to find the data. if the dest_name is in the rename dict, use the renamed name # as the source name, otherwise use the dest_name as the source name src_name = struct_rename . get ( dest_name , dest_name ) data [ dest_name ] = getattr ( adata , struct )[ src_name ] if len ( data ) > 0 : if struct in [ \"obs\" , \"var\" ]: data = pd . concat ( data , axis = 1 ) kwargs [ struct ] = data elif struct in [ \"obs\" , \"var\" ]: # if no columns need to be copied, we still need an 'obs' and a 'var' # to help determine the shape of the adata kwargs [ struct ] = getattr ( adata , struct ) . iloc [:, []] return ad . AnnData ( ** kwargs ) volcano Make an interactive volcano plot from Differential Expression analysis tools outputs data: a df with rows genes and cols [log2FoldChange, pvalue, gene_id] folder: str of location where to save the plot, won't save if empty tohighlight: list[str] of genes to highlight in the plot tooltips: list[tuples(str,str)] if user wants tot specify another bokeh tooltip title: str plot title xlabel: str if user wants to specify the title of the x axis ylabel: str if user wants tot specify the title of the y axis maxvalue: float the max -log2(pvalue authorized usefull when managing inf vals) searchbox: bool whether or not to add a searchBox to interactively highlight genes logfoldtohighlight: float min logfoldchange when to diplay points pvaltohighlight: float min pvalue when to diplay points showlabels: bool whether or not to show a text above each datapoint with its label information The bokeh object Source code in scprint2/utils/utils.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 def volcano ( data , folder = \"\" , tohighlight = None , tooltips = [( \"gene\" , \"@gene_id\" )], title = \"volcano plot\" , xlabel = \"log-fold change\" , ylabel = \"-log(Q)\" , maxvalue = 100 , searchbox = False , logfoldtohighlight = 0.15 , pvaltohighlight = 0.1 , showlabels = False , ): \"\"\" Make an interactive volcano plot from Differential Expression analysis tools outputs Args: ----- data: a df with rows genes and cols [log2FoldChange, pvalue, gene_id] folder: str of location where to save the plot, won't save if empty tohighlight: list[str] of genes to highlight in the plot tooltips: list[tuples(str,str)] if user wants tot specify another bokeh tooltip title: str plot title xlabel: str if user wants to specify the title of the x axis ylabel: str if user wants tot specify the title of the y axis maxvalue: float the max -log2(pvalue authorized usefull when managing inf vals) searchbox: bool whether or not to add a searchBox to interactively highlight genes logfoldtohighlight: float min logfoldchange when to diplay points pvaltohighlight: float min pvalue when to diplay points showlabels: bool whether or not to show a text above each datapoint with its label information Returns: -------- The bokeh object \"\"\" to_plot_not , to_plot_yes = selector ( data , tohighlight if tohighlight is not None else [], logfoldtohighlight , pvaltohighlight , ) hover = HoverTool ( tooltips = tooltips , name = \"circles\" ) # Create figure p = figure ( title = title , width = 650 , height = 450 ) p . xgrid . grid_line_color = \"white\" p . ygrid . grid_line_color = \"white\" p . xaxis . axis_label = xlabel p . yaxis . axis_label = ylabel # Add the hover tool p . add_tools ( hover ) p , source1 = add_points ( p , to_plot_not , \"log2FoldChange\" , \"pvalue\" , color = \"#1a9641\" , maxvalue = maxvalue ) p , source2 = add_points ( p , to_plot_yes , \"log2FoldChange\" , \"pvalue\" , color = \"#fc8d59\" , alpha = 0.6 , outline = True , maxvalue = maxvalue , ) if showlabels : labels = LabelSet ( x = \"log2FoldChange\" , y = \"transformed_q\" , text_font_size = \"7pt\" , text = \"gene_id\" , level = \"glyph\" , x_offset = 5 , y_offset = 5 , source = source2 , # renderers=\"canvas\", ) p . add_layout ( labels ) if searchbox : text = TextInput ( title = \"text\" , value = \"gene\" ) text . js_on_change ( \"value\" , CustomJS ( args = dict ( source = source1 ), code = \"\"\" var data = source.data var value = cb_obj.value var gene_id = data.gene_id var a = -1 for (let i=0; i < gene_id.length; i++) { if ( gene_id[i]===value ) { a=i; console.log(i); data.size[i]=7; data.alpha[i]=1; data.color[i]='#fc8d59' } } source.data = data console.log(source) console.log(cb_obj) source.change.emit() console.log(source) \"\"\" , ), ) p = column ( text , p ) p . output_backend = \"svg\" if folder : save ( p , folder + title . replace ( \" \" , \"_\" ) + \"_volcano.html\" ) try : p . output_backend = \"svg\" export_svg ( p , filename = folder + title . replace ( \" \" , \"_\" ) + \"_volcano.svg\" ) except ( RuntimeError , Exception ) as e : print ( f \"Could not save SVG: { e } \" ) try : show ( p ) except Exception as e : print ( f \"Could not show plot: { e } \" ) return p scprint2.utils.get_seq Functions: Name Description load_fasta_species Downloads and caches FASTA files for a given species from the Ensembl FTP server. seq Fetch nucleotide or amino acid sequence (FASTA) of a gene (and all its isoforms) or transcript by Ensembl, WormBase, or FlyBase ID. subset_fasta subset_fasta: creates a new fasta file with only the sequence which names contain one of gene_names load_fasta_species Downloads and caches FASTA files for a given species from the Ensembl FTP server. Parameters: species ( str , default: 'homo_sapiens' ) \u2013 The species name for which to download FASTA files. Defaults to \"homo_sapiens\". output_path ( str , default: '/tmp/data/fasta/' ) \u2013 The local directory path where the FASTA files will be saved. Defaults to \"/tmp/data/fasta/\". cache ( bool , default: True ) \u2013 If True, use cached files if they exist. If False, re-download the files. Defaults to True. Source code in scprint2/utils/get_seq.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def load_fasta_species ( species : str = \"homo_sapiens\" , output_path : str = \"/tmp/data/fasta/\" , load : List [ str ] = [ \"pep\" , \"ncrna\" , \"cds\" ], cache : bool = True , ) -> None : \"\"\" Downloads and caches FASTA files for a given species from the Ensembl FTP server. Args: species (str, optional): The species name for which to download FASTA files. Defaults to \"homo_sapiens\". output_path (str, optional): The local directory path where the FASTA files will be saved. Defaults to \"/tmp/data/fasta/\". cache (bool, optional): If True, use cached files if they exist. If False, re-download the files. Defaults to True. \"\"\" ftp = ftplib . FTP ( \"ftp.ensembl.org\" ) ftp . login () local_file_path = [] try : ftp . cwd ( \"/pub/release-110/fasta/\" + species + \"/pep/\" ) types = \"animals\" except ftplib . error_perm : try : ftp = ftplib . FTP ( \"ftp.ensemblgenomes.ebi.ac.uk\" ) ftp . login () ftp . cwd ( \"/pub/plants/release-60/fasta/\" + species + \"/pep/\" ) types = \"plants\" except ftplib . error_perm : try : ftp . cwd ( \"/pub/metazoa/release-60/fasta/\" + species + \"/pep/\" ) types = \"metazoa\" except ftplib . error_perm : raise ValueError ( f \"Species { species } not found in Ensembl or Ensembl Genomes.\" ) os . makedirs ( output_path , exist_ok = True ) if \"pep\" in load : file = list_files ( ftp , \".all.fa.gz\" )[ 0 ] local_file_path . append ( output_path + file ) if not os . path . exists ( local_file_path [ - 1 ]) or not cache : with open ( local_file_path [ - 1 ], \"wb\" ) as local_file : ftp . retrbinary ( \"RETR \" + file , local_file . write ) # ncRNA if \"ncrna\" in load : if types == \"animals\" : ftp . cwd ( \"/pub/release-110/fasta/\" + species + \"/ncrna/\" ) elif types == \"plants\" : ftp . cwd ( \"/pub/plants/release-60/fasta/\" + species + \"/ncrna/\" ) file = list_files ( ftp , \".ncrna.fa.gz\" )[ 0 ] local_file_path . append ( output_path + file ) if not os . path . exists ( local_file_path [ - 1 ]) or not cache : with open ( local_file_path [ - 1 ], \"wb\" ) as local_file : ftp . retrbinary ( \"RETR \" + file , local_file . write ) # CDNA: if \"cdna\" in load : if types == \"animals\" : ftp . cwd ( \"/pub/release-110/fasta/\" + species + \"/cdna/\" ) elif types == \"plants\" : ftp . cwd ( \"/pub/plants/release-60/fasta/\" + species + \"/cdna/\" ) file = list_files ( ftp , \".cdna.all.fa.gz\" )[ 0 ] local_file_path . append ( output_path + file ) if not os . path . exists ( local_file_path [ - 1 ]) or not cache : with open ( local_file_path [ - 1 ], \"wb\" ) as local_file : ftp . retrbinary ( \"RETR \" + file , local_file . write ) ftp . quit () return local_file_path seq Fetch nucleotide or amino acid sequence (FASTA) of a gene (and all its isoforms) or transcript by Ensembl, WormBase, or FlyBase ID. Parameters: ens_ids ( Union [ str , List [ str ]] ) \u2013 One or more Ensembl IDs (passed as string or list of strings). Also supports WormBase and FlyBase IDs. translate ( bool , default: False ) \u2013 Defines whether nucleotide or amino acid sequences are returned. Defaults to False (returns nucleotide sequences). Nucleotide sequences are fetched from the Ensembl REST API server. Amino acid sequences are fetched from the UniProt REST API server. isoforms ( bool , default: False ) \u2013 If True, returns the sequences of all known transcripts. Defaults to False. (Only for gene IDs.) parallel ( bool , default: True ) \u2013 If True, fetches sequences in parallel. Defaults to True. save ( bool , default: False ) \u2013 If True, saves output FASTA to current directory. Defaults to False. transcribe ( bool , default: None ) \u2013 Deprecated. Use 'translate' instead. seqtype ( str , default: None ) \u2013 Deprecated. Use 'translate' instead. verbose ( bool , default: True ) \u2013 If True, prints progress information. Defaults to True. Returns: List [ str ] \u2013 List[str]: A list containing the requested sequences, or a FASTA file if 'save' is True. Raises: ValueError \u2013 If an invalid Ensembl ID is provided. Source code in scprint2/utils/get_seq.py 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 def seq ( ens_ids : Union [ str , List [ str ]], translate : bool = False , isoforms : bool = False , parallel : bool = True , save : bool = False , transcribe : Optional [ bool ] = None , seqtype : Optional [ str ] = None , verbose : bool = True , ) -> List [ str ]: \"\"\" Fetch nucleotide or amino acid sequence (FASTA) of a gene (and all its isoforms) or transcript by Ensembl, WormBase, or FlyBase ID. Args: ens_ids (Union[str, List[str]]): One or more Ensembl IDs (passed as string or list of strings). Also supports WormBase and FlyBase IDs. translate (bool, optional): Defines whether nucleotide or amino acid sequences are returned. Defaults to False (returns nucleotide sequences). Nucleotide sequences are fetched from the Ensembl REST API server. Amino acid sequences are fetched from the UniProt REST API server. isoforms (bool, optional): If True, returns the sequences of all known transcripts. Defaults to False. (Only for gene IDs.) parallel (bool, optional): If True, fetches sequences in parallel. Defaults to True. save (bool, optional): If True, saves output FASTA to current directory. Defaults to False. transcribe (bool, optional): Deprecated. Use 'translate' instead. seqtype (str, optional): Deprecated. Use 'translate' instead. verbose (bool, optional): If True, prints progress information. Defaults to True. Returns: List[str]: A list containing the requested sequences, or a FASTA file if 'save' is True. Raises: ValueError: If an invalid Ensembl ID is provided. \"\"\" # Handle deprecated arguments if seqtype : logging . error ( \"'seqtype' argument deprecated! Please use True/False argument 'translate' instead.\" ) return if transcribe : translate = transcribe ## Clean up arguments # Clean up Ensembl IDs # If single Ensembl ID passed as string, convert to list if type ( ens_ids ) is str : ens_ids = [ ens_ids ] # Remove Ensembl ID version if passed ens_ids_clean = [] temp = 0 for ensembl_ID in ens_ids : # But only for Ensembl ID (and not for flybase/wormbase IDs) if ensembl_ID . startswith ( \"ENS\" ): ens_ids_clean . append ( ensembl_ID . split ( \".\" )[ 0 ]) if \".\" in ensembl_ID and temp == 0 : if verbose : logging . info ( \"We noticed that you may have passed a version number with your Ensembl ID. \\n \" \"Please note that gget seq will return information linked to the latest Ensembl ID version.\" ) temp = + 1 else : ens_ids_clean . append ( ensembl_ID ) # Initiate empty 'fasta' fasta = [] ## Fetch nucleotide sequece if translate is False : # Define Ensembl REST API server server = ENSEMBL_REST_API # Define type of returned content from REST content_type = \"application/json\" # Initiate dictionary to save results for all IDs in master_dict = {} # Query REST APIs from https://rest.ensembl.org/ for ensembl_ID in ens_ids_clean : # Create dict to save query results results_dict = { ensembl_ID : {}} # If isoforms False, just fetch sequences of passed Ensembl ID if isoforms is False : # sequence/id/ query: Request sequence by stable identifier query = \"sequence/id/\" + ensembl_ID + \"?\" # Try if query valid try : # Submit query; this will throw RuntimeError if ID not found df_temp = rest_query ( server , query , content_type ) # Delete superfluous entries keys_to_delete = [ \"query\" , \"id\" , \"version\" , \"molecule\" ] for key in keys_to_delete : # Pop keys, None -> do not raise an error if key to delete not found df_temp . pop ( key , None ) # Add results to main dict results_dict [ ensembl_ID ] . update ({ \"seq\" : df_temp }) if verbose : logging . info ( f \"Requesting nucleotide sequence of { ensembl_ID } from Ensembl.\" ) except RuntimeError : logging . error ( f \"ID { ensembl_ID } not found. Please double-check spelling/arguments and try again.\" ) # If isoforms true, fetch sequences of isoforms instead if isoforms is True : # Get ID type (gene, transcript, ...) using gget info info_df = info ( ensembl_ID , verbose = False , pdb = False , ncbi = False , uniprot = False ) # Check if Ensembl ID was found if isinstance ( info_df , type ( None )): logging . warning ( f \"ID ' { ensembl_ID } ' not found. Please double-check spelling/arguments and try again.\" ) continue ens_ID_type = info_df . loc [ ensembl_ID ][ \"object_type\" ] # If the ID is a gene, get the IDs of all its transcripts if ens_ID_type == \"Gene\" : if verbose : logging . info ( f \"Requesting nucleotide sequences of all transcripts of { ensembl_ID } from Ensembl.\" ) for transcipt_id in info_df . loc [ ensembl_ID ][ \"all_transcripts\" ]: # Remove version number for Ensembl IDs (not for flybase/wormbase IDs) if transcipt_id . startswith ( \"ENS\" ): transcipt_id = transcipt_id . split ( \".\" )[ 0 ] # Try if query is valid try : # Define the REST query query = \"sequence/id/\" + transcipt_id + \"?\" # Submit query df_temp = rest_query ( server , query , content_type ) # Delete superfluous entries keys_to_delete = [ \"query\" , \"version\" , \"molecule\" ] for key in keys_to_delete : # Pop keys, None -> do not raise an error if key to delete not found df_temp . pop ( key , None ) # Add results to main dict results_dict [ ensembl_ID ] . update ( { f \" { transcipt_id } \" : df_temp } ) except RuntimeError : logging . error ( f \"ID { transcipt_id } not found. \" \"Please double-check spelling/arguments and try again.\" ) # If isoform true, but ID is not a gene; ignore the isoform parameter else : # Try if query is valid try : # Define the REST query query = \"sequence/id/\" + ensembl_ID + \"?\" # Submit query df_temp = rest_query ( server , query , content_type ) # Delete superfluous entries keys_to_delete = [ \"query\" , \"id\" , \"version\" , \"molecule\" ] for key in keys_to_delete : # Pop keys, None -> do not raise an error if key to delete not found df_temp . pop ( key , None ) # Add results to main dict results_dict [ ensembl_ID ] . update ({ \"seq\" : df_temp }) logging . info ( f \"Requesting nucleotide sequence of { ensembl_ID } from Ensembl.\" ) logging . warning ( \"The isoform option only applies to gene IDs.\" ) except RuntimeError : logging . error ( f \"ID { ensembl_ID } not found. \" \"Please double-check spelling/arguments and try again.\" ) # Add results to master dict master_dict . update ( results_dict ) # Build FASTA file for ens_ID in master_dict : for key in master_dict [ ens_ID ] . keys (): if key == \"seq\" : fasta . append ( \">\" + ens_ID + \" \" + master_dict [ ens_ID ][ key ][ \"desc\" ]) fasta . append ( master_dict [ ens_ID ][ key ][ \"seq\" ]) else : fasta . append ( \">\" + master_dict [ ens_ID ][ key ][ \"id\" ] + \" \" + master_dict [ ens_ID ][ key ][ \"desc\" ] ) fasta . append ( master_dict [ ens_ID ][ key ][ \"seq\" ]) ## Fetch amino acid sequences from UniProt if translate is True : if isoforms is False : # List to collect transcript IDs trans_ids = [] # Get ID type (gene, transcript, ...) using gget info info_df = info ( ens_ids_clean , verbose = False , pdb = False , ncbi = False , uniprot = False ) # Check that Ensembl ID was found missing = set ( ens_ids_clean ) - set ( info_df . index . values ) if len ( missing ) > 0 : logging . warning ( f \" { str ( missing ) } IDs not found. Please double-check spelling/arguments.\" ) ens_ID_type = info_df . loc [ ens_ids_clean [ 0 ]][ \"object_type\" ] # If the ID is a gene, use the ID of its canonical transcript if ens_ID_type == \"Gene\" : # Get ID of canonical transcript for ensembl_ID in info_df . index . values : can_trans = info_df . loc [ ensembl_ID ][ \"canonical_transcript\" ] if ensembl_ID . startswith ( \"ENS\" ): # Remove Ensembl ID version from transcript IDs and append to transcript IDs list temp_trans_id = can_trans . split ( \".\" )[ 0 ] trans_ids . append ( temp_trans_id ) elif ensembl_ID . startswith ( \"WB\" ): # Remove added \".\" at the end of transcript IDs temp_trans_id = \".\" . join ( can_trans . split ( \".\" )[: - 1 ]) # # For WormBase transcript IDs, also remove the version number for submission to UniProt API # temp_trans_id = \".\".join(temp_trans_id1.split(\".\")[:-1]) trans_ids . append ( temp_trans_id ) else : # Remove added \".\" at the end of other transcript IDs temp_trans_id = \".\" . join ( can_trans . split ( \".\" )[: - 1 ]) trans_ids . append ( temp_trans_id ) if verbose : logging . info ( f \"Requesting amino acid sequence of the canonical transcript { temp_trans_id } of gene { ensembl_ID } from UniProt.\" ) # If the ID is a transcript, append the ID directly elif ens_ID_type == \"Transcript\" : # # For WormBase transcript IDs, remove the version number for submission to UniProt API # if ensembl_ID.startswith(\"T\"): # trans_ids.append(\".\".join(ensembl_ID.split(\".\")[:-1])) # else: trans_ids = ens_ids_clean if verbose : logging . info ( f \"Requesting amino acid sequence of { trans_ids } from UniProt.\" ) else : logging . warning ( \"ensembl_IDs not recognized as either a gene or transcript ID. It will not be included in the UniProt query.\" ) # Fetch the amino acid sequences of the transcript Ensembl IDs df_uniprot = get_uniprot_seqs ( UNIPROT_REST_API , trans_ids ) # Add info_df.loc[ensembl_ID] to df_uniprot by joining on \"canonical_transcript\" / \"gene_name\" respectively info_df . set_index ( \"canonical_transcript\" , inplace = True ) df_uniprot . loc [:, \"gene_id\" ] = info_df . loc [ df_uniprot [ \"query\" ], \"gene_name\" ] . values if isoforms is True : # List to collect transcript IDs trans_ids = [] for ensembl_ID in ens_ids_clean : # Get ID type (gene, transcript, ...) using gget info info_df = info ( ensembl_ID , verbose = False , pdb = False , ncbi = False , uniprot = False ) # Check that Ensembl ID was found if isinstance ( info_df , type ( None )): logging . warning ( f \"ID ' { ensembl_ID } ' not found. Please double-check spelling/arguments.\" ) continue ens_ID_type = info_df . loc [ ensembl_ID ][ \"object_type\" ] # If the ID is a gene, get the IDs of all isoforms if ens_ID_type == \"Gene\" : # Get the IDs of all transcripts from the gget info results for transcipt_id in info_df . loc [ ensembl_ID ][ \"all_transcripts\" ]: if ensembl_ID . startswith ( \"ENS\" ): # Append transcript ID (without Ensembl version number) to list of transcripts to fetch trans_ids . append ( transcipt_id . split ( \".\" )[ 0 ]) # elif ensembl_ID.startswith(\"WB\"): # # For WormBase transcript IDs, remove the version number for submission to UniProt API # temp_trans_id = \".\".join(transcipt_id.split(\".\")[:-1]) # trans_ids.append(temp_trans_id) else : # Note: No need to remove the added \".\" at the end of unversioned transcripts here, because \"all_transcripts\" are returned without it trans_ids . append ( transcipt_id ) if verbose : logging . info ( f \"Requesting amino acid sequences of all transcripts of gene { ensembl_ID } from UniProt.\" ) elif ens_ID_type == \"Transcript\" : # # For WormBase transcript IDs, remove the version number for submission to UniProt API # if ensembl_ID.startswith(\"T\"): # trans_ids.append(\".\".join(ensembl_ID.split(\".\")[:-1])) # else: trans_ids . append ( ensembl_ID ) if verbose : logging . info ( f \"Requesting amino acid sequence of { ensembl_ID } from UniProt.\" ) logging . warning ( \"The isoform option only applies to gene IDs.\" ) else : logging . warning ( f \" { ensembl_ID } not recognized as either a gene or transcript ID. It will not be included in the UniProt query.\" ) # Fetch amino acid sequences of all isoforms from the UniProt REST API df_uniprot = get_uniprot_seqs ( UNIPROT_REST_API , trans_ids ) # Check if any results were found if len ( df_uniprot ) < 1 : logging . error ( \"No UniProt amino acid sequences were found for these ID(s).\" ) else : # Build FASTA file from UniProt results for ( uniprot_id , query_ensembl_id , gene_name , organism , sequence_length , uniprot_seq , ) in zip ( df_uniprot [ \"uniprot_id\" ] . values , df_uniprot [ \"query\" ] . values , df_uniprot [ \"gene_name\" ] . values , df_uniprot [ \"gene_id\" ] . values , df_uniprot [ \"organism\" ] . values , df_uniprot [ \"sequence_length\" ] . values , df_uniprot [ \"sequence\" ] . values , ): fasta . append ( \">\" + str ( query_ensembl_id ) + \" uniprot_id: \" + str ( uniprot_id ) + \" ensembl_id: \" + str ( query_ensembl_id ) + \" gene_name: \" + str ( gene_name ) + \" organism: \" + str ( organism ) + \" sequence_length: \" + str ( sequence_length ) ) fasta . append ( str ( uniprot_seq )) # Save if save : file = open ( \"gget_seq_results.fa\" , \"w\" ) for element in fasta : file . write ( element + \" \\n \" ) file . close () # missed samples return ( set ( trans_ids ) - set ( df_uniprot [ \"query\" ] . values )) | set ( missing ) return fasta subset_fasta subset_fasta: creates a new fasta file with only the sequence which names contain one of gene_names Parameters: gene_tosubset ( set , default: None ) \u2013 A set of gene names to subset from the original FASTA file. fasta_path ( str , default: None ) \u2013 The path to the original FASTA file. subfasta_path ( str , default: './data/fasta/subset.fa' ) \u2013 The path to save the subsetted FASTA file. Defaults to \"./data/fasta/subset.fa\". drop_unknown_seq ( bool , default: True ) \u2013 If True, drop sequences containing unknown amino acids (denoted by '*'). Defaults to True. subset_protein_coding ( bool , default: True ) \u2013 If True, subset only protein coding genes. Defaults to True. Returns: set: A set of gene names that were found and included in the subsetted FASTA file. Raises: ValueError \u2013 If a gene name does not start with \"ENS\". Source code in scprint2/utils/get_seq.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def subset_fasta ( gene_tosubset : set = None , fasta_path : str = None , subfasta_path : str = \"./data/fasta/subset.fa\" , drop_unknown_seq : bool = True , subset_protein_coding : bool = True , ) -> set : \"\"\" subset_fasta: creates a new fasta file with only the sequence which names contain one of gene_names Args: gene_tosubset (set): A set of gene names to subset from the original FASTA file. fasta_path (str): The path to the original FASTA file. subfasta_path (str, optional): The path to save the subsetted FASTA file. Defaults to \"./data/fasta/subset.fa\". drop_unknown_seq (bool, optional): If True, drop sequences containing unknown amino acids (denoted by '*'). Defaults to True. subset_protein_coding (bool, optional): If True, subset only protein coding genes. Defaults to True. Returns: set: A set of gene names that were found and included in the subsetted FASTA file. Raises: ValueError: If a gene name does not start with \"ENS\". \"\"\" dup = set () weird = 0 nc = 0 genes_found = set () gene_tosubset = set ( gene_tosubset ) if gene_tosubset else [] names = [] with ( open ( fasta_path , \"r\" ) as original_fasta , open ( subfasta_path , \"w\" ) as subset_fasta , ): for record in SeqIO . parse ( original_fasta , \"fasta\" ): gene_name = ( record . description . split ( \" gene:\" )[ 1 ] . split ( \" \" )[ 0 ] . split ( \".\" )[ 0 ] ) gene_biotype = record . description . split ( \"gene_biotype:\" )[ 1 ] . split ( \" \" )[ 0 ] if \"gene_symbol:\" not in record . description : gene_symbol = gene_name else : gene_symbol = record . description . split ( \"gene_symbol:\" )[ 1 ] . split ( \" \" )[ 0 ] if \"description:\" not in record . description : description = \"\" else : description = record . description . split ( \"description:\" )[ 1 ] names . append ([ gene_name , gene_biotype , record . id , gene_symbol , description ]) if subset_protein_coding and gene_biotype != \"protein_coding\" : nc += 1 continue if len ( gene_tosubset ) == 0 or gene_name in gene_tosubset : if drop_unknown_seq : if \"*\" in record . seq : weird += 1 continue if gene_name in genes_found : dup . add ( gene_name ) continue record . description = \"\" record . id = gene_name SeqIO . write ( record , subset_fasta , \"fasta\" ) genes_found . add ( gene_name ) print ( len ( dup ), \" genes had duplicates\" ) print ( \"dropped\" , weird , \"weird sequences\" ) print ( \"dropped\" , nc , \"non-coding sequences\" ) return genes_found , pd . DataFrame ( names , columns = [ \"name\" , \"biotype\" , \"ensembl_id\" , \"gene_symbol\" , \"description\" ] ) scprint2.utils.graph_refinement Graph-regularized logit refinement implementation. This module implements the GRIT (Graph-Regularized logIT) refinement method for improving cell type predictions using graph structure. Functions: Name Description build_knn_graph Build a k-nearest neighbor graph and store it in adata.obsp. graph_regularized_logit_refinement Refine logits using graph-regularized optimization. test_graph_refinement Test function for graph refinement. zero_shot_annotation_with_refinement Perform zero-shot cell type annotation with graph refinement. build_knn_graph Build a k-nearest neighbor graph and store it in adata.obsp. Parameters: adata ( AnnData ) \u2013 AnnData object representation_key ( str , default: 'X_pca' ) \u2013 Key in adata.obsm for the representation to use. Defaults to \"X_pca\". n_neighbors ( int , default: 15 ) \u2013 Number of nearest neighbors. Defaults to 15. metric ( str , default: 'euclidean' ) \u2013 Distance metric for nearest neighbor search. Defaults to \"euclidean\". Returns: AnnData \u2013 anndata.AnnData: Updated AnnData object with connectivity matrix Source code in scprint2/utils/graph_refinement.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def build_knn_graph ( adata : anndata . AnnData , representation_key : str = \"X_pca\" , n_neighbors : int = 15 , metric : str = \"euclidean\" , ) -> anndata . AnnData : \"\"\" Build a k-nearest neighbor graph and store it in adata.obsp. Args: adata (anndata.AnnData): AnnData object representation_key (str): Key in adata.obsm for the representation to use. Defaults to \"X_pca\". n_neighbors (int): Number of nearest neighbors. Defaults to 15. metric (str): Distance metric for nearest neighbor search. Defaults to \"euclidean\". Returns: anndata.AnnData: Updated AnnData object with connectivity matrix \"\"\" try : import scanpy as sc except ImportError : raise ImportError ( \"scanpy is required for building k-NN graphs\" ) # Compute neighbors sc . pp . neighbors ( adata , use_rep = representation_key , n_neighbors = n_neighbors , metric = metric , ) return adata graph_regularized_logit_refinement Refine logits using graph-regularized optimization. Optimized version that solves for all classes simultaneously. This function implements the optimization problem: P\u0303 = arg min_P ||P - P\u2080||\u00b2_F + \u03bb Tr(P^T L P) where P\u2080 are the initial logits, L is the graph Laplacian, and \u03bb controls the strength of regularization. The solution has a closed form: P\u0303 = (I + \u03bbL)\u207b\u00b9P\u2080 Parameters: pred ( ndarray ) \u2013 Initial logits of shape (n_cells, n_classes) adata ( AnnData ) \u2013 AnnData object containing graph connectivity connectivity_key ( str , default: 'connectivities' ) \u2013 Key in adata.obsp for connectivity matrix lambda_reg ( float , default: 0.1 ) \u2013 Regularization strength \u03bb > 0 use_laplacian ( bool , default: True ) \u2013 If True, use graph Laplacian; if False, use adjacency matrix Returns: ndarray \u2013 np.ndarray: Refined logits of same shape as input pred Raises: ValueError \u2013 If connectivity matrix is not found or dimensions don't match KeyError \u2013 If connectivity_key is not in adata.obsp Source code in scprint2/utils/graph_refinement.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def graph_regularized_logit_refinement ( pred : np . ndarray , adata : anndata . AnnData , connectivity_key : str = \"connectivities\" , lambda_reg : float = 0.1 , use_laplacian : bool = True , ) -> np . ndarray : \"\"\" Refine logits using graph-regularized optimization. Optimized version that solves for all classes simultaneously. This function implements the optimization problem: P\u0303 = arg min_P ||P - P\u2080||\u00b2_F + \u03bb Tr(P^T L P) where P\u2080 are the initial logits, L is the graph Laplacian, and \u03bb controls the strength of regularization. The solution has a closed form: P\u0303 = (I + \u03bbL)\u207b\u00b9P\u2080 Args: pred (np.ndarray): Initial logits of shape (n_cells, n_classes) adata (anndata.AnnData): AnnData object containing graph connectivity connectivity_key (str): Key in adata.obsp for connectivity matrix lambda_reg (float): Regularization strength \u03bb > 0 use_laplacian (bool): If True, use graph Laplacian; if False, use adjacency matrix Returns: np.ndarray: Refined logits of same shape as input pred Raises: ValueError: If connectivity matrix is not found or dimensions don't match KeyError: If connectivity_key is not in adata.obsp \"\"\" # Validate inputs if connectivity_key not in adata . obsp : raise KeyError ( f \"Connectivity key ' { connectivity_key } ' not found in adata.obsp\" ) A = adata . obsp [ connectivity_key ] n_cells , n_classes = pred . shape # Check dimensions if A . shape [ 0 ] != n_cells or A . shape [ 1 ] != n_cells : raise ValueError ( f \"Connectivity matrix shape { A . shape } doesn't match number of cells { n_cells } \" ) # Ensure adjacency matrix is symmetric and sparse if not sp . issparse ( A ): A = sp . csr_matrix ( A ) # Make symmetric if not already A = ( A + A . T ) / 2 if use_laplacian : # Compute graph Laplacian: L = D - A # where D is the diagonal degree matrix degrees = np . array ( A . sum ( axis = 1 )) . flatten () D = sp . diags ( degrees , format = \"csr\" ) L = D - A else : # Use adjacency matrix directly L = A identity_matrix = sp . identity ( n_cells , format = \"csr\" ) system_matrix = identity_matrix + lambda_reg * L # Solve for all classes at once instead of looping # spsolve can handle multiple right-hand sides refined_pred = spsolve ( system_matrix , pred ) # Handle the case where spsolve returns 1D array for single class if refined_pred . ndim == 1 and n_classes == 1 : refined_pred = refined_pred . reshape ( - 1 , 1 ) elif refined_pred . ndim == 1 : refined_pred = refined_pred . reshape ( n_cells , n_classes ) return refined_pred test_graph_refinement Test function for graph refinement. Source code in scprint2/utils/graph_refinement.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def test_graph_refinement (): \"\"\"Test function for graph refinement.\"\"\" # Create synthetic data n_cells , n_classes = 100 , 5 # Random logits np . random . seed ( 42 ) pred = np . random . randn ( n_cells , n_classes ) # Create synthetic AnnData with connectivity adata = anndata . AnnData ( X = np . random . randn ( n_cells , 50 )) # Create a random sparse connectivity matrix from scipy.sparse import random connectivity = random ( n_cells , n_cells , density = 0.1 , format = \"csr\" ) connectivity = ( connectivity + connectivity . T ) / 2 # Make symmetric adata . obsp [ \"connectivities\" ] = connectivity # Test refinement refined_pred = graph_regularized_logit_refinement ( pred , adata , lambda_reg = 0.1 ) print ( f \"Original logits shape: { pred . shape } \" ) print ( f \"Refined logits shape: { refined_pred . shape } \" ) print ( f \"Logits changed: { not np . allclose ( pred , refined_pred ) } \" ) # Test zero-shot annotation predictions , probabilities = zero_shot_annotation_with_refinement ( pred , adata , return_probabilities = True ) print ( f \"Predictions shape: { predictions . shape } \" ) print ( f \"Probabilities shape: { probabilities . shape } \" ) print ( f \"Predicted classes: { np . unique ( predictions ) } \" ) zero_shot_annotation_with_refinement Perform zero-shot cell type annotation with graph refinement. This function first refines the logits using graph regularization, then performs argmax to get final predictions. Parameters: pred ( ndarray ) \u2013 Initial logits of shape (n_cells, n_classes) adata ( AnnData ) \u2013 AnnData object containing graph connectivity connectivity_key ( str , default: 'connectivities' ) \u2013 Key in adata.obsp for connectivity matrix lambda_reg ( float , default: 0.1 ) \u2013 Regularization strength return_probabilities ( bool , default: False ) \u2013 If True, also return refined probabilities Returns: Union [ ndarray , tuple ] \u2013 np.ndarray or tuple: If return_probabilities is False, returns array of predicted class indices. If True, returns tuple of (predictions, refined_probabilities) Source code in scprint2/utils/graph_refinement.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def zero_shot_annotation_with_refinement ( pred : np . ndarray , adata : anndata . AnnData , connectivity_key : str = \"connectivities\" , representation_key : str = \"X_pca\" , n_neighbors : int = 15 , metric : str = \"euclidean\" , lambda_reg : float = 0.1 , return_probabilities : bool = False , return_raw : bool = False , ) -> Union [ np . ndarray , tuple ]: \"\"\" Perform zero-shot cell type annotation with graph refinement. This function first refines the logits using graph regularization, then performs argmax to get final predictions. Args: pred (np.ndarray): Initial logits of shape (n_cells, n_classes) adata (anndata.AnnData): AnnData object containing graph connectivity connectivity_key (str): Key in adata.obsp for connectivity matrix lambda_reg (float): Regularization strength return_probabilities (bool): If True, also return refined probabilities Returns: np.ndarray or tuple: If return_probabilities is False, returns array of predicted class indices. If True, returns tuple of (predictions, refined_probabilities) \"\"\" if pred is type ( pd . DataFrame ): pred = pred . values if adata . obsp . get ( connectivity_key ) is None : # Refine logits adata = build_knn_graph ( adata = adata , representation_key = representation_key , n_neighbors = n_neighbors , metric = metric , ) connectivity_key = \"connectivities\" print ( adata . obsp ) refined_logits = graph_regularized_logit_refinement ( pred , adata , connectivity_key , lambda_reg ) if return_raw : return refined_logits # Get predictions: g(xi) = arg max_j {P\u0303(i)} predictions = np . argmax ( refined_logits , axis = 1 ) if return_probabilities : # Convert to probabilities using softmax refined_probs = np . exp ( refined_logits ) refined_probs = refined_probs / refined_probs . sum ( axis = 1 , keepdims = True ) return predictions , refined_probs return predictions","title":"utils"},{"location":"utils/#documentation-for-the-utils-modules","text":"","title":"Documentation for the utils modules"},{"location":"utils/#scprint2.utils.sinkhorn","text":"Classes: Name Description SinkhornDistance","title":"sinkhorn"},{"location":"utils/#scprint2.utils.sinkhorn.SinkhornDistance","text":"Bases: Module SinkhornDistance Initialize the SinkhornDistance class Parameters: eps ( float , default: 0.01 ) \u2013 Regularization parameter. Defaults to 1e-2. max_iter ( int , default: 100 ) \u2013 Maximum number of Sinkhorn iterations. Defaults to 100. reduction ( str , default: 'none' ) \u2013 Specifies the reduction to apply to the output. Defaults to \"none\". Methods: Name Description M Modified cost for logarithmic updates ave Barycenter subroutine, used by kinetic acceleration through extrapolation. forward forward Compute the Sinkhorn distance between two measures with cost matrix c Source code in scprint2/utils/sinkhorn.py 8 9 10 11 12 13 14 15 16 17 18 19 20 def __init__ ( self , eps : float = 1e-2 , max_iter : int = 100 , reduction : str = \"none\" ): \"\"\" SinkhornDistance Initialize the SinkhornDistance class Args: eps (float, optional): Regularization parameter. Defaults to 1e-2. max_iter (int, optional): Maximum number of Sinkhorn iterations. Defaults to 100. reduction (str, optional): Specifies the reduction to apply to the output. Defaults to \"none\". \"\"\" super ( SinkhornDistance , self ) . __init__ () self . eps = eps self . max_iter = max_iter self . reduction = reduction","title":"SinkhornDistance"},{"location":"utils/#scprint2.utils.sinkhorn.SinkhornDistance.M","text":"Modified cost for logarithmic updates Source code in scprint2/utils/sinkhorn.py 99 100 101 102 def M ( self , C , u , v ): \"Modified cost for logarithmic updates\" \"\"\"$M_{ij} = (-c_{ij} + u_i + v_j) / epsilon$\"\"\" return ( - C + u . unsqueeze ( - 1 ) + v . unsqueeze ( 1 )) / self . eps","title":"M"},{"location":"utils/#scprint2.utils.sinkhorn.SinkhornDistance.ave","text":"Barycenter subroutine, used by kinetic acceleration through extrapolation. Source code in scprint2/utils/sinkhorn.py 104 105 106 107 @staticmethod def ave ( u , u1 , tau ): \"Barycenter subroutine, used by kinetic acceleration through extrapolation.\" return tau * u + ( 1 - tau ) * u1","title":"ave"},{"location":"utils/#scprint2.utils.sinkhorn.SinkhornDistance.forward","text":"forward Compute the Sinkhorn distance between two measures with cost matrix c Parameters: c ( Tensor ) \u2013 The cost matrix between the two measures. Returns: Tensor \u2013 torch.Tensor: The computed Sinkhorn distance. Source code in scprint2/utils/sinkhorn.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def forward ( self , c : torch . Tensor ) -> torch . Tensor : \"\"\" forward Compute the Sinkhorn distance between two measures with cost matrix c Args: c (torch.Tensor): The cost matrix between the two measures. Returns: torch.Tensor: The computed Sinkhorn distance. \"\"\" C = - c x_points = C . shape [ - 2 ] batch_size = C . shape [ 0 ] # both marginals are fixed with equal weights mu = ( torch . empty ( batch_size , x_points , dtype = C . dtype , requires_grad = False , device = C . device , ) . fill_ ( 1.0 / x_points ) . squeeze () ) nu = ( torch . empty ( batch_size , x_points , dtype = C . dtype , requires_grad = False , device = C . device , ) . fill_ ( 1.0 / x_points ) . squeeze () ) u = torch . zeros_like ( mu ) v = torch . zeros_like ( nu ) # Stopping criterion thresh = 1e-12 # Sinkhorn iterations for i in range ( self . max_iter ): if i % 2 == 0 : u1 = u # useful to check the update u = ( self . eps * ( torch . log ( mu ) - torch . logsumexp ( self . M ( C , u , v ), dim =- 1 )) + u ) err = ( u - u1 ) . abs () . sum ( - 1 ) . mean () else : v = ( self . eps * ( torch . log ( nu ) - torch . logsumexp ( self . M ( C , u , v ) . transpose ( - 2 , - 1 ), dim =- 1 ) ) + v ) v = v . detach () . requires_grad_ ( False ) v [ v > 9 * 1e8 ] = 0.0 v = v . detach () . requires_grad_ ( True ) if err . item () < thresh : break U , V = u , v # Transport plan pi = diag(a)*K*diag(b) pi = torch . exp ( self . M ( C , U , V )) # Sinkhorn distance return pi , C , U , V","title":"forward"},{"location":"utils/#scprint2.utils.utils","text":"Functions: Name Description add_points parts of volcano plot category_str2int category_str2int converts a list of category strings to a list of category integers. correlationMatrix Make an interactive correlation matrix from an array using bokeh createFoldersFor will recursively create folders if needed until having all the folders required to save the file in this filepath fileToList loads an input file with a\\n b\\n.. into a list [a,b,..] get_free_gpu get_free_gpu finds the GPU with the most free memory using nvidia-smi. get_git_commit get_git_commit gets the current git commit hash. heatmap Make an interactive heatmap from a dataframe using bokeh inf_loop wrapper function for endless data loader. isnotebook check whether excuting in jupyter notebook. listToFile listToFile loads a list with [a,b,..] into an input file a\\n b\\n.. prepare_device setup GPU device if available. get gpu device indices which are used for DataParallel run_command run_command runs a command in the shell and prints the output. selector Part of Volcano plot: A function to separate tfs from everything else set_seed set random seed. subset_h5ad_by_format Create new anndata object according to slot info specifications. volcano Make an interactive volcano plot from Differential Expression analysis tools outputs","title":"utils"},{"location":"utils/#scprint2.utils.utils.add_points","text":"parts of volcano plot Source code in scprint2/utils/utils.py 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 def add_points ( p , df1 , x , y , color = \"blue\" , alpha = 0.2 , outline = False , maxvalue = 100 ): \"\"\"parts of volcano plot\"\"\" # Define colors in a dictionary to access them with # the key from the pandas groupby funciton. df = df1 . copy () transformed_q = - df [ y ] . apply ( np . log10 ) . values transformed_q [ transformed_q == np . inf ] = maxvalue transformed_q [ transformed_q > maxvalue ] = maxvalue df [ \"transformed_q\" ] = transformed_q df [ \"color\" ] = color df [ \"alpha\" ] = alpha df [ \"size\" ] = 7 source1 = ColumnDataSource ( df ) # Specify data source p . scatter ( x = x , y = \"transformed_q\" , size = \"size\" , alpha = \"alpha\" , source = source1 , color = \"color\" , name = \"circles\" , ) if outline : p . scatter ( x = x , y = \"transformed_q\" , size = 7 , alpha = 1 , source = source1 , color = \"black\" , fill_color = None , name = \"outlines\" , ) # prettify p . background_fill_color = \"#DFDFE5\" p . background_fill_alpha = 0.5 return p , source1","title":"add_points"},{"location":"utils/#scprint2.utils.utils.category_str2int","text":"category_str2int converts a list of category strings to a list of category integers. Parameters: category_strs ( List [ str ] ) \u2013 A list of category strings to be converted. Returns: List [ int ] \u2013 List[int]: A list of integers corresponding to the input category strings. Source code in scprint2/utils/utils.py 105 106 107 108 109 110 111 112 113 114 115 116 117 def category_str2int ( category_strs : List [ str ]) -> List [ int ]: \"\"\" category_str2int converts a list of category strings to a list of category integers. Args: category_strs (List[str]): A list of category strings to be converted. Returns: List[int]: A list of integers corresponding to the input category strings. \"\"\" set_category_strs = set ( category_strs ) name2id = { name : i for i , name in enumerate ( set_category_strs )} return [ name2id [ name ] for name in category_strs ]","title":"category_str2int"},{"location":"utils/#scprint2.utils.utils.correlationMatrix","text":"Make an interactive correlation matrix from an array using bokeh data: arrayLike of int / float/ bool of size(names val) or (names names) names: list[str] of names for each rows colors: list[int] of size(names) a color for each names (good to display clusters) pvals: arraylike of int / float/ bool of size(names val) or (names names) with the corresponding pvalues maxokpval: float threshold when pvalue is considered good. otherwise lowers the size of the square until 10 -3 when it disappears other: arrayLike of int / float/ bool of size(names val) or (names names), an additional information matrix that you want ot display with opacity whereas correlations willl be displayed with title: str the plot title dataIsCorr: bool if not true, we will compute the corrcoef of the data array invert: bool whether or not to invert the matrix before running corrcoef size: int the plot size folder: str of folder location where to save the plot, won't save if empty interactive: bool whether or not to make the plot interactive (else will use matplotlib) maxval: float clamping coloring up to maxval minval: float clamping coloring down to minval the bokeh object if interactive else None Source code in scprint2/utils/utils.py 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 def correlationMatrix ( data , names , colors = None , pvals = None , maxokpval = 10 **- 9 , other = None , title = \"correlation Matrix\" , dataIsCorr = False , invert = False , size = 40 , folder = \"\" , interactive = False , maxval = None , minval = None , ): \"\"\" Make an interactive correlation matrix from an array using bokeh Args: ----- data: arrayLike of int / float/ bool of size(names*val) or (names*names) names: list[str] of names for each rows colors: list[int] of size(names) a color for each names (good to display clusters) pvals: arraylike of int / float/ bool of size(names*val) or (names*names) with the corresponding pvalues maxokpval: float threshold when pvalue is considered good. otherwise lowers the size of the square until 10**-3 when it disappears other: arrayLike of int / float/ bool of size(names*val) or (names*names), an additional information matrix that you want ot display with opacity whereas correlations willl be displayed with title: str the plot title dataIsCorr: bool if not true, we will compute the corrcoef of the data array invert: bool whether or not to invert the matrix before running corrcoef size: int the plot size folder: str of folder location where to save the plot, won't save if empty interactive: bool whether or not to make the plot interactive (else will use matplotlib) maxval: float clamping coloring up to maxval minval: float clamping coloring down to minval Returns: ------- the bokeh object if interactive else None \"\"\" if not dataIsCorr : print ( \"computing correlations\" ) data = np . corrcoef ( np . array ( data ) if not invert else np . array ( data ) . T ) else : data = np . array ( data ) regdata = data . copy () if minval is not None : data [ data < minval ] = minval if maxval is not None : data [ data > maxval ] = maxval data = data / data . max () TOOLS = ( \"hover,crosshair,pan,wheel_zoom,zoom_in,zoom_out,box_zoom,undo,redo,reset,save\" ) xname = [] yname = [] color = [] alpha = [] height = [] width = [] if type ( colors ) is list : print ( \"we are assuming you want to display clusters with colors\" ) elif other is not None : print ( \"we are assuming you want to display the other of your correlation with opacity\" ) if pvals is not None : print ( \"we are assuming you want to display the pvals of your correlation with size\" ) regpvals = pvals . copy () u = pvals < maxokpval pvals [ ~ u ] = np . log10 ( 1 / pvals [ ~ u ]) pvals = pvals / pvals . max () pvals [ u ] = 1 if interactive : xname = [] yname = [] color = [] for i , name1 in enumerate ( names ): for j , name2 in enumerate ( names ): xname . append ( name1 ) yname . append ( name2 ) if pvals is not None : height . append ( max ( 0.1 , min ( 0.9 , pvals [ i , j ]))) color . append ( cc . coolwarm [ int (( data [ i , j ] * 127 ) + 127 )]) alpha . append ( min ( abs ( data [ i , j ]), 0.9 )) elif other is not None : color . append ( cc . coolwarm [ int (( data [ i , j ] * 127 ) + 127 )]) alpha . append ( max ( min ( other [ i , j ], 0.9 ), 0.1 ) if other [ i , j ] != 0 else 0 ) else : alpha . append ( min ( abs ( data [ i , j ]), 0.9 )) if colors is not None : if type ( colors ) is list : if colors [ i ] == colors [ j ]: color . append ( Category10 [ 10 ][ colors [ i ]]) else : color . append ( \"lightgrey\" ) elif pvals is None and other is None : color . append ( \"grey\" if data [ i , j ] > 0 else Category20 [ 3 ][ 2 ]) print ( regdata . max ()) if pvals is not None : width = height . copy () data = dict ( xname = xname , yname = yname , colors = color , alphas = alpha , data = regdata . ravel (), pvals = regpvals . ravel (), width = width , height = height , ) else : data = dict ( xname = xname , yname = yname , colors = color , alphas = alpha , data = data . ravel () ) tt = [( \"names\" , \"@yname, @xname\" ), ( \"value\" , \"@data\" )] if pvals is not None : tt . append (( \"pvals\" , \"@pvals\" )) p = figure ( title = title if title is not None else \"Correlation Matrix\" , x_axis_location = \"above\" , tools = TOOLS , x_range = list ( reversed ( names )), y_range = names , tooltips = tt , ) p . width = 800 p . height = 800 p . grid . grid_line_color = None p . axis . axis_line_color = None p . axis . major_tick_line_color = None p . axis . major_label_text_font_size = \"5pt\" p . axis . major_label_standoff = 0 p . xaxis . major_label_orientation = np . pi / 3 p . output_backend = \"svg\" p . rect ( \"xname\" , \"yname\" , width = 0.9 if not width else \"width\" , height = 0.9 if not height else \"height\" , source = data , color = \"colors\" , alpha = \"alphas\" , line_color = None , hover_line_color = \"black\" , hover_color = \"colors\" , ) save ( p , folder + title . replace ( \" \" , \"_\" ) + \"_correlation.html\" ) try : p . output_backend = \"svg\" export_svg ( p , filename = folder + title . replace ( \" \" , \"_\" ) + \"_correlation.svg\" ) except ( RuntimeError , Exception ) as e : print ( f \"Could not save SVG: { e } \" ) try : show ( p ) except Exception as e : print ( f \"Could not show plot: { e } \" ) return p # show the plot else : plt . figure ( figsize = ( size , 200 )) plt . title ( \"the correlation matrix\" ) plt . imshow ( data ) plt . savefig ( title + \"_correlation.pdf\" ) plt . show ()","title":"correlationMatrix"},{"location":"utils/#scprint2.utils.utils.createFoldersFor","text":"will recursively create folders if needed until having all the folders required to save the file in this filepath Source code in scprint2/utils/utils.py 94 95 96 97 98 99 100 101 102 def createFoldersFor ( filepath ): \"\"\" will recursively create folders if needed until having all the folders required to save the file in this filepath \"\"\" prevval = \"\" for val in os . path . expanduser ( filepath ) . split ( \"/\" )[: - 1 ]: prevval += val + \"/\" if not os . path . exists ( prevval ): os . mkdir ( prevval )","title":"createFoldersFor"},{"location":"utils/#scprint2.utils.utils.fileToList","text":"loads an input file with a\\n b\\n.. into a list [a,b,..] Parameters: filename ( str ) \u2013 The path to the file to be loaded. strconv ( callable , default: lambda x: x ) \u2013 Function to convert each line. Defaults to identity function. Returns: list ( list ) \u2013 The list of converted values from the file. Source code in scprint2/utils/utils.py 49 50 51 52 53 54 55 56 57 58 59 60 61 def fileToList ( filename : str , strconv : callable = lambda x : x ) -> list : \"\"\" loads an input file with a\\\\n b\\\\n.. into a list [a,b,..] Args: filename (str): The path to the file to be loaded. strconv (callable): Function to convert each line. Defaults to identity function. Returns: list: The list of converted values from the file. \"\"\" with open ( filename ) as f : return [ strconv ( val [: - 1 ]) for val in f . readlines ()]","title":"fileToList"},{"location":"utils/#scprint2.utils.utils.get_free_gpu","text":"get_free_gpu finds the GPU with the most free memory using nvidia-smi. Returns: int ( int ) \u2013 The index of the GPU with the most free memory. Source code in scprint2/utils/utils.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def get_free_gpu () -> int : \"\"\" get_free_gpu finds the GPU with the most free memory using nvidia-smi. Returns: int: The index of the GPU with the most free memory. \"\"\" import subprocess import sys from io import StringIO gpu_stats = subprocess . check_output ( [ \"nvidia-smi\" , \"--format=csv\" , \"--query-gpu=memory.used,memory.free\" , ] ) . decode ( \"utf-8\" ) gpu_df = pd . read_csv ( StringIO ( gpu_stats ), names = [ \"memory.used\" , \"memory.free\" ], skiprows = 1 ) print ( \"GPU usage: \\n {} \" . format ( gpu_df )) gpu_df [ \"memory.free\" ] = gpu_df [ \"memory.free\" ] . map ( lambda x : int ( x . rstrip ( \" [MiB]\" ))) idx = gpu_df [ \"memory.free\" ] . idxmax () print ( \"Find free GPU {} with {} free MiB\" . format ( idx , gpu_df . iloc [ idx ][ \"memory.free\" ]) ) return idx","title":"get_free_gpu"},{"location":"utils/#scprint2.utils.utils.get_git_commit","text":"get_git_commit gets the current git commit hash. Returns: str ( str ) \u2013 The current git commit Source code in scprint2/utils/utils.py 165 166 167 168 169 170 171 172 def get_git_commit () -> str : \"\"\" get_git_commit gets the current git commit hash. Returns: str: The current git commit \"\"\" return subprocess . check_output ([ \"git\" , \"rev-parse\" , \"HEAD\" ]) . decode ( \"utf-8\" ) . strip ()","title":"get_git_commit"},{"location":"utils/#scprint2.utils.utils.heatmap","text":"Make an interactive heatmap from a dataframe using bokeh data: dataframe of int / float/ bool of size(names1 names2) colors: list[int] of size(names) a color for each names (good to display clusters) pvals: arraylike of int / float/ bool of size(names val) or (names names) with the corresponding pvalues maxokpval: float threshold when pvalue is considered good. otherwise lowers the size of the square until 10 *-3 when it disappears title: str the plot title size: int the plot size folder: str of folder location where to save the plot, won't save if empty interactive: bool whether or not to make the plot interactive (else will use matplotlib) maxval: float clamping coloring up to maxval minval: float clamping coloring down to minval the bokeh object if interactive else None Source code in scprint2/utils/utils.py 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 def heatmap ( data , colors = None , title = \"correlation Matrix\" , size = 40 , other = None , folder = \"\" , interactive = False , pvals = None , maxokpval = 10 **- 9 , maxval = None , minval = None , ): \"\"\" Make an interactive heatmap from a dataframe using bokeh Args: ----- data: dataframe of int / float/ bool of size(names1*names2) colors: list[int] of size(names) a color for each names (good to display clusters) pvals: arraylike of int / float/ bool of size(names*val) or (names*names) with the corresponding pvalues maxokpval: float threshold when pvalue is considered good. otherwise lowers the size of the square until 10**-3 when it disappears title: str the plot title size: int the plot size folder: str of folder location where to save the plot, won't save if empty interactive: bool whether or not to make the plot interactive (else will use matplotlib) maxval: float clamping coloring up to maxval minval: float clamping coloring down to minval Returns: ------- the bokeh object if interactive else None \"\"\" regdata = data . copy () if minval is not None : data [ data < minval ] = minval if maxval is not None : data [ data > maxval ] = maxval data = data / data . max () data = data . values TOOLS = ( \"hover,crosshair,pan,wheel_zoom,zoom_in,zoom_out,box_zoom,undo,redo,reset,save\" ) xname = [] yname = [] color = [] alpha = [] height = [] width = [] if pvals is not None : print ( \"we are assuming you want to display the pvals of your correlation with size\" ) regpvals = pvals . copy () u = pvals < maxokpval pvals [ ~ u ] = np . log10 ( 1 / pvals [ ~ u ]) pvals = pvals / pvals . max () pvals [ u ] = 1 if interactive : xname = [] yname = [] color = [] for i , name1 in enumerate ( regdata . index ): for j , name2 in enumerate ( regdata . columns ): xname . append ( name2 ) yname . append ( name1 ) if pvals is not None : # import pdb;pdb.set_trace() height . append ( max ( 0.1 , min ( 0.9 , pvals . loc [ name1 ][ name2 ]))) color . append ( cc . coolwarm [ int (( data [ i , j ] * 128 ) + 127 )]) alpha . append ( min ( abs ( data [ i , j ]), 0.9 )) elif other is not None : color . append ( cc . coolwarm [ int (( data [ i , j ] * 128 ) + 127 )]) alpha . append ( max ( min ( other [ i , j ], 0.9 ), 0.1 ) if other [ i , j ] != 0 else 0 ) else : alpha . append ( min ( abs ( data [ i , j ]), 0.9 )) if colors is not None : if type ( colors ) is list : if colors [ i ] == colors [ j ]: color . append ( Category10 [ 10 ][ colors [ i ]]) else : color . append ( \"lightgrey\" ) elif pvals is None and other is None : color . append ( \"grey\" if data [ i , j ] > 0 else Category20 [ 3 ][ 2 ]) if pvals is not None : width = height . copy () data = dict ( xname = xname , yname = yname , colors = color , alphas = alpha , data = regdata . values . ravel (), pvals = regpvals . values . ravel (), width = width , height = height , ) else : data = dict ( xname = xname , yname = yname , colors = color , alphas = alpha , data = data . ravel () ) tt = [( \"names\" , \"@yname, @xname\" ), ( \"value\" , \"@data\" )] if pvals is not None : tt . append (( \"pvals\" , \"@pvals\" )) p = figure ( title = title if title is not None else \"Heatmap\" , x_axis_location = \"above\" , tools = TOOLS , x_range = list ( reversed ( regdata . columns . astype ( str ) . tolist ())), y_range = regdata . index . tolist (), tooltips = tt , ) p . width = 800 p . height = 800 p . grid . grid_line_color = None p . axis . axis_line_color = None p . axis . major_tick_line_color = None p . axis . major_label_text_font_size = \"5pt\" p . axis . major_label_standoff = 0 p . xaxis . major_label_orientation = np . pi / 3 p . output_backend = \"svg\" p . rect ( \"xname\" , \"yname\" , width = 0.9 if not width else \"width\" , height = 0.9 if not height else \"height\" , source = data , color = \"colors\" , alpha = \"alphas\" , line_color = None , hover_line_color = \"black\" , hover_color = \"colors\" , ) save ( p , folder + title . replace ( \" \" , \"_\" ) + \"_heatmap.html\" ) try : p . output_backend = \"svg\" export_svg ( p , filename = folder + title . replace ( \" \" , \"_\" ) + \"_correlation.svg\" ) except ( RuntimeError , Exception ) as e : print ( f \"Could not save SVG: { e } \" ) try : show ( p ) except Exception as e : print ( f \"Could not show plot: { e } \" ) return p # show the plot else : plt . figure ( figsize = size ) plt . title ( \"the correlation matrix\" ) plt . imshow ( data ) plt . savefig ( title + \"_correlation.pdf\" ) plt . show ()","title":"heatmap"},{"location":"utils/#scprint2.utils.utils.inf_loop","text":"wrapper function for endless data loader. Source code in scprint2/utils/utils.py 193 194 195 196 def inf_loop ( data_loader ): \"\"\"wrapper function for endless data loader.\"\"\" for loader in repeat ( data_loader ): yield from loader","title":"inf_loop"},{"location":"utils/#scprint2.utils.utils.isnotebook","text":"check whether excuting in jupyter notebook. Source code in scprint2/utils/utils.py 120 121 122 123 124 125 126 127 128 129 130 131 def isnotebook () -> bool : \"\"\"check whether excuting in jupyter notebook.\"\"\" try : shell = get_ipython () . __class__ . __name__ if shell == \"ZMQInteractiveShell\" : return True # Jupyter notebook or qtconsole elif shell == \"TerminalInteractiveShell\" : return True # Terminal running IPython else : return False # Other type (?) except NameError : return False # Probably standard Python interpreter","title":"isnotebook"},{"location":"utils/#scprint2.utils.utils.listToFile","text":"listToFile loads a list with [a,b,..] into an input file a\\n b\\n.. Parameters: li ( list ) \u2013 The list of elements to be written to the file. filename ( str ) \u2013 The name of the file where the list will be written. strconv ( callable , default: lambda x: str ( x ) ) \u2013 A function to convert each element of the list to a string. Defaults to str. Returns: None \u2013 None Source code in scprint2/utils/utils.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def listToFile ( li : List [ str ], filename : str , strconv : callable = lambda x : str ( x ) ) -> None : \"\"\" listToFile loads a list with [a,b,..] into an input file a\\\\n b\\\\n.. Args: li (list): The list of elements to be written to the file. filename (str): The name of the file where the list will be written. strconv (callable, optional): A function to convert each element of the list to a string. Defaults to str. Returns: None \"\"\" with open ( filename , \"w\" ) as f : for item in li : f . write ( \" %s \\n \" % strconv ( item ))","title":"listToFile"},{"location":"utils/#scprint2.utils.utils.prepare_device","text":"setup GPU device if available. get gpu device indices which are used for DataParallel Source code in scprint2/utils/utils.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def prepare_device ( n_gpu_use ): \"\"\" setup GPU device if available. get gpu device indices which are used for DataParallel \"\"\" n_gpu = torch . cuda . device_count () if n_gpu_use > 0 and n_gpu == 0 : print ( \"Warning: There's no GPU available on this machine,\" \"training will be performed on CPU.\" ) n_gpu_use = 0 if n_gpu_use > n_gpu : print ( f \"Warning: The number of GPU's configured to use is { n_gpu_use } , but only { n_gpu } are \" \"available on this machine.\" ) n_gpu_use = n_gpu device = torch . device ( \"cuda:0\" if n_gpu_use > 0 else \"cpu\" ) list_ids = list ( range ( n_gpu_use )) return device , list_ids","title":"prepare_device"},{"location":"utils/#scprint2.utils.utils.run_command","text":"run_command runs a command in the shell and prints the output. Parameters: command ( str ) \u2013 The command to be executed in the shell. Returns: int ( int ) \u2013 The return code of the command executed. Source code in scprint2/utils/utils.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def run_command ( command : str , ** kwargs ) -> int : \"\"\" run_command runs a command in the shell and prints the output. Args: command (str): The command to be executed in the shell. Returns: int: The return code of the command executed. \"\"\" process = subprocess . Popen ( command , stdout = subprocess . PIPE , ** kwargs ) while True : if process . poll () is not None : break output = process . stdout . readline () if output : print ( output . strip ()) rc = process . poll () return rc","title":"run_command"},{"location":"utils/#scprint2.utils.utils.selector","text":"Part of Volcano plot: A function to separate tfs from everything else Source code in scprint2/utils/utils.py 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 def selector ( df , valtoextract = [], logfoldtohighlight = 0.15 , pvaltohighlight = 0.1 , minlogfold = 0.15 , minpval = 0.1 , ): \"\"\"Part of Volcano plot: A function to separate tfs from everything else\"\"\" toshow = ( df . pvalue < minpval ) & ( abs ( df . log2FoldChange ) > minlogfold ) df = df [ toshow ] sig = ( df . pvalue < pvaltohighlight ) & ( abs ( df . log2FoldChange ) > logfoldtohighlight ) if valtoextract : not_tf = ~ df . gene_id . isin ( valtoextract ) is_tf = df . gene_id . isin ( valtoextract ) to_plot_not = df [ ~ sig | not_tf ] to_plot_yes = df [ sig & is_tf ] else : to_plot_not = df [ ~ sig ] to_plot_yes = df [ sig ] return to_plot_not , to_plot_yes","title":"selector"},{"location":"utils/#scprint2.utils.utils.set_seed","text":"set random seed. Source code in scprint2/utils/utils.py 83 84 85 86 87 88 89 def set_seed ( seed : int = 42 ): \"\"\"set random seed.\"\"\" random . seed ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False","title":"set_seed"},{"location":"utils/#scprint2.utils.utils.subset_h5ad_by_format","text":"Create new anndata object according to slot info specifications. Arguments: adata -- An AnnData object to subset (required) config -- A Viash config object as read by openproblems.project.read_viash_config (required) arg_name -- The name of the argument in the config file that specifies the output format (required) field_rename_dict -- A mapping between the slots of the source h5ad and the slots of the destination h5ad. Example of slot_mapping: ``` slot_mapping = { \"layers\": { \"counts\": par[\"layer_counts\"], }, \"obs\": { \"cell_type\": par[\"obs_cell_type\"], \"batch\": par[\"obs_batch\"], } } Source code in scprint2/utils/utils.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 def subset_h5ad_by_format ( adata , config , arg_name , field_rename_dict = {}): \"\"\"Create new anndata object according to slot info specifications. Arguments: adata -- An AnnData object to subset (required) config -- A Viash config object as read by openproblems.project.read_viash_config (required) arg_name -- The name of the argument in the config file that specifies the output format (required) field_rename_dict -- A mapping between the slots of the source h5ad and the slots of the destination h5ad. Example of slot_mapping: ``` slot_mapping = { \"layers\": { \"counts\": par[\"layer_counts\"], }, \"obs\": { \"cell_type\": par[\"obs_cell_type\"], \"batch\": par[\"obs_batch\"], } } \"\"\" import anndata as ad import pandas as pd assert isinstance ( adata , ad . AnnData ), \"adata must be an AnnData object\" assert isinstance ( config , dict ), \"config must be a dictionary\" # find argument arg = next ( ( x for x in config [ \"all_arguments\" ] if x [ \"clean_name\" ] == arg_name ), None ) assert arg , f \"Argument ' { arg_name } ' not found in config\" # find file format file_format = ( arg . get ( \"info\" ) or {}) . get ( \"format\" ) assert file_format , f \"Argument ' { arg_name } ' has no .info.format\" # find file format type file_format_type = file_format . get ( \"type\" ) assert file_format_type == \"h5ad\" , \"format must be a h5ad type\" structs = [ \"layers\" , \"obs\" , \"var\" , \"uns\" , \"obsp\" , \"obsm\" , \"varp\" , \"varm\" ] kwargs = {} for struct in structs : struct_format = file_format . get ( struct , {}) struct_rename = field_rename_dict . get ( struct , {}) # fetch data from adata data = {} for field_format in struct_format : dest_name = field_format [ \"name\" ] # where to find the data. if the dest_name is in the rename dict, use the renamed name # as the source name, otherwise use the dest_name as the source name src_name = struct_rename . get ( dest_name , dest_name ) data [ dest_name ] = getattr ( adata , struct )[ src_name ] if len ( data ) > 0 : if struct in [ \"obs\" , \"var\" ]: data = pd . concat ( data , axis = 1 ) kwargs [ struct ] = data elif struct in [ \"obs\" , \"var\" ]: # if no columns need to be copied, we still need an 'obs' and a 'var' # to help determine the shape of the adata kwargs [ struct ] = getattr ( adata , struct ) . iloc [:, []] return ad . AnnData ( ** kwargs )","title":"subset_h5ad_by_format"},{"location":"utils/#scprint2.utils.utils.volcano","text":"Make an interactive volcano plot from Differential Expression analysis tools outputs data: a df with rows genes and cols [log2FoldChange, pvalue, gene_id] folder: str of location where to save the plot, won't save if empty tohighlight: list[str] of genes to highlight in the plot tooltips: list[tuples(str,str)] if user wants tot specify another bokeh tooltip title: str plot title xlabel: str if user wants to specify the title of the x axis ylabel: str if user wants tot specify the title of the y axis maxvalue: float the max -log2(pvalue authorized usefull when managing inf vals) searchbox: bool whether or not to add a searchBox to interactively highlight genes logfoldtohighlight: float min logfoldchange when to diplay points pvaltohighlight: float min pvalue when to diplay points showlabels: bool whether or not to show a text above each datapoint with its label information The bokeh object Source code in scprint2/utils/utils.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 def volcano ( data , folder = \"\" , tohighlight = None , tooltips = [( \"gene\" , \"@gene_id\" )], title = \"volcano plot\" , xlabel = \"log-fold change\" , ylabel = \"-log(Q)\" , maxvalue = 100 , searchbox = False , logfoldtohighlight = 0.15 , pvaltohighlight = 0.1 , showlabels = False , ): \"\"\" Make an interactive volcano plot from Differential Expression analysis tools outputs Args: ----- data: a df with rows genes and cols [log2FoldChange, pvalue, gene_id] folder: str of location where to save the plot, won't save if empty tohighlight: list[str] of genes to highlight in the plot tooltips: list[tuples(str,str)] if user wants tot specify another bokeh tooltip title: str plot title xlabel: str if user wants to specify the title of the x axis ylabel: str if user wants tot specify the title of the y axis maxvalue: float the max -log2(pvalue authorized usefull when managing inf vals) searchbox: bool whether or not to add a searchBox to interactively highlight genes logfoldtohighlight: float min logfoldchange when to diplay points pvaltohighlight: float min pvalue when to diplay points showlabels: bool whether or not to show a text above each datapoint with its label information Returns: -------- The bokeh object \"\"\" to_plot_not , to_plot_yes = selector ( data , tohighlight if tohighlight is not None else [], logfoldtohighlight , pvaltohighlight , ) hover = HoverTool ( tooltips = tooltips , name = \"circles\" ) # Create figure p = figure ( title = title , width = 650 , height = 450 ) p . xgrid . grid_line_color = \"white\" p . ygrid . grid_line_color = \"white\" p . xaxis . axis_label = xlabel p . yaxis . axis_label = ylabel # Add the hover tool p . add_tools ( hover ) p , source1 = add_points ( p , to_plot_not , \"log2FoldChange\" , \"pvalue\" , color = \"#1a9641\" , maxvalue = maxvalue ) p , source2 = add_points ( p , to_plot_yes , \"log2FoldChange\" , \"pvalue\" , color = \"#fc8d59\" , alpha = 0.6 , outline = True , maxvalue = maxvalue , ) if showlabels : labels = LabelSet ( x = \"log2FoldChange\" , y = \"transformed_q\" , text_font_size = \"7pt\" , text = \"gene_id\" , level = \"glyph\" , x_offset = 5 , y_offset = 5 , source = source2 , # renderers=\"canvas\", ) p . add_layout ( labels ) if searchbox : text = TextInput ( title = \"text\" , value = \"gene\" ) text . js_on_change ( \"value\" , CustomJS ( args = dict ( source = source1 ), code = \"\"\" var data = source.data var value = cb_obj.value var gene_id = data.gene_id var a = -1 for (let i=0; i < gene_id.length; i++) { if ( gene_id[i]===value ) { a=i; console.log(i); data.size[i]=7; data.alpha[i]=1; data.color[i]='#fc8d59' } } source.data = data console.log(source) console.log(cb_obj) source.change.emit() console.log(source) \"\"\" , ), ) p = column ( text , p ) p . output_backend = \"svg\" if folder : save ( p , folder + title . replace ( \" \" , \"_\" ) + \"_volcano.html\" ) try : p . output_backend = \"svg\" export_svg ( p , filename = folder + title . replace ( \" \" , \"_\" ) + \"_volcano.svg\" ) except ( RuntimeError , Exception ) as e : print ( f \"Could not save SVG: { e } \" ) try : show ( p ) except Exception as e : print ( f \"Could not show plot: { e } \" ) return p","title":"volcano"},{"location":"utils/#scprint2.utils.get_seq","text":"Functions: Name Description load_fasta_species Downloads and caches FASTA files for a given species from the Ensembl FTP server. seq Fetch nucleotide or amino acid sequence (FASTA) of a gene (and all its isoforms) or transcript by Ensembl, WormBase, or FlyBase ID. subset_fasta subset_fasta: creates a new fasta file with only the sequence which names contain one of gene_names","title":"get_seq"},{"location":"utils/#scprint2.utils.get_seq.load_fasta_species","text":"Downloads and caches FASTA files for a given species from the Ensembl FTP server. Parameters: species ( str , default: 'homo_sapiens' ) \u2013 The species name for which to download FASTA files. Defaults to \"homo_sapiens\". output_path ( str , default: '/tmp/data/fasta/' ) \u2013 The local directory path where the FASTA files will be saved. Defaults to \"/tmp/data/fasta/\". cache ( bool , default: True ) \u2013 If True, use cached files if they exist. If False, re-download the files. Defaults to True. Source code in scprint2/utils/get_seq.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def load_fasta_species ( species : str = \"homo_sapiens\" , output_path : str = \"/tmp/data/fasta/\" , load : List [ str ] = [ \"pep\" , \"ncrna\" , \"cds\" ], cache : bool = True , ) -> None : \"\"\" Downloads and caches FASTA files for a given species from the Ensembl FTP server. Args: species (str, optional): The species name for which to download FASTA files. Defaults to \"homo_sapiens\". output_path (str, optional): The local directory path where the FASTA files will be saved. Defaults to \"/tmp/data/fasta/\". cache (bool, optional): If True, use cached files if they exist. If False, re-download the files. Defaults to True. \"\"\" ftp = ftplib . FTP ( \"ftp.ensembl.org\" ) ftp . login () local_file_path = [] try : ftp . cwd ( \"/pub/release-110/fasta/\" + species + \"/pep/\" ) types = \"animals\" except ftplib . error_perm : try : ftp = ftplib . FTP ( \"ftp.ensemblgenomes.ebi.ac.uk\" ) ftp . login () ftp . cwd ( \"/pub/plants/release-60/fasta/\" + species + \"/pep/\" ) types = \"plants\" except ftplib . error_perm : try : ftp . cwd ( \"/pub/metazoa/release-60/fasta/\" + species + \"/pep/\" ) types = \"metazoa\" except ftplib . error_perm : raise ValueError ( f \"Species { species } not found in Ensembl or Ensembl Genomes.\" ) os . makedirs ( output_path , exist_ok = True ) if \"pep\" in load : file = list_files ( ftp , \".all.fa.gz\" )[ 0 ] local_file_path . append ( output_path + file ) if not os . path . exists ( local_file_path [ - 1 ]) or not cache : with open ( local_file_path [ - 1 ], \"wb\" ) as local_file : ftp . retrbinary ( \"RETR \" + file , local_file . write ) # ncRNA if \"ncrna\" in load : if types == \"animals\" : ftp . cwd ( \"/pub/release-110/fasta/\" + species + \"/ncrna/\" ) elif types == \"plants\" : ftp . cwd ( \"/pub/plants/release-60/fasta/\" + species + \"/ncrna/\" ) file = list_files ( ftp , \".ncrna.fa.gz\" )[ 0 ] local_file_path . append ( output_path + file ) if not os . path . exists ( local_file_path [ - 1 ]) or not cache : with open ( local_file_path [ - 1 ], \"wb\" ) as local_file : ftp . retrbinary ( \"RETR \" + file , local_file . write ) # CDNA: if \"cdna\" in load : if types == \"animals\" : ftp . cwd ( \"/pub/release-110/fasta/\" + species + \"/cdna/\" ) elif types == \"plants\" : ftp . cwd ( \"/pub/plants/release-60/fasta/\" + species + \"/cdna/\" ) file = list_files ( ftp , \".cdna.all.fa.gz\" )[ 0 ] local_file_path . append ( output_path + file ) if not os . path . exists ( local_file_path [ - 1 ]) or not cache : with open ( local_file_path [ - 1 ], \"wb\" ) as local_file : ftp . retrbinary ( \"RETR \" + file , local_file . write ) ftp . quit () return local_file_path","title":"load_fasta_species"},{"location":"utils/#scprint2.utils.get_seq.seq","text":"Fetch nucleotide or amino acid sequence (FASTA) of a gene (and all its isoforms) or transcript by Ensembl, WormBase, or FlyBase ID. Parameters: ens_ids ( Union [ str , List [ str ]] ) \u2013 One or more Ensembl IDs (passed as string or list of strings). Also supports WormBase and FlyBase IDs. translate ( bool , default: False ) \u2013 Defines whether nucleotide or amino acid sequences are returned. Defaults to False (returns nucleotide sequences). Nucleotide sequences are fetched from the Ensembl REST API server. Amino acid sequences are fetched from the UniProt REST API server. isoforms ( bool , default: False ) \u2013 If True, returns the sequences of all known transcripts. Defaults to False. (Only for gene IDs.) parallel ( bool , default: True ) \u2013 If True, fetches sequences in parallel. Defaults to True. save ( bool , default: False ) \u2013 If True, saves output FASTA to current directory. Defaults to False. transcribe ( bool , default: None ) \u2013 Deprecated. Use 'translate' instead. seqtype ( str , default: None ) \u2013 Deprecated. Use 'translate' instead. verbose ( bool , default: True ) \u2013 If True, prints progress information. Defaults to True. Returns: List [ str ] \u2013 List[str]: A list containing the requested sequences, or a FASTA file if 'save' is True. Raises: ValueError \u2013 If an invalid Ensembl ID is provided. Source code in scprint2/utils/get_seq.py 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 def seq ( ens_ids : Union [ str , List [ str ]], translate : bool = False , isoforms : bool = False , parallel : bool = True , save : bool = False , transcribe : Optional [ bool ] = None , seqtype : Optional [ str ] = None , verbose : bool = True , ) -> List [ str ]: \"\"\" Fetch nucleotide or amino acid sequence (FASTA) of a gene (and all its isoforms) or transcript by Ensembl, WormBase, or FlyBase ID. Args: ens_ids (Union[str, List[str]]): One or more Ensembl IDs (passed as string or list of strings). Also supports WormBase and FlyBase IDs. translate (bool, optional): Defines whether nucleotide or amino acid sequences are returned. Defaults to False (returns nucleotide sequences). Nucleotide sequences are fetched from the Ensembl REST API server. Amino acid sequences are fetched from the UniProt REST API server. isoforms (bool, optional): If True, returns the sequences of all known transcripts. Defaults to False. (Only for gene IDs.) parallel (bool, optional): If True, fetches sequences in parallel. Defaults to True. save (bool, optional): If True, saves output FASTA to current directory. Defaults to False. transcribe (bool, optional): Deprecated. Use 'translate' instead. seqtype (str, optional): Deprecated. Use 'translate' instead. verbose (bool, optional): If True, prints progress information. Defaults to True. Returns: List[str]: A list containing the requested sequences, or a FASTA file if 'save' is True. Raises: ValueError: If an invalid Ensembl ID is provided. \"\"\" # Handle deprecated arguments if seqtype : logging . error ( \"'seqtype' argument deprecated! Please use True/False argument 'translate' instead.\" ) return if transcribe : translate = transcribe ## Clean up arguments # Clean up Ensembl IDs # If single Ensembl ID passed as string, convert to list if type ( ens_ids ) is str : ens_ids = [ ens_ids ] # Remove Ensembl ID version if passed ens_ids_clean = [] temp = 0 for ensembl_ID in ens_ids : # But only for Ensembl ID (and not for flybase/wormbase IDs) if ensembl_ID . startswith ( \"ENS\" ): ens_ids_clean . append ( ensembl_ID . split ( \".\" )[ 0 ]) if \".\" in ensembl_ID and temp == 0 : if verbose : logging . info ( \"We noticed that you may have passed a version number with your Ensembl ID. \\n \" \"Please note that gget seq will return information linked to the latest Ensembl ID version.\" ) temp = + 1 else : ens_ids_clean . append ( ensembl_ID ) # Initiate empty 'fasta' fasta = [] ## Fetch nucleotide sequece if translate is False : # Define Ensembl REST API server server = ENSEMBL_REST_API # Define type of returned content from REST content_type = \"application/json\" # Initiate dictionary to save results for all IDs in master_dict = {} # Query REST APIs from https://rest.ensembl.org/ for ensembl_ID in ens_ids_clean : # Create dict to save query results results_dict = { ensembl_ID : {}} # If isoforms False, just fetch sequences of passed Ensembl ID if isoforms is False : # sequence/id/ query: Request sequence by stable identifier query = \"sequence/id/\" + ensembl_ID + \"?\" # Try if query valid try : # Submit query; this will throw RuntimeError if ID not found df_temp = rest_query ( server , query , content_type ) # Delete superfluous entries keys_to_delete = [ \"query\" , \"id\" , \"version\" , \"molecule\" ] for key in keys_to_delete : # Pop keys, None -> do not raise an error if key to delete not found df_temp . pop ( key , None ) # Add results to main dict results_dict [ ensembl_ID ] . update ({ \"seq\" : df_temp }) if verbose : logging . info ( f \"Requesting nucleotide sequence of { ensembl_ID } from Ensembl.\" ) except RuntimeError : logging . error ( f \"ID { ensembl_ID } not found. Please double-check spelling/arguments and try again.\" ) # If isoforms true, fetch sequences of isoforms instead if isoforms is True : # Get ID type (gene, transcript, ...) using gget info info_df = info ( ensembl_ID , verbose = False , pdb = False , ncbi = False , uniprot = False ) # Check if Ensembl ID was found if isinstance ( info_df , type ( None )): logging . warning ( f \"ID ' { ensembl_ID } ' not found. Please double-check spelling/arguments and try again.\" ) continue ens_ID_type = info_df . loc [ ensembl_ID ][ \"object_type\" ] # If the ID is a gene, get the IDs of all its transcripts if ens_ID_type == \"Gene\" : if verbose : logging . info ( f \"Requesting nucleotide sequences of all transcripts of { ensembl_ID } from Ensembl.\" ) for transcipt_id in info_df . loc [ ensembl_ID ][ \"all_transcripts\" ]: # Remove version number for Ensembl IDs (not for flybase/wormbase IDs) if transcipt_id . startswith ( \"ENS\" ): transcipt_id = transcipt_id . split ( \".\" )[ 0 ] # Try if query is valid try : # Define the REST query query = \"sequence/id/\" + transcipt_id + \"?\" # Submit query df_temp = rest_query ( server , query , content_type ) # Delete superfluous entries keys_to_delete = [ \"query\" , \"version\" , \"molecule\" ] for key in keys_to_delete : # Pop keys, None -> do not raise an error if key to delete not found df_temp . pop ( key , None ) # Add results to main dict results_dict [ ensembl_ID ] . update ( { f \" { transcipt_id } \" : df_temp } ) except RuntimeError : logging . error ( f \"ID { transcipt_id } not found. \" \"Please double-check spelling/arguments and try again.\" ) # If isoform true, but ID is not a gene; ignore the isoform parameter else : # Try if query is valid try : # Define the REST query query = \"sequence/id/\" + ensembl_ID + \"?\" # Submit query df_temp = rest_query ( server , query , content_type ) # Delete superfluous entries keys_to_delete = [ \"query\" , \"id\" , \"version\" , \"molecule\" ] for key in keys_to_delete : # Pop keys, None -> do not raise an error if key to delete not found df_temp . pop ( key , None ) # Add results to main dict results_dict [ ensembl_ID ] . update ({ \"seq\" : df_temp }) logging . info ( f \"Requesting nucleotide sequence of { ensembl_ID } from Ensembl.\" ) logging . warning ( \"The isoform option only applies to gene IDs.\" ) except RuntimeError : logging . error ( f \"ID { ensembl_ID } not found. \" \"Please double-check spelling/arguments and try again.\" ) # Add results to master dict master_dict . update ( results_dict ) # Build FASTA file for ens_ID in master_dict : for key in master_dict [ ens_ID ] . keys (): if key == \"seq\" : fasta . append ( \">\" + ens_ID + \" \" + master_dict [ ens_ID ][ key ][ \"desc\" ]) fasta . append ( master_dict [ ens_ID ][ key ][ \"seq\" ]) else : fasta . append ( \">\" + master_dict [ ens_ID ][ key ][ \"id\" ] + \" \" + master_dict [ ens_ID ][ key ][ \"desc\" ] ) fasta . append ( master_dict [ ens_ID ][ key ][ \"seq\" ]) ## Fetch amino acid sequences from UniProt if translate is True : if isoforms is False : # List to collect transcript IDs trans_ids = [] # Get ID type (gene, transcript, ...) using gget info info_df = info ( ens_ids_clean , verbose = False , pdb = False , ncbi = False , uniprot = False ) # Check that Ensembl ID was found missing = set ( ens_ids_clean ) - set ( info_df . index . values ) if len ( missing ) > 0 : logging . warning ( f \" { str ( missing ) } IDs not found. Please double-check spelling/arguments.\" ) ens_ID_type = info_df . loc [ ens_ids_clean [ 0 ]][ \"object_type\" ] # If the ID is a gene, use the ID of its canonical transcript if ens_ID_type == \"Gene\" : # Get ID of canonical transcript for ensembl_ID in info_df . index . values : can_trans = info_df . loc [ ensembl_ID ][ \"canonical_transcript\" ] if ensembl_ID . startswith ( \"ENS\" ): # Remove Ensembl ID version from transcript IDs and append to transcript IDs list temp_trans_id = can_trans . split ( \".\" )[ 0 ] trans_ids . append ( temp_trans_id ) elif ensembl_ID . startswith ( \"WB\" ): # Remove added \".\" at the end of transcript IDs temp_trans_id = \".\" . join ( can_trans . split ( \".\" )[: - 1 ]) # # For WormBase transcript IDs, also remove the version number for submission to UniProt API # temp_trans_id = \".\".join(temp_trans_id1.split(\".\")[:-1]) trans_ids . append ( temp_trans_id ) else : # Remove added \".\" at the end of other transcript IDs temp_trans_id = \".\" . join ( can_trans . split ( \".\" )[: - 1 ]) trans_ids . append ( temp_trans_id ) if verbose : logging . info ( f \"Requesting amino acid sequence of the canonical transcript { temp_trans_id } of gene { ensembl_ID } from UniProt.\" ) # If the ID is a transcript, append the ID directly elif ens_ID_type == \"Transcript\" : # # For WormBase transcript IDs, remove the version number for submission to UniProt API # if ensembl_ID.startswith(\"T\"): # trans_ids.append(\".\".join(ensembl_ID.split(\".\")[:-1])) # else: trans_ids = ens_ids_clean if verbose : logging . info ( f \"Requesting amino acid sequence of { trans_ids } from UniProt.\" ) else : logging . warning ( \"ensembl_IDs not recognized as either a gene or transcript ID. It will not be included in the UniProt query.\" ) # Fetch the amino acid sequences of the transcript Ensembl IDs df_uniprot = get_uniprot_seqs ( UNIPROT_REST_API , trans_ids ) # Add info_df.loc[ensembl_ID] to df_uniprot by joining on \"canonical_transcript\" / \"gene_name\" respectively info_df . set_index ( \"canonical_transcript\" , inplace = True ) df_uniprot . loc [:, \"gene_id\" ] = info_df . loc [ df_uniprot [ \"query\" ], \"gene_name\" ] . values if isoforms is True : # List to collect transcript IDs trans_ids = [] for ensembl_ID in ens_ids_clean : # Get ID type (gene, transcript, ...) using gget info info_df = info ( ensembl_ID , verbose = False , pdb = False , ncbi = False , uniprot = False ) # Check that Ensembl ID was found if isinstance ( info_df , type ( None )): logging . warning ( f \"ID ' { ensembl_ID } ' not found. Please double-check spelling/arguments.\" ) continue ens_ID_type = info_df . loc [ ensembl_ID ][ \"object_type\" ] # If the ID is a gene, get the IDs of all isoforms if ens_ID_type == \"Gene\" : # Get the IDs of all transcripts from the gget info results for transcipt_id in info_df . loc [ ensembl_ID ][ \"all_transcripts\" ]: if ensembl_ID . startswith ( \"ENS\" ): # Append transcript ID (without Ensembl version number) to list of transcripts to fetch trans_ids . append ( transcipt_id . split ( \".\" )[ 0 ]) # elif ensembl_ID.startswith(\"WB\"): # # For WormBase transcript IDs, remove the version number for submission to UniProt API # temp_trans_id = \".\".join(transcipt_id.split(\".\")[:-1]) # trans_ids.append(temp_trans_id) else : # Note: No need to remove the added \".\" at the end of unversioned transcripts here, because \"all_transcripts\" are returned without it trans_ids . append ( transcipt_id ) if verbose : logging . info ( f \"Requesting amino acid sequences of all transcripts of gene { ensembl_ID } from UniProt.\" ) elif ens_ID_type == \"Transcript\" : # # For WormBase transcript IDs, remove the version number for submission to UniProt API # if ensembl_ID.startswith(\"T\"): # trans_ids.append(\".\".join(ensembl_ID.split(\".\")[:-1])) # else: trans_ids . append ( ensembl_ID ) if verbose : logging . info ( f \"Requesting amino acid sequence of { ensembl_ID } from UniProt.\" ) logging . warning ( \"The isoform option only applies to gene IDs.\" ) else : logging . warning ( f \" { ensembl_ID } not recognized as either a gene or transcript ID. It will not be included in the UniProt query.\" ) # Fetch amino acid sequences of all isoforms from the UniProt REST API df_uniprot = get_uniprot_seqs ( UNIPROT_REST_API , trans_ids ) # Check if any results were found if len ( df_uniprot ) < 1 : logging . error ( \"No UniProt amino acid sequences were found for these ID(s).\" ) else : # Build FASTA file from UniProt results for ( uniprot_id , query_ensembl_id , gene_name , organism , sequence_length , uniprot_seq , ) in zip ( df_uniprot [ \"uniprot_id\" ] . values , df_uniprot [ \"query\" ] . values , df_uniprot [ \"gene_name\" ] . values , df_uniprot [ \"gene_id\" ] . values , df_uniprot [ \"organism\" ] . values , df_uniprot [ \"sequence_length\" ] . values , df_uniprot [ \"sequence\" ] . values , ): fasta . append ( \">\" + str ( query_ensembl_id ) + \" uniprot_id: \" + str ( uniprot_id ) + \" ensembl_id: \" + str ( query_ensembl_id ) + \" gene_name: \" + str ( gene_name ) + \" organism: \" + str ( organism ) + \" sequence_length: \" + str ( sequence_length ) ) fasta . append ( str ( uniprot_seq )) # Save if save : file = open ( \"gget_seq_results.fa\" , \"w\" ) for element in fasta : file . write ( element + \" \\n \" ) file . close () # missed samples return ( set ( trans_ids ) - set ( df_uniprot [ \"query\" ] . values )) | set ( missing ) return fasta","title":"seq"},{"location":"utils/#scprint2.utils.get_seq.subset_fasta","text":"subset_fasta: creates a new fasta file with only the sequence which names contain one of gene_names Parameters: gene_tosubset ( set , default: None ) \u2013 A set of gene names to subset from the original FASTA file. fasta_path ( str , default: None ) \u2013 The path to the original FASTA file. subfasta_path ( str , default: './data/fasta/subset.fa' ) \u2013 The path to save the subsetted FASTA file. Defaults to \"./data/fasta/subset.fa\". drop_unknown_seq ( bool , default: True ) \u2013 If True, drop sequences containing unknown amino acids (denoted by '*'). Defaults to True. subset_protein_coding ( bool , default: True ) \u2013 If True, subset only protein coding genes. Defaults to True. Returns: set: A set of gene names that were found and included in the subsetted FASTA file. Raises: ValueError \u2013 If a gene name does not start with \"ENS\". Source code in scprint2/utils/get_seq.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def subset_fasta ( gene_tosubset : set = None , fasta_path : str = None , subfasta_path : str = \"./data/fasta/subset.fa\" , drop_unknown_seq : bool = True , subset_protein_coding : bool = True , ) -> set : \"\"\" subset_fasta: creates a new fasta file with only the sequence which names contain one of gene_names Args: gene_tosubset (set): A set of gene names to subset from the original FASTA file. fasta_path (str): The path to the original FASTA file. subfasta_path (str, optional): The path to save the subsetted FASTA file. Defaults to \"./data/fasta/subset.fa\". drop_unknown_seq (bool, optional): If True, drop sequences containing unknown amino acids (denoted by '*'). Defaults to True. subset_protein_coding (bool, optional): If True, subset only protein coding genes. Defaults to True. Returns: set: A set of gene names that were found and included in the subsetted FASTA file. Raises: ValueError: If a gene name does not start with \"ENS\". \"\"\" dup = set () weird = 0 nc = 0 genes_found = set () gene_tosubset = set ( gene_tosubset ) if gene_tosubset else [] names = [] with ( open ( fasta_path , \"r\" ) as original_fasta , open ( subfasta_path , \"w\" ) as subset_fasta , ): for record in SeqIO . parse ( original_fasta , \"fasta\" ): gene_name = ( record . description . split ( \" gene:\" )[ 1 ] . split ( \" \" )[ 0 ] . split ( \".\" )[ 0 ] ) gene_biotype = record . description . split ( \"gene_biotype:\" )[ 1 ] . split ( \" \" )[ 0 ] if \"gene_symbol:\" not in record . description : gene_symbol = gene_name else : gene_symbol = record . description . split ( \"gene_symbol:\" )[ 1 ] . split ( \" \" )[ 0 ] if \"description:\" not in record . description : description = \"\" else : description = record . description . split ( \"description:\" )[ 1 ] names . append ([ gene_name , gene_biotype , record . id , gene_symbol , description ]) if subset_protein_coding and gene_biotype != \"protein_coding\" : nc += 1 continue if len ( gene_tosubset ) == 0 or gene_name in gene_tosubset : if drop_unknown_seq : if \"*\" in record . seq : weird += 1 continue if gene_name in genes_found : dup . add ( gene_name ) continue record . description = \"\" record . id = gene_name SeqIO . write ( record , subset_fasta , \"fasta\" ) genes_found . add ( gene_name ) print ( len ( dup ), \" genes had duplicates\" ) print ( \"dropped\" , weird , \"weird sequences\" ) print ( \"dropped\" , nc , \"non-coding sequences\" ) return genes_found , pd . DataFrame ( names , columns = [ \"name\" , \"biotype\" , \"ensembl_id\" , \"gene_symbol\" , \"description\" ] )","title":"subset_fasta"},{"location":"utils/#scprint2.utils.graph_refinement","text":"Graph-regularized logit refinement implementation. This module implements the GRIT (Graph-Regularized logIT) refinement method for improving cell type predictions using graph structure. Functions: Name Description build_knn_graph Build a k-nearest neighbor graph and store it in adata.obsp. graph_regularized_logit_refinement Refine logits using graph-regularized optimization. test_graph_refinement Test function for graph refinement. zero_shot_annotation_with_refinement Perform zero-shot cell type annotation with graph refinement.","title":"graph_refinement"},{"location":"utils/#scprint2.utils.graph_refinement.build_knn_graph","text":"Build a k-nearest neighbor graph and store it in adata.obsp. Parameters: adata ( AnnData ) \u2013 AnnData object representation_key ( str , default: 'X_pca' ) \u2013 Key in adata.obsm for the representation to use. Defaults to \"X_pca\". n_neighbors ( int , default: 15 ) \u2013 Number of nearest neighbors. Defaults to 15. metric ( str , default: 'euclidean' ) \u2013 Distance metric for nearest neighbor search. Defaults to \"euclidean\". Returns: AnnData \u2013 anndata.AnnData: Updated AnnData object with connectivity matrix Source code in scprint2/utils/graph_refinement.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def build_knn_graph ( adata : anndata . AnnData , representation_key : str = \"X_pca\" , n_neighbors : int = 15 , metric : str = \"euclidean\" , ) -> anndata . AnnData : \"\"\" Build a k-nearest neighbor graph and store it in adata.obsp. Args: adata (anndata.AnnData): AnnData object representation_key (str): Key in adata.obsm for the representation to use. Defaults to \"X_pca\". n_neighbors (int): Number of nearest neighbors. Defaults to 15. metric (str): Distance metric for nearest neighbor search. Defaults to \"euclidean\". Returns: anndata.AnnData: Updated AnnData object with connectivity matrix \"\"\" try : import scanpy as sc except ImportError : raise ImportError ( \"scanpy is required for building k-NN graphs\" ) # Compute neighbors sc . pp . neighbors ( adata , use_rep = representation_key , n_neighbors = n_neighbors , metric = metric , ) return adata","title":"build_knn_graph"},{"location":"utils/#scprint2.utils.graph_refinement.graph_regularized_logit_refinement","text":"Refine logits using graph-regularized optimization. Optimized version that solves for all classes simultaneously. This function implements the optimization problem: P\u0303 = arg min_P ||P - P\u2080||\u00b2_F + \u03bb Tr(P^T L P) where P\u2080 are the initial logits, L is the graph Laplacian, and \u03bb controls the strength of regularization. The solution has a closed form: P\u0303 = (I + \u03bbL)\u207b\u00b9P\u2080 Parameters: pred ( ndarray ) \u2013 Initial logits of shape (n_cells, n_classes) adata ( AnnData ) \u2013 AnnData object containing graph connectivity connectivity_key ( str , default: 'connectivities' ) \u2013 Key in adata.obsp for connectivity matrix lambda_reg ( float , default: 0.1 ) \u2013 Regularization strength \u03bb > 0 use_laplacian ( bool , default: True ) \u2013 If True, use graph Laplacian; if False, use adjacency matrix Returns: ndarray \u2013 np.ndarray: Refined logits of same shape as input pred Raises: ValueError \u2013 If connectivity matrix is not found or dimensions don't match KeyError \u2013 If connectivity_key is not in adata.obsp Source code in scprint2/utils/graph_refinement.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def graph_regularized_logit_refinement ( pred : np . ndarray , adata : anndata . AnnData , connectivity_key : str = \"connectivities\" , lambda_reg : float = 0.1 , use_laplacian : bool = True , ) -> np . ndarray : \"\"\" Refine logits using graph-regularized optimization. Optimized version that solves for all classes simultaneously. This function implements the optimization problem: P\u0303 = arg min_P ||P - P\u2080||\u00b2_F + \u03bb Tr(P^T L P) where P\u2080 are the initial logits, L is the graph Laplacian, and \u03bb controls the strength of regularization. The solution has a closed form: P\u0303 = (I + \u03bbL)\u207b\u00b9P\u2080 Args: pred (np.ndarray): Initial logits of shape (n_cells, n_classes) adata (anndata.AnnData): AnnData object containing graph connectivity connectivity_key (str): Key in adata.obsp for connectivity matrix lambda_reg (float): Regularization strength \u03bb > 0 use_laplacian (bool): If True, use graph Laplacian; if False, use adjacency matrix Returns: np.ndarray: Refined logits of same shape as input pred Raises: ValueError: If connectivity matrix is not found or dimensions don't match KeyError: If connectivity_key is not in adata.obsp \"\"\" # Validate inputs if connectivity_key not in adata . obsp : raise KeyError ( f \"Connectivity key ' { connectivity_key } ' not found in adata.obsp\" ) A = adata . obsp [ connectivity_key ] n_cells , n_classes = pred . shape # Check dimensions if A . shape [ 0 ] != n_cells or A . shape [ 1 ] != n_cells : raise ValueError ( f \"Connectivity matrix shape { A . shape } doesn't match number of cells { n_cells } \" ) # Ensure adjacency matrix is symmetric and sparse if not sp . issparse ( A ): A = sp . csr_matrix ( A ) # Make symmetric if not already A = ( A + A . T ) / 2 if use_laplacian : # Compute graph Laplacian: L = D - A # where D is the diagonal degree matrix degrees = np . array ( A . sum ( axis = 1 )) . flatten () D = sp . diags ( degrees , format = \"csr\" ) L = D - A else : # Use adjacency matrix directly L = A identity_matrix = sp . identity ( n_cells , format = \"csr\" ) system_matrix = identity_matrix + lambda_reg * L # Solve for all classes at once instead of looping # spsolve can handle multiple right-hand sides refined_pred = spsolve ( system_matrix , pred ) # Handle the case where spsolve returns 1D array for single class if refined_pred . ndim == 1 and n_classes == 1 : refined_pred = refined_pred . reshape ( - 1 , 1 ) elif refined_pred . ndim == 1 : refined_pred = refined_pred . reshape ( n_cells , n_classes ) return refined_pred","title":"graph_regularized_logit_refinement"},{"location":"utils/#scprint2.utils.graph_refinement.test_graph_refinement","text":"Test function for graph refinement. Source code in scprint2/utils/graph_refinement.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def test_graph_refinement (): \"\"\"Test function for graph refinement.\"\"\" # Create synthetic data n_cells , n_classes = 100 , 5 # Random logits np . random . seed ( 42 ) pred = np . random . randn ( n_cells , n_classes ) # Create synthetic AnnData with connectivity adata = anndata . AnnData ( X = np . random . randn ( n_cells , 50 )) # Create a random sparse connectivity matrix from scipy.sparse import random connectivity = random ( n_cells , n_cells , density = 0.1 , format = \"csr\" ) connectivity = ( connectivity + connectivity . T ) / 2 # Make symmetric adata . obsp [ \"connectivities\" ] = connectivity # Test refinement refined_pred = graph_regularized_logit_refinement ( pred , adata , lambda_reg = 0.1 ) print ( f \"Original logits shape: { pred . shape } \" ) print ( f \"Refined logits shape: { refined_pred . shape } \" ) print ( f \"Logits changed: { not np . allclose ( pred , refined_pred ) } \" ) # Test zero-shot annotation predictions , probabilities = zero_shot_annotation_with_refinement ( pred , adata , return_probabilities = True ) print ( f \"Predictions shape: { predictions . shape } \" ) print ( f \"Probabilities shape: { probabilities . shape } \" ) print ( f \"Predicted classes: { np . unique ( predictions ) } \" )","title":"test_graph_refinement"},{"location":"utils/#scprint2.utils.graph_refinement.zero_shot_annotation_with_refinement","text":"Perform zero-shot cell type annotation with graph refinement. This function first refines the logits using graph regularization, then performs argmax to get final predictions. Parameters: pred ( ndarray ) \u2013 Initial logits of shape (n_cells, n_classes) adata ( AnnData ) \u2013 AnnData object containing graph connectivity connectivity_key ( str , default: 'connectivities' ) \u2013 Key in adata.obsp for connectivity matrix lambda_reg ( float , default: 0.1 ) \u2013 Regularization strength return_probabilities ( bool , default: False ) \u2013 If True, also return refined probabilities Returns: Union [ ndarray , tuple ] \u2013 np.ndarray or tuple: If return_probabilities is False, returns array of predicted class indices. If True, returns tuple of (predictions, refined_probabilities) Source code in scprint2/utils/graph_refinement.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def zero_shot_annotation_with_refinement ( pred : np . ndarray , adata : anndata . AnnData , connectivity_key : str = \"connectivities\" , representation_key : str = \"X_pca\" , n_neighbors : int = 15 , metric : str = \"euclidean\" , lambda_reg : float = 0.1 , return_probabilities : bool = False , return_raw : bool = False , ) -> Union [ np . ndarray , tuple ]: \"\"\" Perform zero-shot cell type annotation with graph refinement. This function first refines the logits using graph regularization, then performs argmax to get final predictions. Args: pred (np.ndarray): Initial logits of shape (n_cells, n_classes) adata (anndata.AnnData): AnnData object containing graph connectivity connectivity_key (str): Key in adata.obsp for connectivity matrix lambda_reg (float): Regularization strength return_probabilities (bool): If True, also return refined probabilities Returns: np.ndarray or tuple: If return_probabilities is False, returns array of predicted class indices. If True, returns tuple of (predictions, refined_probabilities) \"\"\" if pred is type ( pd . DataFrame ): pred = pred . values if adata . obsp . get ( connectivity_key ) is None : # Refine logits adata = build_knn_graph ( adata = adata , representation_key = representation_key , n_neighbors = n_neighbors , metric = metric , ) connectivity_key = \"connectivities\" print ( adata . obsp ) refined_logits = graph_regularized_logit_refinement ( pred , adata , connectivity_key , lambda_reg ) if return_raw : return refined_logits # Get predictions: g(xi) = arg max_j {P\u0303(i)} predictions = np . argmax ( refined_logits , axis = 1 ) if return_probabilities : # Convert to probabilities using softmax refined_probs = np . exp ( refined_logits ) refined_probs = refined_probs / refined_probs . sum ( axis = 1 , keepdims = True ) return predictions , refined_probs return predictions","title":"zero_shot_annotation_with_refinement"}]}