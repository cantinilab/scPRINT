{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"scprint Awesome Large Transcriptional Model created by Jeremie Kalfon scprint = single cell pretrained regulation inference neural network from transcripts using: Install it from PyPI first have a good version of pytorch installed you might need to make it match your cuda version etc.. We only support torch>=2.0.0 then install laminDB pip install 'lamindb[jupyter,bionty]' then install scPrint pip install scprint I had to install a specific version of pytorch, torchaudio, torchtext.. for my cuda version. My cuda compiler nvcc compiles cuda 11.7. my cuda-smi (api) is currently 12.1. Please install all of it for your cuda version and it should still work. for more information on this, please see [installation.md](installation.md). Usage from lightning.pytorch import Trainer from scprint import scPrint from scdataloader import DataModule ... model = scPrint(...) trainer = Trainer(...) trainer.fit(model, datamodule=datamodule) $ python -m scPrint/__main__.py #or $ scprint fit/train/predict/test for more information on usage please see the documentation in https://jkobject.com/scPrint Development Read the CONTRIBUTING.md file. What is included? \ud83d\udcc3 Documentation structure using mkdocs \ud83e\uddea Testing structure using pytest If you want codecov Reports and Automatic Release to PyPI On the new repository settings->secrets add your PYPI_API_TOKEN and CODECOV_TOKEN (get the tokens on respective websites) \u2705 Code linting using flake8 \ud83d\udcca Code coverage reports using codecov \ud83d\udef3\ufe0f Automatic release to PyPI using twine and github actions. acknowledgement: python template scGPT laminDB","title":"Home"},{"location":"#scprint","text":"Awesome Large Transcriptional Model created by Jeremie Kalfon scprint = single cell pretrained regulation inference neural network from transcripts using:","title":"scprint"},{"location":"#install-it-from-pypi","text":"first have a good version of pytorch installed you might need to make it match your cuda version etc.. We only support torch>=2.0.0 then install laminDB pip install 'lamindb[jupyter,bionty]' then install scPrint pip install scprint I had to install a specific version of pytorch, torchaudio, torchtext.. for my cuda version. My cuda compiler nvcc compiles cuda 11.7. my cuda-smi (api) is currently 12.1. Please install all of it for your cuda version and it should still work. for more information on this, please see [installation.md](installation.md).","title":"Install it from PyPI"},{"location":"#usage","text":"from lightning.pytorch import Trainer from scprint import scPrint from scdataloader import DataModule ... model = scPrint(...) trainer = Trainer(...) trainer.fit(model, datamodule=datamodule) $ python -m scPrint/__main__.py #or $ scprint fit/train/predict/test for more information on usage please see the documentation in https://jkobject.com/scPrint","title":"Usage"},{"location":"#development","text":"Read the CONTRIBUTING.md file.","title":"Development"},{"location":"#what-is-included","text":"\ud83d\udcc3 Documentation structure using mkdocs \ud83e\uddea Testing structure using pytest If you want codecov Reports and Automatic Release to PyPI On the new repository settings->secrets add your PYPI_API_TOKEN and CODECOV_TOKEN (get the tokens on respective websites) \u2705 Code linting using flake8 \ud83d\udcca Code coverage reports using codecov \ud83d\udef3\ufe0f Automatic release to PyPI using twine and github actions. acknowledgement: python template scGPT laminDB","title":"What is included?"},{"location":"loaders/","text":"Documentation for the loaders scprint . loaders . embedder . embed embed embed a set of genes using fasta file and LLMs Parameters: genedf ( DataFrame ) \u2013 A DataFrame containing gene information. organism ( str , default: 'homo_sapiens' ) \u2013 The organism to which the genes belong. Defaults to \"homo_sapiens\". cache ( bool , default: True ) \u2013 If True, the function will use cached data if available. Defaults to True. fasta_path ( str , default: '/tmp/data/fasta/' ) \u2013 The path to the directory where the fasta files are stored. Defaults to \"/tmp/data/fasta/\". embedding_size ( int , default: 512 ) \u2013 The size of the embeddings to be generated. Defaults to 512. Returns: \u2013 pd.DataFrame: Returns a DataFrame containing the protein embeddings, and the RNA embeddings. Source code in scprint/loaders/embedder.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def embed ( genedf : pd . DataFrame , organism : str = \"homo_sapiens\" , cache : bool = True , fasta_path : str = \"/tmp/data/fasta/\" , embedding_size : int = 512 , ): \"\"\" embed embed a set of genes using fasta file and LLMs Args: genedf (pd.DataFrame): A DataFrame containing gene information. organism (str, optional): The organism to which the genes belong. Defaults to \"homo_sapiens\". cache (bool, optional): If True, the function will use cached data if available. Defaults to True. fasta_path (str, optional): The path to the directory where the fasta files are stored. Defaults to \"/tmp/data/fasta/\". embedding_size (int, optional): The size of the embeddings to be generated. Defaults to 512. Returns: pd.DataFrame: Returns a DataFrame containing the protein embeddings, and the RNA embeddings. \"\"\" # given a gene file and organism # load the organism fasta if not already done utils . load_fasta_species ( species = organism , output_path = fasta_path , cache = cache ) # subset the fasta fasta_file = next ( file for file in os . listdir ( fasta_path ) if file . endswith ( \".all.fa.gz\" ) ) protgenedf = genedf [ genedf [ \"biotype\" ] == \"protein_coding\" ] utils . utils . run_command ([ \"gunzip\" , fasta_path + fasta_file ]) utils . subset_fasta ( protgenedf [ \"ensembl_gene_id\" ] . tolist (), subfasta_path = fasta_path + \"subset.fa\" , fasta_path = fasta_path + fasta_file [: - 3 ], drop_unknown_seq = True , ) # subset the gene file # embed prot_embedder = PROTBERT () # TODO: to redebug prot_embeddings = prot_embedder ( fasta_path + \"subset.fa\" , output_folder = fasta_path + \"esm_out/\" , cache = cache ) # load the data and erase / zip the rest utils . utils . run_command ([ \"gzip\" , fasta_path + fasta_file [: - 3 ]]) # return the embedding and gene file # do the same for RNA rnagenedf = genedf [ genedf [ \"biotype\" ] != \"protein_coding\" ] fasta_file = next ( file for file in os . listdir ( fasta_path ) if file . endswith ( \".ncrna.fa.gz\" ) ) utils . utils . run_command ([ \"gunzip\" , fasta_path + fasta_file ]) utils . subset_fasta ( rnagenedf [ \"ensembl_gene_id\" ] . tolist (), subfasta_path = fasta_path + \"subset.ncrna.fa\" , fasta_path = fasta_path + fasta_file [: - 3 ], drop_unknown_seq = True , ) rna_embedder = RNABERT () rna_embeddings = rna_embedder ( fasta_path + \"subset.ncrna.fa\" ) # Check if the sizes of the cembeddings are not the same utils . utils . run_command ([ \"gzip\" , fasta_path + fasta_file [: - 3 ]]) m = AdaptiveAvgPool1d ( embedding_size ) prot_embeddings = pd . DataFrame ( data = m ( torch . tensor ( prot_embeddings . values )), index = prot_embeddings . index ) rna_embeddings = pd . DataFrame ( data = m ( torch . tensor ( rna_embeddings . values )), index = rna_embeddings . index ) # Concatenate the embeddings return pd . concat ([ prot_embeddings , rna_embeddings ]) scprint . loaders . PROTBERT PROTBERT a ghost class to call protein LLMs to encode protein sequences. Parameters: config ( str , default: 'esm-extract' ) \u2013 The configuration for the model. Defaults to \"esm-extract\". pretrained_model ( str , default: 'esm2_t33_650M_UR50D' ) \u2013 The pretrained model to be used. Defaults to \"esm2_t33_650M_UR50D\". Source code in scprint/loaders/protein_embedder.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , config : str = \"esm-extract\" , pretrained_model : str = \"esm2_t33_650M_UR50D\" , ): \"\"\" PROTBERT a ghost class to call protein LLMs to encode protein sequences. Args: config (str, optional): The configuration for the model. Defaults to \"esm-extract\". pretrained_model (str, optional): The pretrained model to be used. Defaults to \"esm2_t33_650M_UR50D\". \"\"\" self . config = config self . pretrained_model = pretrained_model __call__ Call the PROTBERT model on the input file. Parameters: input_file ( str ) \u2013 The input file to be processed. output_folder ( str , default: '/tmp/esm_out/' ) \u2013 The folder where the output will be stored. Defaults to \"/tmp/esm_out/\". cache ( bool , default: True ) \u2013 If True, use cached data if available. Defaults to True. Returns: \u2013 pd.DataFrame: The results of the model as a DataFrame. Source code in scprint/loaders/protein_embedder.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __call__ ( self , input_file , output_folder = \"/tmp/esm_out/\" , cache = True ): \"\"\" Call the PROTBERT model on the input file. Args: input_file (str): The input file to be processed. output_folder (str, optional): The folder where the output will be stored. Defaults to \"/tmp/esm_out/\". cache (bool, optional): If True, use cached data if available. Defaults to True. Returns: pd.DataFrame: The results of the model as a DataFrame. \"\"\" if not os . path . exists ( output_folder ) or not cache : os . makedirs ( output_folder , exist_ok = True ) print ( \"running protbert\" ) cmd = ( self . config + \" \" + self . pretrained_model + \" \" + input_file + \" \" + output_folder + \" --include mean\" ) try : run_command ( cmd , shell = True ) except Exception as e : raise RuntimeError ( \"An error occurred while running the esm-extract command: \" + str ( e ) ) return self . read_results ( output_folder ) read_results Read multiple .pt files in a folder and convert them into a DataFrame. Parameters: output_folder ( str ) \u2013 The folder where the .pt files are stored. Returns: \u2013 pd.DataFrame: The results of the model as a DataFrame. Source code in scprint/loaders/protein_embedder.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def read_results ( self , output_folder ): \"\"\" Read multiple .pt files in a folder and convert them into a DataFrame. Args: output_folder (str): The folder where the .pt files are stored. Returns: pd.DataFrame: The results of the model as a DataFrame. \"\"\" files = os . listdir ( output_folder ) files = [ i for i in files if i . endswith ( \".pt\" )] results = [] for file in files : results . append ( load ( output_folder + file )[ \"mean_representations\" ][ 33 ] . numpy () . tolist () ) return pd . DataFrame ( data = results , index = [ file . split ( \".\" )[ 0 ] for file in files ]) RNABERT.main","title":"Documentation for the loaders"},{"location":"loaders/#documentation-for-the-loaders","text":"","title":"Documentation for the loaders"},{"location":"loaders/#scprint.loaders.embedder.embed","text":"embed embed a set of genes using fasta file and LLMs Parameters: genedf ( DataFrame ) \u2013 A DataFrame containing gene information. organism ( str , default: 'homo_sapiens' ) \u2013 The organism to which the genes belong. Defaults to \"homo_sapiens\". cache ( bool , default: True ) \u2013 If True, the function will use cached data if available. Defaults to True. fasta_path ( str , default: '/tmp/data/fasta/' ) \u2013 The path to the directory where the fasta files are stored. Defaults to \"/tmp/data/fasta/\". embedding_size ( int , default: 512 ) \u2013 The size of the embeddings to be generated. Defaults to 512. Returns: \u2013 pd.DataFrame: Returns a DataFrame containing the protein embeddings, and the RNA embeddings. Source code in scprint/loaders/embedder.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def embed ( genedf : pd . DataFrame , organism : str = \"homo_sapiens\" , cache : bool = True , fasta_path : str = \"/tmp/data/fasta/\" , embedding_size : int = 512 , ): \"\"\" embed embed a set of genes using fasta file and LLMs Args: genedf (pd.DataFrame): A DataFrame containing gene information. organism (str, optional): The organism to which the genes belong. Defaults to \"homo_sapiens\". cache (bool, optional): If True, the function will use cached data if available. Defaults to True. fasta_path (str, optional): The path to the directory where the fasta files are stored. Defaults to \"/tmp/data/fasta/\". embedding_size (int, optional): The size of the embeddings to be generated. Defaults to 512. Returns: pd.DataFrame: Returns a DataFrame containing the protein embeddings, and the RNA embeddings. \"\"\" # given a gene file and organism # load the organism fasta if not already done utils . load_fasta_species ( species = organism , output_path = fasta_path , cache = cache ) # subset the fasta fasta_file = next ( file for file in os . listdir ( fasta_path ) if file . endswith ( \".all.fa.gz\" ) ) protgenedf = genedf [ genedf [ \"biotype\" ] == \"protein_coding\" ] utils . utils . run_command ([ \"gunzip\" , fasta_path + fasta_file ]) utils . subset_fasta ( protgenedf [ \"ensembl_gene_id\" ] . tolist (), subfasta_path = fasta_path + \"subset.fa\" , fasta_path = fasta_path + fasta_file [: - 3 ], drop_unknown_seq = True , ) # subset the gene file # embed prot_embedder = PROTBERT () # TODO: to redebug prot_embeddings = prot_embedder ( fasta_path + \"subset.fa\" , output_folder = fasta_path + \"esm_out/\" , cache = cache ) # load the data and erase / zip the rest utils . utils . run_command ([ \"gzip\" , fasta_path + fasta_file [: - 3 ]]) # return the embedding and gene file # do the same for RNA rnagenedf = genedf [ genedf [ \"biotype\" ] != \"protein_coding\" ] fasta_file = next ( file for file in os . listdir ( fasta_path ) if file . endswith ( \".ncrna.fa.gz\" ) ) utils . utils . run_command ([ \"gunzip\" , fasta_path + fasta_file ]) utils . subset_fasta ( rnagenedf [ \"ensembl_gene_id\" ] . tolist (), subfasta_path = fasta_path + \"subset.ncrna.fa\" , fasta_path = fasta_path + fasta_file [: - 3 ], drop_unknown_seq = True , ) rna_embedder = RNABERT () rna_embeddings = rna_embedder ( fasta_path + \"subset.ncrna.fa\" ) # Check if the sizes of the cembeddings are not the same utils . utils . run_command ([ \"gzip\" , fasta_path + fasta_file [: - 3 ]]) m = AdaptiveAvgPool1d ( embedding_size ) prot_embeddings = pd . DataFrame ( data = m ( torch . tensor ( prot_embeddings . values )), index = prot_embeddings . index ) rna_embeddings = pd . DataFrame ( data = m ( torch . tensor ( rna_embeddings . values )), index = rna_embeddings . index ) # Concatenate the embeddings return pd . concat ([ prot_embeddings , rna_embeddings ])","title":"embed()"},{"location":"loaders/#scprint.loaders.PROTBERT","text":"PROTBERT a ghost class to call protein LLMs to encode protein sequences. Parameters: config ( str , default: 'esm-extract' ) \u2013 The configuration for the model. Defaults to \"esm-extract\". pretrained_model ( str , default: 'esm2_t33_650M_UR50D' ) \u2013 The pretrained model to be used. Defaults to \"esm2_t33_650M_UR50D\". Source code in scprint/loaders/protein_embedder.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , config : str = \"esm-extract\" , pretrained_model : str = \"esm2_t33_650M_UR50D\" , ): \"\"\" PROTBERT a ghost class to call protein LLMs to encode protein sequences. Args: config (str, optional): The configuration for the model. Defaults to \"esm-extract\". pretrained_model (str, optional): The pretrained model to be used. Defaults to \"esm2_t33_650M_UR50D\". \"\"\" self . config = config self . pretrained_model = pretrained_model","title":"PROTBERT"},{"location":"loaders/#scprint.loaders.PROTBERT.__call__","text":"Call the PROTBERT model on the input file. Parameters: input_file ( str ) \u2013 The input file to be processed. output_folder ( str , default: '/tmp/esm_out/' ) \u2013 The folder where the output will be stored. Defaults to \"/tmp/esm_out/\". cache ( bool , default: True ) \u2013 If True, use cached data if available. Defaults to True. Returns: \u2013 pd.DataFrame: The results of the model as a DataFrame. Source code in scprint/loaders/protein_embedder.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __call__ ( self , input_file , output_folder = \"/tmp/esm_out/\" , cache = True ): \"\"\" Call the PROTBERT model on the input file. Args: input_file (str): The input file to be processed. output_folder (str, optional): The folder where the output will be stored. Defaults to \"/tmp/esm_out/\". cache (bool, optional): If True, use cached data if available. Defaults to True. Returns: pd.DataFrame: The results of the model as a DataFrame. \"\"\" if not os . path . exists ( output_folder ) or not cache : os . makedirs ( output_folder , exist_ok = True ) print ( \"running protbert\" ) cmd = ( self . config + \" \" + self . pretrained_model + \" \" + input_file + \" \" + output_folder + \" --include mean\" ) try : run_command ( cmd , shell = True ) except Exception as e : raise RuntimeError ( \"An error occurred while running the esm-extract command: \" + str ( e ) ) return self . read_results ( output_folder )","title":"__call__()"},{"location":"loaders/#scprint.loaders.PROTBERT.read_results","text":"Read multiple .pt files in a folder and convert them into a DataFrame. Parameters: output_folder ( str ) \u2013 The folder where the .pt files are stored. Returns: \u2013 pd.DataFrame: The results of the model as a DataFrame. Source code in scprint/loaders/protein_embedder.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def read_results ( self , output_folder ): \"\"\" Read multiple .pt files in a folder and convert them into a DataFrame. Args: output_folder (str): The folder where the .pt files are stored. Returns: pd.DataFrame: The results of the model as a DataFrame. \"\"\" files = os . listdir ( output_folder ) files = [ i for i in files if i . endswith ( \".pt\" )] results = [] for file in files : results . append ( load ( output_folder + file )[ \"mean_representations\" ][ 33 ] . numpy () . tolist () ) return pd . DataFrame ( data = results , index = [ file . split ( \".\" )[ 0 ] for file in files ])","title":"read_results()"},{"location":"loaders/#RNABERT.main","text":"","title":"main"},{"location":"model/","text":"Documentation for the model scprint.model.utils downsample_profile This function downsamples the expression profile of a given single cell RNA matrix. The noise is applied based on the renoise parameter, the total counts of the matrix, and the number of genes. The function first calculates the noise threshold (tnoise) based on the renoise parameter. It then generates an initial matrix count by applying a Poisson distribution to a random tensor scaled by the total counts and the number of genes. The function then models the sampling zeros by applying a Poisson distribution to a random tensor scaled by the noise threshold, the total counts, and the number of genes. The function also models the technical zeros by generating a random tensor and comparing it to the noise threshold. The final matrix count is calculated by subtracting the sampling zeros from the initial matrix count and multiplying by the technical zeros. The function ensures that the final matrix count is not less than zero by taking the maximum of the final matrix count and a tensor of zeros. The function returns the final matrix count. Parameters: mat ( Tensor ) \u2013 The input matrix. renoise ( float ) \u2013 The renoise parameter. totcounts ( Tensor ) \u2013 The total counts of the matrix. ngenes ( int ) \u2013 The number of genes. Returns: \u2013 torch.Tensor: The matrix count after applying noise. Source code in scprint/model/utils.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def downsample_profile ( mat , renoise ): \"\"\" This function downsamples the expression profile of a given single cell RNA matrix. The noise is applied based on the renoise parameter, the total counts of the matrix, and the number of genes. The function first calculates the noise threshold (tnoise) based on the renoise parameter. It then generates an initial matrix count by applying a Poisson distribution to a random tensor scaled by the total counts and the number of genes. The function then models the sampling zeros by applying a Poisson distribution to a random tensor scaled by the noise threshold, the total counts, and the number of genes. The function also models the technical zeros by generating a random tensor and comparing it to the noise threshold. The final matrix count is calculated by subtracting the sampling zeros from the initial matrix count and multiplying by the technical zeros. The function ensures that the final matrix count is not less than zero by taking the maximum of the final matrix count and a tensor of zeros. The function returns the final matrix count. Args: mat (torch.Tensor): The input matrix. renoise (float): The renoise parameter. totcounts (torch.Tensor): The total counts of the matrix. ngenes (int): The number of genes. Returns: torch.Tensor: The matrix count after applying noise. \"\"\" # Randomly drop on average N counts to each element of expression using a heavy tail Gaussian distribution # here we try to get the scale of the distribution so as to remove the right number of counts from each gene # https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02601-5#:~:text=Zero%20measurements%20in%20scRNA%2Dseq,generation%20of%20scRNA%2Dseq%20data. totcounts = mat . sum ( 1 ) batch = mat . shape [ 0 ] ngenes = mat . shape [ 1 ] tnoise = 1 - ( 1 - renoise ) ** ( 1 / 2 ) # we model the sampling zeros (dropping 30% of the reads) res = torch . poisson ( torch . rand (( batch , ngenes )) . to ( device = mat . device ) * (( tnoise * totcounts . unsqueeze ( 1 )) / ( 0.5 * ngenes )) ) . int () # we model the technical zeros (dropping 50% of the genes) drop = ( torch . rand (( batch , ngenes )) > tnoise ) . int () . to ( device = mat . device ) mat = ( mat - res ) * drop return torch . maximum ( mat , torch . Tensor ([[ 0 ]]) . to ( device = mat . device )) . int () make_adata This function creates an AnnData object from the given input parameters. Parameters: pred ( Tensor ) \u2013 Predicted labels. The shape of the tensor is (n_cells, n_classes) embs ( Tensor ) \u2013 Embeddings of the cells. The shape of the tensor is (n_cells, n_features) labels ( list ) \u2013 List of labels for the predicted classes. step ( int , default: 0 ) \u2013 Step number. Default is 0. (for storing the anndata without overwriting others) label_decoders ( dict , default: None ) \u2013 Dictionary to map class codes to class names. Default is None. gtclass ( Tensor , default: None ) \u2013 Ground truth class. Default is None. name ( str , default: '' ) \u2013 Name of the AnnData object. Default is an empty string. mdir ( str , default: '/tmp' ) \u2013 Directory to save the AnnData object. Default is \"/tmp\". Returns: adata ( AnnData ) \u2013 The created AnnData object. Source code in scprint/model/utils.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def make_adata ( pred , embs , labels , step = 0 , label_decoders = None , gtclass = None , name = \"\" , mdir = \"/tmp\" ): \"\"\" This function creates an AnnData object from the given input parameters. Args: pred (torch.Tensor): Predicted labels. The shape of the tensor is (n_cells, n_classes) embs (torch.Tensor): Embeddings of the cells. The shape of the tensor is (n_cells, n_features) labels (list): List of labels for the predicted classes. step (int, optional): Step number. Default is 0. (for storing the anndata without overwriting others) label_decoders (dict, optional): Dictionary to map class codes to class names. Default is None. gtclass (torch.Tensor, optional): Ground truth class. Default is None. name (str, optional): Name of the AnnData object. Default is an empty string. mdir (str, optional): Directory to save the AnnData object. Default is \"/tmp\". Returns: adata (anndata.AnnData): The created AnnData object. \"\"\" colname = [ \"pred_\" + i for i in labels ] obs = np . array ( pred . to ( device = \"cpu\" , dtype = torch . int32 )) # label decoders is not cls_decoders. one is a dict to map class codes (ints) # to class names the other is the module the predict the class if label_decoders is not None : obs = np . array ( [ [ label_decoders [ labels [ i ]][ n ] for n in name ] for i , name in enumerate ( obs . T ) ] ) . T if gtclass is not None : colname += labels nobs = np . array ( gtclass . to ( device = \"cpu\" , dtype = torch . int32 )) if label_decoders is not None : nobs = np . array ( [ [ label_decoders [ labels [ i ]][ n ] for n in name ] for i , name in enumerate ( nobs . T ) ] ) . T obs = np . hstack ([ obs , nobs ]) adata = AnnData ( np . array ( embs . to ( device = \"cpu\" , dtype = torch . float32 )), obs = pd . DataFrame ( obs , columns = colname , ), ) for n in labels : if gtclass is not None : tr = translate ( adata . obs [ n ] . tolist (), n ) if tr is not None : adata . obs [ \"conv_\" + n ] = adata . obs [ n ] . replace ( tr ) tr = translate ( adata . obs [ \"pred_\" + n ] . tolist (), n ) if tr is not None : adata . obs [ \"conv_pred_\" + n ] = adata . obs [ \"pred_\" + n ] . replace ( tr ) sc . pp . neighbors ( adata ) sc . tl . umap ( adata ) sc . tl . leiden ( adata ) adata . obs = adata . obs . astype ( \"category\" ) print ( adata ) if gtclass is not None : color = [ i for pair in zip ( [ \"conv_\" + i if \"conv_\" + i in adata . obs . columns else i for i in labels ], [ \"conv_pred_\" + i if \"conv_pred_\" + i in adata . obs . columns else \"pred_\" + i for i in labels ], ) for i in pair ] _ , axs = plt . subplots ( int ( len ( color ) / 2 ), 2 , figsize = ( 24 , len ( color ) * 4 )) plt . subplots_adjust ( wspace = 1 ) for i , col in enumerate ( color ): sc . pl . umap ( adata , color = col , ax = axs [ i // 2 , i % 2 ], show = False , ) else : color = [ \"conv_pred_\" + i if \"conv_pred_\" + i in adata . obs . columns else \"pred_\" + i for i in labels ] fig , axs = plt . subplots ( len ( color ), 1 , figsize = ( 16 , len ( color ) * 8 )) for i , col in enumerate ( color ): sc . pl . umap ( adata , color = col , ax = axs [ i ], show = False , ) adata . write ( mdir + \"/step_\" + str ( step ) + \"_\" + name + \".h5ad\" ) return adata masker Randomly mask a batch of data. Parameters: values ( array - like ) \u2013 A batch of tokenized data, with shape (batch_size, n_features). mask_ratio ( float , default: 0.15 ) \u2013 The ratio of genes to mask, default to 0.15. mask_value ( int , default: 1 ) \u2013 The value to mask with, default to -1. pad_value ( int ) \u2013 The value of padding in the values, will be kept unchanged. Returns: Tensor \u2013 torch.Tensor: A tensor of masked data. Source code in scprint/model/utils.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def masker ( length : int , batch_size : int = 1 , mask_ratio : float = 0.15 , mask_prob : Optional [ Union [ torch . Tensor , np . ndarray ]] = None , # n_features mask_value : int = 1 , ) -> torch . Tensor : \"\"\" Randomly mask a batch of data. Args: values (array-like): A batch of tokenized data, with shape (batch_size, n_features). mask_ratio (float): The ratio of genes to mask, default to 0.15. mask_value (int): The value to mask with, default to -1. pad_value (int): The value of padding in the values, will be kept unchanged. Returns: torch.Tensor: A tensor of masked data. \"\"\" mask = [] for _ in range ( batch_size ): m = np . zeros ( length ) loc = np . random . choice ( a = length , size = int ( length * mask_ratio ), replace = False , p = mask_prob ) m [ loc ] = mask_value mask . append ( m ) return torch . Tensor ( np . array ( mask )) . to ( torch . bool ) translate translate This function translates the given value based on the specified type. Parameters: val ( str / list / set / dict / Counter ) \u2013 The value to be translated. t ( str , default: 'cell_type_ontology_term_id' ) \u2013 The type of translation to be performed. Defaults to \"cell_type_ontology_term_id\". Returns: dict \u2013 A dictionary with the translated values. Source code in scprint/model/utils.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def translate ( val , t = \"cell_type_ontology_term_id\" ): \"\"\" translate This function translates the given value based on the specified type. Args: val (str/list/set/dict/Counter): The value to be translated. t (str, optional): The type of translation to be performed. Defaults to \"cell_type_ontology_term_id\". Returns: dict: A dictionary with the translated values. \"\"\" if t == \"cell_type_ontology_term_id\" : obj = bt . CellType . df () . set_index ( \"ontology_id\" ) elif t == \"assay_ontology_term_id\" : obj = bt . ExperimentalFactor . df () . set_index ( \"ontology_id\" ) elif t == \"tissue_ontology_term_id\" : obj = bt . Tissue . df () . set_index ( \"ontology_id\" ) elif t == \"disease_ontology_term_id\" : obj = bt . Disease . df () . set_index ( \"ontology_id\" ) elif t == \"self_reported_ethnicity_ontology_term_id\" : obj = bt . Ethnicity . df () . set_index ( \"ontology_id\" ) else : return None if type ( val ) is str : return { val : obj . loc [ val ][ \"name\" ]} elif type ( val ) is list or type ( val ) is set : return { i : obj . loc [ i ][ \"name\" ] for i in set ( val )} elif type ( val ) is dict or type ( val ) is Counter : return { obj . loc [ k ][ \"name\" ]: v for k , v in val . items ()} zinb_sample zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution. Parameters: mu ( Tensor ) \u2013 The mean of the Negative Binomial (NB) distribution. theta ( Tensor ) \u2013 The dispersion parameter of the NB distribution. zi_probs ( Tensor ) \u2013 The zero-inflation probabilities. sample_shape ( Size , default: Size ([]) ) \u2013 The output shape. Defaults to torch.Size([]). Returns: \u2013 torch.Tensor: A sample from the ZINB distribution. Source code in scprint/model/utils.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def zinb_sample ( mu , theta , zi_probs , sample_shape = torch . Size ([])): \"\"\" zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution. Args: mu (torch.Tensor): The mean of the Negative Binomial (NB) distribution. theta (torch.Tensor): The dispersion parameter of the NB distribution. zi_probs (torch.Tensor): The zero-inflation probabilities. sample_shape (torch.Size, optional): The output shape. Defaults to torch.Size([]). Returns: torch.Tensor: A sample from the ZINB distribution. \"\"\" concentration = theta rate = theta / mu # Important remark: Gamma is parametrized by the rate = 1/scale! gamma_d = Gamma ( concentration = concentration , rate = rate ) p_means = gamma_d . sample ( sample_shape ) # Clamping as distributions objects can have buggy behaviors when # their parameters are too high l_train = torch . clamp ( p_means , max = 1e8 ) samp = Poisson ( l_train ) . sample () # Shape : (n_samples, n_cells_batch, n_vars) is_zero = torch . rand_like ( samp ) <= zi_probs samp_ = torch . where ( is_zero , torch . zeros_like ( samp ), samp ) return samp_ scprint.model.encoders CategoryValueEncoder Bases: Module Encodes categorical values into a vector using an embedding layer and layer normalization. Parameters: num_embeddings ( int ) \u2013 The number of possible values. embedding_dim ( int ) \u2013 The dimension of the output vectors. padding_idx ( int , default: None ) \u2013 The index of the padding token. Defaults to None. Returns: \u2013 torch.Tensor: A tensor representing the encoded categorical values. Note: not used in the current version of scprint. Source code in scprint/model/encoders.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def __init__ ( self , num_embeddings : int , embedding_dim : int , padding_idx : Optional [ int ] = None , ): \"\"\" Encodes categorical values into a vector using an embedding layer and layer normalization. Args: num_embeddings (int): The number of possible values. embedding_dim (int): The dimension of the output vectors. padding_idx (int, optional): The index of the padding token. Defaults to None. Returns: torch.Tensor: A tensor representing the encoded categorical values. Note: not used in the current version of scprint. \"\"\" super ( CategoryValueEncoder , self ) . __init__ () self . embedding = nn . Embedding ( num_embeddings , embedding_dim , padding_idx = padding_idx ) self . enc_norm = nn . LayerNorm ( embedding_dim ) ContinuousValueEncoder Bases: Module Encode real number values to a vector using neural nets projection. Parameters: d_model ( int ) \u2013 The dimension of the input vectors. dropout ( float , default: 0.1 ) \u2013 The dropout rate to apply to the output of the positional encoding. max_value ( int , default: 100000 ) \u2013 The maximum value of the input. Defaults to 100_000. layers ( int , default: 1 ) \u2013 The number of layers in the encoder. Defaults to 1. size ( int , default: 1 ) \u2013 The size of the input. Defaults to 1. Returns: \u2013 torch.Tensor: A tensor representing the encoded continuous values. Source code in scprint/model/encoders.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def __init__ ( self , d_model : int , dropout : float = 0.1 , max_value : int = 100_000 , layers : int = 1 , size : int = 1 , ): \"\"\" Encode real number values to a vector using neural nets projection. Args: d_model (int): The dimension of the input vectors. dropout (float, optional): The dropout rate to apply to the output of the positional encoding. max_value (int, optional): The maximum value of the input. Defaults to 100_000. layers (int, optional): The number of layers in the encoder. Defaults to 1. size (int, optional): The size of the input. Defaults to 1. Returns: torch.Tensor: A tensor representing the encoded continuous values. \"\"\" super ( ContinuousValueEncoder , self ) . __init__ () self . max_value = max_value self . encoder = nn . ModuleList () for i in range ( layers ): self . encoder . append ( nn . Linear ( size if i == 0 else d_model , d_model )) self . encoder . append ( nn . LayerNorm ( d_model )) self . encoder . append ( nn . ReLU ()) self . encoder . append ( nn . Dropout ( p = dropout )) forward Parameters: x ( Tensor ) \u2013 Tensor, shape [batch_size, seq_len] Source code in scprint/model/encoders.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def forward ( self , x : Tensor , mask : Tensor = None ) -> Tensor : \"\"\" Args: x: Tensor, shape [batch_size, seq_len] \"\"\" # TODO: test using actual embedding layer if input is categorical # expand last dimension x = x . unsqueeze ( - 1 ) # use the mask embedding when x=-1 # mask = (x == -1).float() x = torch . clamp ( x , min = 0 , max = self . max_value ) for val in self . encoder : x = val ( x ) if mask is not None : x = x . masked_fill_ ( mask . unsqueeze ( - 1 ), 0 ) return x DPositionalEncoding Bases: Module The PositionalEncoding module applies a positional encoding to a sequence of vectors. This is necessary for the Transformer model, which does not have any inherent notion of position in a sequence. The positional encoding is added to the input embeddings and allows the model to attend to positions in the sequence. Parameters: d_model ( int ) \u2013 The dimension of the input vectors. dropout ( float , default: 0.1 ) \u2013 The dropout rate to apply to the output of the positional encoding. max_len ( int ) \u2013 The maximum length of a sequence that this module can handle. Note: not used in the current version of scprint. Source code in scprint/model/encoders.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def __init__ ( self , d_model : int , max_len_x : int , max_len_y : int , maxvalue_x = 10000.0 , maxvalue_y = 10000.0 , dropout : float = 0.1 , ): super ( DPositionalEncoding , self ) . __init__ () self . dropout = nn . Dropout ( p = dropout ) position2 = torch . arange ( max_len_y ) . unsqueeze ( 1 ) position1 = torch . arange ( max_len_x ) . unsqueeze ( 1 ) half_n = d_model // 2 div_term2 = torch . exp ( torch . arange ( 0 , half_n , 2 ) * ( - math . log ( maxvalue_y ) / d_model ) ) div_term1 = torch . exp ( torch . arange ( 0 , half_n , 2 ) * ( - math . log ( maxvalue_x ) / d_model ) ) pe1 = torch . zeros ( max_len_x , 1 , d_model ) pe2 = torch . zeros ( max_len_y , 1 , d_model ) pe1 [:, 0 , 0 : half_n : 2 ] = torch . sin ( position1 * div_term1 ) pe1 [:, 0 , 1 : half_n : 2 ] = torch . cos ( position1 * div_term1 ) pe2 [:, 0 , half_n :: 2 ] = torch . sin ( position2 * div_term2 ) pe2 [:, 0 , 1 + half_n :: 2 ] = torch . cos ( position2 * div_term2 ) # https://github.com/tatp22/multidim-positional-encoding/blob/master/positional_encodings/torch_encodings.py # TODO: seems to do it differently. I hope it still works ok!! self . register_buffer ( \"pe1\" , pe1 ) self . register_buffer ( \"pe2\" , pe2 ) forward Parameters: x ( Tensor ) \u2013 Tensor, shape [seq_len, batch_size, embedding_dim] Source code in scprint/model/encoders.py 162 163 164 165 166 167 168 169 170 def forward ( self , x : Tensor , pos_x : Tensor , pos_y : Tensor ) -> Tensor : \"\"\" Args: x: Tensor, shape [seq_len, batch_size, embedding_dim] \"\"\" # TODO: try with a continuous value encoder of size 2 (start, end where they are normalized to 0-1) x = x + self . pe1 [ pos_x ] x = x + self . pe2 [ pos_y ] return self . dropout ( x ) GeneEncoder Bases: Module Encodes gene sequences into a continuous vector space using an embedding layer. The output is then normalized using a LayerNorm. Parameters: num_embeddings ( int ) \u2013 The number of possible values. embedding_dim ( int ) \u2013 The dimension of the output vectors. padding_idx ( int , default: None ) \u2013 The index of the padding token. Defaults to None. weights ( Tensor , default: None ) \u2013 The initial weights for the embedding layer. Defaults to None. dropout ( float , default: 0.1 ) \u2013 The dropout rate to apply to the output of the positional encoding. Defaults to 0.1. freeze ( bool , default: False ) \u2013 Whether to freeze the weights of the embedding layer. Defaults to False. Note: not used in the current version of scprint. Source code in scprint/model/encoders.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , num_embeddings : int , embedding_dim : int , padding_idx : Optional [ int ] = None , weights : Optional [ Tensor ] = None , dropout : float = 0.1 , freeze : bool = False , ): \"\"\" Encodes gene sequences into a continuous vector space using an embedding layer. The output is then normalized using a LayerNorm. Args: num_embeddings (int): The number of possible values. embedding_dim (int): The dimension of the output vectors. padding_idx (int, optional): The index of the padding token. Defaults to None. weights (Tensor, optional): The initial weights for the embedding layer. Defaults to None. dropout (float, optional): The dropout rate to apply to the output of the positional encoding. Defaults to 0.1. freeze (bool, optional): Whether to freeze the weights of the embedding layer. Defaults to False. Note: not used in the current version of scprint. \"\"\" super ( GeneEncoder , self ) . __init__ () self . embedding = nn . Embedding ( num_embeddings , embedding_dim , padding_idx = padding_idx , _freeze = freeze ) if weights is not None : # concat a zero vector to the weight # this is to make the embedding of the padding token to be zero # weights = torch.cat( # [torch.Tensor(weights), torch.zeros(1, embedding_dim)], dim=0 # ) self . embedding . weight . data . copy_ ( torch . Tensor ( weights )) self . enc_norm = nn . LayerNorm ( embedding_dim ) self . dropout = nn . Dropout ( p = dropout ) PositionalEncoding Bases: Module The PositionalEncoding module applies a positional encoding to a sequence of vectors. This is necessary for the Transformer model, which does not have any inherent notion of position in a sequence. The positional encoding is added to the input embeddings and allows the model to attend to positions in the sequence. Parameters: d_model ( int ) \u2013 The dimension of the input vectors. dropout ( float , default: 0.1 ) \u2013 The dropout rate to apply to the output of the positional encoding. max_len ( int ) \u2013 The maximum length of a sequence that this module can handle. Note: not used in the current version of scprint. Source code in scprint/model/encoders.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , d_model : int , max_len : int , token_to_pos : dict [ str , int ], # [token, pos] dropout : float = 0.1 , maxval = 10000.0 , ): \"\"\" The PositionalEncoding module applies a positional encoding to a sequence of vectors. This is necessary for the Transformer model, which does not have any inherent notion of position in a sequence. The positional encoding is added to the input embeddings and allows the model to attend to positions in the sequence. Args: d_model (int): The dimension of the input vectors. dropout (float, optional): The dropout rate to apply to the output of the positional encoding. max_len (int, optional): The maximum length of a sequence that this module can handle. Note: not used in the current version of scprint. \"\"\" super ( PositionalEncoding , self ) . __init__ () self . dropout = nn . Dropout ( p = dropout ) position = torch . arange ( max_len ) . unsqueeze ( 1 ) # Create a dictionary to convert token to position div_term = torch . exp ( torch . arange ( 0 , d_model , 2 ) * ( - math . log ( maxval ) / d_model ) ) pe = torch . zeros ( max_len , 1 , d_model ) pe [:, 0 , 0 :: 2 ] = torch . sin ( position * div_term ) pe [:, 0 , 1 :: 2 ] = torch . cos ( position * div_term ) # we reorder them and map them to gene_id (position) arr = [] for k , v in token_to_pos . items (): arr . append ( pe [ v - 1 ] . numpy ()) pe = torch . Tensor ( np . array ( arr )) self . register_buffer ( \"pe\" , pe ) forward Parameters: x \u2013 Tensor, shape [seq_len, batch_size, embedding_dim] Source code in scprint/model/encoders.py 96 97 98 99 100 101 102 103 104 105 def forward ( self , gene_pos : Tensor ) -> Tensor : \"\"\" Args: x: Tensor, shape [seq_len, batch_size, embedding_dim] \"\"\" return self . dropout ( torch . index_select ( self . pe , 0 , gene_pos . view ( - 1 )) . view ( gene_pos . shape + ( - 1 ,) ) ) scprint.model.decoders ClsDecoder Bases: Module ClsDecoder Decoder for classification task. Parameters: d_model ( int ) \u2013 int, dimension of the input. n_cls ( int ) \u2013 int, number of classes. layers ( list [ int ] , default: [256, 128] ) \u2013 list[int], list of hidden layers. activation ( Callable , default: ReLU ) \u2013 nn.Module, activation function. dropout ( float , default: 0.1 ) \u2013 float, dropout rate. Returns: \u2013 Tensor, shape [batch_size, n_cls] Source code in scprint/model/decoders.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def __init__ ( self , d_model : int , n_cls : int , layers : list [ int ] = [ 256 , 128 ], activation : Callable = nn . ReLU , dropout : float = 0.1 , ): \"\"\" ClsDecoder Decoder for classification task. Args: d_model: int, dimension of the input. n_cls: int, number of classes. layers: list[int], list of hidden layers. activation: nn.Module, activation function. dropout: float, dropout rate. Returns: Tensor, shape [batch_size, n_cls] \"\"\" super ( ClsDecoder , self ) . __init__ () # module list layers = [ d_model ] + layers self . decoder = nn . Sequential () for i , l in enumerate ( layers [ 1 :]): self . decoder . append ( nn . Linear ( layers [ i ], l )) self . decoder . append ( nn . LayerNorm ( l )) self . decoder . append ( activation ()) self . decoder . append ( nn . Dropout ( dropout )) self . out_layer = nn . Linear ( layers [ - 1 ], n_cls ) forward Parameters: x ( Tensor ) \u2013 Tensor, shape [batch_size, embsize] Source code in scprint/model/decoders.py 197 198 199 200 201 202 203 def forward ( self , x : Tensor ) -> Tensor : \"\"\" Args: x: Tensor, shape [batch_size, embsize] \"\"\" x = self . decoder ( x ) return self . out_layer ( x ) ExprDecoder Bases: Module ExprDecoder Decoder for the gene expression prediction. Will output the mean, variance and zero logits, parameters of a zero inflated negative binomial distribution. Parameters: d_model ( int ) \u2013 The dimension of the model. This is the size of the input feature vector. nfirst_labels_to_skip ( int , default: 0 ) \u2013 The number of initial labels to skip in the sequence. Defaults to 0. dropout ( float , default: 0.1 ) \u2013 The dropout rate applied during training to prevent overfitting. Defaults to 0.1. Source code in scprint/model/decoders.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , d_model : int , nfirst_labels_to_skip : int = 0 , dropout : float = 0.1 , ): \"\"\" ExprDecoder Decoder for the gene expression prediction. Will output the mean, variance and zero logits, parameters of a zero inflated negative binomial distribution. Args: d_model (int): The dimension of the model. This is the size of the input feature vector. nfirst_labels_to_skip (int, optional): The number of initial labels to skip in the sequence. Defaults to 0. dropout (float, optional): The dropout rate applied during training to prevent overfitting. Defaults to 0.1. \"\"\" super ( ExprDecoder , self ) . __init__ () self . nfirst_labels_to_skip = nfirst_labels_to_skip self . fc = nn . Sequential ( nn . Linear ( d_model , d_model ), nn . LayerNorm ( d_model ), nn . LeakyReLU (), nn . Dropout ( dropout ), nn . Linear ( d_model , d_model ), nn . LeakyReLU (), ) self . pred_var_zero = nn . Linear ( d_model , 3 ) forward x is the output of the transformer, (batch, seq_len, d_model) Source code in scprint/model/decoders.py 59 60 61 62 63 64 65 66 67 68 69 70 71 def forward ( self , x : Tensor ) -> Dict [ str , Tensor ]: \"\"\"x is the output of the transformer, (batch, seq_len, d_model)\"\"\" # we don't do it on the labels x = self . fc ( x [:, self . nfirst_labels_to_skip :, :]) pred_value , var_value , zero_logits = self . pred_var_zero ( x ) . split ( 1 , dim =- 1 ) # (batch, seq_len) # The sigmoid function is used to map the zero_logits to a probability between 0 and 1. return dict ( mean = F . softmax ( pred_value . squeeze ( - 1 ), dim =- 1 ), disp = torch . exp ( torch . clamp ( var_value . squeeze ( - 1 ), max = 15 )), zero_logits = zero_logits . squeeze ( - 1 ), ) GraphSDEExprDecoder Bases: Module Initialize the ExprNeuralSDEDecoder module. Parameters: d_model (int): The dimension of the model. drift (nn.Module): The drift component of the SDE. diffusion (nn.Module): The diffusion component of the SDE. Source code in scprint/model/decoders.py 9 10 11 12 13 14 15 16 17 18 19 20 21 def __init__ ( self , d_model : int , drift : nn . Module , diffusion : nn . Module ): \"\"\" Initialize the ExprNeuralSDEDecoder module. Parameters: d_model (int): The dimension of the model. drift (nn.Module): The drift component of the SDE. diffusion (nn.Module): The diffusion component of the SDE. \"\"\" super () . __init__ () self . d_model = d_model self . drift = drift self . diffusion = diffusion MVCDecoder Bases: Module MVCDecoder Decoder for the masked value prediction for cell embeddings. Will use the gene embeddings with the cell embeddings to predict the mean, variance and zero logits Parameters: d_model \u2013 obj: int ): dimension of the gene embedding. arch_style \u2013 obj: str ): architecture style of the decoder, choice from 1. \"inner product\" or 2. \"cell product\" 3. \"concat query\" or 4. \"sum query\". query_activation \u2013 obj: nn.Module ): activation function for the query vectors. Defaults to nn.Sigmoid. hidden_activation \u2013 obj: nn.Module ): activation function for the hidden layers. Defaults to nn.PReLU. Source code in scprint/model/decoders.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __init__ ( self , d_model : int , arch_style : str = \"inner product\" , query_activation : nn . Module = nn . Sigmoid , hidden_activation : nn . Module = nn . PReLU , ) -> None : \"\"\" MVCDecoder Decoder for the masked value prediction for cell embeddings. Will use the gene embeddings with the cell embeddings to predict the mean, variance and zero logits Args: d_model (:obj:`int`): dimension of the gene embedding. arch_style (:obj:`str`): architecture style of the decoder, choice from 1. \"inner product\" or 2. \"cell product\" 3. \"concat query\" or 4. \"sum query\". query_activation (:obj:`nn.Module`): activation function for the query vectors. Defaults to nn.Sigmoid. hidden_activation (:obj:`nn.Module`): activation function for the hidden layers. Defaults to nn.PReLU. \"\"\" super ( MVCDecoder , self ) . __init__ () if arch_style == \"inner product\" : self . gene2query = nn . Linear ( d_model , d_model ) self . query_activation = query_activation () self . pred_var_zero = nn . Linear ( d_model , d_model * 3 , bias = False ) elif arch_style == \"concat query\" : self . gene2query = nn . Linear ( d_model , 64 ) self . query_activation = query_activation () self . fc1 = nn . Linear ( d_model + 64 , 64 ) self . hidden_activation = hidden_activation () self . fc2 = nn . Linear ( 64 , 3 ) elif arch_style == \"sum query\" : self . gene2query = nn . Linear ( d_model , d_model ) self . query_activation = query_activation () self . fc1 = nn . Linear ( d_model , 64 ) self . hidden_activation = hidden_activation () self . fc2 = nn . Linear ( 64 , 3 ) else : raise ValueError ( f \"Unknown arch_style: { arch_style } \" ) self . arch_style = arch_style self . do_detach = arch_style . endswith ( \"detach\" ) self . d_model = d_model forward Parameters: cell_emb ( Tensor ) \u2013 Tensor, shape (batch, embsize=d_model) gene_embs ( Tensor ) \u2013 Tensor, shape (batch, seq_len, embsize=d_model) Source code in scprint/model/decoders.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def forward ( self , cell_emb : Tensor , gene_embs : Tensor , ) -> Union [ Tensor , Dict [ str , Tensor ]]: \"\"\" Args: cell_emb: Tensor, shape (batch, embsize=d_model) gene_embs: Tensor, shape (batch, seq_len, embsize=d_model) \"\"\" if self . arch_style == \"inner product\" : query_vecs = self . query_activation ( self . gene2query ( gene_embs )) pred , var , zero_logits = self . pred_var_zero ( query_vecs ) . split ( self . d_model , dim =- 1 ) cell_emb = cell_emb . unsqueeze ( 2 ) pred , var , zero_logits = ( torch . bmm ( pred , cell_emb ) . squeeze ( 2 ), torch . bmm ( var , cell_emb ) . squeeze ( 2 ), torch . bmm ( zero_logits , cell_emb ) . squeeze ( 2 ), ) # zero logits need to based on the cell_emb, because of input exprs elif self . arch_style == \"concat query\" : query_vecs = self . query_activation ( self . gene2query ( gene_embs )) # expand cell_emb to (batch, seq_len, embsize) cell_emb = cell_emb . unsqueeze ( 1 ) . expand ( - 1 , gene_embs . shape [ 1 ], - 1 ) h = self . hidden_activation ( self . fc1 ( torch . cat ([ cell_emb , query_vecs ], dim = 2 )) ) pred , var , zero_logits = self . fc2 ( h ) . split ( 1 , dim =- 1 ) elif self . arch_style == \"sum query\" : query_vecs = self . query_activation ( self . gene2query ( gene_embs )) cell_emb = cell_emb . unsqueeze ( 1 ) h = self . hidden_activation ( self . fc1 ( cell_emb + query_vecs )) pred , var , zero_logits = self . fc2 ( h ) . split ( 1 , dim =- 1 ) return dict ( mvc_mean = F . softmax ( pred , dim =- 1 ), mvc_disp = torch . exp ( torch . clamp ( var , max = 15 )), mvc_zero_logits = zero_logits , ) scprint.model.flash_attn.flashformer FlashTransformerEncoder Bases: Module FlashTransformerEncoder a transformer encoder with flash attention. Parameters: d_model ( int ) \u2013 The dimension of the input vectors. nhead ( int ) \u2013 The number of attention heads. nlayers ( int ) \u2013 The number of layers in the transformer. dropout ( float , default: 0.1 ) \u2013 The dropout rate to apply to the output of the positional encoding. Defaults to 0.1. residual_in_fp32 ( bool , default: True ) \u2013 Whether to force the residual to be in fp32 format. Defaults to True. num_heads_kv ( _type_ , default: None ) \u2013 The number of heads for key/value. Defaults to None. checkpointing ( bool , default: False ) \u2013 Whether to use gradient checkpointing. Defaults to False. fused_dropout_add_ln ( bool , default: False ) \u2013 Whether to fuse dropout, addition and layer normalization operations. Defaults to False. return_residual ( bool , default: False ) \u2013 Whether to return the residual. Defaults to False. prenorm ( bool , default: True ) \u2013 Whether to use pre-normalization. Defaults to True. mlp_ratio ( float , default: 4.0 ) \u2013 The ratio for MLP. Defaults to 4.0. fused_mlp ( bool , default: False ) \u2013 Whether to use fused MLP. Defaults to False. fused_bias_fc ( bool , default: False ) \u2013 Whether to fuse bias and fully connected layers. Defaults to False. sequence_parallel ( bool , default: False ) \u2013 Whether to use sequence parallelism. Defaults to False. drop_path_rate ( float , default: 0.0 ) \u2013 The drop path rate. Defaults to 0.0. weight_init ( str , default: '' ) \u2013 The weight initialization method. Defaults to \"\". Raises: ImportError \u2013 Raised when Triton is not installed but fused_dropout_add_ln is set to True. NotImplementedError \u2013 Raised when an unsupported operation is attempted. Source code in scprint/model/flash_attn/flashformer.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def __init__ ( self , d_model : int , nhead : int , nlayers : int , dropout : float = 0.1 , residual_in_fp32 : bool = True , num_heads_kv : Optional [ int ] = None , checkpointing : bool = False , fused_dropout_add_ln : bool = False , return_residual : bool = False , prenorm : bool = True , mlp_ratio : float = 4.0 , fused_mlp : bool = False , fused_bias_fc : bool = False , sequence_parallel : bool = False , drop_path_rate : float = 0.0 , use_flash_attn : bool = True , weight_init : str = \"\" , ): \"\"\" FlashTransformerEncoder a transformer encoder with flash attention. Args: d_model (int): The dimension of the input vectors. nhead (int): The number of attention heads. nlayers (int): The number of layers in the transformer. dropout (float, optional): The dropout rate to apply to the output of the positional encoding. Defaults to 0.1. residual_in_fp32 (bool, optional): Whether to force the residual to be in fp32 format. Defaults to True. num_heads_kv (_type_, optional): The number of heads for key/value. Defaults to None. checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False. fused_dropout_add_ln (bool, optional): Whether to fuse dropout, addition and layer normalization operations. Defaults to False. return_residual (bool, optional): Whether to return the residual. Defaults to False. prenorm (bool, optional): Whether to use pre-normalization. Defaults to True. mlp_ratio (float, optional): The ratio for MLP. Defaults to 4.0. fused_mlp (bool, optional): Whether to use fused MLP. Defaults to False. fused_bias_fc (bool, optional): Whether to fuse bias and fully connected layers. Defaults to False. sequence_parallel (bool, optional): Whether to use sequence parallelism. Defaults to False. drop_path_rate (float, optional): The drop path rate. Defaults to 0.0. weight_init (str, optional): The weight initialization method. Defaults to \"\". Raises: ImportError: Raised when Triton is not installed but fused_dropout_add_ln is set to True. NotImplementedError: Raised when an unsupported operation is attempted. \"\"\" super ( FlashTransformerEncoder , self ) . __init__ () self . blocks = nn . ModuleList () dpr = [ x . item () for x in torch . linspace ( 0 , drop_path_rate , nlayers ) ] # stochastic depth decay rule for i in range ( nlayers ): mlp = create_mlp_cls ( d_model , mlp_ratio , nn . GELU , fused_mlp ) attention = partial ( MHA , num_heads = nhead , dropout = dropout , causal = False , use_flash_attn = use_flash_attn , num_heads_kv = num_heads_kv , checkpointing = checkpointing , fused_bias_fc = fused_bias_fc , layer_idx = i , ) # or use parallelBlock where attn & MLP are done in parallel encoder_layers = Block ( d_model , attention , mlp , prenorm = prenorm , # need to set it here for now although it hinders some performances as it returns the residual and I need to see what to do with it # TD [2022-07-30]: Force residual in fp32, seems to make fp16 training more stable residual_in_fp32 = residual_in_fp32 , sequence_parallel = sequence_parallel , # for more parallelism resid_dropout1 = dropout , resid_dropout2 = dropout , drop_path1 = dpr [ i - 1 ] if i > 0 else 0.0 , drop_path2 = dpr [ i ], fused_dropout_add_ln = fused_dropout_add_ln , return_residual = return_residual , ) self . blocks . append ( encoder_layers ) self . dropout = nn . Dropout ( p = dropout ) self . drop_path = StochasticDepth ( p = dpr [ - 1 ], mode = \"row\" ) self . norm = torch . nn . LayerNorm ( d_model , eps = 1e-6 ) self . fused_dropout_add_ln = fused_dropout_add_ln if self . fused_dropout_add_ln and layer_norm_fn is None : raise ImportError ( \"Triton is not installed\" ) if sequence_parallel : # This seems to only be important when doing tensor parallelism across GPUs, to increase even more the context length I guess? # not really necessary here I think raise NotImplementedError ( \"sequence_parallel not implemented yet\" ) self . init_weights ( weight_init ) scprint.model.model scPrint Bases: LightningModule scPrint transformer for single cell biology and the inference of Gene Regulatory networks Parameters: genes ( list ) \u2013 the genenames with which the model will work precpt_gene_emb ( array , default: None ) \u2013 The gene embeddings. should be of size len(genes), d_model. it should be in the same order as the genes. Defaults to None. gene_pos_enc ( list , default: None ) \u2013 The gene position encoding. Should be of the same size as genes. for each gene in genes, gives it a location value. Defaults to None. d_model ( int , default: 512 ) \u2013 The dimension of the model. Defaults to 512. nhead ( int , default: 8 ) \u2013 The number of heads in the multiheadattention models. Defaults to 8. d_hid ( int , default: 512 ) \u2013 The dimension of the feedforward network model. Defaults to 512. nlayers ( int , default: 6 ) \u2013 The number of layers in the transformer model. Defaults to 6. nlayers_cls ( int ) \u2013 The number of layers in the classifier. Defaults to 3. labels ( dict , default: {} ) \u2013 The classes to predict with number of labels for each. Defaults to {}. cls_hierarchy ( dict , default: {} ) \u2013 The class hierarchy for classes that have hierarchical labels. Defaults to {}. dropout ( float , default: 0.2 ) \u2013 The dropout value. Defaults to 0.5. transformer ( str , default: 'fast' ) \u2013 (flag, optional) the transformer type to use. one of \"linear\", \"flash\", \"flashsparse\", \"scprint\". Defaults to \"flash\". domain_spec_batchnorm ( str , default: 'None' ) \u2013 Whether to apply domain specific batch normalization. Defaults to False. expr_emb_style ( str , default: 'continuous' ) \u2013 The style of input embedding (one of \"continuous_concat\", \"binned_pos\", \"full_pos\"). Defaults to \"continuous_concat\". mvc_decoder ( str , default: 'None' ) \u2013 The style of MVC decoder one of \"None\", \"inner product\", \"concat query\", \"sum query\". Defaults to \"inner product\". pred_embedding ( list , default: [] ) \u2013 The list of labels to use for plotting embeddings. Defaults to []. cell_emb_style ( str , default: 'cls' ) \u2013 The style of cell embedding. one of \"cls\", \"avg-pool\", \"w-pool\". Defaults to \"cls\". lr ( float , default: 0.001 ) \u2013 The learning rate. Defaults to 0.001. label_decoders ( Optional [ Dict [ str , Dict [ int , str ]]] , default: None ) \u2013 (dict, optional) the label decoders to use for plotting the umap during validations. Defaults to None. Raises: ValueError \u2013 If the expr_emb_style is not one of \"category\", \"continuous\", \"none\". Source code in scprint/model/model.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def __init__ ( self , genes : list , precpt_gene_emb : Optional [ str ] = None , gene_pos_enc : Optional [ list ] = None , d_model : int = 512 , nhead : int = 8 , d_hid : int = 512 , edge_dim : int = 12 , nlayers : int = 6 , layers_cls : list [ int ] = [], labels : Dict [ str , int ] = {}, cls_hierarchy : Dict [ str , Dict [ int , list [ int ]]] = {}, dropout : float = 0.2 , transformer : str = \"fast\" , expr_emb_style : str = \"continuous\" , # \"binned_pos\", \"cont_pos\" domain_spec_batchnorm : str = \"None\" , n_input_bins : int = 0 , mvc_decoder : str = \"None\" , pred_embedding : list [ str ] = [], cell_emb_style : str = \"cls\" , lr : float = 0.001 , label_decoders : Optional [ Dict [ str , Dict [ int , str ]]] = None , ** flash_attention_kwargs , ): \"\"\" scPrint transformer for single cell biology and the inference of Gene Regulatory networks Args: genes (list): the genenames with which the model will work precpt_gene_emb (np.array, optional): The gene embeddings. should be of size len(genes), d_model. it should be in the same order as the genes. Defaults to None. gene_pos_enc (list, optional): The gene position encoding. Should be of the same size as genes. for each gene in genes, gives it a location value. Defaults to None. d_model (int, optional): The dimension of the model. Defaults to 512. nhead (int, optional): The number of heads in the multiheadattention models. Defaults to 8. d_hid (int, optional): The dimension of the feedforward network model. Defaults to 512. nlayers (int, optional): The number of layers in the transformer model. Defaults to 6. nlayers_cls (int, optional): The number of layers in the classifier. Defaults to 3. labels (dict, optional): The classes to predict with number of labels for each. Defaults to {}. cls_hierarchy (dict, optional): The class hierarchy for classes that have hierarchical labels. Defaults to {}. dropout (float, optional): The dropout value. Defaults to 0.5. transformer: (flag, optional) the transformer type to use. one of \"linear\", \"flash\", \"flashsparse\", \"scprint\". Defaults to \"flash\". domain_spec_batchnorm (str, optional): Whether to apply domain specific batch normalization. Defaults to False. expr_emb_style (str, optional): The style of input embedding (one of \"continuous_concat\", \"binned_pos\", \"full_pos\"). Defaults to \"continuous_concat\". mvc_decoder (str, optional): The style of MVC decoder one of \"None\", \"inner product\", \"concat query\", \"sum query\". Defaults to \"inner product\". pred_embedding (list, optional): The list of labels to use for plotting embeddings. Defaults to []. cell_emb_style (str, optional): The style of cell embedding. one of \"cls\", \"avg-pool\", \"w-pool\". Defaults to \"cls\". lr (float, optional): The learning rate. Defaults to 0.001. label_decoders: (dict, optional) the label decoders to use for plotting the umap during validations. Defaults to None. Raises: ValueError: If the expr_emb_style is not one of \"category\", \"continuous\", \"none\". \"\"\" super () . __init__ () # default self . do_denoise = False self . noise = [] self . do_cce = True self . cce_sim = 0.5 self . do_ecs = True self . ecs_threshold = 0.3 self . ecs_scale = 1.0 self . do_mvc = False self . do_adv_cls = False self . do_next_tp = False self . do_generate = False self . class_scale = 1000 self . mask_ratio = [ 0.15 ] self . warmup_duration = 500 self . weight_decay = 0.0 self . fused_adam = False self . lr_patience = 3 self . lrfinder_steps = 0 self . get_attention_layer = [] self . embs = None # should be stored somehow self . d_model = d_model self . edge_dim = edge_dim self . nlayers = nlayers self . gene_pos_enc = gene_pos_enc self . mvc_decoder = mvc_decoder self . domain_spec_batchnorm = domain_spec_batchnorm # need to store self . n_input_bins = n_input_bins self . transformer = transformer self . labels_counts = labels self . labels = list ( labels . keys ()) self . cell_emb_style = cell_emb_style self . label_decoders = label_decoders self . pred_embedding = pred_embedding self . lr = lr # compute tensor for mat_cls_hierarchy self . mat_cls_hierarchy = {} self . cls_hierarchy = cls_hierarchy for k , v in cls_hierarchy . items (): tens = torch . zeros (( len ( v ), labels [ k ])) for k2 , v2 in v . items (): tens [ k2 - labels [ k ], v2 ] = 1 self . mat_cls_hierarchy [ k ] = tens . to ( bool ) self . expr_emb_style = expr_emb_style if self . expr_emb_style not in [ \"category\" , \"continuous\" , \"none\" ]: raise ValueError ( f \"expr_emb_style should be one of category, continuous, scaling, \" f \"got { expr_emb_style } \" ) if cell_emb_style not in [ \"cls\" , \"avg-pool\" , \"w-pool\" ]: raise ValueError ( f \"Unknown cell_emb_style: { cell_emb_style } \" ) self . genes = genes self . vocab = { i : n for i , n in enumerate ( genes )} # encoder # gene encoder if precpt_gene_emb is not None : embeddings = pd . read_parquet ( precpt_gene_emb ) . loc [ self . genes ] if len ( embeddings ) == 0 : raise ValueError ( f \"the gene embeddings file { precpt_gene_emb } does not contain any of the genes given to the model\" ) elif len ( embeddings ) < len ( self . genes ): print ( \"Warning: only a subset of the genes available in the embeddings file.\" ) print ( \"number of genes: \" , len ( embeddings )) sembeddings = torch . nn . AdaptiveAvgPool1d ( d_model )( torch . tensor ( embeddings . values ) ) self . gene_encoder = encoders . GeneEncoder ( len ( self . vocab ), d_model , weights = sembeddings , freeze = True ) else : self . gene_encoder = encoders . GeneEncoder ( len ( self . vocab ), d_model ) # Value Encoder, NOTE: the scaling style is also handled in _encode method if expr_emb_style in [ \"continuous\" , \"full_pos\" ]: self . expr_encoder = encoders . ContinuousValueEncoder ( d_model , dropout ) elif expr_emb_style == \"binned_pos\" : assert n_input_bins > 0 self . expr_encoder = encoders . CategoryValueEncoder ( n_input_bins , d_model ) else : self . expr_encoder = nn . Identity () # Positional Encoding if self . gene_pos_enc is not None : max_len = max ( gene_pos_enc ) token_to_pos = { token : pos for token , pos in enumerate ( self . gene_pos_enc )} self . pos_encoder = encoders . PositionalEncoding ( d_model , max_len = max_len , token_to_pos = token_to_pos ) # Batch Encoder # always have [base_cell_emb, time_embedding, depth_embedding] + any other class info # base cell embedding will store other cell specific information self . label_encoder = encoders . CategoryValueEncoder ( len ( self . labels ) + 2 , d_model ) # self.time_encoder = encoders.ContinuousValueEncoder(d_model, dropout) self . depth_decoder = encoders . ContinuousValueEncoder ( d_model , dropout ) # Model # Batch Norm if domain_spec_batchnorm is True or domain_spec_batchnorm == \"dsbn\" : use_affine = True if domain_spec_batchnorm == \"do_affine\" else False print ( f \"Use domain specific batchnorm with affine= { use_affine } \" ) self . dsbn = DomainSpecificBatchNorm1d ( d_model , len ( self . labels ), eps = 6.1e-5 , affine = use_affine ) elif domain_spec_batchnorm == \"batchnorm\" : print ( \"Using simple batchnorm instead of domain specific batchnorm\" ) self . bn = nn . BatchNorm1d ( d_model , eps = 6.1e-5 ) # Transformer # Linear if transformer == \"linear\" : # linear transformer using the fast transformer package self . transformer = FastTransformerEncoder ( d_model , nhead , d_hid , nlayers , dropout , \"linear\" ) # flashsparse elif transformer == \"flashsparse\" : if Hashformer is None : raise ValueError ( \"Hashformer transformer requires cuda kernels\" ) self . transformer = Hashformer ( d_model , nlayers , 2 , nhead , ) # flash EGT # We found that the results can be further improved by freezing the # node channel layers and training the edge channel layers for a # few additional epochs. # However, its effect on transfer learning has not yet been studied. # That is why we include checkpoints for both tuned and untuned models. # https://github.com/shamim-hussain/egt/blob/master/README.md # https://github.com/shamim-hussain/egt_pytorch elif transformer == \"scprint\" : self . transformer = EGT ( num_layers = nlayers , feat_size = d_model , edge_feat_size = edge_dim , num_heads = nhead , num_virtual_nodes = len ( self . labels ), ) # regular or flash else : if transformer == \"flash\" and FlashTransformerEncoder is None : raise ValueError ( \"flash transformer requires flash package\" ) # NOT flash transformer using the special tritton kernel # or parallelMHA (add the process group thing and faster) self . transformer = FlashTransformerEncoder ( d_model , nhead , nlayers , dropout = dropout , use_flash_attn = ( transformer == \"flash\" ), ** flash_attention_kwargs , ) # decoders # expression self . expr_decoder = decoders . ExprDecoder ( d_model , nfirst_labels_to_skip = len ( self . labels ) + 2 , dropout = dropout , ) # cls decoder self . cls_decoders = nn . ModuleDict () # should be a very simple classifier for most things # (maybe scale with the number of classes) should be 1 layer... for label , n_cls in labels . items (): self . cls_decoders [ label ] = decoders . ClsDecoder ( d_model , n_cls , layers = layers_cls , dropout = dropout ) # expression decoder from batch embbedding if mvc_decoder != \"None\" : self . mvc_decoder = decoders . MVCDecoder ( d_model , arch_style = mvc_decoder , dropout = dropout , ) else : self . mvc_decoder = None self . apply ( partial ( utils . _init_weights , n_layer = nlayers , ) ) self . save_hyperparameters () print ( self ) configure_optimizers @see pl.LightningModule Source code in scprint/model/model.py 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 def configure_optimizers ( self ): \"\"\"@see pl.LightningModule\"\"\" # https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam # optimizer = optim.Adam( # self.parameters(), # lr=self.hparams.lr, # betas=(0.9, 0.999), # eps=1e-08, # weight_decay=0, # amsgrad=False, # fused=False, # ) optimizer = optim . AdamW ( self . parameters (), lr = self . hparams . lr , betas = ( 0.9 , 0.999 ), eps = 1e-08 , weight_decay = self . weight_decay , amsgrad = False , fused = self . fused_adam , ) lr_scheduler = optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = \"min\" , patience = self . lr_patience , factor = 0.5 ) lr_dict = { \"scheduler\" : lr_scheduler , # The unit of the scheduler's step size, could also be 'step'. # 'epoch' updates the scheduler on epoch end whereas 'step' # updates it after a optimizer update. \"interval\" : \"epoch\" , # How many epochs/steps should pass between calls to # `scheduler.step()`. 1 corresponds to updating the learning # rate after every epoch/step. \"frequency\" : 1 , # Metric to to monitor for schedulers like `ReduceLROnPlateau` \"monitor\" : \"val_loss\" if self . trainer . val_dataloaders else \"train_loss\" , } self . lrfinder_steps = 0 for val in self . trainer . callbacks : if type ( val ) is _LRCallback : self . lrfinder_steps = val . num_training if type ( val ) is LearningRateFinder : self . lrfinder_steps = val . _num_training_steps return [ optimizer ], [ lr_dict ] forward forward also called on self(), a full forward pass on the model Parameters: gene_pos ( Tensor ) \u2013 A tensor of shape (minibatch, seq_len) representing the genes used for each cell in the minibatch. expression ( Tensor , default: None ) \u2013 A tensor of shape (minibatch, seq_len) representing the expression levels of genes in the minibatch. Defaults to None. mask ( Tensor , default: None ) \u2013 A tensor of shape (minibatch, seq_len) used to mask certain elements in the sequence during the forward pass. Defaults to None. full_depth ( Tensor , default: None ) \u2013 A tensor of shape (minibatch,) representing the full depth of each sequence in the minibatch. Defaults to None. timepoint ( Tensor , default: None ) \u2013 A tensor of shape (minibatch,) representing the timepoint associated with each sequence in the minibatch. Defaults to None. get_gene_emb ( bool , default: False ) \u2013 A flag indicating whether to return the gene embeddings. If True, the gene embeddings are included in the output. Defaults to False. do_sample ( bool , default: False ) \u2013 A flag indicating whether to sample the expression levels. If True, the expression levels are sampled during the forward pass. Defaults to False. get_attention_layer ( list , default: [] ) \u2013 A list indicating which attention layers to return. If not empty, the specified attention layers are included in the output. Defaults to []. Returns: \u2013 dict of output Tensors: A dictionary containing the output tensors from the forward pass. The keys of the dictionary depend on the input flags (get_gene_emb, do_sample, get_attention_layer). at minima, the dictionary contains the following: - \"mean\": the mean expression levels - \"zero_logits\": the logits for zero-inflated expression levels - \"disp\": the dispersion parameter - \"cell_embs\": the cell embeddings per class - \"cell_emb\": the main cell embedding - \"cls_output\": the output of the classifier Source code in scprint/model/model.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 def forward ( self , gene_pos : Tensor , depth_mult : Tensor , expression : Optional [ Tensor ] = None , mask : Optional [ Tensor ] = None , # (minibatch,) unormalized total counts full_depth : Optional [ Tensor ] = None , timepoint : Optional [ Tensor ] = None , # (new_minibatch_of_nxt_cells,) get_gene_emb : bool = False , do_sample : bool = False , get_attention_layer : list = [], ): \"\"\" forward also called on self(), a full forward pass on the model Args: gene_pos (Tensor): A tensor of shape (minibatch, seq_len) representing the genes used for each cell in the minibatch. expression (Tensor, optional): A tensor of shape (minibatch, seq_len) representing the expression levels of genes in the minibatch. Defaults to None. mask (Tensor, optional): A tensor of shape (minibatch, seq_len) used to mask certain elements in the sequence during the forward pass. Defaults to None. full_depth (Tensor, optional): A tensor of shape (minibatch,) representing the full depth of each sequence in the minibatch. Defaults to None. timepoint (Tensor, optional): A tensor of shape (minibatch,) representing the timepoint associated with each sequence in the minibatch. Defaults to None. get_gene_emb (bool, optional): A flag indicating whether to return the gene embeddings. If True, the gene embeddings are included in the output. Defaults to False. do_sample (bool, optional): A flag indicating whether to sample the expression levels. If True, the expression levels are sampled during the forward pass. Defaults to False. get_attention_layer (list, optional): A list indicating which attention layers to return. If not empty, the specified attention layers are included in the output. Defaults to []. Returns: dict of output Tensors: A dictionary containing the output tensors from the forward pass. The keys of the dictionary depend on the input flags (get_gene_emb, do_sample, get_attention_layer). at minima, the dictionary contains the following: - \"mean\": the mean expression levels - \"zero_logits\": the logits for zero-inflated expression levels - \"disp\": the dispersion parameter - \"cell_embs\": the cell embeddings per class - \"cell_emb\": the main cell embedding - \"cls_output\": the output of the classifier \"\"\" encoding = self . _encoder ( gene_pos , expression , mask , full_depth , timepoint ) transformer_output = self . transformer ( encoding , return_qkv = get_attention_layer ) if len ( get_attention_layer ) > 0 : transformer_output , qkvs = transformer_output return ( self . _decoder ( transformer_output , depth_mult , get_gene_emb , do_sample ), qkvs , ) else : return self . _decoder ( transformer_output , depth_mult , get_gene_emb , do_sample ) get_cell_embs get_cell_embs Parameters: layer_output ( Tensor ) \u2013 The output tensor from a layer in the model. Raises: ValueError \u2013 Raised when an unknown cell embedding style is encountered. Returns: Tensor \u2013 The cell embeddings tensor. Source code in scprint/model/model.py 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 def get_cell_embs ( self , layer_output ): \"\"\" get_cell_embs Args: layer_output (Tensor): The output tensor from a layer in the model. Raises: ValueError: Raised when an unknown cell embedding style is encountered. Returns: Tensor: The cell embeddings tensor. \"\"\" if self . cell_emb_style == \"cls\" and self . labels is not None : # (minibatch, embsize) cell_emb = layer_output [:, : 2 + len ( self . labels )] elif self . cell_emb_style == \"avg-pool\" : cell_emb = torch . mean ( layer_output , dim = 1 ) else : raise ValueError ( f \"Unknown cell_emb_style: { self . cell_emb_style } \" ) return cell_emb log_adata log_adata will log an adata from predictions. It will log to tensorboard and wandb if available see @utils.log_adata Source code in scprint/model/model.py 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 def log_adata ( self , gtclass = None , name = \"\" ): \"\"\" log_adata will log an adata from predictions. It will log to tensorboard and wandb if available see @utils.log_adata \"\"\" try : mdir = self . logger . save_dir if self . logger . save_dir is not None else \"/tmp\" except : mdir = \"/tmp\" adata = utils . make_adata ( self . pred , self . embs , self . labels , self . trainer . global_step , self . label_decoders , gtclass , name , mdir , ) try : self . logger . experiment . add_figure ( fig ) except : print ( \"couldn't log to tensorboard\" ) try : self . logger . log_image ( key = \"umaps\" , images = [ fig ]) except : print ( \"couldn't log to wandb\" ) return adata on_fit_start @see pl.LightningModule Source code in scprint/model/model.py 516 517 518 519 520 521 522 def on_fit_start ( self ): \"\"\"@see pl.LightningModule\"\"\" if type ( self . transformer ) is FlashTransformerEncoder : for encoder_layers in self . transformer . blocks : encoder_layers . set_seq_parallel ( True ) for k , v in self . mat_cls_hierarchy . items (): self . mat_cls_hierarchy [ k ] = v . to ( self . device ) on_predict_epoch_end @see pl.LightningModule will Source code in scprint/model/model.py 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 def on_predict_epoch_end ( self ): \"\"\"@see pl.LightningModule will\"\"\" if not self . trainer . is_global_zero : print ( \"you are not on the main node. cancelling logging step\" ) return self . expr_pred = [ i . to ( device = \"cpu\" , dtype = torch . float32 ) for i in self . expr_pred ] self . pred = self . pred . to ( device = \"cpu\" , dtype = torch . float32 ) self . embs = self . embs . to ( device = \"cpu\" , dtype = torch . float32 ) self . pos = self . pos . to ( device = \"cpu\" , dtype = torch . int32 ) self . mean_attn = [ i / self . num_pred_batch for i in self . mean_attn ] return self . log_adata () on_predict_epoch_start @see pl.LightningModule Source code in scprint/model/model.py 945 946 947 948 949 950 951 def on_predict_epoch_start ( self ): \"\"\"@see pl.LightningModule\"\"\" self . embs = None self . num_pred_batch = 0 if type ( self . transformer ) is FlashTransformerEncoder : for encoder_layers in self . transformer . blocks : encoder_layers . set_seq_parallel ( False ) on_validation_epoch_end @see pl.LightningModule Source code in scprint/model/model.py 912 913 914 915 916 917 def on_validation_epoch_end ( self ): \"\"\"@see pl.LightningModule\"\"\" if not self . trainer . is_global_zero : print ( \"you are not on the main node. cancelling logging step\" ) return self . log_adata ( gtclass = self . info ) optimizer_step @see pl.LightningModule Source code in scprint/model/model.py 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 def optimizer_step ( self , epoch , batch_idx , optimizer , optimizer_closure ): \"\"\"@see pl.LightningModule\"\"\" # update params optimizer . step ( closure = optimizer_closure ) # manually warm up lr without a scheduler # making sure that we don't do this during lrfinder if ( self . trainer . global_step < self . warmup_duration + self . lrfinder_steps ) and self . lrfinder_steps < self . trainer . global_step : lr_scale = min ( 1.0 , float ( self . trainer . global_step + 1 ) / self . warmup_duration ) for pg in optimizer . param_groups : pg [ \"lr\" ] = lr_scale * self . hparams . lr self . log ( \"lr\" , lr_scale * self . hparams . lr ) else : self . log ( \"lr\" , self . lr ) predict_step embed given gene expression, encode the gene embedding and cell embedding. Returns: Tensor \u2013 description Source code in scprint/model/model.py 953 954 955 956 957 958 959 960 961 962 963 def predict_step ( self , batch , batch_idx ): \"\"\" embed given gene expression, encode the gene embedding and cell embedding. Args: batch @see training_step Returns: Tensor: _description_ \"\"\" return self . _predict ( batch [ \"genes\" ], batch [ \"x\" ], batch [ \"depth\" ]) test_step @see pl.LightningModule Source code in scprint/model/model.py 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 def test_step ( self , batch , batch_idx ): \"\"\" @see pl.LightningModule Args: batch @see training_step \"\"\" total_loss , losses = self . _full_training ( batch , self . do_denoise , self . noise , self . do_next_tp , self . do_cce , self . cce_sim , self . do_ecs , self . do_mvc , self . do_adv_cls , self . do_generate , self . mask_ratio , ) self . log ( \"test_loss: \" , total_loss , sync_dist = True ) self . log_dict ( losses , sync_dist = True ) return total_loss training_step training_step defines the train loop. It is independent of forward @see pl.LightningModule Returns: _type_ \u2013 description Source code in scprint/model/model.py 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 def training_step ( self , batch : Dict [ Tensor ], batch_idx , ): \"\"\" training_step defines the train loop. It is independent of forward @see pl.LightningModule Returns: _type_: _description_ \"\"\" # TASK 1 & 2 & 3 (first pass, expression reconstruction, label prediction) total_loss , losses = self . _full_training ( batch , self . do_denoise , self . noise , self . do_next_tp , self . do_cce , self . cce_sim , self . do_ecs , self . do_mvc , self . do_adv_cls , self . do_generate , self . mask_ratio , ) self . log ( \"train_loss\" , total_loss , prog_bar = True ) self . log_dict ( losses , prog_bar = True ) return total_loss validation_step validation_step defines the validation loop. It is independent of forward @see pl.LightningModule Parameters: batch ( list [ Tensor ] ) \u2013 @see training_step Source code in scprint/model/model.py 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 def validation_step ( self , batch , batch_idx , ): \"\"\" validation_step defines the validation loop. It is independent of forward @see pl.LightningModule Args: batch (list[Tensor]): @see training_step \"\"\" val_loss , losses = self . _full_training ( batch , self . do_denoise , self . noise , self . do_next_tp , self . do_cce , self . cce_sim , self . do_ecs , self . do_mvc , self . do_adv_cls , self . do_generate , self . mask_ratio , ) expression = batch [ \"x\" ] gene_pos = batch [ \"genes\" ] depth = batch [ \"depth\" ] if self . embs is not None : if self . embs . shape [ 0 ] < 10000 : self . _predict ( gene_pos , expression , depth ) self . info = torch . cat ([ self . info , batch [ \"class\" ]]) else : self . _predict ( gene_pos , expression , depth ) self . info = batch [ \"class\" ] self . log ( \"val_loss\" , val_loss , sync_dist = True ) self . log_dict ( losses , sync_dist = True ) return val_loss scprint.model.loss classification Computes the classification loss for a given batch of predictions and ground truth labels. Parameters: labelname ( str ) \u2013 The name of the label. pred ( Tensor ) \u2013 The predicted logits for the batch. cl ( Tensor ) \u2013 The ground truth labels for the batch. maxsize ( int ) \u2013 The number of possible labels. cls_hierarchy ( dict , default: {} ) \u2013 The hierarchical structure of the labels. Defaults to {}. Raises: ValueError \u2013 If the labelname is not found in the cls_hierarchy dictionary. Returns: \u2013 torch.Tensor: The computed binary cross entropy loss for the given batch. Source code in scprint/model/loss.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 def classification ( labelname , pred , cl , maxsize , cls_hierarchy = {}): \"\"\" Computes the classification loss for a given batch of predictions and ground truth labels. Args: labelname (str): The name of the label. pred (torch.Tensor): The predicted logits for the batch. cl (torch.Tensor): The ground truth labels for the batch. maxsize (int): The number of possible labels. cls_hierarchy (dict, optional): The hierarchical structure of the labels. Defaults to {}. Raises: ValueError: If the labelname is not found in the cls_hierarchy dictionary. Returns: torch.Tensor: The computed binary cross entropy loss for the given batch. \"\"\" newcl = torch . zeros ( ( cl . shape [ 0 ], maxsize ), device = cl . device ) # batchsize * n_labels # if we don't know the label we set the weight to 0 else to 1 valid_indices = ( cl != - 1 ) & ( cl < maxsize ) valid_cl = cl [ valid_indices ] newcl [ valid_indices , valid_cl ] = 1 weight = torch . ones_like ( newcl , device = cl . device ) weight [ cl == - 1 , :] = 0 inv = cl >= maxsize # if we have non leaf values, we don't know so we don't compute grad and set weight to 0 # and add labels that won't be counted but so that we can still use them if inv . any (): if labelname in cls_hierarchy . keys (): clhier = cls_hierarchy [ labelname ] invw = weight [ inv ] invw [ clhier [ cl [ inv ] - maxsize ]] = 0 weight [ inv ] = invw addnewcl = torch . ones ( weight . shape [ 0 ], device = pred . device ) # no need to set the other to 0 addweight = torch . zeros ( weight . shape [ 0 ], device = pred . device ) addweight [ inv ] = 1 # computing hierarchical labels and adding them to cl cpred = pred . clone () cpred [ ~ inv ] = torch . finfo ( pred . dtype ) . min cpred = torch . logsumexp ( cpred , dim =- 1 ) newcl = torch . cat ([ newcl , addnewcl . unsqueeze ( 1 )], dim = 1 ) pred = torch . cat ([ pred , cpred . unsqueeze ( 1 )], dim = 1 ) weight = torch . cat ([ weight , addweight . unsqueeze ( 1 )], dim = 1 ) else : raise ValueError ( \"need to use cls_hierarchy for this usecase\" ) myloss = torch . nn . functional . binary_cross_entropy_with_logits ( pred , target = newcl , weight = weight ) return myloss classifier_loss Compute the cross entropy loss between prediction and target. Source code in scprint/model/loss.py 151 152 153 154 155 156 def classifier_loss ( pred : torch . Tensor , target : torch . Tensor ) -> torch . Tensor : \"\"\" Compute the cross entropy loss between prediction and target. \"\"\" loss = F . cross_entropy ( pred , target ) return loss criterion_neg_log_bernoulli Compute the negative log-likelihood of Bernoulli distribution Source code in scprint/model/loss.py 159 160 161 162 163 164 165 166 167 168 def criterion_neg_log_bernoulli ( input : torch . Tensor , target : torch . Tensor , mask : torch . Tensor ) -> torch . Tensor : \"\"\" Compute the negative log-likelihood of Bernoulli distribution \"\"\" mask = mask . float () bernoulli = torch . distributions . Bernoulli ( probs = input ) masked_log_probs = bernoulli . log_prob (( target > 0 ) . float ()) * mask return - masked_log_probs . sum () / mask . sum () ecs ecs Computes the similarity of cell embeddings based on a threshold. Parameters: cell_emb ( Tensor ) \u2013 A tensor representing cell embeddings. ecs_threshold ( float , default: 0.5 ) \u2013 A threshold for determining similarity. Defaults to 0.5. Returns: \u2013 torch.Tensor: A tensor representing the mean of 1 minus the square of the difference between the cosine similarity and the threshold. Source code in scprint/model/loss.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def ecs ( cell_emb , ecs_threshold = 0.5 ): \"\"\" ecs Computes the similarity of cell embeddings based on a threshold. Args: cell_emb (torch.Tensor): A tensor representing cell embeddings. ecs_threshold (float, optional): A threshold for determining similarity. Defaults to 0.5. Returns: torch.Tensor: A tensor representing the mean of 1 minus the square of the difference between the cosine similarity and the threshold. \"\"\" # Here using customized cosine similarity instead of F.cosine_similarity # to avoid the pytorch issue of similarity larger than 1.0, pytorch # 78064 # normalize the embedding cell_emb_normed = F . normalize ( cell_emb , p = 2 , dim = 1 ) ecs_sim = torch . mm ( cell_emb_normed , cell_emb_normed . t ()) # mask out diagnal elements mask = torch . eye ( ecs_sim . size ( 0 )) . bool () . to ( ecs_sim . device ) cos_sim = torch . mm ( cell_emb_normed , cell_emb_normed . t ()) cos_sim = cos_sim . masked_fill ( mask , 0.0 ) # only optimize positive similarities cos_sim = F . relu ( cos_sim ) return torch . mean ( 1 - ( cos_sim - ecs_threshold ) ** 2 ) graph_similarity_loss Compute the similarity of 2 generated graphs. Source code in scprint/model/loss.py 182 183 184 185 186 187 188 189 190 def graph_similarity_loss ( input1 : torch . Tensor , input2 : torch . Tensor , mask : torch . Tensor ) -> torch . Tensor : \"\"\" Compute the similarity of 2 generated graphs. \"\"\" mask = mask . float () loss = F . mse_loss ( input1 * mask , input2 * mask , reduction = \"sum\" ) return loss / mask . sum () graph_sparsity_loss Compute the sparsity of generated graphs. Source code in scprint/model/loss.py 193 194 195 196 197 198 199 def graph_sparsity_loss ( input : torch . Tensor , mask : torch . Tensor ) -> torch . Tensor : \"\"\" Compute the sparsity of generated graphs. \"\"\" mask = mask . float () loss = F . mse_loss ( input * mask , torch . zeros_like ( input ) * mask , reduction = \"sum\" ) return loss / mask . sum () masked_mae_loss Compute the masked MAE loss between input and target. MAE = mean absolute error Source code in scprint/model/loss.py 18 19 20 21 22 23 24 25 26 27 def masked_mae_loss ( input : torch . Tensor , target : torch . Tensor , mask : torch . Tensor ) -> torch . Tensor : \"\"\" Compute the masked MAE loss between input and target. MAE = mean absolute error \"\"\" mask = mask . float () loss = F . l1_loss ( input * mask , target * mask , reduction = \"sum\" ) return loss / mask . sum () masked_mse_loss Compute the masked MSE loss between input and target. Source code in scprint/model/loss.py 7 8 9 10 11 12 13 14 15 def masked_mse_loss ( input : torch . Tensor , target : torch . Tensor , mask : torch . Tensor ) -> torch . Tensor : \"\"\" Compute the masked MSE loss between input and target. \"\"\" mask = mask . float () loss = F . mse_loss ( input * mask , target * mask , reduction = \"sum\" ) return loss / mask . sum () masked_nb_loss Compute the masked negative binomial loss between input and target. Source code in scprint/model/loss.py 30 31 32 33 34 35 36 37 38 39 def masked_nb_loss ( input : torch . Tensor , target : torch . Tensor , mask : torch . Tensor ) -> torch . Tensor : \"\"\" Compute the masked negative binomial loss between input and target. \"\"\" mask = mask . float () nb = torch . distributions . NegativeBinomial ( total_count = target , probs = input ) masked_log_probs = nb . log_prob ( target ) * mask return - masked_log_probs . sum () / mask . sum () masked_relative_error Compute the masked relative error between input and target. Source code in scprint/model/loss.py 171 172 173 174 175 176 177 178 179 def masked_relative_error ( input : torch . Tensor , target : torch . Tensor , mask : torch . LongTensor ) -> torch . Tensor : \"\"\" Compute the masked relative error between input and target. \"\"\" assert mask . any () loss = torch . abs ( input [ mask ] - target [ mask ]) / ( target [ mask ] + 1e-6 ) return loss . mean () nb This negative binomial function was taken from: Title: scvi-tools Authors: Romain Lopez romain_lopez@gmail.com , Adam Gayoso adamgayoso@berkeley.edu , Galen Xing gx2113@columbia.edu Date: 16th November 2020 Code version: 0.8.1 Availability: https://github.com/YosefLab/scvi-tools/blob/8f5a9cc362325abbb7be1e07f9523cfcf7e55ec0/scvi/core/distributions/_negative_binomial.py Computes negative binomial loss. Parameters x: torch.Tensor Torch Tensor of ground truth data. mu: torch.Tensor Torch Tensor of means of the negative binomial (has to be positive support). theta: torch.Tensor Torch Tensor of inverse dispersion parameter (has to be positive support). eps: Float numerical stability constant. Returns If 'mean' is 'True' NB loss value gets returned, otherwise Torch tensor of losses gets returned. Source code in scprint/model/loss.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def nb ( x : torch . Tensor , mu : torch . Tensor , theta : torch . Tensor , eps = 1e-8 ): \"\"\" This negative binomial function was taken from: Title: scvi-tools Authors: Romain Lopez <romain_lopez@gmail.com>, Adam Gayoso <adamgayoso@berkeley.edu>, Galen Xing <gx2113@columbia.edu> Date: 16th November 2020 Code version: 0.8.1 Availability: https://github.com/YosefLab/scvi-tools/blob/8f5a9cc362325abbb7be1e07f9523cfcf7e55ec0/scvi/core/distributions/_negative_binomial.py Computes negative binomial loss. Parameters ---------- x: torch.Tensor Torch Tensor of ground truth data. mu: torch.Tensor Torch Tensor of means of the negative binomial (has to be positive support). theta: torch.Tensor Torch Tensor of inverse dispersion parameter (has to be positive support). eps: Float numerical stability constant. Returns ------- If 'mean' is 'True' NB loss value gets returned, otherwise Torch tensor of losses gets returned. \"\"\" if theta . ndimension () == 1 : theta = theta . view ( 1 , theta . size ( 0 )) log_theta_mu_eps = torch . log ( theta + mu + eps ) res = ( theta * ( torch . log ( theta + eps ) - log_theta_mu_eps ) + x * ( torch . log ( mu + eps ) - log_theta_mu_eps ) + torch . lgamma ( x + theta ) - torch . lgamma ( theta ) - torch . lgamma ( x + 1 ) ) return res . sum ( - 1 ) . mean () similarity Dot product or cosine similarity Source code in scprint/model/loss.py 202 203 204 205 206 207 208 def similarity ( x , y , temp ): \"\"\" Dot product or cosine similarity \"\"\" res = F . cosine_similarity ( x , y ) / temp labels = torch . arange ( res . size ( 0 )) . long () . to ( device = res . device ) return F . cross_entropy ( res , labels ) zinb This zero-inflated negative binomial function was taken from: Title: scvi-tools Authors: Romain Lopez romain_lopez@gmail.com , Adam Gayoso adamgayoso@berkeley.edu , Galen Xing gx2113@columbia.edu Date: 16th November 2020 Code version: 0.8.1 Availability: https://github.com/YosefLab/scvi-tools/blob/8f5a9cc362325abbb7be1e07f9523cfcf7e55ec0/scvi/core/distributions/_negative_binomial.py Computes zero inflated negative binomial loss. Parameters x: torch.Tensor Torch Tensor of ground truth data. mu: torch.Tensor Torch Tensor of means of the negative binomial (has to be positive support). theta: torch.Tensor Torch Tensor of inverses dispersion parameter (has to be positive support). pi: torch.Tensor Torch Tensor of logits of the dropout parameter (real support) eps: Float numerical stability constant. Returns If 'mean' is 'True' ZINB loss value gets returned, otherwise Torch tensor of losses gets returned. Source code in scprint/model/loss.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def zinb ( target : torch . Tensor , mu : torch . Tensor , theta : torch . Tensor , pi : torch . Tensor , eps = 1e-6 , mask = None , ): \"\"\" This zero-inflated negative binomial function was taken from: Title: scvi-tools Authors: Romain Lopez <romain_lopez@gmail.com>, Adam Gayoso <adamgayoso@berkeley.edu>, Galen Xing <gx2113@columbia.edu> Date: 16th November 2020 Code version: 0.8.1 Availability: https://github.com/YosefLab/scvi-tools/blob/8f5a9cc362325abbb7be1e07f9523cfcf7e55ec0/scvi/core/distributions/_negative_binomial.py Computes zero inflated negative binomial loss. Parameters ---------- x: torch.Tensor Torch Tensor of ground truth data. mu: torch.Tensor Torch Tensor of means of the negative binomial (has to be positive support). theta: torch.Tensor Torch Tensor of inverses dispersion parameter (has to be positive support). pi: torch.Tensor Torch Tensor of logits of the dropout parameter (real support) eps: Float numerical stability constant. Returns ------- If 'mean' is 'True' ZINB loss value gets returned, otherwise Torch tensor of losses gets returned. \"\"\" softplus_pi = F . softplus ( - pi ) # uses log(sigmoid(x)) = -softplus(-x) log_theta_eps = torch . log ( theta + eps ) log_theta_mu_eps = torch . log ( theta + mu + eps ) pi_theta_log = - pi + theta * ( log_theta_eps - log_theta_mu_eps ) case_zero = F . softplus ( pi_theta_log ) - softplus_pi mul_case_zero = torch . mul (( target < eps ) . type ( torch . float32 ), case_zero ) case_non_zero = ( - softplus_pi + pi_theta_log + target * ( torch . log ( mu + eps ) - log_theta_mu_eps ) + torch . lgamma ( target + theta ) - torch . lgamma ( theta ) - torch . lgamma ( target + 1 ) ) mul_case_non_zero = torch . mul (( target > eps ) . type ( torch . float32 ), case_non_zero ) res = mul_case_zero + mul_case_non_zero # we want to minize the loss but maximize the log likelyhood return - res . sum ( - 1 ) . mean ()","title":"model"},{"location":"model/#documentation-for-the-model","text":"","title":"Documentation for the model"},{"location":"model/#scprint.model.utils","text":"","title":"utils"},{"location":"model/#scprint.model.utils.downsample_profile","text":"This function downsamples the expression profile of a given single cell RNA matrix. The noise is applied based on the renoise parameter, the total counts of the matrix, and the number of genes. The function first calculates the noise threshold (tnoise) based on the renoise parameter. It then generates an initial matrix count by applying a Poisson distribution to a random tensor scaled by the total counts and the number of genes. The function then models the sampling zeros by applying a Poisson distribution to a random tensor scaled by the noise threshold, the total counts, and the number of genes. The function also models the technical zeros by generating a random tensor and comparing it to the noise threshold. The final matrix count is calculated by subtracting the sampling zeros from the initial matrix count and multiplying by the technical zeros. The function ensures that the final matrix count is not less than zero by taking the maximum of the final matrix count and a tensor of zeros. The function returns the final matrix count. Parameters: mat ( Tensor ) \u2013 The input matrix. renoise ( float ) \u2013 The renoise parameter. totcounts ( Tensor ) \u2013 The total counts of the matrix. ngenes ( int ) \u2013 The number of genes. Returns: \u2013 torch.Tensor: The matrix count after applying noise. Source code in scprint/model/utils.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def downsample_profile ( mat , renoise ): \"\"\" This function downsamples the expression profile of a given single cell RNA matrix. The noise is applied based on the renoise parameter, the total counts of the matrix, and the number of genes. The function first calculates the noise threshold (tnoise) based on the renoise parameter. It then generates an initial matrix count by applying a Poisson distribution to a random tensor scaled by the total counts and the number of genes. The function then models the sampling zeros by applying a Poisson distribution to a random tensor scaled by the noise threshold, the total counts, and the number of genes. The function also models the technical zeros by generating a random tensor and comparing it to the noise threshold. The final matrix count is calculated by subtracting the sampling zeros from the initial matrix count and multiplying by the technical zeros. The function ensures that the final matrix count is not less than zero by taking the maximum of the final matrix count and a tensor of zeros. The function returns the final matrix count. Args: mat (torch.Tensor): The input matrix. renoise (float): The renoise parameter. totcounts (torch.Tensor): The total counts of the matrix. ngenes (int): The number of genes. Returns: torch.Tensor: The matrix count after applying noise. \"\"\" # Randomly drop on average N counts to each element of expression using a heavy tail Gaussian distribution # here we try to get the scale of the distribution so as to remove the right number of counts from each gene # https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02601-5#:~:text=Zero%20measurements%20in%20scRNA%2Dseq,generation%20of%20scRNA%2Dseq%20data. totcounts = mat . sum ( 1 ) batch = mat . shape [ 0 ] ngenes = mat . shape [ 1 ] tnoise = 1 - ( 1 - renoise ) ** ( 1 / 2 ) # we model the sampling zeros (dropping 30% of the reads) res = torch . poisson ( torch . rand (( batch , ngenes )) . to ( device = mat . device ) * (( tnoise * totcounts . unsqueeze ( 1 )) / ( 0.5 * ngenes )) ) . int () # we model the technical zeros (dropping 50% of the genes) drop = ( torch . rand (( batch , ngenes )) > tnoise ) . int () . to ( device = mat . device ) mat = ( mat - res ) * drop return torch . maximum ( mat , torch . Tensor ([[ 0 ]]) . to ( device = mat . device )) . int ()","title":"downsample_profile()"},{"location":"model/#scprint.model.utils.make_adata","text":"This function creates an AnnData object from the given input parameters. Parameters: pred ( Tensor ) \u2013 Predicted labels. The shape of the tensor is (n_cells, n_classes) embs ( Tensor ) \u2013 Embeddings of the cells. The shape of the tensor is (n_cells, n_features) labels ( list ) \u2013 List of labels for the predicted classes. step ( int , default: 0 ) \u2013 Step number. Default is 0. (for storing the anndata without overwriting others) label_decoders ( dict , default: None ) \u2013 Dictionary to map class codes to class names. Default is None. gtclass ( Tensor , default: None ) \u2013 Ground truth class. Default is None. name ( str , default: '' ) \u2013 Name of the AnnData object. Default is an empty string. mdir ( str , default: '/tmp' ) \u2013 Directory to save the AnnData object. Default is \"/tmp\". Returns: adata ( AnnData ) \u2013 The created AnnData object. Source code in scprint/model/utils.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def make_adata ( pred , embs , labels , step = 0 , label_decoders = None , gtclass = None , name = \"\" , mdir = \"/tmp\" ): \"\"\" This function creates an AnnData object from the given input parameters. Args: pred (torch.Tensor): Predicted labels. The shape of the tensor is (n_cells, n_classes) embs (torch.Tensor): Embeddings of the cells. The shape of the tensor is (n_cells, n_features) labels (list): List of labels for the predicted classes. step (int, optional): Step number. Default is 0. (for storing the anndata without overwriting others) label_decoders (dict, optional): Dictionary to map class codes to class names. Default is None. gtclass (torch.Tensor, optional): Ground truth class. Default is None. name (str, optional): Name of the AnnData object. Default is an empty string. mdir (str, optional): Directory to save the AnnData object. Default is \"/tmp\". Returns: adata (anndata.AnnData): The created AnnData object. \"\"\" colname = [ \"pred_\" + i for i in labels ] obs = np . array ( pred . to ( device = \"cpu\" , dtype = torch . int32 )) # label decoders is not cls_decoders. one is a dict to map class codes (ints) # to class names the other is the module the predict the class if label_decoders is not None : obs = np . array ( [ [ label_decoders [ labels [ i ]][ n ] for n in name ] for i , name in enumerate ( obs . T ) ] ) . T if gtclass is not None : colname += labels nobs = np . array ( gtclass . to ( device = \"cpu\" , dtype = torch . int32 )) if label_decoders is not None : nobs = np . array ( [ [ label_decoders [ labels [ i ]][ n ] for n in name ] for i , name in enumerate ( nobs . T ) ] ) . T obs = np . hstack ([ obs , nobs ]) adata = AnnData ( np . array ( embs . to ( device = \"cpu\" , dtype = torch . float32 )), obs = pd . DataFrame ( obs , columns = colname , ), ) for n in labels : if gtclass is not None : tr = translate ( adata . obs [ n ] . tolist (), n ) if tr is not None : adata . obs [ \"conv_\" + n ] = adata . obs [ n ] . replace ( tr ) tr = translate ( adata . obs [ \"pred_\" + n ] . tolist (), n ) if tr is not None : adata . obs [ \"conv_pred_\" + n ] = adata . obs [ \"pred_\" + n ] . replace ( tr ) sc . pp . neighbors ( adata ) sc . tl . umap ( adata ) sc . tl . leiden ( adata ) adata . obs = adata . obs . astype ( \"category\" ) print ( adata ) if gtclass is not None : color = [ i for pair in zip ( [ \"conv_\" + i if \"conv_\" + i in adata . obs . columns else i for i in labels ], [ \"conv_pred_\" + i if \"conv_pred_\" + i in adata . obs . columns else \"pred_\" + i for i in labels ], ) for i in pair ] _ , axs = plt . subplots ( int ( len ( color ) / 2 ), 2 , figsize = ( 24 , len ( color ) * 4 )) plt . subplots_adjust ( wspace = 1 ) for i , col in enumerate ( color ): sc . pl . umap ( adata , color = col , ax = axs [ i // 2 , i % 2 ], show = False , ) else : color = [ \"conv_pred_\" + i if \"conv_pred_\" + i in adata . obs . columns else \"pred_\" + i for i in labels ] fig , axs = plt . subplots ( len ( color ), 1 , figsize = ( 16 , len ( color ) * 8 )) for i , col in enumerate ( color ): sc . pl . umap ( adata , color = col , ax = axs [ i ], show = False , ) adata . write ( mdir + \"/step_\" + str ( step ) + \"_\" + name + \".h5ad\" ) return adata","title":"make_adata()"},{"location":"model/#scprint.model.utils.masker","text":"Randomly mask a batch of data. Parameters: values ( array - like ) \u2013 A batch of tokenized data, with shape (batch_size, n_features). mask_ratio ( float , default: 0.15 ) \u2013 The ratio of genes to mask, default to 0.15. mask_value ( int , default: 1 ) \u2013 The value to mask with, default to -1. pad_value ( int ) \u2013 The value of padding in the values, will be kept unchanged. Returns: Tensor \u2013 torch.Tensor: A tensor of masked data. Source code in scprint/model/utils.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def masker ( length : int , batch_size : int = 1 , mask_ratio : float = 0.15 , mask_prob : Optional [ Union [ torch . Tensor , np . ndarray ]] = None , # n_features mask_value : int = 1 , ) -> torch . Tensor : \"\"\" Randomly mask a batch of data. Args: values (array-like): A batch of tokenized data, with shape (batch_size, n_features). mask_ratio (float): The ratio of genes to mask, default to 0.15. mask_value (int): The value to mask with, default to -1. pad_value (int): The value of padding in the values, will be kept unchanged. Returns: torch.Tensor: A tensor of masked data. \"\"\" mask = [] for _ in range ( batch_size ): m = np . zeros ( length ) loc = np . random . choice ( a = length , size = int ( length * mask_ratio ), replace = False , p = mask_prob ) m [ loc ] = mask_value mask . append ( m ) return torch . Tensor ( np . array ( mask )) . to ( torch . bool )","title":"masker()"},{"location":"model/#scprint.model.utils.translate","text":"translate This function translates the given value based on the specified type. Parameters: val ( str / list / set / dict / Counter ) \u2013 The value to be translated. t ( str , default: 'cell_type_ontology_term_id' ) \u2013 The type of translation to be performed. Defaults to \"cell_type_ontology_term_id\". Returns: dict \u2013 A dictionary with the translated values. Source code in scprint/model/utils.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def translate ( val , t = \"cell_type_ontology_term_id\" ): \"\"\" translate This function translates the given value based on the specified type. Args: val (str/list/set/dict/Counter): The value to be translated. t (str, optional): The type of translation to be performed. Defaults to \"cell_type_ontology_term_id\". Returns: dict: A dictionary with the translated values. \"\"\" if t == \"cell_type_ontology_term_id\" : obj = bt . CellType . df () . set_index ( \"ontology_id\" ) elif t == \"assay_ontology_term_id\" : obj = bt . ExperimentalFactor . df () . set_index ( \"ontology_id\" ) elif t == \"tissue_ontology_term_id\" : obj = bt . Tissue . df () . set_index ( \"ontology_id\" ) elif t == \"disease_ontology_term_id\" : obj = bt . Disease . df () . set_index ( \"ontology_id\" ) elif t == \"self_reported_ethnicity_ontology_term_id\" : obj = bt . Ethnicity . df () . set_index ( \"ontology_id\" ) else : return None if type ( val ) is str : return { val : obj . loc [ val ][ \"name\" ]} elif type ( val ) is list or type ( val ) is set : return { i : obj . loc [ i ][ \"name\" ] for i in set ( val )} elif type ( val ) is dict or type ( val ) is Counter : return { obj . loc [ k ][ \"name\" ]: v for k , v in val . items ()}","title":"translate()"},{"location":"model/#scprint.model.utils.zinb_sample","text":"zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution. Parameters: mu ( Tensor ) \u2013 The mean of the Negative Binomial (NB) distribution. theta ( Tensor ) \u2013 The dispersion parameter of the NB distribution. zi_probs ( Tensor ) \u2013 The zero-inflation probabilities. sample_shape ( Size , default: Size ([]) ) \u2013 The output shape. Defaults to torch.Size([]). Returns: \u2013 torch.Tensor: A sample from the ZINB distribution. Source code in scprint/model/utils.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def zinb_sample ( mu , theta , zi_probs , sample_shape = torch . Size ([])): \"\"\" zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution. Args: mu (torch.Tensor): The mean of the Negative Binomial (NB) distribution. theta (torch.Tensor): The dispersion parameter of the NB distribution. zi_probs (torch.Tensor): The zero-inflation probabilities. sample_shape (torch.Size, optional): The output shape. Defaults to torch.Size([]). Returns: torch.Tensor: A sample from the ZINB distribution. \"\"\" concentration = theta rate = theta / mu # Important remark: Gamma is parametrized by the rate = 1/scale! gamma_d = Gamma ( concentration = concentration , rate = rate ) p_means = gamma_d . sample ( sample_shape ) # Clamping as distributions objects can have buggy behaviors when # their parameters are too high l_train = torch . clamp ( p_means , max = 1e8 ) samp = Poisson ( l_train ) . sample () # Shape : (n_samples, n_cells_batch, n_vars) is_zero = torch . rand_like ( samp ) <= zi_probs samp_ = torch . where ( is_zero , torch . zeros_like ( samp ), samp ) return samp_","title":"zinb_sample()"},{"location":"model/#scprint.model.encoders","text":"","title":"encoders"},{"location":"model/#scprint.model.encoders.CategoryValueEncoder","text":"Bases: Module Encodes categorical values into a vector using an embedding layer and layer normalization. Parameters: num_embeddings ( int ) \u2013 The number of possible values. embedding_dim ( int ) \u2013 The dimension of the output vectors. padding_idx ( int , default: None ) \u2013 The index of the padding token. Defaults to None. Returns: \u2013 torch.Tensor: A tensor representing the encoded categorical values. Note: not used in the current version of scprint. Source code in scprint/model/encoders.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def __init__ ( self , num_embeddings : int , embedding_dim : int , padding_idx : Optional [ int ] = None , ): \"\"\" Encodes categorical values into a vector using an embedding layer and layer normalization. Args: num_embeddings (int): The number of possible values. embedding_dim (int): The dimension of the output vectors. padding_idx (int, optional): The index of the padding token. Defaults to None. Returns: torch.Tensor: A tensor representing the encoded categorical values. Note: not used in the current version of scprint. \"\"\" super ( CategoryValueEncoder , self ) . __init__ () self . embedding = nn . Embedding ( num_embeddings , embedding_dim , padding_idx = padding_idx ) self . enc_norm = nn . LayerNorm ( embedding_dim )","title":"CategoryValueEncoder"},{"location":"model/#scprint.model.encoders.ContinuousValueEncoder","text":"Bases: Module Encode real number values to a vector using neural nets projection. Parameters: d_model ( int ) \u2013 The dimension of the input vectors. dropout ( float , default: 0.1 ) \u2013 The dropout rate to apply to the output of the positional encoding. max_value ( int , default: 100000 ) \u2013 The maximum value of the input. Defaults to 100_000. layers ( int , default: 1 ) \u2013 The number of layers in the encoder. Defaults to 1. size ( int , default: 1 ) \u2013 The size of the input. Defaults to 1. Returns: \u2013 torch.Tensor: A tensor representing the encoded continuous values. Source code in scprint/model/encoders.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def __init__ ( self , d_model : int , dropout : float = 0.1 , max_value : int = 100_000 , layers : int = 1 , size : int = 1 , ): \"\"\" Encode real number values to a vector using neural nets projection. Args: d_model (int): The dimension of the input vectors. dropout (float, optional): The dropout rate to apply to the output of the positional encoding. max_value (int, optional): The maximum value of the input. Defaults to 100_000. layers (int, optional): The number of layers in the encoder. Defaults to 1. size (int, optional): The size of the input. Defaults to 1. Returns: torch.Tensor: A tensor representing the encoded continuous values. \"\"\" super ( ContinuousValueEncoder , self ) . __init__ () self . max_value = max_value self . encoder = nn . ModuleList () for i in range ( layers ): self . encoder . append ( nn . Linear ( size if i == 0 else d_model , d_model )) self . encoder . append ( nn . LayerNorm ( d_model )) self . encoder . append ( nn . ReLU ()) self . encoder . append ( nn . Dropout ( p = dropout ))","title":"ContinuousValueEncoder"},{"location":"model/#scprint.model.encoders.ContinuousValueEncoder.forward","text":"Parameters: x ( Tensor ) \u2013 Tensor, shape [batch_size, seq_len] Source code in scprint/model/encoders.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def forward ( self , x : Tensor , mask : Tensor = None ) -> Tensor : \"\"\" Args: x: Tensor, shape [batch_size, seq_len] \"\"\" # TODO: test using actual embedding layer if input is categorical # expand last dimension x = x . unsqueeze ( - 1 ) # use the mask embedding when x=-1 # mask = (x == -1).float() x = torch . clamp ( x , min = 0 , max = self . max_value ) for val in self . encoder : x = val ( x ) if mask is not None : x = x . masked_fill_ ( mask . unsqueeze ( - 1 ), 0 ) return x","title":"forward()"},{"location":"model/#scprint.model.encoders.DPositionalEncoding","text":"Bases: Module The PositionalEncoding module applies a positional encoding to a sequence of vectors. This is necessary for the Transformer model, which does not have any inherent notion of position in a sequence. The positional encoding is added to the input embeddings and allows the model to attend to positions in the sequence. Parameters: d_model ( int ) \u2013 The dimension of the input vectors. dropout ( float , default: 0.1 ) \u2013 The dropout rate to apply to the output of the positional encoding. max_len ( int ) \u2013 The maximum length of a sequence that this module can handle. Note: not used in the current version of scprint. Source code in scprint/model/encoders.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def __init__ ( self , d_model : int , max_len_x : int , max_len_y : int , maxvalue_x = 10000.0 , maxvalue_y = 10000.0 , dropout : float = 0.1 , ): super ( DPositionalEncoding , self ) . __init__ () self . dropout = nn . Dropout ( p = dropout ) position2 = torch . arange ( max_len_y ) . unsqueeze ( 1 ) position1 = torch . arange ( max_len_x ) . unsqueeze ( 1 ) half_n = d_model // 2 div_term2 = torch . exp ( torch . arange ( 0 , half_n , 2 ) * ( - math . log ( maxvalue_y ) / d_model ) ) div_term1 = torch . exp ( torch . arange ( 0 , half_n , 2 ) * ( - math . log ( maxvalue_x ) / d_model ) ) pe1 = torch . zeros ( max_len_x , 1 , d_model ) pe2 = torch . zeros ( max_len_y , 1 , d_model ) pe1 [:, 0 , 0 : half_n : 2 ] = torch . sin ( position1 * div_term1 ) pe1 [:, 0 , 1 : half_n : 2 ] = torch . cos ( position1 * div_term1 ) pe2 [:, 0 , half_n :: 2 ] = torch . sin ( position2 * div_term2 ) pe2 [:, 0 , 1 + half_n :: 2 ] = torch . cos ( position2 * div_term2 ) # https://github.com/tatp22/multidim-positional-encoding/blob/master/positional_encodings/torch_encodings.py # TODO: seems to do it differently. I hope it still works ok!! self . register_buffer ( \"pe1\" , pe1 ) self . register_buffer ( \"pe2\" , pe2 )","title":"DPositionalEncoding"},{"location":"model/#scprint.model.encoders.DPositionalEncoding.forward","text":"Parameters: x ( Tensor ) \u2013 Tensor, shape [seq_len, batch_size, embedding_dim] Source code in scprint/model/encoders.py 162 163 164 165 166 167 168 169 170 def forward ( self , x : Tensor , pos_x : Tensor , pos_y : Tensor ) -> Tensor : \"\"\" Args: x: Tensor, shape [seq_len, batch_size, embedding_dim] \"\"\" # TODO: try with a continuous value encoder of size 2 (start, end where they are normalized to 0-1) x = x + self . pe1 [ pos_x ] x = x + self . pe2 [ pos_y ] return self . dropout ( x )","title":"forward()"},{"location":"model/#scprint.model.encoders.GeneEncoder","text":"Bases: Module Encodes gene sequences into a continuous vector space using an embedding layer. The output is then normalized using a LayerNorm. Parameters: num_embeddings ( int ) \u2013 The number of possible values. embedding_dim ( int ) \u2013 The dimension of the output vectors. padding_idx ( int , default: None ) \u2013 The index of the padding token. Defaults to None. weights ( Tensor , default: None ) \u2013 The initial weights for the embedding layer. Defaults to None. dropout ( float , default: 0.1 ) \u2013 The dropout rate to apply to the output of the positional encoding. Defaults to 0.1. freeze ( bool , default: False ) \u2013 Whether to freeze the weights of the embedding layer. Defaults to False. Note: not used in the current version of scprint. Source code in scprint/model/encoders.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , num_embeddings : int , embedding_dim : int , padding_idx : Optional [ int ] = None , weights : Optional [ Tensor ] = None , dropout : float = 0.1 , freeze : bool = False , ): \"\"\" Encodes gene sequences into a continuous vector space using an embedding layer. The output is then normalized using a LayerNorm. Args: num_embeddings (int): The number of possible values. embedding_dim (int): The dimension of the output vectors. padding_idx (int, optional): The index of the padding token. Defaults to None. weights (Tensor, optional): The initial weights for the embedding layer. Defaults to None. dropout (float, optional): The dropout rate to apply to the output of the positional encoding. Defaults to 0.1. freeze (bool, optional): Whether to freeze the weights of the embedding layer. Defaults to False. Note: not used in the current version of scprint. \"\"\" super ( GeneEncoder , self ) . __init__ () self . embedding = nn . Embedding ( num_embeddings , embedding_dim , padding_idx = padding_idx , _freeze = freeze ) if weights is not None : # concat a zero vector to the weight # this is to make the embedding of the padding token to be zero # weights = torch.cat( # [torch.Tensor(weights), torch.zeros(1, embedding_dim)], dim=0 # ) self . embedding . weight . data . copy_ ( torch . Tensor ( weights )) self . enc_norm = nn . LayerNorm ( embedding_dim ) self . dropout = nn . Dropout ( p = dropout )","title":"GeneEncoder"},{"location":"model/#scprint.model.encoders.PositionalEncoding","text":"Bases: Module The PositionalEncoding module applies a positional encoding to a sequence of vectors. This is necessary for the Transformer model, which does not have any inherent notion of position in a sequence. The positional encoding is added to the input embeddings and allows the model to attend to positions in the sequence. Parameters: d_model ( int ) \u2013 The dimension of the input vectors. dropout ( float , default: 0.1 ) \u2013 The dropout rate to apply to the output of the positional encoding. max_len ( int ) \u2013 The maximum length of a sequence that this module can handle. Note: not used in the current version of scprint. Source code in scprint/model/encoders.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __init__ ( self , d_model : int , max_len : int , token_to_pos : dict [ str , int ], # [token, pos] dropout : float = 0.1 , maxval = 10000.0 , ): \"\"\" The PositionalEncoding module applies a positional encoding to a sequence of vectors. This is necessary for the Transformer model, which does not have any inherent notion of position in a sequence. The positional encoding is added to the input embeddings and allows the model to attend to positions in the sequence. Args: d_model (int): The dimension of the input vectors. dropout (float, optional): The dropout rate to apply to the output of the positional encoding. max_len (int, optional): The maximum length of a sequence that this module can handle. Note: not used in the current version of scprint. \"\"\" super ( PositionalEncoding , self ) . __init__ () self . dropout = nn . Dropout ( p = dropout ) position = torch . arange ( max_len ) . unsqueeze ( 1 ) # Create a dictionary to convert token to position div_term = torch . exp ( torch . arange ( 0 , d_model , 2 ) * ( - math . log ( maxval ) / d_model ) ) pe = torch . zeros ( max_len , 1 , d_model ) pe [:, 0 , 0 :: 2 ] = torch . sin ( position * div_term ) pe [:, 0 , 1 :: 2 ] = torch . cos ( position * div_term ) # we reorder them and map them to gene_id (position) arr = [] for k , v in token_to_pos . items (): arr . append ( pe [ v - 1 ] . numpy ()) pe = torch . Tensor ( np . array ( arr )) self . register_buffer ( \"pe\" , pe )","title":"PositionalEncoding"},{"location":"model/#scprint.model.encoders.PositionalEncoding.forward","text":"Parameters: x \u2013 Tensor, shape [seq_len, batch_size, embedding_dim] Source code in scprint/model/encoders.py 96 97 98 99 100 101 102 103 104 105 def forward ( self , gene_pos : Tensor ) -> Tensor : \"\"\" Args: x: Tensor, shape [seq_len, batch_size, embedding_dim] \"\"\" return self . dropout ( torch . index_select ( self . pe , 0 , gene_pos . view ( - 1 )) . view ( gene_pos . shape + ( - 1 ,) ) )","title":"forward()"},{"location":"model/#scprint.model.decoders","text":"","title":"decoders"},{"location":"model/#scprint.model.decoders.ClsDecoder","text":"Bases: Module ClsDecoder Decoder for classification task. Parameters: d_model ( int ) \u2013 int, dimension of the input. n_cls ( int ) \u2013 int, number of classes. layers ( list [ int ] , default: [256, 128] ) \u2013 list[int], list of hidden layers. activation ( Callable , default: ReLU ) \u2013 nn.Module, activation function. dropout ( float , default: 0.1 ) \u2013 float, dropout rate. Returns: \u2013 Tensor, shape [batch_size, n_cls] Source code in scprint/model/decoders.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def __init__ ( self , d_model : int , n_cls : int , layers : list [ int ] = [ 256 , 128 ], activation : Callable = nn . ReLU , dropout : float = 0.1 , ): \"\"\" ClsDecoder Decoder for classification task. Args: d_model: int, dimension of the input. n_cls: int, number of classes. layers: list[int], list of hidden layers. activation: nn.Module, activation function. dropout: float, dropout rate. Returns: Tensor, shape [batch_size, n_cls] \"\"\" super ( ClsDecoder , self ) . __init__ () # module list layers = [ d_model ] + layers self . decoder = nn . Sequential () for i , l in enumerate ( layers [ 1 :]): self . decoder . append ( nn . Linear ( layers [ i ], l )) self . decoder . append ( nn . LayerNorm ( l )) self . decoder . append ( activation ()) self . decoder . append ( nn . Dropout ( dropout )) self . out_layer = nn . Linear ( layers [ - 1 ], n_cls )","title":"ClsDecoder"},{"location":"model/#scprint.model.decoders.ClsDecoder.forward","text":"Parameters: x ( Tensor ) \u2013 Tensor, shape [batch_size, embsize] Source code in scprint/model/decoders.py 197 198 199 200 201 202 203 def forward ( self , x : Tensor ) -> Tensor : \"\"\" Args: x: Tensor, shape [batch_size, embsize] \"\"\" x = self . decoder ( x ) return self . out_layer ( x )","title":"forward()"},{"location":"model/#scprint.model.decoders.ExprDecoder","text":"Bases: Module ExprDecoder Decoder for the gene expression prediction. Will output the mean, variance and zero logits, parameters of a zero inflated negative binomial distribution. Parameters: d_model ( int ) \u2013 The dimension of the model. This is the size of the input feature vector. nfirst_labels_to_skip ( int , default: 0 ) \u2013 The number of initial labels to skip in the sequence. Defaults to 0. dropout ( float , default: 0.1 ) \u2013 The dropout rate applied during training to prevent overfitting. Defaults to 0.1. Source code in scprint/model/decoders.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , d_model : int , nfirst_labels_to_skip : int = 0 , dropout : float = 0.1 , ): \"\"\" ExprDecoder Decoder for the gene expression prediction. Will output the mean, variance and zero logits, parameters of a zero inflated negative binomial distribution. Args: d_model (int): The dimension of the model. This is the size of the input feature vector. nfirst_labels_to_skip (int, optional): The number of initial labels to skip in the sequence. Defaults to 0. dropout (float, optional): The dropout rate applied during training to prevent overfitting. Defaults to 0.1. \"\"\" super ( ExprDecoder , self ) . __init__ () self . nfirst_labels_to_skip = nfirst_labels_to_skip self . fc = nn . Sequential ( nn . Linear ( d_model , d_model ), nn . LayerNorm ( d_model ), nn . LeakyReLU (), nn . Dropout ( dropout ), nn . Linear ( d_model , d_model ), nn . LeakyReLU (), ) self . pred_var_zero = nn . Linear ( d_model , 3 )","title":"ExprDecoder"},{"location":"model/#scprint.model.decoders.ExprDecoder.forward","text":"x is the output of the transformer, (batch, seq_len, d_model) Source code in scprint/model/decoders.py 59 60 61 62 63 64 65 66 67 68 69 70 71 def forward ( self , x : Tensor ) -> Dict [ str , Tensor ]: \"\"\"x is the output of the transformer, (batch, seq_len, d_model)\"\"\" # we don't do it on the labels x = self . fc ( x [:, self . nfirst_labels_to_skip :, :]) pred_value , var_value , zero_logits = self . pred_var_zero ( x ) . split ( 1 , dim =- 1 ) # (batch, seq_len) # The sigmoid function is used to map the zero_logits to a probability between 0 and 1. return dict ( mean = F . softmax ( pred_value . squeeze ( - 1 ), dim =- 1 ), disp = torch . exp ( torch . clamp ( var_value . squeeze ( - 1 ), max = 15 )), zero_logits = zero_logits . squeeze ( - 1 ), )","title":"forward()"},{"location":"model/#scprint.model.decoders.GraphSDEExprDecoder","text":"Bases: Module Initialize the ExprNeuralSDEDecoder module. Parameters: d_model (int): The dimension of the model. drift (nn.Module): The drift component of the SDE. diffusion (nn.Module): The diffusion component of the SDE. Source code in scprint/model/decoders.py 9 10 11 12 13 14 15 16 17 18 19 20 21 def __init__ ( self , d_model : int , drift : nn . Module , diffusion : nn . Module ): \"\"\" Initialize the ExprNeuralSDEDecoder module. Parameters: d_model (int): The dimension of the model. drift (nn.Module): The drift component of the SDE. diffusion (nn.Module): The diffusion component of the SDE. \"\"\" super () . __init__ () self . d_model = d_model self . drift = drift self . diffusion = diffusion","title":"GraphSDEExprDecoder"},{"location":"model/#scprint.model.decoders.MVCDecoder","text":"Bases: Module MVCDecoder Decoder for the masked value prediction for cell embeddings. Will use the gene embeddings with the cell embeddings to predict the mean, variance and zero logits Parameters: d_model \u2013 obj: int ): dimension of the gene embedding. arch_style \u2013 obj: str ): architecture style of the decoder, choice from 1. \"inner product\" or 2. \"cell product\" 3. \"concat query\" or 4. \"sum query\". query_activation \u2013 obj: nn.Module ): activation function for the query vectors. Defaults to nn.Sigmoid. hidden_activation \u2013 obj: nn.Module ): activation function for the hidden layers. Defaults to nn.PReLU. Source code in scprint/model/decoders.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __init__ ( self , d_model : int , arch_style : str = \"inner product\" , query_activation : nn . Module = nn . Sigmoid , hidden_activation : nn . Module = nn . PReLU , ) -> None : \"\"\" MVCDecoder Decoder for the masked value prediction for cell embeddings. Will use the gene embeddings with the cell embeddings to predict the mean, variance and zero logits Args: d_model (:obj:`int`): dimension of the gene embedding. arch_style (:obj:`str`): architecture style of the decoder, choice from 1. \"inner product\" or 2. \"cell product\" 3. \"concat query\" or 4. \"sum query\". query_activation (:obj:`nn.Module`): activation function for the query vectors. Defaults to nn.Sigmoid. hidden_activation (:obj:`nn.Module`): activation function for the hidden layers. Defaults to nn.PReLU. \"\"\" super ( MVCDecoder , self ) . __init__ () if arch_style == \"inner product\" : self . gene2query = nn . Linear ( d_model , d_model ) self . query_activation = query_activation () self . pred_var_zero = nn . Linear ( d_model , d_model * 3 , bias = False ) elif arch_style == \"concat query\" : self . gene2query = nn . Linear ( d_model , 64 ) self . query_activation = query_activation () self . fc1 = nn . Linear ( d_model + 64 , 64 ) self . hidden_activation = hidden_activation () self . fc2 = nn . Linear ( 64 , 3 ) elif arch_style == \"sum query\" : self . gene2query = nn . Linear ( d_model , d_model ) self . query_activation = query_activation () self . fc1 = nn . Linear ( d_model , 64 ) self . hidden_activation = hidden_activation () self . fc2 = nn . Linear ( 64 , 3 ) else : raise ValueError ( f \"Unknown arch_style: { arch_style } \" ) self . arch_style = arch_style self . do_detach = arch_style . endswith ( \"detach\" ) self . d_model = d_model","title":"MVCDecoder"},{"location":"model/#scprint.model.decoders.MVCDecoder.forward","text":"Parameters: cell_emb ( Tensor ) \u2013 Tensor, shape (batch, embsize=d_model) gene_embs ( Tensor ) \u2013 Tensor, shape (batch, seq_len, embsize=d_model) Source code in scprint/model/decoders.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def forward ( self , cell_emb : Tensor , gene_embs : Tensor , ) -> Union [ Tensor , Dict [ str , Tensor ]]: \"\"\" Args: cell_emb: Tensor, shape (batch, embsize=d_model) gene_embs: Tensor, shape (batch, seq_len, embsize=d_model) \"\"\" if self . arch_style == \"inner product\" : query_vecs = self . query_activation ( self . gene2query ( gene_embs )) pred , var , zero_logits = self . pred_var_zero ( query_vecs ) . split ( self . d_model , dim =- 1 ) cell_emb = cell_emb . unsqueeze ( 2 ) pred , var , zero_logits = ( torch . bmm ( pred , cell_emb ) . squeeze ( 2 ), torch . bmm ( var , cell_emb ) . squeeze ( 2 ), torch . bmm ( zero_logits , cell_emb ) . squeeze ( 2 ), ) # zero logits need to based on the cell_emb, because of input exprs elif self . arch_style == \"concat query\" : query_vecs = self . query_activation ( self . gene2query ( gene_embs )) # expand cell_emb to (batch, seq_len, embsize) cell_emb = cell_emb . unsqueeze ( 1 ) . expand ( - 1 , gene_embs . shape [ 1 ], - 1 ) h = self . hidden_activation ( self . fc1 ( torch . cat ([ cell_emb , query_vecs ], dim = 2 )) ) pred , var , zero_logits = self . fc2 ( h ) . split ( 1 , dim =- 1 ) elif self . arch_style == \"sum query\" : query_vecs = self . query_activation ( self . gene2query ( gene_embs )) cell_emb = cell_emb . unsqueeze ( 1 ) h = self . hidden_activation ( self . fc1 ( cell_emb + query_vecs )) pred , var , zero_logits = self . fc2 ( h ) . split ( 1 , dim =- 1 ) return dict ( mvc_mean = F . softmax ( pred , dim =- 1 ), mvc_disp = torch . exp ( torch . clamp ( var , max = 15 )), mvc_zero_logits = zero_logits , )","title":"forward()"},{"location":"model/#scprint.model.flash_attn.flashformer","text":"","title":"flashformer"},{"location":"model/#scprint.model.flash_attn.flashformer.FlashTransformerEncoder","text":"Bases: Module FlashTransformerEncoder a transformer encoder with flash attention. Parameters: d_model ( int ) \u2013 The dimension of the input vectors. nhead ( int ) \u2013 The number of attention heads. nlayers ( int ) \u2013 The number of layers in the transformer. dropout ( float , default: 0.1 ) \u2013 The dropout rate to apply to the output of the positional encoding. Defaults to 0.1. residual_in_fp32 ( bool , default: True ) \u2013 Whether to force the residual to be in fp32 format. Defaults to True. num_heads_kv ( _type_ , default: None ) \u2013 The number of heads for key/value. Defaults to None. checkpointing ( bool , default: False ) \u2013 Whether to use gradient checkpointing. Defaults to False. fused_dropout_add_ln ( bool , default: False ) \u2013 Whether to fuse dropout, addition and layer normalization operations. Defaults to False. return_residual ( bool , default: False ) \u2013 Whether to return the residual. Defaults to False. prenorm ( bool , default: True ) \u2013 Whether to use pre-normalization. Defaults to True. mlp_ratio ( float , default: 4.0 ) \u2013 The ratio for MLP. Defaults to 4.0. fused_mlp ( bool , default: False ) \u2013 Whether to use fused MLP. Defaults to False. fused_bias_fc ( bool , default: False ) \u2013 Whether to fuse bias and fully connected layers. Defaults to False. sequence_parallel ( bool , default: False ) \u2013 Whether to use sequence parallelism. Defaults to False. drop_path_rate ( float , default: 0.0 ) \u2013 The drop path rate. Defaults to 0.0. weight_init ( str , default: '' ) \u2013 The weight initialization method. Defaults to \"\". Raises: ImportError \u2013 Raised when Triton is not installed but fused_dropout_add_ln is set to True. NotImplementedError \u2013 Raised when an unsupported operation is attempted. Source code in scprint/model/flash_attn/flashformer.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def __init__ ( self , d_model : int , nhead : int , nlayers : int , dropout : float = 0.1 , residual_in_fp32 : bool = True , num_heads_kv : Optional [ int ] = None , checkpointing : bool = False , fused_dropout_add_ln : bool = False , return_residual : bool = False , prenorm : bool = True , mlp_ratio : float = 4.0 , fused_mlp : bool = False , fused_bias_fc : bool = False , sequence_parallel : bool = False , drop_path_rate : float = 0.0 , use_flash_attn : bool = True , weight_init : str = \"\" , ): \"\"\" FlashTransformerEncoder a transformer encoder with flash attention. Args: d_model (int): The dimension of the input vectors. nhead (int): The number of attention heads. nlayers (int): The number of layers in the transformer. dropout (float, optional): The dropout rate to apply to the output of the positional encoding. Defaults to 0.1. residual_in_fp32 (bool, optional): Whether to force the residual to be in fp32 format. Defaults to True. num_heads_kv (_type_, optional): The number of heads for key/value. Defaults to None. checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False. fused_dropout_add_ln (bool, optional): Whether to fuse dropout, addition and layer normalization operations. Defaults to False. return_residual (bool, optional): Whether to return the residual. Defaults to False. prenorm (bool, optional): Whether to use pre-normalization. Defaults to True. mlp_ratio (float, optional): The ratio for MLP. Defaults to 4.0. fused_mlp (bool, optional): Whether to use fused MLP. Defaults to False. fused_bias_fc (bool, optional): Whether to fuse bias and fully connected layers. Defaults to False. sequence_parallel (bool, optional): Whether to use sequence parallelism. Defaults to False. drop_path_rate (float, optional): The drop path rate. Defaults to 0.0. weight_init (str, optional): The weight initialization method. Defaults to \"\". Raises: ImportError: Raised when Triton is not installed but fused_dropout_add_ln is set to True. NotImplementedError: Raised when an unsupported operation is attempted. \"\"\" super ( FlashTransformerEncoder , self ) . __init__ () self . blocks = nn . ModuleList () dpr = [ x . item () for x in torch . linspace ( 0 , drop_path_rate , nlayers ) ] # stochastic depth decay rule for i in range ( nlayers ): mlp = create_mlp_cls ( d_model , mlp_ratio , nn . GELU , fused_mlp ) attention = partial ( MHA , num_heads = nhead , dropout = dropout , causal = False , use_flash_attn = use_flash_attn , num_heads_kv = num_heads_kv , checkpointing = checkpointing , fused_bias_fc = fused_bias_fc , layer_idx = i , ) # or use parallelBlock where attn & MLP are done in parallel encoder_layers = Block ( d_model , attention , mlp , prenorm = prenorm , # need to set it here for now although it hinders some performances as it returns the residual and I need to see what to do with it # TD [2022-07-30]: Force residual in fp32, seems to make fp16 training more stable residual_in_fp32 = residual_in_fp32 , sequence_parallel = sequence_parallel , # for more parallelism resid_dropout1 = dropout , resid_dropout2 = dropout , drop_path1 = dpr [ i - 1 ] if i > 0 else 0.0 , drop_path2 = dpr [ i ], fused_dropout_add_ln = fused_dropout_add_ln , return_residual = return_residual , ) self . blocks . append ( encoder_layers ) self . dropout = nn . Dropout ( p = dropout ) self . drop_path = StochasticDepth ( p = dpr [ - 1 ], mode = \"row\" ) self . norm = torch . nn . LayerNorm ( d_model , eps = 1e-6 ) self . fused_dropout_add_ln = fused_dropout_add_ln if self . fused_dropout_add_ln and layer_norm_fn is None : raise ImportError ( \"Triton is not installed\" ) if sequence_parallel : # This seems to only be important when doing tensor parallelism across GPUs, to increase even more the context length I guess? # not really necessary here I think raise NotImplementedError ( \"sequence_parallel not implemented yet\" ) self . init_weights ( weight_init )","title":"FlashTransformerEncoder"},{"location":"model/#scprint.model.model","text":"","title":"model"},{"location":"model/#scprint.model.model.scPrint","text":"Bases: LightningModule scPrint transformer for single cell biology and the inference of Gene Regulatory networks Parameters: genes ( list ) \u2013 the genenames with which the model will work precpt_gene_emb ( array , default: None ) \u2013 The gene embeddings. should be of size len(genes), d_model. it should be in the same order as the genes. Defaults to None. gene_pos_enc ( list , default: None ) \u2013 The gene position encoding. Should be of the same size as genes. for each gene in genes, gives it a location value. Defaults to None. d_model ( int , default: 512 ) \u2013 The dimension of the model. Defaults to 512. nhead ( int , default: 8 ) \u2013 The number of heads in the multiheadattention models. Defaults to 8. d_hid ( int , default: 512 ) \u2013 The dimension of the feedforward network model. Defaults to 512. nlayers ( int , default: 6 ) \u2013 The number of layers in the transformer model. Defaults to 6. nlayers_cls ( int ) \u2013 The number of layers in the classifier. Defaults to 3. labels ( dict , default: {} ) \u2013 The classes to predict with number of labels for each. Defaults to {}. cls_hierarchy ( dict , default: {} ) \u2013 The class hierarchy for classes that have hierarchical labels. Defaults to {}. dropout ( float , default: 0.2 ) \u2013 The dropout value. Defaults to 0.5. transformer ( str , default: 'fast' ) \u2013 (flag, optional) the transformer type to use. one of \"linear\", \"flash\", \"flashsparse\", \"scprint\". Defaults to \"flash\". domain_spec_batchnorm ( str , default: 'None' ) \u2013 Whether to apply domain specific batch normalization. Defaults to False. expr_emb_style ( str , default: 'continuous' ) \u2013 The style of input embedding (one of \"continuous_concat\", \"binned_pos\", \"full_pos\"). Defaults to \"continuous_concat\". mvc_decoder ( str , default: 'None' ) \u2013 The style of MVC decoder one of \"None\", \"inner product\", \"concat query\", \"sum query\". Defaults to \"inner product\". pred_embedding ( list , default: [] ) \u2013 The list of labels to use for plotting embeddings. Defaults to []. cell_emb_style ( str , default: 'cls' ) \u2013 The style of cell embedding. one of \"cls\", \"avg-pool\", \"w-pool\". Defaults to \"cls\". lr ( float , default: 0.001 ) \u2013 The learning rate. Defaults to 0.001. label_decoders ( Optional [ Dict [ str , Dict [ int , str ]]] , default: None ) \u2013 (dict, optional) the label decoders to use for plotting the umap during validations. Defaults to None. Raises: ValueError \u2013 If the expr_emb_style is not one of \"category\", \"continuous\", \"none\". Source code in scprint/model/model.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def __init__ ( self , genes : list , precpt_gene_emb : Optional [ str ] = None , gene_pos_enc : Optional [ list ] = None , d_model : int = 512 , nhead : int = 8 , d_hid : int = 512 , edge_dim : int = 12 , nlayers : int = 6 , layers_cls : list [ int ] = [], labels : Dict [ str , int ] = {}, cls_hierarchy : Dict [ str , Dict [ int , list [ int ]]] = {}, dropout : float = 0.2 , transformer : str = \"fast\" , expr_emb_style : str = \"continuous\" , # \"binned_pos\", \"cont_pos\" domain_spec_batchnorm : str = \"None\" , n_input_bins : int = 0 , mvc_decoder : str = \"None\" , pred_embedding : list [ str ] = [], cell_emb_style : str = \"cls\" , lr : float = 0.001 , label_decoders : Optional [ Dict [ str , Dict [ int , str ]]] = None , ** flash_attention_kwargs , ): \"\"\" scPrint transformer for single cell biology and the inference of Gene Regulatory networks Args: genes (list): the genenames with which the model will work precpt_gene_emb (np.array, optional): The gene embeddings. should be of size len(genes), d_model. it should be in the same order as the genes. Defaults to None. gene_pos_enc (list, optional): The gene position encoding. Should be of the same size as genes. for each gene in genes, gives it a location value. Defaults to None. d_model (int, optional): The dimension of the model. Defaults to 512. nhead (int, optional): The number of heads in the multiheadattention models. Defaults to 8. d_hid (int, optional): The dimension of the feedforward network model. Defaults to 512. nlayers (int, optional): The number of layers in the transformer model. Defaults to 6. nlayers_cls (int, optional): The number of layers in the classifier. Defaults to 3. labels (dict, optional): The classes to predict with number of labels for each. Defaults to {}. cls_hierarchy (dict, optional): The class hierarchy for classes that have hierarchical labels. Defaults to {}. dropout (float, optional): The dropout value. Defaults to 0.5. transformer: (flag, optional) the transformer type to use. one of \"linear\", \"flash\", \"flashsparse\", \"scprint\". Defaults to \"flash\". domain_spec_batchnorm (str, optional): Whether to apply domain specific batch normalization. Defaults to False. expr_emb_style (str, optional): The style of input embedding (one of \"continuous_concat\", \"binned_pos\", \"full_pos\"). Defaults to \"continuous_concat\". mvc_decoder (str, optional): The style of MVC decoder one of \"None\", \"inner product\", \"concat query\", \"sum query\". Defaults to \"inner product\". pred_embedding (list, optional): The list of labels to use for plotting embeddings. Defaults to []. cell_emb_style (str, optional): The style of cell embedding. one of \"cls\", \"avg-pool\", \"w-pool\". Defaults to \"cls\". lr (float, optional): The learning rate. Defaults to 0.001. label_decoders: (dict, optional) the label decoders to use for plotting the umap during validations. Defaults to None. Raises: ValueError: If the expr_emb_style is not one of \"category\", \"continuous\", \"none\". \"\"\" super () . __init__ () # default self . do_denoise = False self . noise = [] self . do_cce = True self . cce_sim = 0.5 self . do_ecs = True self . ecs_threshold = 0.3 self . ecs_scale = 1.0 self . do_mvc = False self . do_adv_cls = False self . do_next_tp = False self . do_generate = False self . class_scale = 1000 self . mask_ratio = [ 0.15 ] self . warmup_duration = 500 self . weight_decay = 0.0 self . fused_adam = False self . lr_patience = 3 self . lrfinder_steps = 0 self . get_attention_layer = [] self . embs = None # should be stored somehow self . d_model = d_model self . edge_dim = edge_dim self . nlayers = nlayers self . gene_pos_enc = gene_pos_enc self . mvc_decoder = mvc_decoder self . domain_spec_batchnorm = domain_spec_batchnorm # need to store self . n_input_bins = n_input_bins self . transformer = transformer self . labels_counts = labels self . labels = list ( labels . keys ()) self . cell_emb_style = cell_emb_style self . label_decoders = label_decoders self . pred_embedding = pred_embedding self . lr = lr # compute tensor for mat_cls_hierarchy self . mat_cls_hierarchy = {} self . cls_hierarchy = cls_hierarchy for k , v in cls_hierarchy . items (): tens = torch . zeros (( len ( v ), labels [ k ])) for k2 , v2 in v . items (): tens [ k2 - labels [ k ], v2 ] = 1 self . mat_cls_hierarchy [ k ] = tens . to ( bool ) self . expr_emb_style = expr_emb_style if self . expr_emb_style not in [ \"category\" , \"continuous\" , \"none\" ]: raise ValueError ( f \"expr_emb_style should be one of category, continuous, scaling, \" f \"got { expr_emb_style } \" ) if cell_emb_style not in [ \"cls\" , \"avg-pool\" , \"w-pool\" ]: raise ValueError ( f \"Unknown cell_emb_style: { cell_emb_style } \" ) self . genes = genes self . vocab = { i : n for i , n in enumerate ( genes )} # encoder # gene encoder if precpt_gene_emb is not None : embeddings = pd . read_parquet ( precpt_gene_emb ) . loc [ self . genes ] if len ( embeddings ) == 0 : raise ValueError ( f \"the gene embeddings file { precpt_gene_emb } does not contain any of the genes given to the model\" ) elif len ( embeddings ) < len ( self . genes ): print ( \"Warning: only a subset of the genes available in the embeddings file.\" ) print ( \"number of genes: \" , len ( embeddings )) sembeddings = torch . nn . AdaptiveAvgPool1d ( d_model )( torch . tensor ( embeddings . values ) ) self . gene_encoder = encoders . GeneEncoder ( len ( self . vocab ), d_model , weights = sembeddings , freeze = True ) else : self . gene_encoder = encoders . GeneEncoder ( len ( self . vocab ), d_model ) # Value Encoder, NOTE: the scaling style is also handled in _encode method if expr_emb_style in [ \"continuous\" , \"full_pos\" ]: self . expr_encoder = encoders . ContinuousValueEncoder ( d_model , dropout ) elif expr_emb_style == \"binned_pos\" : assert n_input_bins > 0 self . expr_encoder = encoders . CategoryValueEncoder ( n_input_bins , d_model ) else : self . expr_encoder = nn . Identity () # Positional Encoding if self . gene_pos_enc is not None : max_len = max ( gene_pos_enc ) token_to_pos = { token : pos for token , pos in enumerate ( self . gene_pos_enc )} self . pos_encoder = encoders . PositionalEncoding ( d_model , max_len = max_len , token_to_pos = token_to_pos ) # Batch Encoder # always have [base_cell_emb, time_embedding, depth_embedding] + any other class info # base cell embedding will store other cell specific information self . label_encoder = encoders . CategoryValueEncoder ( len ( self . labels ) + 2 , d_model ) # self.time_encoder = encoders.ContinuousValueEncoder(d_model, dropout) self . depth_decoder = encoders . ContinuousValueEncoder ( d_model , dropout ) # Model # Batch Norm if domain_spec_batchnorm is True or domain_spec_batchnorm == \"dsbn\" : use_affine = True if domain_spec_batchnorm == \"do_affine\" else False print ( f \"Use domain specific batchnorm with affine= { use_affine } \" ) self . dsbn = DomainSpecificBatchNorm1d ( d_model , len ( self . labels ), eps = 6.1e-5 , affine = use_affine ) elif domain_spec_batchnorm == \"batchnorm\" : print ( \"Using simple batchnorm instead of domain specific batchnorm\" ) self . bn = nn . BatchNorm1d ( d_model , eps = 6.1e-5 ) # Transformer # Linear if transformer == \"linear\" : # linear transformer using the fast transformer package self . transformer = FastTransformerEncoder ( d_model , nhead , d_hid , nlayers , dropout , \"linear\" ) # flashsparse elif transformer == \"flashsparse\" : if Hashformer is None : raise ValueError ( \"Hashformer transformer requires cuda kernels\" ) self . transformer = Hashformer ( d_model , nlayers , 2 , nhead , ) # flash EGT # We found that the results can be further improved by freezing the # node channel layers and training the edge channel layers for a # few additional epochs. # However, its effect on transfer learning has not yet been studied. # That is why we include checkpoints for both tuned and untuned models. # https://github.com/shamim-hussain/egt/blob/master/README.md # https://github.com/shamim-hussain/egt_pytorch elif transformer == \"scprint\" : self . transformer = EGT ( num_layers = nlayers , feat_size = d_model , edge_feat_size = edge_dim , num_heads = nhead , num_virtual_nodes = len ( self . labels ), ) # regular or flash else : if transformer == \"flash\" and FlashTransformerEncoder is None : raise ValueError ( \"flash transformer requires flash package\" ) # NOT flash transformer using the special tritton kernel # or parallelMHA (add the process group thing and faster) self . transformer = FlashTransformerEncoder ( d_model , nhead , nlayers , dropout = dropout , use_flash_attn = ( transformer == \"flash\" ), ** flash_attention_kwargs , ) # decoders # expression self . expr_decoder = decoders . ExprDecoder ( d_model , nfirst_labels_to_skip = len ( self . labels ) + 2 , dropout = dropout , ) # cls decoder self . cls_decoders = nn . ModuleDict () # should be a very simple classifier for most things # (maybe scale with the number of classes) should be 1 layer... for label , n_cls in labels . items (): self . cls_decoders [ label ] = decoders . ClsDecoder ( d_model , n_cls , layers = layers_cls , dropout = dropout ) # expression decoder from batch embbedding if mvc_decoder != \"None\" : self . mvc_decoder = decoders . MVCDecoder ( d_model , arch_style = mvc_decoder , dropout = dropout , ) else : self . mvc_decoder = None self . apply ( partial ( utils . _init_weights , n_layer = nlayers , ) ) self . save_hyperparameters () print ( self )","title":"scPrint"},{"location":"model/#scprint.model.model.scPrint.configure_optimizers","text":"@see pl.LightningModule Source code in scprint/model/model.py 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 def configure_optimizers ( self ): \"\"\"@see pl.LightningModule\"\"\" # https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam # optimizer = optim.Adam( # self.parameters(), # lr=self.hparams.lr, # betas=(0.9, 0.999), # eps=1e-08, # weight_decay=0, # amsgrad=False, # fused=False, # ) optimizer = optim . AdamW ( self . parameters (), lr = self . hparams . lr , betas = ( 0.9 , 0.999 ), eps = 1e-08 , weight_decay = self . weight_decay , amsgrad = False , fused = self . fused_adam , ) lr_scheduler = optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = \"min\" , patience = self . lr_patience , factor = 0.5 ) lr_dict = { \"scheduler\" : lr_scheduler , # The unit of the scheduler's step size, could also be 'step'. # 'epoch' updates the scheduler on epoch end whereas 'step' # updates it after a optimizer update. \"interval\" : \"epoch\" , # How many epochs/steps should pass between calls to # `scheduler.step()`. 1 corresponds to updating the learning # rate after every epoch/step. \"frequency\" : 1 , # Metric to to monitor for schedulers like `ReduceLROnPlateau` \"monitor\" : \"val_loss\" if self . trainer . val_dataloaders else \"train_loss\" , } self . lrfinder_steps = 0 for val in self . trainer . callbacks : if type ( val ) is _LRCallback : self . lrfinder_steps = val . num_training if type ( val ) is LearningRateFinder : self . lrfinder_steps = val . _num_training_steps return [ optimizer ], [ lr_dict ]","title":"configure_optimizers()"},{"location":"model/#scprint.model.model.scPrint.forward","text":"forward also called on self(), a full forward pass on the model Parameters: gene_pos ( Tensor ) \u2013 A tensor of shape (minibatch, seq_len) representing the genes used for each cell in the minibatch. expression ( Tensor , default: None ) \u2013 A tensor of shape (minibatch, seq_len) representing the expression levels of genes in the minibatch. Defaults to None. mask ( Tensor , default: None ) \u2013 A tensor of shape (minibatch, seq_len) used to mask certain elements in the sequence during the forward pass. Defaults to None. full_depth ( Tensor , default: None ) \u2013 A tensor of shape (minibatch,) representing the full depth of each sequence in the minibatch. Defaults to None. timepoint ( Tensor , default: None ) \u2013 A tensor of shape (minibatch,) representing the timepoint associated with each sequence in the minibatch. Defaults to None. get_gene_emb ( bool , default: False ) \u2013 A flag indicating whether to return the gene embeddings. If True, the gene embeddings are included in the output. Defaults to False. do_sample ( bool , default: False ) \u2013 A flag indicating whether to sample the expression levels. If True, the expression levels are sampled during the forward pass. Defaults to False. get_attention_layer ( list , default: [] ) \u2013 A list indicating which attention layers to return. If not empty, the specified attention layers are included in the output. Defaults to []. Returns: \u2013 dict of output Tensors: A dictionary containing the output tensors from the forward pass. The keys of the dictionary depend on the input flags (get_gene_emb, do_sample, get_attention_layer). at minima, the dictionary contains the following: - \"mean\": the mean expression levels - \"zero_logits\": the logits for zero-inflated expression levels - \"disp\": the dispersion parameter - \"cell_embs\": the cell embeddings per class - \"cell_emb\": the main cell embedding - \"cls_output\": the output of the classifier Source code in scprint/model/model.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 def forward ( self , gene_pos : Tensor , depth_mult : Tensor , expression : Optional [ Tensor ] = None , mask : Optional [ Tensor ] = None , # (minibatch,) unormalized total counts full_depth : Optional [ Tensor ] = None , timepoint : Optional [ Tensor ] = None , # (new_minibatch_of_nxt_cells,) get_gene_emb : bool = False , do_sample : bool = False , get_attention_layer : list = [], ): \"\"\" forward also called on self(), a full forward pass on the model Args: gene_pos (Tensor): A tensor of shape (minibatch, seq_len) representing the genes used for each cell in the minibatch. expression (Tensor, optional): A tensor of shape (minibatch, seq_len) representing the expression levels of genes in the minibatch. Defaults to None. mask (Tensor, optional): A tensor of shape (minibatch, seq_len) used to mask certain elements in the sequence during the forward pass. Defaults to None. full_depth (Tensor, optional): A tensor of shape (minibatch,) representing the full depth of each sequence in the minibatch. Defaults to None. timepoint (Tensor, optional): A tensor of shape (minibatch,) representing the timepoint associated with each sequence in the minibatch. Defaults to None. get_gene_emb (bool, optional): A flag indicating whether to return the gene embeddings. If True, the gene embeddings are included in the output. Defaults to False. do_sample (bool, optional): A flag indicating whether to sample the expression levels. If True, the expression levels are sampled during the forward pass. Defaults to False. get_attention_layer (list, optional): A list indicating which attention layers to return. If not empty, the specified attention layers are included in the output. Defaults to []. Returns: dict of output Tensors: A dictionary containing the output tensors from the forward pass. The keys of the dictionary depend on the input flags (get_gene_emb, do_sample, get_attention_layer). at minima, the dictionary contains the following: - \"mean\": the mean expression levels - \"zero_logits\": the logits for zero-inflated expression levels - \"disp\": the dispersion parameter - \"cell_embs\": the cell embeddings per class - \"cell_emb\": the main cell embedding - \"cls_output\": the output of the classifier \"\"\" encoding = self . _encoder ( gene_pos , expression , mask , full_depth , timepoint ) transformer_output = self . transformer ( encoding , return_qkv = get_attention_layer ) if len ( get_attention_layer ) > 0 : transformer_output , qkvs = transformer_output return ( self . _decoder ( transformer_output , depth_mult , get_gene_emb , do_sample ), qkvs , ) else : return self . _decoder ( transformer_output , depth_mult , get_gene_emb , do_sample )","title":"forward()"},{"location":"model/#scprint.model.model.scPrint.get_cell_embs","text":"get_cell_embs Parameters: layer_output ( Tensor ) \u2013 The output tensor from a layer in the model. Raises: ValueError \u2013 Raised when an unknown cell embedding style is encountered. Returns: Tensor \u2013 The cell embeddings tensor. Source code in scprint/model/model.py 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 def get_cell_embs ( self , layer_output ): \"\"\" get_cell_embs Args: layer_output (Tensor): The output tensor from a layer in the model. Raises: ValueError: Raised when an unknown cell embedding style is encountered. Returns: Tensor: The cell embeddings tensor. \"\"\" if self . cell_emb_style == \"cls\" and self . labels is not None : # (minibatch, embsize) cell_emb = layer_output [:, : 2 + len ( self . labels )] elif self . cell_emb_style == \"avg-pool\" : cell_emb = torch . mean ( layer_output , dim = 1 ) else : raise ValueError ( f \"Unknown cell_emb_style: { self . cell_emb_style } \" ) return cell_emb","title":"get_cell_embs()"},{"location":"model/#scprint.model.model.scPrint.log_adata","text":"log_adata will log an adata from predictions. It will log to tensorboard and wandb if available see @utils.log_adata Source code in scprint/model/model.py 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 def log_adata ( self , gtclass = None , name = \"\" ): \"\"\" log_adata will log an adata from predictions. It will log to tensorboard and wandb if available see @utils.log_adata \"\"\" try : mdir = self . logger . save_dir if self . logger . save_dir is not None else \"/tmp\" except : mdir = \"/tmp\" adata = utils . make_adata ( self . pred , self . embs , self . labels , self . trainer . global_step , self . label_decoders , gtclass , name , mdir , ) try : self . logger . experiment . add_figure ( fig ) except : print ( \"couldn't log to tensorboard\" ) try : self . logger . log_image ( key = \"umaps\" , images = [ fig ]) except : print ( \"couldn't log to wandb\" ) return adata","title":"log_adata()"},{"location":"model/#scprint.model.model.scPrint.on_fit_start","text":"@see pl.LightningModule Source code in scprint/model/model.py 516 517 518 519 520 521 522 def on_fit_start ( self ): \"\"\"@see pl.LightningModule\"\"\" if type ( self . transformer ) is FlashTransformerEncoder : for encoder_layers in self . transformer . blocks : encoder_layers . set_seq_parallel ( True ) for k , v in self . mat_cls_hierarchy . items (): self . mat_cls_hierarchy [ k ] = v . to ( self . device )","title":"on_fit_start()"},{"location":"model/#scprint.model.model.scPrint.on_predict_epoch_end","text":"@see pl.LightningModule will Source code in scprint/model/model.py 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 def on_predict_epoch_end ( self ): \"\"\"@see pl.LightningModule will\"\"\" if not self . trainer . is_global_zero : print ( \"you are not on the main node. cancelling logging step\" ) return self . expr_pred = [ i . to ( device = \"cpu\" , dtype = torch . float32 ) for i in self . expr_pred ] self . pred = self . pred . to ( device = \"cpu\" , dtype = torch . float32 ) self . embs = self . embs . to ( device = \"cpu\" , dtype = torch . float32 ) self . pos = self . pos . to ( device = \"cpu\" , dtype = torch . int32 ) self . mean_attn = [ i / self . num_pred_batch for i in self . mean_attn ] return self . log_adata ()","title":"on_predict_epoch_end()"},{"location":"model/#scprint.model.model.scPrint.on_predict_epoch_start","text":"@see pl.LightningModule Source code in scprint/model/model.py 945 946 947 948 949 950 951 def on_predict_epoch_start ( self ): \"\"\"@see pl.LightningModule\"\"\" self . embs = None self . num_pred_batch = 0 if type ( self . transformer ) is FlashTransformerEncoder : for encoder_layers in self . transformer . blocks : encoder_layers . set_seq_parallel ( False )","title":"on_predict_epoch_start()"},{"location":"model/#scprint.model.model.scPrint.on_validation_epoch_end","text":"@see pl.LightningModule Source code in scprint/model/model.py 912 913 914 915 916 917 def on_validation_epoch_end ( self ): \"\"\"@see pl.LightningModule\"\"\" if not self . trainer . is_global_zero : print ( \"you are not on the main node. cancelling logging step\" ) return self . log_adata ( gtclass = self . info )","title":"on_validation_epoch_end()"},{"location":"model/#scprint.model.model.scPrint.optimizer_step","text":"@see pl.LightningModule Source code in scprint/model/model.py 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 def optimizer_step ( self , epoch , batch_idx , optimizer , optimizer_closure ): \"\"\"@see pl.LightningModule\"\"\" # update params optimizer . step ( closure = optimizer_closure ) # manually warm up lr without a scheduler # making sure that we don't do this during lrfinder if ( self . trainer . global_step < self . warmup_duration + self . lrfinder_steps ) and self . lrfinder_steps < self . trainer . global_step : lr_scale = min ( 1.0 , float ( self . trainer . global_step + 1 ) / self . warmup_duration ) for pg in optimizer . param_groups : pg [ \"lr\" ] = lr_scale * self . hparams . lr self . log ( \"lr\" , lr_scale * self . hparams . lr ) else : self . log ( \"lr\" , self . lr )","title":"optimizer_step()"},{"location":"model/#scprint.model.model.scPrint.predict_step","text":"embed given gene expression, encode the gene embedding and cell embedding. Returns: Tensor \u2013 description Source code in scprint/model/model.py 953 954 955 956 957 958 959 960 961 962 963 def predict_step ( self , batch , batch_idx ): \"\"\" embed given gene expression, encode the gene embedding and cell embedding. Args: batch @see training_step Returns: Tensor: _description_ \"\"\" return self . _predict ( batch [ \"genes\" ], batch [ \"x\" ], batch [ \"depth\" ])","title":"predict_step()"},{"location":"model/#scprint.model.model.scPrint.test_step","text":"@see pl.LightningModule Source code in scprint/model/model.py 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 def test_step ( self , batch , batch_idx ): \"\"\" @see pl.LightningModule Args: batch @see training_step \"\"\" total_loss , losses = self . _full_training ( batch , self . do_denoise , self . noise , self . do_next_tp , self . do_cce , self . cce_sim , self . do_ecs , self . do_mvc , self . do_adv_cls , self . do_generate , self . mask_ratio , ) self . log ( \"test_loss: \" , total_loss , sync_dist = True ) self . log_dict ( losses , sync_dist = True ) return total_loss","title":"test_step()"},{"location":"model/#scprint.model.model.scPrint.training_step","text":"training_step defines the train loop. It is independent of forward @see pl.LightningModule Returns: _type_ \u2013 description Source code in scprint/model/model.py 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 def training_step ( self , batch : Dict [ Tensor ], batch_idx , ): \"\"\" training_step defines the train loop. It is independent of forward @see pl.LightningModule Returns: _type_: _description_ \"\"\" # TASK 1 & 2 & 3 (first pass, expression reconstruction, label prediction) total_loss , losses = self . _full_training ( batch , self . do_denoise , self . noise , self . do_next_tp , self . do_cce , self . cce_sim , self . do_ecs , self . do_mvc , self . do_adv_cls , self . do_generate , self . mask_ratio , ) self . log ( \"train_loss\" , total_loss , prog_bar = True ) self . log_dict ( losses , prog_bar = True ) return total_loss","title":"training_step()"},{"location":"model/#scprint.model.model.scPrint.validation_step","text":"validation_step defines the validation loop. It is independent of forward @see pl.LightningModule Parameters: batch ( list [ Tensor ] ) \u2013 @see training_step Source code in scprint/model/model.py 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 def validation_step ( self , batch , batch_idx , ): \"\"\" validation_step defines the validation loop. It is independent of forward @see pl.LightningModule Args: batch (list[Tensor]): @see training_step \"\"\" val_loss , losses = self . _full_training ( batch , self . do_denoise , self . noise , self . do_next_tp , self . do_cce , self . cce_sim , self . do_ecs , self . do_mvc , self . do_adv_cls , self . do_generate , self . mask_ratio , ) expression = batch [ \"x\" ] gene_pos = batch [ \"genes\" ] depth = batch [ \"depth\" ] if self . embs is not None : if self . embs . shape [ 0 ] < 10000 : self . _predict ( gene_pos , expression , depth ) self . info = torch . cat ([ self . info , batch [ \"class\" ]]) else : self . _predict ( gene_pos , expression , depth ) self . info = batch [ \"class\" ] self . log ( \"val_loss\" , val_loss , sync_dist = True ) self . log_dict ( losses , sync_dist = True ) return val_loss","title":"validation_step()"},{"location":"model/#scprint.model.loss","text":"","title":"loss"},{"location":"model/#scprint.model.loss.classification","text":"Computes the classification loss for a given batch of predictions and ground truth labels. Parameters: labelname ( str ) \u2013 The name of the label. pred ( Tensor ) \u2013 The predicted logits for the batch. cl ( Tensor ) \u2013 The ground truth labels for the batch. maxsize ( int ) \u2013 The number of possible labels. cls_hierarchy ( dict , default: {} ) \u2013 The hierarchical structure of the labels. Defaults to {}. Raises: ValueError \u2013 If the labelname is not found in the cls_hierarchy dictionary. Returns: \u2013 torch.Tensor: The computed binary cross entropy loss for the given batch. Source code in scprint/model/loss.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 def classification ( labelname , pred , cl , maxsize , cls_hierarchy = {}): \"\"\" Computes the classification loss for a given batch of predictions and ground truth labels. Args: labelname (str): The name of the label. pred (torch.Tensor): The predicted logits for the batch. cl (torch.Tensor): The ground truth labels for the batch. maxsize (int): The number of possible labels. cls_hierarchy (dict, optional): The hierarchical structure of the labels. Defaults to {}. Raises: ValueError: If the labelname is not found in the cls_hierarchy dictionary. Returns: torch.Tensor: The computed binary cross entropy loss for the given batch. \"\"\" newcl = torch . zeros ( ( cl . shape [ 0 ], maxsize ), device = cl . device ) # batchsize * n_labels # if we don't know the label we set the weight to 0 else to 1 valid_indices = ( cl != - 1 ) & ( cl < maxsize ) valid_cl = cl [ valid_indices ] newcl [ valid_indices , valid_cl ] = 1 weight = torch . ones_like ( newcl , device = cl . device ) weight [ cl == - 1 , :] = 0 inv = cl >= maxsize # if we have non leaf values, we don't know so we don't compute grad and set weight to 0 # and add labels that won't be counted but so that we can still use them if inv . any (): if labelname in cls_hierarchy . keys (): clhier = cls_hierarchy [ labelname ] invw = weight [ inv ] invw [ clhier [ cl [ inv ] - maxsize ]] = 0 weight [ inv ] = invw addnewcl = torch . ones ( weight . shape [ 0 ], device = pred . device ) # no need to set the other to 0 addweight = torch . zeros ( weight . shape [ 0 ], device = pred . device ) addweight [ inv ] = 1 # computing hierarchical labels and adding them to cl cpred = pred . clone () cpred [ ~ inv ] = torch . finfo ( pred . dtype ) . min cpred = torch . logsumexp ( cpred , dim =- 1 ) newcl = torch . cat ([ newcl , addnewcl . unsqueeze ( 1 )], dim = 1 ) pred = torch . cat ([ pred , cpred . unsqueeze ( 1 )], dim = 1 ) weight = torch . cat ([ weight , addweight . unsqueeze ( 1 )], dim = 1 ) else : raise ValueError ( \"need to use cls_hierarchy for this usecase\" ) myloss = torch . nn . functional . binary_cross_entropy_with_logits ( pred , target = newcl , weight = weight ) return myloss","title":"classification()"},{"location":"model/#scprint.model.loss.classifier_loss","text":"Compute the cross entropy loss between prediction and target. Source code in scprint/model/loss.py 151 152 153 154 155 156 def classifier_loss ( pred : torch . Tensor , target : torch . Tensor ) -> torch . Tensor : \"\"\" Compute the cross entropy loss between prediction and target. \"\"\" loss = F . cross_entropy ( pred , target ) return loss","title":"classifier_loss()"},{"location":"model/#scprint.model.loss.criterion_neg_log_bernoulli","text":"Compute the negative log-likelihood of Bernoulli distribution Source code in scprint/model/loss.py 159 160 161 162 163 164 165 166 167 168 def criterion_neg_log_bernoulli ( input : torch . Tensor , target : torch . Tensor , mask : torch . Tensor ) -> torch . Tensor : \"\"\" Compute the negative log-likelihood of Bernoulli distribution \"\"\" mask = mask . float () bernoulli = torch . distributions . Bernoulli ( probs = input ) masked_log_probs = bernoulli . log_prob (( target > 0 ) . float ()) * mask return - masked_log_probs . sum () / mask . sum ()","title":"criterion_neg_log_bernoulli()"},{"location":"model/#scprint.model.loss.ecs","text":"ecs Computes the similarity of cell embeddings based on a threshold. Parameters: cell_emb ( Tensor ) \u2013 A tensor representing cell embeddings. ecs_threshold ( float , default: 0.5 ) \u2013 A threshold for determining similarity. Defaults to 0.5. Returns: \u2013 torch.Tensor: A tensor representing the mean of 1 minus the square of the difference between the cosine similarity and the threshold. Source code in scprint/model/loss.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def ecs ( cell_emb , ecs_threshold = 0.5 ): \"\"\" ecs Computes the similarity of cell embeddings based on a threshold. Args: cell_emb (torch.Tensor): A tensor representing cell embeddings. ecs_threshold (float, optional): A threshold for determining similarity. Defaults to 0.5. Returns: torch.Tensor: A tensor representing the mean of 1 minus the square of the difference between the cosine similarity and the threshold. \"\"\" # Here using customized cosine similarity instead of F.cosine_similarity # to avoid the pytorch issue of similarity larger than 1.0, pytorch # 78064 # normalize the embedding cell_emb_normed = F . normalize ( cell_emb , p = 2 , dim = 1 ) ecs_sim = torch . mm ( cell_emb_normed , cell_emb_normed . t ()) # mask out diagnal elements mask = torch . eye ( ecs_sim . size ( 0 )) . bool () . to ( ecs_sim . device ) cos_sim = torch . mm ( cell_emb_normed , cell_emb_normed . t ()) cos_sim = cos_sim . masked_fill ( mask , 0.0 ) # only optimize positive similarities cos_sim = F . relu ( cos_sim ) return torch . mean ( 1 - ( cos_sim - ecs_threshold ) ** 2 )","title":"ecs()"},{"location":"model/#scprint.model.loss.graph_similarity_loss","text":"Compute the similarity of 2 generated graphs. Source code in scprint/model/loss.py 182 183 184 185 186 187 188 189 190 def graph_similarity_loss ( input1 : torch . Tensor , input2 : torch . Tensor , mask : torch . Tensor ) -> torch . Tensor : \"\"\" Compute the similarity of 2 generated graphs. \"\"\" mask = mask . float () loss = F . mse_loss ( input1 * mask , input2 * mask , reduction = \"sum\" ) return loss / mask . sum ()","title":"graph_similarity_loss()"},{"location":"model/#scprint.model.loss.graph_sparsity_loss","text":"Compute the sparsity of generated graphs. Source code in scprint/model/loss.py 193 194 195 196 197 198 199 def graph_sparsity_loss ( input : torch . Tensor , mask : torch . Tensor ) -> torch . Tensor : \"\"\" Compute the sparsity of generated graphs. \"\"\" mask = mask . float () loss = F . mse_loss ( input * mask , torch . zeros_like ( input ) * mask , reduction = \"sum\" ) return loss / mask . sum ()","title":"graph_sparsity_loss()"},{"location":"model/#scprint.model.loss.masked_mae_loss","text":"Compute the masked MAE loss between input and target. MAE = mean absolute error Source code in scprint/model/loss.py 18 19 20 21 22 23 24 25 26 27 def masked_mae_loss ( input : torch . Tensor , target : torch . Tensor , mask : torch . Tensor ) -> torch . Tensor : \"\"\" Compute the masked MAE loss between input and target. MAE = mean absolute error \"\"\" mask = mask . float () loss = F . l1_loss ( input * mask , target * mask , reduction = \"sum\" ) return loss / mask . sum ()","title":"masked_mae_loss()"},{"location":"model/#scprint.model.loss.masked_mse_loss","text":"Compute the masked MSE loss between input and target. Source code in scprint/model/loss.py 7 8 9 10 11 12 13 14 15 def masked_mse_loss ( input : torch . Tensor , target : torch . Tensor , mask : torch . Tensor ) -> torch . Tensor : \"\"\" Compute the masked MSE loss between input and target. \"\"\" mask = mask . float () loss = F . mse_loss ( input * mask , target * mask , reduction = \"sum\" ) return loss / mask . sum ()","title":"masked_mse_loss()"},{"location":"model/#scprint.model.loss.masked_nb_loss","text":"Compute the masked negative binomial loss between input and target. Source code in scprint/model/loss.py 30 31 32 33 34 35 36 37 38 39 def masked_nb_loss ( input : torch . Tensor , target : torch . Tensor , mask : torch . Tensor ) -> torch . Tensor : \"\"\" Compute the masked negative binomial loss between input and target. \"\"\" mask = mask . float () nb = torch . distributions . NegativeBinomial ( total_count = target , probs = input ) masked_log_probs = nb . log_prob ( target ) * mask return - masked_log_probs . sum () / mask . sum ()","title":"masked_nb_loss()"},{"location":"model/#scprint.model.loss.masked_relative_error","text":"Compute the masked relative error between input and target. Source code in scprint/model/loss.py 171 172 173 174 175 176 177 178 179 def masked_relative_error ( input : torch . Tensor , target : torch . Tensor , mask : torch . LongTensor ) -> torch . Tensor : \"\"\" Compute the masked relative error between input and target. \"\"\" assert mask . any () loss = torch . abs ( input [ mask ] - target [ mask ]) / ( target [ mask ] + 1e-6 ) return loss . mean ()","title":"masked_relative_error()"},{"location":"model/#scprint.model.loss.nb","text":"This negative binomial function was taken from: Title: scvi-tools Authors: Romain Lopez romain_lopez@gmail.com , Adam Gayoso adamgayoso@berkeley.edu , Galen Xing gx2113@columbia.edu Date: 16th November 2020 Code version: 0.8.1 Availability: https://github.com/YosefLab/scvi-tools/blob/8f5a9cc362325abbb7be1e07f9523cfcf7e55ec0/scvi/core/distributions/_negative_binomial.py Computes negative binomial loss. Parameters x: torch.Tensor Torch Tensor of ground truth data. mu: torch.Tensor Torch Tensor of means of the negative binomial (has to be positive support). theta: torch.Tensor Torch Tensor of inverse dispersion parameter (has to be positive support). eps: Float numerical stability constant.","title":"nb()"},{"location":"model/#scprint.model.loss.nb--returns","text":"If 'mean' is 'True' NB loss value gets returned, otherwise Torch tensor of losses gets returned. Source code in scprint/model/loss.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def nb ( x : torch . Tensor , mu : torch . Tensor , theta : torch . Tensor , eps = 1e-8 ): \"\"\" This negative binomial function was taken from: Title: scvi-tools Authors: Romain Lopez <romain_lopez@gmail.com>, Adam Gayoso <adamgayoso@berkeley.edu>, Galen Xing <gx2113@columbia.edu> Date: 16th November 2020 Code version: 0.8.1 Availability: https://github.com/YosefLab/scvi-tools/blob/8f5a9cc362325abbb7be1e07f9523cfcf7e55ec0/scvi/core/distributions/_negative_binomial.py Computes negative binomial loss. Parameters ---------- x: torch.Tensor Torch Tensor of ground truth data. mu: torch.Tensor Torch Tensor of means of the negative binomial (has to be positive support). theta: torch.Tensor Torch Tensor of inverse dispersion parameter (has to be positive support). eps: Float numerical stability constant. Returns ------- If 'mean' is 'True' NB loss value gets returned, otherwise Torch tensor of losses gets returned. \"\"\" if theta . ndimension () == 1 : theta = theta . view ( 1 , theta . size ( 0 )) log_theta_mu_eps = torch . log ( theta + mu + eps ) res = ( theta * ( torch . log ( theta + eps ) - log_theta_mu_eps ) + x * ( torch . log ( mu + eps ) - log_theta_mu_eps ) + torch . lgamma ( x + theta ) - torch . lgamma ( theta ) - torch . lgamma ( x + 1 ) ) return res . sum ( - 1 ) . mean ()","title":"Returns"},{"location":"model/#scprint.model.loss.similarity","text":"Dot product or cosine similarity Source code in scprint/model/loss.py 202 203 204 205 206 207 208 def similarity ( x , y , temp ): \"\"\" Dot product or cosine similarity \"\"\" res = F . cosine_similarity ( x , y ) / temp labels = torch . arange ( res . size ( 0 )) . long () . to ( device = res . device ) return F . cross_entropy ( res , labels )","title":"similarity()"},{"location":"model/#scprint.model.loss.zinb","text":"This zero-inflated negative binomial function was taken from: Title: scvi-tools Authors: Romain Lopez romain_lopez@gmail.com , Adam Gayoso adamgayoso@berkeley.edu , Galen Xing gx2113@columbia.edu Date: 16th November 2020 Code version: 0.8.1 Availability: https://github.com/YosefLab/scvi-tools/blob/8f5a9cc362325abbb7be1e07f9523cfcf7e55ec0/scvi/core/distributions/_negative_binomial.py Computes zero inflated negative binomial loss. Parameters x: torch.Tensor Torch Tensor of ground truth data. mu: torch.Tensor Torch Tensor of means of the negative binomial (has to be positive support). theta: torch.Tensor Torch Tensor of inverses dispersion parameter (has to be positive support). pi: torch.Tensor Torch Tensor of logits of the dropout parameter (real support) eps: Float numerical stability constant.","title":"zinb()"},{"location":"model/#scprint.model.loss.zinb--returns","text":"If 'mean' is 'True' ZINB loss value gets returned, otherwise Torch tensor of losses gets returned. Source code in scprint/model/loss.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def zinb ( target : torch . Tensor , mu : torch . Tensor , theta : torch . Tensor , pi : torch . Tensor , eps = 1e-6 , mask = None , ): \"\"\" This zero-inflated negative binomial function was taken from: Title: scvi-tools Authors: Romain Lopez <romain_lopez@gmail.com>, Adam Gayoso <adamgayoso@berkeley.edu>, Galen Xing <gx2113@columbia.edu> Date: 16th November 2020 Code version: 0.8.1 Availability: https://github.com/YosefLab/scvi-tools/blob/8f5a9cc362325abbb7be1e07f9523cfcf7e55ec0/scvi/core/distributions/_negative_binomial.py Computes zero inflated negative binomial loss. Parameters ---------- x: torch.Tensor Torch Tensor of ground truth data. mu: torch.Tensor Torch Tensor of means of the negative binomial (has to be positive support). theta: torch.Tensor Torch Tensor of inverses dispersion parameter (has to be positive support). pi: torch.Tensor Torch Tensor of logits of the dropout parameter (real support) eps: Float numerical stability constant. Returns ------- If 'mean' is 'True' ZINB loss value gets returned, otherwise Torch tensor of losses gets returned. \"\"\" softplus_pi = F . softplus ( - pi ) # uses log(sigmoid(x)) = -softplus(-x) log_theta_eps = torch . log ( theta + eps ) log_theta_mu_eps = torch . log ( theta + mu + eps ) pi_theta_log = - pi + theta * ( log_theta_eps - log_theta_mu_eps ) case_zero = F . softplus ( pi_theta_log ) - softplus_pi mul_case_zero = torch . mul (( target < eps ) . type ( torch . float32 ), case_zero ) case_non_zero = ( - softplus_pi + pi_theta_log + target * ( torch . log ( mu + eps ) - log_theta_mu_eps ) + torch . lgamma ( target + theta ) - torch . lgamma ( theta ) - torch . lgamma ( target + 1 ) ) mul_case_non_zero = torch . mul (( target > eps ) . type ( torch . float32 ), case_non_zero ) res = mul_case_zero + mul_case_non_zero # we want to minize the loss but maximize the log likelyhood return - res . sum ( - 1 ) . mean ()","title":"Returns"},{"location":"structure/","text":"structure gene embedders Function to get embeddings from a set of genes, given their ensembl ids. For now use 2 different models: RNABert : for non coding genes ESM2 : for protein coding genes given ids, a fasta file, will use the models to compute an embedding of each gene. This could be potentially applied to genes with mutations and from different species. data_loaders From scDataloader. (see more in the available readmes and website https://jkobject.com/scDataLoader) For now can work with either one to many AnnData's or a laminDB Collection of AnnDatas allows you to preprocess your anndatas too. They can be stored locally or remotely stores them in a Dataset class. Creates the DataLoaders from a Datamodule Class. Collates the results using a Collator function. model Extends from lightning data module to implement all the necessary functions to do: training validation testing prediction (inference) is subdivided into multiple parts: encoder transformer decoder trainer & cli the model uses lightning's training toolkit and CLI tools. to use CLI, just call scprint ... (will call the __main__.py function). Additional, training-specific informations are passed to the model using the trainer.py function. specific training schemes are available under the config folder as yaml files. Moreover the model can be trained on multiple compute types. SLURM scripts are available under the slurm folder. tasks Implement different tasks that a pretrained model would perform. for now: GRN prediction: given a single cell dataset and a group (cell type, cluster, ...) will output a GRnnData completed with a predicted GRN from the attention of the model. denoising: from a single cell dataset, will modify the count matrices to predict what it would have looked like if it had been sequenced deeper, according to the model. embedding: from a single cell dataset, will create embeddings (low dimensional representations) of each cells, as well as prediction of the cell labels the model has been trained on (cell type, disease, ethnicity, sex...). It will also output a umap and predicted expression from the zinb, post bottleneck (similar to a VAE decoder prediction)","title":"structure"},{"location":"structure/#structure","text":"","title":"structure"},{"location":"structure/#gene-embedders","text":"Function to get embeddings from a set of genes, given their ensembl ids. For now use 2 different models: RNABert : for non coding genes ESM2 : for protein coding genes given ids, a fasta file, will use the models to compute an embedding of each gene. This could be potentially applied to genes with mutations and from different species.","title":"gene embedders"},{"location":"structure/#data_loaders","text":"From scDataloader. (see more in the available readmes and website https://jkobject.com/scDataLoader) For now can work with either one to many AnnData's or a laminDB Collection of AnnDatas allows you to preprocess your anndatas too. They can be stored locally or remotely stores them in a Dataset class. Creates the DataLoaders from a Datamodule Class. Collates the results using a Collator function.","title":"data_loaders"},{"location":"structure/#model","text":"Extends from lightning data module to implement all the necessary functions to do: training validation testing prediction (inference) is subdivided into multiple parts: encoder transformer decoder","title":"model"},{"location":"structure/#trainer-cli","text":"the model uses lightning's training toolkit and CLI tools. to use CLI, just call scprint ... (will call the __main__.py function). Additional, training-specific informations are passed to the model using the trainer.py function. specific training schemes are available under the config folder as yaml files. Moreover the model can be trained on multiple compute types. SLURM scripts are available under the slurm folder.","title":"trainer &amp; cli"},{"location":"structure/#tasks","text":"Implement different tasks that a pretrained model would perform. for now: GRN prediction: given a single cell dataset and a group (cell type, cluster, ...) will output a GRnnData completed with a predicted GRN from the attention of the model. denoising: from a single cell dataset, will modify the count matrices to predict what it would have looked like if it had been sequenced deeper, according to the model. embedding: from a single cell dataset, will create embeddings (low dimensional representations) of each cells, as well as prediction of the cell labels the model has been trained on (cell type, disease, ethnicity, sex...). It will also output a umap and predicted expression from the zinb, post bottleneck (similar to a VAE decoder prediction)","title":"tasks"},{"location":"tasks/","text":"Documentation for the tasks scprint . tasks . cell_emb . Embedder Embedder a class to embed and annotate cells using a model Parameters: model ( Module ) \u2013 The model to be used for embedding and annotating cells. batch_size ( int , default: 64 ) \u2013 The size of the batches to be used in the DataLoader. Defaults to 64. num_workers ( int , default: 8 ) \u2013 The number of worker processes to use for data loading. Defaults to 8. how ( str , default: 'most expr' ) \u2013 The method to be used for selecting valid genes. Defaults to \"most expr\". max_len ( int , default: 1000 ) \u2013 The maximum length of the gene sequence. Defaults to 1000. add_zero_genes ( int , default: 100 ) \u2013 The number of zero genes to add to the gene sequence. Defaults to 100. precision ( str , default: '16-mixed' ) \u2013 The precision to be used in the Trainer. Defaults to \"16-mixed\". organisms ( List [ str ] , default: ['NCBITaxon:9606'] ) \u2013 The list of organisms to be considered. Defaults to [ \"NCBITaxon:9606\", ]. pred_embedding ( List [ str ] , default: ['cell_type_ontology_term_id', 'disease_ontology_term_id', 'self_reported_ethnicity_ontology_term_id', 'sex_ontology_term_id'] ) \u2013 The list of labels to be used for plotting embeddings. Defaults to [ \"cell_type_ontology_term_id\", \"disease_ontology_term_id\", \"self_reported_ethnicity_ontology_term_id\", \"sex_ontology_term_id\", ]. model_name ( str , default: 'scprint' ) \u2013 The name of the model to be used. Defaults to \"scprint\". output_expression ( str , default: 'sample' ) \u2013 The type of output expression to be used. Can be one of \"all\", \"sample\", \"none\". Defaults to \"sample\". Source code in scprint/tasks/cell_emb.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , model : torch . nn . Module , batch_size : int = 64 , num_workers : int = 8 , how : str = \"most expr\" , max_len : int = 1000 , add_zero_genes : int = 100 , precision : str = \"16-mixed\" , organisms : List [ str ] = [ \"NCBITaxon:9606\" , ], pred_embedding : List [ str ] = [ \"cell_type_ontology_term_id\" , \"disease_ontology_term_id\" , \"self_reported_ethnicity_ontology_term_id\" , \"sex_ontology_term_id\" , ], model_name : str = \"scprint\" , output_expression : str = \"sample\" , # one of \"all\" \"sample\" \"none\" plot_corr_size : int = 64 , ): \"\"\" Embedder a class to embed and annotate cells using a model Args: model (torch.nn.Module): The model to be used for embedding and annotating cells. batch_size (int, optional): The size of the batches to be used in the DataLoader. Defaults to 64. num_workers (int, optional): The number of worker processes to use for data loading. Defaults to 8. how (str, optional): The method to be used for selecting valid genes. Defaults to \"most expr\". max_len (int, optional): The maximum length of the gene sequence. Defaults to 1000. add_zero_genes (int, optional): The number of zero genes to add to the gene sequence. Defaults to 100. precision (str, optional): The precision to be used in the Trainer. Defaults to \"16-mixed\". organisms (List[str], optional): The list of organisms to be considered. Defaults to [ \"NCBITaxon:9606\", ]. pred_embedding (List[str], optional): The list of labels to be used for plotting embeddings. Defaults to [ \"cell_type_ontology_term_id\", \"disease_ontology_term_id\", \"self_reported_ethnicity_ontology_term_id\", \"sex_ontology_term_id\", ]. model_name (str, optional): The name of the model to be used. Defaults to \"scprint\". output_expression (str, optional): The type of output expression to be used. Can be one of \"all\", \"sample\", \"none\". Defaults to \"sample\". \"\"\" self . model = model self . batch_size = batch_size self . num_workers = num_workers self . how = how self . max_len = max_len self . add_zero_genes = add_zero_genes self . organisms = organisms self . model . pred_embedding = pred_embedding self . model_name = model_name self . output_expression = output_expression self . plot_corr_size = plot_corr_size self . precision = precision self . trainer = Trainer ( precision = precision ) scprint.tasks.grn","title":"tasks"},{"location":"tasks/#documentation-for-the-tasks","text":"","title":"Documentation for the tasks"},{"location":"tasks/#scprint.tasks.cell_emb.Embedder","text":"Embedder a class to embed and annotate cells using a model Parameters: model ( Module ) \u2013 The model to be used for embedding and annotating cells. batch_size ( int , default: 64 ) \u2013 The size of the batches to be used in the DataLoader. Defaults to 64. num_workers ( int , default: 8 ) \u2013 The number of worker processes to use for data loading. Defaults to 8. how ( str , default: 'most expr' ) \u2013 The method to be used for selecting valid genes. Defaults to \"most expr\". max_len ( int , default: 1000 ) \u2013 The maximum length of the gene sequence. Defaults to 1000. add_zero_genes ( int , default: 100 ) \u2013 The number of zero genes to add to the gene sequence. Defaults to 100. precision ( str , default: '16-mixed' ) \u2013 The precision to be used in the Trainer. Defaults to \"16-mixed\". organisms ( List [ str ] , default: ['NCBITaxon:9606'] ) \u2013 The list of organisms to be considered. Defaults to [ \"NCBITaxon:9606\", ]. pred_embedding ( List [ str ] , default: ['cell_type_ontology_term_id', 'disease_ontology_term_id', 'self_reported_ethnicity_ontology_term_id', 'sex_ontology_term_id'] ) \u2013 The list of labels to be used for plotting embeddings. Defaults to [ \"cell_type_ontology_term_id\", \"disease_ontology_term_id\", \"self_reported_ethnicity_ontology_term_id\", \"sex_ontology_term_id\", ]. model_name ( str , default: 'scprint' ) \u2013 The name of the model to be used. Defaults to \"scprint\". output_expression ( str , default: 'sample' ) \u2013 The type of output expression to be used. Can be one of \"all\", \"sample\", \"none\". Defaults to \"sample\". Source code in scprint/tasks/cell_emb.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , model : torch . nn . Module , batch_size : int = 64 , num_workers : int = 8 , how : str = \"most expr\" , max_len : int = 1000 , add_zero_genes : int = 100 , precision : str = \"16-mixed\" , organisms : List [ str ] = [ \"NCBITaxon:9606\" , ], pred_embedding : List [ str ] = [ \"cell_type_ontology_term_id\" , \"disease_ontology_term_id\" , \"self_reported_ethnicity_ontology_term_id\" , \"sex_ontology_term_id\" , ], model_name : str = \"scprint\" , output_expression : str = \"sample\" , # one of \"all\" \"sample\" \"none\" plot_corr_size : int = 64 , ): \"\"\" Embedder a class to embed and annotate cells using a model Args: model (torch.nn.Module): The model to be used for embedding and annotating cells. batch_size (int, optional): The size of the batches to be used in the DataLoader. Defaults to 64. num_workers (int, optional): The number of worker processes to use for data loading. Defaults to 8. how (str, optional): The method to be used for selecting valid genes. Defaults to \"most expr\". max_len (int, optional): The maximum length of the gene sequence. Defaults to 1000. add_zero_genes (int, optional): The number of zero genes to add to the gene sequence. Defaults to 100. precision (str, optional): The precision to be used in the Trainer. Defaults to \"16-mixed\". organisms (List[str], optional): The list of organisms to be considered. Defaults to [ \"NCBITaxon:9606\", ]. pred_embedding (List[str], optional): The list of labels to be used for plotting embeddings. Defaults to [ \"cell_type_ontology_term_id\", \"disease_ontology_term_id\", \"self_reported_ethnicity_ontology_term_id\", \"sex_ontology_term_id\", ]. model_name (str, optional): The name of the model to be used. Defaults to \"scprint\". output_expression (str, optional): The type of output expression to be used. Can be one of \"all\", \"sample\", \"none\". Defaults to \"sample\". \"\"\" self . model = model self . batch_size = batch_size self . num_workers = num_workers self . how = how self . max_len = max_len self . add_zero_genes = add_zero_genes self . organisms = organisms self . model . pred_embedding = pred_embedding self . model_name = model_name self . output_expression = output_expression self . plot_corr_size = plot_corr_size self . precision = precision self . trainer = Trainer ( precision = precision )","title":"Embedder"},{"location":"tasks/#scprint.tasks.grn","text":"","title":"grn"},{"location":"utils/","text":"Documentation for the utils scprint.cli MyCLI Bases: LightningCLI MyCLI is a subclass of LightningCLI to add some missing params and create bindings between params of the model and the data. scprint.utils scprint.__main__ Entry point for scprint. MySaveConfig Bases: SaveConfigCallback MySaveConfig is a subclass of SaveConfigCallback to parametrize the wandb logger further in cli mode scprint.trainer.trainer TrainingMode Bases: Callback TrainingMode a callback to set the training specific info to the model. This is because lightning is unfortunately setup this way. the model should be separated from training but at the same time it has training specific methods... so we have to do this. Source code in scprint/trainer/trainer.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , do_denoise : bool = True , noise : List [ float ] = [ 0.3 ], do_cce : bool = True , cce_sim : float = 0.5 , do_ecs : bool = True , ecs_threshold : float = 0.3 , ecs_scale : float = 1.0 , do_mvc : bool = False , do_adv_cls : bool = False , do_next_tp : bool = False , do_generate : bool = False , class_scale : float = 1.0 , mask_ratio : List [ float ] = [ 0.15 , 0.3 ], warmup_duration : int = 500 , weight_decay : float = 0.01 , fused_adam : bool = False , lr_patience : int = 3 , ): \"\"\" TrainingMode a callback to set the training specific info to the model. This is because lightning is unfortunately setup this way. the model should be separated from training but at the same time it has training specific methods... so we have to do this. Args: see @model.py \"\"\" super () . __init__ () self . do_denoise = do_denoise self . noise = noise self . do_cce = do_cce self . cce_sim = cce_sim self . do_ecs = do_ecs self . ecs_threshold = ecs_threshold self . ecs_scale = ecs_scale self . do_mvc = do_mvc self . do_adv_cls = do_adv_cls self . do_next_tp = do_next_tp self . do_generate = do_generate self . class_scale = class_scale self . mask_ratio = mask_ratio self . warmup_duration = warmup_duration self . weight_decay = weight_decay self . fused_adam = fused_adam self . lr_patience = lr_patience","title":"utils"},{"location":"utils/#documentation-for-the-utils","text":"","title":"Documentation for the utils"},{"location":"utils/#scprint.cli","text":"","title":"cli"},{"location":"utils/#scprint.cli.MyCLI","text":"Bases: LightningCLI MyCLI is a subclass of LightningCLI to add some missing params and create bindings between params of the model and the data.","title":"MyCLI"},{"location":"utils/#scprint.utils","text":"","title":"utils"},{"location":"utils/#scprint.__main__","text":"Entry point for scprint.","title":"__main__"},{"location":"utils/#scprint.__main__.MySaveConfig","text":"Bases: SaveConfigCallback MySaveConfig is a subclass of SaveConfigCallback to parametrize the wandb logger further in cli mode","title":"MySaveConfig"},{"location":"utils/#scprint.trainer.trainer","text":"","title":"trainer"},{"location":"utils/#scprint.trainer.trainer.TrainingMode","text":"Bases: Callback TrainingMode a callback to set the training specific info to the model. This is because lightning is unfortunately setup this way. the model should be separated from training but at the same time it has training specific methods... so we have to do this. Source code in scprint/trainer/trainer.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def __init__ ( self , do_denoise : bool = True , noise : List [ float ] = [ 0.3 ], do_cce : bool = True , cce_sim : float = 0.5 , do_ecs : bool = True , ecs_threshold : float = 0.3 , ecs_scale : float = 1.0 , do_mvc : bool = False , do_adv_cls : bool = False , do_next_tp : bool = False , do_generate : bool = False , class_scale : float = 1.0 , mask_ratio : List [ float ] = [ 0.15 , 0.3 ], warmup_duration : int = 500 , weight_decay : float = 0.01 , fused_adam : bool = False , lr_patience : int = 3 , ): \"\"\" TrainingMode a callback to set the training specific info to the model. This is because lightning is unfortunately setup this way. the model should be separated from training but at the same time it has training specific methods... so we have to do this. Args: see @model.py \"\"\" super () . __init__ () self . do_denoise = do_denoise self . noise = noise self . do_cce = do_cce self . cce_sim = cce_sim self . do_ecs = do_ecs self . ecs_threshold = ecs_threshold self . ecs_scale = ecs_scale self . do_mvc = do_mvc self . do_adv_cls = do_adv_cls self . do_next_tp = do_next_tp self . do_generate = do_generate self . class_scale = class_scale self . mask_ratio = mask_ratio self . warmup_duration = warmup_duration self . weight_decay = weight_decay self . fused_adam = fused_adam self . lr_patience = lr_patience","title":"TrainingMode"}]}