<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://www.jkobject.com/scPRINT-2/model/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>model - scPRINT-2</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "model";
        var mkdocs_page_input_path = "model.md";
        var mkdocs_page_url = "/scPRINT-2/model/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="..">
          <img src="../fig.png" class="logo" alt="Logo"/>
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../structure/">structure</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../pretrain/">pre-training</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../umap/nice_umap_scprint2.html">350M subset umap</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">documentation</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">model</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#model-description">model description</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint2.model.model">model</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.model.scPRINT2">scPRINT2</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2--initialize-model">Initialize model</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2--configure-training">Configure training</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2--train-with-pytorch-lightning">Train with PyTorch Lightning</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2--generate-embeddings">Generate embeddings</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2.genes">genes</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2.add_organism">add_organism</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2.configure_optimizers">configure_optimizers</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2.forward">forward</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2.log_adata">log_adata</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2.on_fit_start">on_fit_start</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2.on_load_checkpoint">on_load_checkpoint</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2.on_predict_epoch_end">on_predict_epoch_end</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2.on_predict_epoch_start">on_predict_epoch_start</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2.on_test_start">on_test_start</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2.on_validation_epoch_end">on_validation_epoch_end</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2.optimizer_step">optimizer_step</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2.predict_step">predict_step</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2.training_step">training_step</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.model.scPRINT2.validation_step">validation_step</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#losses">losses</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint2.model.loss">loss</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.loss.AdversarialDiscriminatorLoss">AdversarialDiscriminatorLoss</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.loss.AdversarialDiscriminatorLoss.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.loss.contrastive_loss">contrastive_loss</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.loss.criterion_neg_log_bernoulli">criterion_neg_log_bernoulli</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.loss.ecs">ecs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.loss.grad_reverse">grad_reverse</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.loss.hierarchical_classification">hierarchical_classification</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.loss.masked_mae">masked_mae</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.loss.masked_mse">masked_mse</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.loss.masked_nb">masked_nb</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.loss.masked_relative_error">masked_relative_error</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.loss.mse">mse</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.loss.nb">nb</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.loss.within_sample">within_sample</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.loss.zinb">zinb</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#utils">utils</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint2.model.utils">utils</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.utils.Attention">Attention</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.utils.Attention.add_attn">add_attn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.utils.Attention.add_qk">add_qk</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.utils.Attention.get">get</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.utils.WeightedMasker">WeightedMasker</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.utils.downsample_profile">downsample_profile</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.utils.make_adata">make_adata</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.utils.simple_masker">simple_masker</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.utils.test">test</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.utils.zinb_sample">zinb_sample</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#encoder-and-decoder-modules">encoder and decoder modules</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint2.model.encoders">encoders</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.encoders.CategoryValueEncoder">CategoryValueEncoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.encoders.ContinuousValueEncoder">ContinuousValueEncoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.encoders.ContinuousValueEncoder.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.encoders.DPositionalEncoding">DPositionalEncoding</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.encoders.DPositionalEncoding.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.encoders.EasyExprGNN">EasyExprGNN</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.encoders.EasyExprGNN.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.encoders.ExprBasedFT">ExprBasedFT</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.encoders.ExprBasedFT.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.encoders.GNN">GNN</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.encoders.GNN.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.encoders.GeneEncoder">GeneEncoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.encoders.GeneEncoder.__del__">__del__</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.encoders.GeneEncoder.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.encoders.PositionalEncoding">PositionalEncoding</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.encoders.PositionalEncoding.forward">forward</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint2.model.decoders">decoders</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.decoders.ClsDecoder">ClsDecoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.decoders.ClsDecoder.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.decoders.ExprDecoder">ExprDecoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.decoders.ExprDecoder.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.decoders.GraphSDEExprDecoder">GraphSDEExprDecoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.decoders.MVCDecoder">MVCDecoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.decoders.MVCDecoder.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.decoders.VAEDecoder">VAEDecoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.decoders.VAEDecoder.forward">forward</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.decoders.VAEDecoder.kl_divergence">kl_divergence</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.decoders.VAEDecoder.reparameterize">reparameterize</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint2.model.fsq">fsq</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.fsq.FSQ">FSQ</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.fsq.FSQ.bound">bound</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.fsq.FSQ.codes_to_indices">codes_to_indices</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.fsq.FSQ.forward">forward</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.fsq.FSQ.indices_to_codes">indices_to_codes</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint2.model.fsq.FSQ.quantize">quantize</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint2.model.fsq.round_ste">round_ste</a>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tokenizers/">tokenizers</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tasks/">tasks</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../utils/">utils</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../cli/">cli</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">scPRINT-2</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">documentation</li>
      <li class="breadcrumb-item active">model</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="documentation-for-the-model">Documentation for the <code>model</code></h1>
<h2 id="model-description">model description</h2>


<div class="doc doc-object doc-module">



<h2 id="scprint2.model.model" class="doc doc-heading">
            <code>scprint2.model.model</code>


</h2>

    <div class="doc doc-contents first">










<p><span class="doc-section-title">Classes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="scPRINT2 (scprint2.model.model.scPRINT2)" href="#scprint2.model.model.scPRINT2">scPRINT2</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
      </tbody>
    </table>







<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="scprint2.model.model.scPRINT2" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">scPRINT2</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="lightning.LightningModule">LightningModule</span></code>, <code><span title="huggingface_hub.PyTorchModelHubMixin">PyTorchModelHubMixin</span></code></p>



        <p>scPRINT-2: Single-Cell Pretrained Regulatory Inference Network Transformer.</p>
<p>A foundation model for single-cell biology that learns cell and gene representations
through self-supervised learning on large-scale single-cell RNA-seq data. The model
can be used for:
- Cell type classification and annotation
- Gene expression denoising and imputation
- Cell embedding generation for downstream analysis
- Gene regulatory network inference via attention patterns
- Multi-species gene expression modeling</p>


<details class="architecture-overview" open>
  <summary>Architecture Overview</summary>
  <ol>
<li>Gene Encoder: Embeds gene identities (optionally with pretrained embeddings)</li>
<li>Expression Encoder: Encodes expression values (continuous, binned, or metacell)</li>
<li>Position Encoder: Optional genomic position encoding</li>
<li>Transformer: Main attention-based encoder (various attention mechanisms)</li>
<li>Cell Transformer: Optional separate transformer for cell embeddings</li>
<li>Decoders: Expression reconstruction, classification, and MVC decoders</li>
</ol>
</details>

<details class="the-model-supports-multiple-training-objectives" open>
  <summary>The model supports multiple training objectives</summary>
  <ul>
<li>Masked expression prediction (like BERT)</li>
<li>Denoising autoencoding</li>
<li>Cell embedding contrastive learning (ECS and CCE losses)</li>
<li>Multi-class cell type classification with hierarchical labels</li>
<li>Multi-view coding (MVC) for robust representations</li>
</ul>
</details>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>genes</code></b>
                  (<code><span title="list">list</span> | <span title="dict">dict</span></code>)
              –
              <div class="doc-md-description">
                <p>Gene vocabulary. Either a list of gene names or a dict
mapping organism names to lists of genes for multi-species models.</p>
              </div>
            </li>
            <li>
              <b><code>organisms</code></b>
                  (<code><span title="list">list</span>[<span title="str">str</span>]</code>)
              –
              <div class="doc-md-description">
                <p>List of organism ontology term IDs the model supports.</p>
              </div>
            </li>
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>256</code>
)
              –
              <div class="doc-md-description">
                <p>Hidden dimension of the transformer. Defaults to 256.</p>
              </div>
            </li>
            <li>
              <b><code>nhead</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>4</code>
)
              –
              <div class="doc-md-description">
                <p>Number of attention heads. Defaults to 4.</p>
              </div>
            </li>
            <li>
              <b><code>nlayers</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>8</code>
)
              –
              <div class="doc-md-description">
                <p>Number of transformer layers. Defaults to 8.</p>
              </div>
            </li>
            <li>
              <b><code>precpt_gene_emb</code></b>
                  (<code><span title="str">str</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Path to parquet file with pretrained gene
embeddings. Index should match gene names. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>memmap_gene_emb</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Memory-map gene embeddings for large files.
Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>finetune_gene_emb</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Add trainable adapter layers on top of
frozen pretrained embeddings. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>freeze_embeddings</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Freeze gene embeddings during training.
Defaults to True.</p>
              </div>
            </li>
            <li>
              <b><code>gene_pos_file</code></b>
                  (<code><span title="str">str</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Path to parquet file with genomic positions.
Must have 'pos' column with integer positions. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>normalization</code></b>
                  (<code><span title="str">str</span></code>, default:
                      <code>&#39;sum&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Expression normalization method. One of:
- "sum": Divide by total counts (TPM-like)
- "log": Log2(1 + x) transform
- "both": Sum normalization then log transform
- "raw": No normalization
Defaults to "sum".</p>
              </div>
            </li>
            <li>
              <b><code>attn_bias</code></b>
                  (<code><span title="str">str</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Path to sparse matrix (.npz) with attention biases
(e.g., gene-gene regulatory priors). Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>expr_encoder_layers</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>3</code>
)
              –
              <div class="doc-md-description">
                <p>Number of layers in expression encoder MLP.
Defaults to 3.</p>
              </div>
            </li>
            <li>
              <b><code>attention</code></b>
                  (<code><span title="str">str</span></code>, default:
                      <code>&#39;normal&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Attention mechanism type. One of:
- "normal": Standard PyTorch attention
- "legacy-flash": Flash attention via simpler-flash
- "performer": Performer linear attention
- "hyper": Compressed hyperbolic attention
- "criss-cross": Criss-cross attention
Defaults to "normal".</p>
              </div>
            </li>
            <li>
              <b><code>expr_emb_style</code></b>
                  (<code><span title="str">str</span></code>, default:
                      <code>&#39;continuous&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Expression embedding approach. One of:
- "continuous": MLP on continuous expression values
- "binned": Learned embeddings for discretized expression bins
- "metacell": DeepSet encoder aggregating KNN neighbors
Defaults to "continuous".</p>
              </div>
            </li>
            <li>
              <b><code>n_input_bins</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>0</code>
)
              –
              <div class="doc-md-description">
                <p>Number of expression bins when using binned
embedding. Required if expr_emb_style="binned". Defaults to 0.</p>
              </div>
            </li>
            <li>
              <b><code>mvc_decoder</code></b>
                  (<code><span title="str">str</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Multi-view coding decoder architecture. One of:
- None: No MVC decoder
- "inner product": Dot product between cell and gene embeddings
- "concat query": Concatenate cell embedding with gene queries
- "sum query": Add cell embedding to gene queries
Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>pred_embedding</code></b>
                  (<code><span title="list">list</span>[<span title="str">str</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Class names to use for cell embeddings
during prediction/logging. Defaults to None (use all).</p>
              </div>
            </li>
            <li>
              <b><code>layers_cls</code></b>
                  (<code><span title="list">list</span>[<span title="int">int</span>]</code>, default:
                      <code>[256, 128]</code>
)
              –
              <div class="doc-md-description">
                <p>Hidden layer sizes for classification heads.
Defaults to [256, 128].</p>
              </div>
            </li>
            <li>
              <b><code>classes</code></b>
                  (<code><span title="dict">dict</span>[<span title="str">str</span>, <span title="int">int</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Classification targets mapping class names
to number of categories. E.g., {"cell_type_ontology_term_id": 100}.
Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>labels_hierarchy</code></b>
                  (<code><span title="dict">dict</span>[<span title="str">str</span>, <span title="dict">dict</span>[<span title="int">int</span>, <span title="list">list</span>[<span title="int">int</span>]]]</code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>Hierarchical
label structure for ontology-based classes. Maps parent indices to lists
of children indices. Defaults to {}.</p>
              </div>
            </li>
            <li>
              <b><code>label_decoders</code></b>
                  (<code><span title="dict">dict</span>[<span title="str">str</span>, <span title="dict">dict</span>[<span title="int">int</span>, <span title="str">str</span>]]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Mapping from encoded
integers back to label strings for each class. Used for logging/plotting.
Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>compress_class_dim</code></b>
                  (<code><span title="dict">dict</span>[<span title="str">str</span>, <span title="int">int</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Compressed embedding dimension
for each class. Uses VAE or FSQ compression. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>cell_specific_blocks</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Use separate transformer for cell
embeddings with cross-attention to gene transformer. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>zinb</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Use Zero-Inflated Negative Binomial distribution for
expression reconstruction. If False, uses MSE loss. Defaults to True.</p>
              </div>
            </li>
            <li>
              <b><code>splicing_head</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Add separate decoder for spliced/unspliced
expression. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>do_adv_cls</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Use adversarial classification to remove batch
effects from cell type embeddings. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>Dropout rate throughout the model. Defaults to 0.1.</p>
              </div>
            </li>
            <li>
              <b><code>use_metacell_token</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Add learnable metacell token to distinguish
single cells from metacells. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>lr</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.0001</code>
)
              –
              <div class="doc-md-description">
                <p>Base learning rate. Defaults to 0.0001.</p>
              </div>
            </li>
            <li>
              <b><code>nb_features</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Number of random features for Performer attention.
Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>sketcher_size</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>200</code>
)
              –
              <div class="doc-md-description">
                <p>Sketch size for sparse attention methods.
Defaults to 200.</p>
              </div>
            </li>
            <li>
              <b><code>feature_redraw_interval</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Steps between random feature redraws
for Performer. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>num_heads_kv</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>4</code>
)
              –
              <div class="doc-md-description">
                <p>Number of key-value heads (for MQA/GQA).
Defaults to 4.</p>
              </div>
            </li>
            <li>
              <b><code>d_model_cell</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>128</code>
)
              –
              <div class="doc-md-description">
                <p>Hidden dim for cell transformer when using
cell_specific_blocks. Defaults to 128.</p>
              </div>
            </li>
            <li>
              <b><code>nhead_cell</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>4</code>
)
              –
              <div class="doc-md-description">
                <p>Attention heads for cell transformer. Defaults to 4.</p>
              </div>
            </li>
            <li>
              <b><code>nlayers_cell</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>6</code>
)
              –
              <div class="doc-md-description">
                <p>Layers in cell transformer. Defaults to 6.</p>
              </div>
            </li>
            <li>
              <b><code>num_heads_kv_cell</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>4</code>
)
              –
              <div class="doc-md-description">
                <p>KV heads for cell transformer. Defaults to 4.</p>
              </div>
            </li>
            <li>
              <b><code>drop_path_rate</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.0</code>
)
              –
              <div class="doc-md-description">
                <p>Stochastic depth rate. Defaults to 0.0.</p>
              </div>
            </li>
            <li>
              <b><code>**attention_kwargs</code></b>
                  (<code><span title="dict">dict</span></code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>Additional arguments passed to FlashTransformer.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Attributes:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>Training</code></b>
                  (<code>Configuration (set these before training</code>)
              –
              <div class="doc-md-description">
                <p>noise (list[float]): Dropout rates for denoising task. E.g., [0.6].
mask_ratio (list[float]): Mask ratios for masked prediction. E.g., [0.15].
cce_temp (float): Temperature for contrastive loss.
cce_scale (float): Weight for contrastive cell embedding loss.
ecs_scale (float): Weight for elastic cell similarity loss.
ecs_threshold (float): Similarity threshold for ECS loss.
mvc_scale (float): Weight for MVC reconstruction loss.
class_scale (float): Weight for classification loss.
lr_reduce_patience (int): Epochs before reducing learning rate.
lr_reduce_factor (float): Factor to reduce learning rate by.
warmup_duration (int): Steps for learning rate warmup.</p>
              </div>
            </li>
            <li>
              <b><code>Prediction</code></b>
                  (<code>Configuration (set before predict</code>)
              –
              <div class="doc-md-description">
                <p>predict_mode (str): "none" or "generate" for expression generation.
pred_embedding (list[str]): Classes to include in cell embeddings.
get_attention_layer (list[int]): Layers to extract attention from.
predict_depth_mult (float): Multiplier for depth in generation.
pred_log_adata (bool): Whether to log predictions as AnnData.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<h4 id="scprint2.model.model.scPRINT2--initialize-model">Initialize model</h4>
<p>model = scPrint2(
...     genes=gene_list,
...     organisms=["NCBITaxon:9606"],
...     d_model=512,
...     nlayers=12,
...     classes={"cell_type_ontology_term_id": 100},
... )</p>
<h4 id="scprint2.model.model.scPRINT2--configure-training">Configure training</h4>
<p>model.noise = [0.4, 0.6]
model.mask_ratio = [0.15, 0.3]</p>
<h4 id="scprint2.model.model.scPRINT2--train-with-pytorch-lightning">Train with PyTorch Lightning</h4>
<p>trainer = L.Trainer(max_epochs=100)
trainer.fit(model, datamodule)</p>
<h4 id="scprint2.model.model.scPRINT2--generate-embeddings">Generate embeddings</h4>
<p>model.pred_embedding = ["cell_type_ontology_term_id"]
predictions = trainer.predict(model, datamodule)</p>
</blockquote>
</blockquote>
</blockquote>
</details>

<details class="note" open>
  <summary>Note</summary>
  <p>The model is designed to work with scDataLoader's DataModule and Collator.
Gene order must match between model initialization and data loading.</p>
</details>










<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="add_organism (scprint2.model.model.scPRINT2.add_organism)" href="#scprint2.model.model.scPRINT2.add_organism">add_organism</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Add a new organism to an existing model for transfer learning.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="configure_optimizers (scprint2.model.model.scPRINT2.configure_optimizers)" href="#scprint2.model.model.scPRINT2.configure_optimizers">configure_optimizers</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>@see pl.LightningModule</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint2.model.model.scPRINT2.forward)" href="#scprint2.model.model.scPRINT2.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Complete forward pass through the scPRINT-2</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="log_adata (scprint2.model.model.scPRINT2.log_adata)" href="#scprint2.model.model.scPRINT2.log_adata">log_adata</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>log_adata will log an adata from predictions.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="on_fit_start (scprint2.model.model.scPRINT2.on_fit_start)" href="#scprint2.model.model.scPRINT2.on_fit_start">on_fit_start</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>@see pl.LightningModule</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="on_load_checkpoint (scprint2.model.model.scPRINT2.on_load_checkpoint)" href="#scprint2.model.model.scPRINT2.on_load_checkpoint">on_load_checkpoint</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Handle checkpoint loading with backward compatibility.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="on_predict_epoch_end (scprint2.model.model.scPRINT2.on_predict_epoch_end)" href="#scprint2.model.model.scPRINT2.on_predict_epoch_end">on_predict_epoch_end</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>@see pl.LightningModule will</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="on_predict_epoch_start (scprint2.model.model.scPRINT2.on_predict_epoch_start)" href="#scprint2.model.model.scPRINT2.on_predict_epoch_start">on_predict_epoch_start</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>@see pl.LightningModule</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="on_test_start (scprint2.model.model.scPRINT2.on_test_start)" href="#scprint2.model.model.scPRINT2.on_test_start">on_test_start</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>@see pl.LightningModule</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="on_validation_epoch_end (scprint2.model.model.scPRINT2.on_validation_epoch_end)" href="#scprint2.model.model.scPRINT2.on_validation_epoch_end">on_validation_epoch_end</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>@see pl.LightningModule</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="optimizer_step (scprint2.model.model.scPRINT2.optimizer_step)" href="#scprint2.model.model.scPRINT2.optimizer_step">optimizer_step</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>@see pl.LightningModule</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="predict_step (scprint2.model.model.scPRINT2.predict_step)" href="#scprint2.model.model.scPRINT2.predict_step">predict_step</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>embed given gene expression, encode the gene embedding and cell embedding.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="training_step (scprint2.model.model.scPRINT2.training_step)" href="#scprint2.model.model.scPRINT2.training_step">training_step</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>training_step defines the train loop. It is independent of forward</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="validation_step (scprint2.model.model.scPRINT2.validation_step)" href="#scprint2.model.model.scPRINT2.validation_step">validation_step</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>validation_step defines the validation loop. It is independent of forward</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>




<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Attributes:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>genes</code></b>
                  (<code><span title="list">list</span>[<span title="str">str</span>]</code>)
              –
              <div class="doc-md-description">
                <p>Get flattened list of all genes in the model's vocabulary.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/model.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">genes</span><span class="p">,</span>
    <span class="n">organisms</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
    <span class="n">nhead</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">nlayers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">precpt_gene_emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">memmap_gene_emb</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">finetune_gene_emb</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">freeze_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">gene_pos_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">normalization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span>  <span class="c1"># log, sum, both, raw</span>
    <span class="n">attn_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">expr_encoder_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">attention</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;normal&quot;</span><span class="p">,</span>  <span class="c1"># &quot;performer&quot;, &quot;legacy-flash&quot;, &quot;normal&quot;, &quot;criss-cross&quot;, &quot;hyper&quot;, &quot;adasplash&quot;, &quot;softpick&quot;, &quot;softpick-flash&quot;</span>
    <span class="n">expr_emb_style</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;continuous&quot;</span><span class="p">,</span>  <span class="c1"># &quot;binned&quot;, &quot;continuous&quot;, &quot;metacell&quot;</span>
    <span class="n">n_input_bins</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">mvc_decoder</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
        <span class="nb">str</span>
    <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># &quot;inner product&quot;, &quot;concat query&quot;, &quot;sum query&quot;</span>
    <span class="n">pred_embedding</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">layers_cls</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
    <span class="n">classes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">labels_hierarchy</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">label_decoders</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">compress_class_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cell_specific_blocks</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">zinb</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">splicing_head</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">do_adv_cls</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_metacell_token</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0001</span><span class="p">,</span>
    <span class="n">nb_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sketcher_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span>
    <span class="n">feature_redraw_interval</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_heads_kv</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">d_model_cell</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">nhead_cell</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">nlayers_cell</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">num_heads_kv_cell</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">transformer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">drop_path_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="c1"># unused args from older versions kept for loading old models</span>
    <span class="n">gene_pos_enc</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">max_cont_len</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">residual_in_fp32</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">checkpointing</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">fused_dropout_add_ln</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">strict_loading</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">optim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">prenorm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">domain_spec_batchnorm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_flash_attn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cell_emb_style</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">num_batch_labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">fused_mlp</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">fused_bias_fc</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">attention_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    scPRINT-2: Single-Cell Pretrained Regulatory Inference Network Transformer.</span>

<span class="sd">    A foundation model for single-cell biology that learns cell and gene representations</span>
<span class="sd">    through self-supervised learning on large-scale single-cell RNA-seq data. The model</span>
<span class="sd">    can be used for:</span>
<span class="sd">    - Cell type classification and annotation</span>
<span class="sd">    - Gene expression denoising and imputation</span>
<span class="sd">    - Cell embedding generation for downstream analysis</span>
<span class="sd">    - Gene regulatory network inference via attention patterns</span>
<span class="sd">    - Multi-species gene expression modeling</span>

<span class="sd">    Architecture Overview:</span>
<span class="sd">        1. Gene Encoder: Embeds gene identities (optionally with pretrained embeddings)</span>
<span class="sd">        2. Expression Encoder: Encodes expression values (continuous, binned, or metacell)</span>
<span class="sd">        3. Position Encoder: Optional genomic position encoding</span>
<span class="sd">        4. Transformer: Main attention-based encoder (various attention mechanisms)</span>
<span class="sd">        5. Cell Transformer: Optional separate transformer for cell embeddings</span>
<span class="sd">        6. Decoders: Expression reconstruction, classification, and MVC decoders</span>

<span class="sd">    The model supports multiple training objectives:</span>
<span class="sd">        - Masked expression prediction (like BERT)</span>
<span class="sd">        - Denoising autoencoding</span>
<span class="sd">        - Cell embedding contrastive learning (ECS and CCE losses)</span>
<span class="sd">        - Multi-class cell type classification with hierarchical labels</span>
<span class="sd">        - Multi-view coding (MVC) for robust representations</span>

<span class="sd">    Args:</span>
<span class="sd">        genes (list | dict): Gene vocabulary. Either a list of gene names or a dict</span>
<span class="sd">            mapping organism names to lists of genes for multi-species models.</span>
<span class="sd">        organisms (list[str]): List of organism ontology term IDs the model supports.</span>
<span class="sd">        d_model (int, optional): Hidden dimension of the transformer. Defaults to 256.</span>
<span class="sd">        nhead (int, optional): Number of attention heads. Defaults to 4.</span>
<span class="sd">        nlayers (int, optional): Number of transformer layers. Defaults to 8.</span>
<span class="sd">        precpt_gene_emb (str, optional): Path to parquet file with pretrained gene</span>
<span class="sd">            embeddings. Index should match gene names. Defaults to None.</span>
<span class="sd">        memmap_gene_emb (bool, optional): Memory-map gene embeddings for large files.</span>
<span class="sd">            Defaults to False.</span>
<span class="sd">        finetune_gene_emb (bool, optional): Add trainable adapter layers on top of</span>
<span class="sd">            frozen pretrained embeddings. Defaults to False.</span>
<span class="sd">        freeze_embeddings (bool, optional): Freeze gene embeddings during training.</span>
<span class="sd">            Defaults to True.</span>
<span class="sd">        gene_pos_file (str, optional): Path to parquet file with genomic positions.</span>
<span class="sd">            Must have &#39;pos&#39; column with integer positions. Defaults to None.</span>
<span class="sd">        normalization (str, optional): Expression normalization method. One of:</span>
<span class="sd">            - &quot;sum&quot;: Divide by total counts (TPM-like)</span>
<span class="sd">            - &quot;log&quot;: Log2(1 + x) transform</span>
<span class="sd">            - &quot;both&quot;: Sum normalization then log transform</span>
<span class="sd">            - &quot;raw&quot;: No normalization</span>
<span class="sd">            Defaults to &quot;sum&quot;.</span>
<span class="sd">        attn_bias (str, optional): Path to sparse matrix (.npz) with attention biases</span>
<span class="sd">            (e.g., gene-gene regulatory priors). Defaults to None.</span>
<span class="sd">        expr_encoder_layers (int, optional): Number of layers in expression encoder MLP.</span>
<span class="sd">            Defaults to 3.</span>
<span class="sd">        attention (str, optional): Attention mechanism type. One of:</span>
<span class="sd">            - &quot;normal&quot;: Standard PyTorch attention</span>
<span class="sd">            - &quot;legacy-flash&quot;: Flash attention via simpler-flash</span>
<span class="sd">            - &quot;performer&quot;: Performer linear attention</span>
<span class="sd">            - &quot;hyper&quot;: Compressed hyperbolic attention</span>
<span class="sd">            - &quot;criss-cross&quot;: Criss-cross attention</span>
<span class="sd">            Defaults to &quot;normal&quot;.</span>
<span class="sd">        expr_emb_style (str, optional): Expression embedding approach. One of:</span>
<span class="sd">            - &quot;continuous&quot;: MLP on continuous expression values</span>
<span class="sd">            - &quot;binned&quot;: Learned embeddings for discretized expression bins</span>
<span class="sd">            - &quot;metacell&quot;: DeepSet encoder aggregating KNN neighbors</span>
<span class="sd">            Defaults to &quot;continuous&quot;.</span>
<span class="sd">        n_input_bins (int, optional): Number of expression bins when using binned</span>
<span class="sd">            embedding. Required if expr_emb_style=&quot;binned&quot;. Defaults to 0.</span>
<span class="sd">        mvc_decoder (str, optional): Multi-view coding decoder architecture. One of:</span>
<span class="sd">            - None: No MVC decoder</span>
<span class="sd">            - &quot;inner product&quot;: Dot product between cell and gene embeddings</span>
<span class="sd">            - &quot;concat query&quot;: Concatenate cell embedding with gene queries</span>
<span class="sd">            - &quot;sum query&quot;: Add cell embedding to gene queries</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        pred_embedding (list[str], optional): Class names to use for cell embeddings</span>
<span class="sd">            during prediction/logging. Defaults to None (use all).</span>
<span class="sd">        layers_cls (list[int], optional): Hidden layer sizes for classification heads.</span>
<span class="sd">            Defaults to [256, 128].</span>
<span class="sd">        classes (dict[str, int], optional): Classification targets mapping class names</span>
<span class="sd">            to number of categories. E.g., {&quot;cell_type_ontology_term_id&quot;: 100}.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        labels_hierarchy (dict[str, dict[int, list[int]]], optional): Hierarchical</span>
<span class="sd">            label structure for ontology-based classes. Maps parent indices to lists</span>
<span class="sd">            of children indices. Defaults to {}.</span>
<span class="sd">        label_decoders (dict[str, dict[int, str]], optional): Mapping from encoded</span>
<span class="sd">            integers back to label strings for each class. Used for logging/plotting.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        compress_class_dim (dict[str, int], optional): Compressed embedding dimension</span>
<span class="sd">            for each class. Uses VAE or FSQ compression. Defaults to None.</span>
<span class="sd">        cell_specific_blocks (bool, optional): Use separate transformer for cell</span>
<span class="sd">            embeddings with cross-attention to gene transformer. Defaults to False.</span>
<span class="sd">        zinb (bool, optional): Use Zero-Inflated Negative Binomial distribution for</span>
<span class="sd">            expression reconstruction. If False, uses MSE loss. Defaults to True.</span>
<span class="sd">        splicing_head (bool, optional): Add separate decoder for spliced/unspliced</span>
<span class="sd">            expression. Defaults to False.</span>
<span class="sd">        do_adv_cls (bool, optional): Use adversarial classification to remove batch</span>
<span class="sd">            effects from cell type embeddings. Defaults to False.</span>
<span class="sd">        dropout (float, optional): Dropout rate throughout the model. Defaults to 0.1.</span>
<span class="sd">        use_metacell_token (bool, optional): Add learnable metacell token to distinguish</span>
<span class="sd">            single cells from metacells. Defaults to False.</span>
<span class="sd">        lr (float, optional): Base learning rate. Defaults to 0.0001.</span>
<span class="sd">        nb_features (int, optional): Number of random features for Performer attention.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        sketcher_size (int, optional): Sketch size for sparse attention methods.</span>
<span class="sd">            Defaults to 200.</span>
<span class="sd">        feature_redraw_interval (int, optional): Steps between random feature redraws</span>
<span class="sd">            for Performer. Defaults to None.</span>
<span class="sd">        num_heads_kv (int, optional): Number of key-value heads (for MQA/GQA).</span>
<span class="sd">            Defaults to 4.</span>
<span class="sd">        d_model_cell (int, optional): Hidden dim for cell transformer when using</span>
<span class="sd">            cell_specific_blocks. Defaults to 128.</span>
<span class="sd">        nhead_cell (int, optional): Attention heads for cell transformer. Defaults to 4.</span>
<span class="sd">        nlayers_cell (int, optional): Layers in cell transformer. Defaults to 6.</span>
<span class="sd">        num_heads_kv_cell (int, optional): KV heads for cell transformer. Defaults to 4.</span>
<span class="sd">        drop_path_rate (float, optional): Stochastic depth rate. Defaults to 0.0.</span>
<span class="sd">        **attention_kwargs (dict): Additional arguments passed to FlashTransformer.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        Training Configuration (set these before training):</span>
<span class="sd">            noise (list[float]): Dropout rates for denoising task. E.g., [0.6].</span>
<span class="sd">            mask_ratio (list[float]): Mask ratios for masked prediction. E.g., [0.15].</span>
<span class="sd">            cce_temp (float): Temperature for contrastive loss.</span>
<span class="sd">            cce_scale (float): Weight for contrastive cell embedding loss.</span>
<span class="sd">            ecs_scale (float): Weight for elastic cell similarity loss.</span>
<span class="sd">            ecs_threshold (float): Similarity threshold for ECS loss.</span>
<span class="sd">            mvc_scale (float): Weight for MVC reconstruction loss.</span>
<span class="sd">            class_scale (float): Weight for classification loss.</span>
<span class="sd">            lr_reduce_patience (int): Epochs before reducing learning rate.</span>
<span class="sd">            lr_reduce_factor (float): Factor to reduce learning rate by.</span>
<span class="sd">            warmup_duration (int): Steps for learning rate warmup.</span>

<span class="sd">        Prediction Configuration (set before predict):</span>
<span class="sd">            predict_mode (str): &quot;none&quot; or &quot;generate&quot; for expression generation.</span>
<span class="sd">            pred_embedding (list[str]): Classes to include in cell embeddings.</span>
<span class="sd">            get_attention_layer (list[int]): Layers to extract attention from.</span>
<span class="sd">            predict_depth_mult (float): Multiplier for depth in generation.</span>
<span class="sd">            pred_log_adata (bool): Whether to log predictions as AnnData.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; # Initialize model</span>
<span class="sd">        &gt;&gt;&gt; model = scPrint2(</span>
<span class="sd">        ...     genes=gene_list,</span>
<span class="sd">        ...     organisms=[&quot;NCBITaxon:9606&quot;],</span>
<span class="sd">        ...     d_model=512,</span>
<span class="sd">        ...     nlayers=12,</span>
<span class="sd">        ...     classes={&quot;cell_type_ontology_term_id&quot;: 100},</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Configure training</span>
<span class="sd">        &gt;&gt;&gt; model.noise = [0.4, 0.6]</span>
<span class="sd">        &gt;&gt;&gt; model.mask_ratio = [0.15, 0.3]</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Train with PyTorch Lightning</span>
<span class="sd">        &gt;&gt;&gt; trainer = L.Trainer(max_epochs=100)</span>
<span class="sd">        &gt;&gt;&gt; trainer.fit(model, datamodule)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Generate embeddings</span>
<span class="sd">        &gt;&gt;&gt; model.pred_embedding = [&quot;cell_type_ontology_term_id&quot;]</span>
<span class="sd">        &gt;&gt;&gt; predictions = trainer.predict(model, datamodule)</span>

<span class="sd">    Note:</span>
<span class="sd">        The model is designed to work with scDataLoader&#39;s DataModule and Collator.</span>
<span class="sd">        Gene order must match between model initialization and data loading.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
    <span class="c1"># training flags</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.6</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cce_temp</span> <span class="o">=</span> <span class="mf">0.3</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cce_scale</span> <span class="o">=</span> <span class="mf">0.2</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ecs_threshold</span> <span class="o">=</span> <span class="mf">0.4</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ecs_scale</span> <span class="o">=</span> <span class="mf">0.2</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mvc_scale</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">class_embd_diss_scale</span> <span class="o">=</span> <span class="mf">0.3</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">adv_class_scale</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_adv_cls</span> <span class="o">=</span> <span class="n">do_adv_cls</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">run_full_forward</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">class_scale</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">zinb_and_mse</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_next_tp</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_generate</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">var_context_length</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mask_ratio</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">warmup_duration</span> <span class="o">=</span> <span class="mi">500</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">0.01</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optim</span> <span class="o">=</span> <span class="s2">&quot;adamW&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fused_adam</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_patience</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_factor</span> <span class="o">=</span> <span class="mf">0.6</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">test_every</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">randsamp</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_monitor</span> <span class="o">=</span> <span class="s2">&quot;val_loss&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_step</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">doplot</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">get_attention_layer</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embs</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred_log_adata</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">predict_depth_mult</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">predict_mode</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">keep_all_labels_pred</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mask_zeros</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vae_kl_scale</span> <span class="o">=</span> <span class="mf">0.05</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vae_kl_warmup_steps</span> <span class="o">=</span> <span class="mi">40_000</span>  <span class="c1"># Default value, can be adjusted</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">save_expr</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># should be stored somehow</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">normalization</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_bias</span> <span class="o">=</span> <span class="n">attn_bias</span> <span class="k">if</span> <span class="n">attn_bias</span> <span class="o">!=</span> <span class="s2">&quot;none&quot;</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">organisms</span> <span class="o">=</span> <span class="n">organisms</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nlayers</span> <span class="o">=</span> <span class="n">nlayers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_metacell_token</span> <span class="o">=</span> <span class="n">use_metacell_token</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mvc_decoder</span> <span class="o">=</span> <span class="n">mvc_decoder</span>
    <span class="c1"># need to store</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_input_bins</span> <span class="o">=</span> <span class="n">n_input_bins</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">attention</span>

    <span class="k">if</span> <span class="n">classes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">label_counts</span> <span class="o">=</span> <span class="n">classes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">classes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">classes</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">label_decoders</span> <span class="o">=</span> <span class="n">label_decoders</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred_embedding</span> <span class="o">=</span> <span class="n">pred_embedding</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_genes</span> <span class="o">=</span> <span class="n">genes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">expr_emb_style</span> <span class="o">=</span> <span class="n">expr_emb_style</span>
    <span class="k">if</span> <span class="n">labels_hierarchy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">labels_hierarchy</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">labels_hierarchy</span> <span class="o">=</span> <span class="n">labels_hierarchy</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="p">[</span><span class="s2">&quot;classes&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">classes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="p">[</span><span class="s2">&quot;label_decoders&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">label_decoders</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="p">[</span><span class="s2">&quot;organisms&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">organisms</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="p">[</span><span class="s2">&quot;use_metacell_token&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">use_metacell_token</span>
    <span class="c1"># 20x more likely to drop a non TF compared to a TF</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tf_masker</span> <span class="o">=</span> <span class="n">WeightedMasker</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">,</span> <span class="n">tf_weight</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">Attention</span><span class="p">(</span>
        <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">),</span>
        <span class="n">additional_tokens</span><span class="o">=</span><span class="p">(</span>
            <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_metacell_token</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
            <span class="o">+</span> <span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">cell_specific_blocks</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">),</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mat_labels_hierarchy</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">labels_hierarchy</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">tens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">classes</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
        <span class="k">for</span> <span class="n">k2</span><span class="p">,</span> <span class="n">v2</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">tens</span><span class="p">[</span><span class="n">k2</span> <span class="o">-</span> <span class="n">classes</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">v2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mat_labels_hierarchy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">tens</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>

    <span class="c1"># encoder</span>
    <span class="c1"># gene encoder</span>
    <span class="k">if</span> <span class="n">gene_pos_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">gene_pos_enc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">gene_pos_file</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">gene_pos_enc</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warning: only a subset of the genes available in the loc file.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_genes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">tokeep</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">gene_pos_enc</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_genes</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">u</span> <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">v</span> <span class="k">if</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">tokeep</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_genes</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mi">100</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;the gene pos file </span><span class="si">{</span><span class="n">gene_pos_file</span><span class="si">}</span><span class="s2"> does not match most of the genes given to the model for species </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
        <span class="n">gene_pos_enc</span> <span class="o">=</span> <span class="n">gene_pos_enc</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;pos&quot;</span><span class="p">]]</span>

    <span class="k">if</span> <span class="n">precpt_gene_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">precpt_gene_emb</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="s2">&quot;Warning: only a subset of the genes available in the embeddings file.&quot;</span>
            <span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_genes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">tokeep</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_genes</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">u</span> <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">v</span> <span class="k">if</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">tokeep</span><span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_genes</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mi">100</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;the gene embeddings file </span><span class="si">{</span><span class="n">precpt_gene_emb</span><span class="si">}</span><span class="s2"> does not match most of the genes given to the model for species </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;number of genes: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">memmap_gene_emb</span><span class="p">:</span>
            <span class="n">sembeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="n">d_model</span><span class="p">)(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">gene_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">GeneEncoder</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">),</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="n">weights_file</span><span class="o">=</span><span class="n">precpt_gene_emb</span> <span class="k">if</span> <span class="n">memmap_gene_emb</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">weights</span><span class="o">=</span><span class="n">sembeddings</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">memmap_gene_emb</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">freeze</span><span class="o">=</span><span class="n">freeze_embeddings</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">gene_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">GeneEncoder</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">),</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">freeze</span><span class="o">=</span><span class="n">freeze_embeddings</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">finetune_gene_emb</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">freeze_embeddings</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;finetune_gene_emb is True but freeze_embeddings is False&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Create adapter layers after the frozen base encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene_encoder</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">gene_encoder</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene_encoder</span> <span class="o">=</span> <span class="n">gene_encoder</span>
    <span class="c1"># Positional Encoding</span>
    <span class="k">if</span> <span class="n">gene_pos_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># redoing it just in case some were dropped with embbeding file step</span>
        <span class="n">gene_pos_enc</span> <span class="o">=</span> <span class="n">gene_pos_enc</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">,</span> <span class="s2">&quot;pos&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">PositionalEncoding</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="n">gene_pos_enc</span><span class="o">=</span><span class="n">gene_pos_enc</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Value Encoder, NOTE: the scaling style is also handled in _encode method</span>
    <span class="n">expr_d_model</span> <span class="o">=</span> <span class="n">d_model</span>  <span class="c1"># // 8 if finetune_gene_emb else d_model</span>
    <span class="k">if</span> <span class="n">expr_emb_style</span> <span class="ow">in</span> <span class="s2">&quot;continuous&quot;</span><span class="p">:</span>
        <span class="n">expr_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">ContinuousValueEncoder</span><span class="p">(</span>
            <span class="n">expr_d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">expr_encoder_layers</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">expr_emb_style</span> <span class="o">==</span> <span class="s2">&quot;binned&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">n_input_bins</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">assert</span> <span class="n">normalization</span> <span class="o">==</span> <span class="s2">&quot;raw&quot;</span><span class="p">,</span> <span class="s2">&quot;shouldn&#39;t use normalization&quot;</span>
        <span class="n">expr_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">CategoryValueEncoder</span><span class="p">(</span><span class="n">n_input_bins</span><span class="p">,</span> <span class="n">expr_d_model</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">expr_emb_style</span> <span class="o">==</span> <span class="s2">&quot;metacell&quot;</span><span class="p">:</span>
        <span class="n">expr_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">EasyExprGNN</span><span class="p">(</span>
            <span class="n">self_dim</span><span class="o">=</span><span class="n">expr_d_model</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span>
            <span class="n">output_dim</span><span class="o">=</span><span class="n">expr_d_model</span><span class="p">,</span>
            <span class="n">shared_layers</span><span class="o">=</span><span class="n">expr_encoder_layers</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;expr_emb_style should be one of binned, continuous, metacell, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;got </span><span class="si">{</span><span class="n">expr_emb_style</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">finetune_gene_emb</span> <span class="ow">and</span> <span class="kc">False</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">ExprBasedFT</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="n">gene_encoder</span><span class="p">,</span>
            <span class="n">expr_encoder</span><span class="p">,</span>
            <span class="n">dropout</span><span class="p">,</span>
            <span class="n">layers</span><span class="o">=</span><span class="n">expr_encoder_layers</span><span class="p">,</span>
            <span class="n">intermediary_d</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">d_model</span> <span class="o">*</span> <span class="mf">1.5</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span> <span class="o">=</span> <span class="n">expr_encoder</span>

    <span class="c1"># Class Encoder</span>
    <span class="c1"># always have [base_cell_emb, time_embedding, depth_embedding] + any other class info</span>
    <span class="c1"># base cell embedding will store other cell specific information</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">class_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">CategoryValueEncoder</span><span class="p">(</span>
        <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">d_model</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">cell_specific_blocks</span> <span class="k">else</span> <span class="n">d_model_cell</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_metacell_token</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metacell_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">CategoryValueEncoder</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="c1"># compute tensor for mat_labels_hierarchy</span>
    <span class="c1"># old parameters that can still be passed when loading older models (managed in the _on_load_ckpt function)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="s2">&quot;strict_loading&quot;</span><span class="p">,</span>
        <span class="s2">&quot;optim&quot;</span><span class="p">,</span>
        <span class="s2">&quot;weight_decay&quot;</span><span class="p">,</span>
        <span class="s2">&quot;d_hid&quot;</span><span class="p">,</span>
        <span class="s2">&quot;edge_dim&quot;</span><span class="p">,</span>
        <span class="s2">&quot;prenorm&quot;</span><span class="p">,</span>
        <span class="s2">&quot;domain_spec_batchnorm&quot;</span><span class="p">,</span>
        <span class="s2">&quot;use_flash_attn&quot;</span><span class="p">,</span>
        <span class="s2">&quot;cell_emb_style&quot;</span><span class="p">,</span>
        <span class="s2">&quot;num_batch_labels&quot;</span><span class="p">,</span>
        <span class="s2">&quot;transformer&quot;</span><span class="p">,</span>
        <span class="s2">&quot;residual_in_fp32&quot;</span><span class="p">,</span>
        <span class="s2">&quot;max_cont_len&quot;</span><span class="p">,</span>
    <span class="p">]:</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">attention_kwargs</span><span class="p">:</span>
            <span class="n">attention_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="c1"># attention</span>
    <span class="c1"># Linear</span>
    <span class="k">if</span> <span class="n">attention</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
        <span class="c1"># linear attention using the fast attention package</span>
        <span class="c1"># self.attention = FastattentionEncoder(</span>
        <span class="c1">#    d_model, nhead, d_hid, nlayers, dropout, &quot;linear&quot;</span>
        <span class="c1"># )</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Linear attention is not implemented&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">attention</span> <span class="o">==</span> <span class="s2">&quot;performer&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">Performer</span><span class="p">(</span>
            <span class="n">dim</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
            <span class="n">depth</span><span class="o">=</span><span class="n">nlayers</span><span class="p">,</span>
            <span class="n">heads</span><span class="o">=</span><span class="n">nhead</span><span class="p">,</span>
            <span class="n">dim_head</span><span class="o">=</span><span class="n">d_model</span> <span class="o">//</span> <span class="n">nhead</span><span class="p">,</span>
            <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">attn_dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">ff_dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">nb_features</span><span class="o">=</span><span class="n">nb_features</span><span class="p">,</span>
            <span class="n">feature_redraw_interval</span><span class="o">=</span><span class="n">feature_redraw_interval</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">FlashTransformer</span><span class="p">(</span>
            <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
            <span class="n">nhead</span><span class="o">=</span><span class="n">nhead</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">attn_dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">nlayers</span><span class="o">=</span><span class="n">nlayers</span><span class="p">,</span>
            <span class="n">cross_attn</span><span class="o">=</span><span class="n">cell_specific_blocks</span><span class="p">,</span>
            <span class="n">cross_dim</span><span class="o">=</span><span class="n">d_model_cell</span><span class="p">,</span>
            <span class="n">attn_type</span><span class="o">=</span><span class="s2">&quot;flash&quot;</span> <span class="k">if</span> <span class="n">attention</span> <span class="o">==</span> <span class="s2">&quot;legacy-flash&quot;</span> <span class="k">else</span> <span class="n">attention</span><span class="p">,</span>
            <span class="n">num_heads_kv</span><span class="o">=</span><span class="n">num_heads_kv</span><span class="p">,</span>
            <span class="n">sketcher_size</span><span class="o">=</span><span class="n">sketcher_size</span><span class="p">,</span>
            <span class="n">drop_path_rate</span><span class="o">=</span><span class="n">drop_path_rate</span><span class="p">,</span>
            <span class="o">**</span><span class="n">attention_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">cell_specific_blocks</span><span class="p">:</span>
        <span class="n">attention_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;num_heads_kv&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_transformer</span> <span class="o">=</span> <span class="n">FlashTransformer</span><span class="p">(</span>
            <span class="n">d_model</span><span class="o">=</span><span class="n">d_model_cell</span><span class="p">,</span>
            <span class="n">nhead</span><span class="o">=</span><span class="n">nhead_cell</span><span class="p">,</span>
            <span class="n">num_heads_kv</span><span class="o">=</span><span class="n">num_heads_kv_cell</span><span class="p">,</span>
            <span class="n">nlayers</span><span class="o">=</span><span class="n">nlayers_cell</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">cross_attn</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">cross_dim</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
            <span class="n">attn_type</span><span class="o">=</span><span class="s2">&quot;flash&quot;</span> <span class="k">if</span> <span class="n">attention</span> <span class="o">==</span> <span class="s2">&quot;legacy-flash&quot;</span> <span class="k">else</span> <span class="s2">&quot;normal&quot;</span><span class="p">,</span>
            <span class="o">**</span><span class="n">attention_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_transformer</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># decoders</span>
    <span class="c1"># expression</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">splicing_head</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">expr_emb_style</span> <span class="o">==</span> <span class="s2">&quot;binned&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expr_decoder</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">ClsDecoder</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="n">n_input_bins</span><span class="p">,</span>
            <span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="n">d_model</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">//</span> <span class="mi">4</span><span class="p">],</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expr_decoder</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">ExprDecoder</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">zinb</span><span class="o">=</span><span class="n">zinb</span><span class="p">,</span>
            <span class="n">use_depth</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">splicing_head</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">splicing_head</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">ExprDecoder</span><span class="p">(</span>
                <span class="n">d_model</span><span class="p">,</span>
                <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
                <span class="n">zinb</span><span class="o">=</span><span class="n">zinb</span><span class="p">,</span>
                <span class="n">use_depth</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="c1"># cls decoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cls_decoders</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
    <span class="c1"># should be a very simple classifier for most things</span>
    <span class="c1"># (maybe scale with the number of classes) should be 1 layer...</span>
    <span class="k">for</span> <span class="n">clss</span><span class="p">,</span> <span class="n">n_cls</span> <span class="ow">in</span> <span class="n">classes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">mdim</span> <span class="o">=</span> <span class="n">d_model_cell</span> <span class="k">if</span> <span class="n">cell_specific_blocks</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">compress_class_dim</span><span class="p">[</span><span class="n">clss</span><span class="p">]</span> <span class="k">if</span> <span class="n">compress_class_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">mdim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cls_decoders</span><span class="p">[</span><span class="n">clss</span><span class="p">]</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">ClsDecoder</span><span class="p">(</span>
            <span class="n">dim</span><span class="p">,</span>
            <span class="n">n_cls</span><span class="p">,</span>
            <span class="n">layers</span><span class="o">=</span><span class="n">layers_cls</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="s2">&quot;cell_type_ontology_term_id&quot;</span> <span class="ow">in</span> <span class="n">classes</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">do_adv_cls</span><span class="p">:</span>
        <span class="n">mdim</span> <span class="o">=</span> <span class="n">d_model_cell</span> <span class="k">if</span> <span class="n">cell_specific_blocks</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">compress_class_dim</span><span class="p">[</span><span class="s2">&quot;cell_type_ontology_term_id&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">compress_class_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">mdim</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;assay_ontology_term_id&quot;</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">assay_relab</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">relabel_assay_for_adv</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">label_decoders</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels_hierarchy</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">adv_assay_decoder</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">ClsDecoder</span><span class="p">(</span>
                <span class="n">dim</span><span class="p">,</span>
                <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">assay_relab</span><span class="o">.</span><span class="n">values</span><span class="p">())),</span>
                <span class="n">layers</span><span class="o">=</span><span class="n">layers_cls</span><span class="p">,</span>
                <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">organisms</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">adv_organism_decoder</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">ClsDecoder</span><span class="p">(</span>
                <span class="n">dim</span><span class="p">,</span>
                <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">organisms</span><span class="p">),</span>
                <span class="n">layers</span><span class="o">=</span><span class="n">layers_cls</span><span class="p">,</span>
                <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="c1"># expression decoder from batch embbedding</span>
    <span class="k">if</span> <span class="n">mvc_decoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">cell_specific_blocks</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;MVC decoder is not supported for cell specific blocks&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mvc_decoder</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">MVCDecoder</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="n">arch_style</span><span class="o">=</span><span class="n">mvc_decoder</span><span class="p">,</span> <span class="n">zinb</span><span class="o">=</span><span class="n">zinb</span><span class="p">,</span> <span class="n">use_depth</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mvc_decoder</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">compress_class_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compressor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">d_model_cell</span> <span class="k">if</span> <span class="n">cell_specific_blocks</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">compress_class_dim</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">v</span> <span class="o">&gt;=</span> <span class="mi">8</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">compressor</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">VAEDecoder</span><span class="p">(</span>
                    <span class="n">dim</span><span class="p">,</span>
                    <span class="n">layers</span><span class="o">=</span><span class="p">[</span>
                        <span class="mi">128</span><span class="p">,</span>
                        <span class="n">v</span><span class="p">,</span>
                    <span class="p">],</span>
                    <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
                    <span class="n">return_latent</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">compressor</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">fsq</span><span class="o">.</span><span class="n">FSQ</span><span class="p">(</span><span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compressor</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span>
            <span class="n">utils</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">,</span>
            <span class="n">n_layer</span><span class="o">=</span><span class="n">nlayers</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dec</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_decoders</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">dec</span><span class="o">.</span><span class="n">out_layer</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.13</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">







<div class="doc doc-object doc-attribute">



<h4 id="scprint2.model.model.scPRINT2.genes" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">genes</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-property"><code>property</code></small>
  </span>

</h4>


    <div class="doc doc-contents ">

        <p>Get flattened list of all genes in the model's vocabulary.</p>
<p>For multi-organism models, concatenates genes from all organisms
in consistent order.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="list">list</span>[<span title="str">str</span>]</code>
              –
              <div class="doc-md-description">
                <p>list[str]: Gene names in model vocabulary order.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
    </div>

</div>




<div class="doc doc-object doc-function">


<h4 id="scprint2.model.model.scPRINT2.add_organism" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">add_organism</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Add a new organism to an existing model for transfer learning.</p>
<p>Extends the gene vocabulary and embeddings to include genes from a new
organism. Useful for applying a pretrained model to a new species.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>organism</code></b>
                  (<code><span title="str">str</span></code>)
              –
              <div class="doc-md-description">
                <p>Organism ontology term ID (e.g., "NCBITaxon:10090" for mouse).</p>
              </div>
            </li>
            <li>
              <b><code>genes</code></b>
                  (<code><span title="pandas.Index">Index</span></code>)
              –
              <div class="doc-md-description">
                <p>Gene names/IDs for the new organism.</p>
              </div>
            </li>
            <li>
              <b><code>emb</code></b>
                  (<code><span title="pandas.DataFrame">DataFrame</span></code>)
              –
              <div class="doc-md-description">
                <p>Gene embeddings DataFrame with genes as index.
Will be resized to match model's d_model.</p>
              </div>
            </li>
            <li>
              <b><code>locs</code></b>
                  (<code><span title="pandas.DataFrame">DataFrame</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Genomic positions with 'pos' column.
Required if model uses positional encoding. Defaults to None.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="ValueError">ValueError</span></code>
              –
              <div class="doc-md-description">
                <p>If model requires gene locations but none provided.</p>
              </div>
            </li>
            <li>
                  <code><span title="ValueError">ValueError</span></code>
              –
              <div class="doc-md-description">
                <p>If gene positions exceed model's maximum position encoding.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="note" open>
  <summary>Note</summary>
  <p>Only genes present in both <code>genes</code> and <code>emb</code> (and <code>locs</code> if provided)
will be added. The model's gene encoder is expanded in-place.</p>
</details>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">add_organism</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">organism</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">genes</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">Index</span><span class="p">,</span> <span class="n">emb</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">locs</span><span class="o">=</span><span class="kc">None</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add a new organism to an existing model for transfer learning.</span>

<span class="sd">    Extends the gene vocabulary and embeddings to include genes from a new</span>
<span class="sd">    organism. Useful for applying a pretrained model to a new species.</span>

<span class="sd">    Args:</span>
<span class="sd">        organism (str): Organism ontology term ID (e.g., &quot;NCBITaxon:10090&quot; for mouse).</span>
<span class="sd">        genes (pd.Index): Gene names/IDs for the new organism.</span>
<span class="sd">        emb (pd.DataFrame): Gene embeddings DataFrame with genes as index.</span>
<span class="sd">            Will be resized to match model&#39;s d_model.</span>
<span class="sd">        locs (pd.DataFrame, optional): Genomic positions with &#39;pos&#39; column.</span>
<span class="sd">            Required if model uses positional encoding. Defaults to None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If model requires gene locations but none provided.</span>
<span class="sd">        ValueError: If gene positions exceed model&#39;s maximum position encoding.</span>

<span class="sd">    Note:</span>
<span class="sd">        Only genes present in both `genes` and `emb` (and `locs` if provided)</span>
<span class="sd">        will be added. The model&#39;s gene encoder is expanded in-place.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">locs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;this model needs gene locations to add a new organism&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">organisms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">organism</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">locs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">overlap</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">locs</span><span class="o">.</span><span class="n">index</span><span class="p">)</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">index</span><span class="p">)</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">genes</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
        <span class="n">genes</span> <span class="o">=</span> <span class="n">genes</span><span class="p">[</span><span class="n">genes</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">overlap</span><span class="p">)]</span>
        <span class="n">locs</span> <span class="o">=</span> <span class="n">locs</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">genes</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">locs</span><span class="p">[</span><span class="s2">&quot;pos&quot;</span><span class="p">]</span>
        <span class="n">token_to_pos</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">pos</span> <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pos</span><span class="p">)}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="nb">max</span><span class="p">(</span><span class="n">pos</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;the number of gene locs in the added organism needs to be less than </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">token_to_pos</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">pos</span> <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pos</span><span class="p">)}</span>
        <span class="n">arr</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">token_to_pos</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">arr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span><span class="o">.</span><span class="n">pe</span><span class="p">[</span><span class="n">v</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">arr</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span><span class="o">.</span><span class="n">pe</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span><span class="o">.</span><span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span><span class="o">.</span><span class="n">pe</span><span class="p">,</span> <span class="n">pe</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">overlap</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">index</span><span class="p">)</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">genes</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
        <span class="n">genes</span> <span class="o">=</span> <span class="n">genes</span><span class="p">[</span><span class="n">genes</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">overlap</span><span class="p">)]</span>

    <span class="n">emb</span> <span class="o">=</span> <span class="n">emb</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">genes</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_genes</span><span class="p">[</span><span class="n">organism</span><span class="p">]</span> <span class="o">=</span> <span class="n">genes</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gene_encoder</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">genc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span><span class="o">.</span><span class="n">gene_encoder</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">genc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gene_encoder</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">genc</span><span class="p">)</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
        <span class="n">enc</span> <span class="o">=</span> <span class="n">genc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">enc</span> <span class="o">=</span> <span class="n">genc</span>
    <span class="n">semb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">enc</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">enc</span><span class="o">.</span><span class="n">memmap</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;todev.. will fail for now&quot;</span><span class="p">)</span>

    <span class="n">embs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">enc</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">semb</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">enc</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
        <span class="n">embs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">embs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">padding_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">_freeze</span><span class="o">=</span><span class="n">enc</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">enc</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">embs</span><span class="p">)</span>
    <span class="n">enc</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">enc</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">genc</span><span class="p">)</span> <span class="ow">is</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
        <span class="n">genc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">enc</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">genc</span> <span class="o">=</span> <span class="n">enc</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gene_encoder</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span><span class="o">.</span><span class="n">gene_encoder</span> <span class="o">=</span> <span class="n">genc</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene_encoder</span> <span class="o">=</span> <span class="n">genc</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.model.scPRINT2.configure_optimizers" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">configure_optimizers</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>@see pl.LightningModule</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1503</span>
<span class="normal">1504</span>
<span class="normal">1505</span>
<span class="normal">1506</span>
<span class="normal">1507</span>
<span class="normal">1508</span>
<span class="normal">1509</span>
<span class="normal">1510</span>
<span class="normal">1511</span>
<span class="normal">1512</span>
<span class="normal">1513</span>
<span class="normal">1514</span>
<span class="normal">1515</span>
<span class="normal">1516</span>
<span class="normal">1517</span>
<span class="normal">1518</span>
<span class="normal">1519</span>
<span class="normal">1520</span>
<span class="normal">1521</span>
<span class="normal">1522</span>
<span class="normal">1523</span>
<span class="normal">1524</span>
<span class="normal">1525</span>
<span class="normal">1526</span>
<span class="normal">1527</span>
<span class="normal">1528</span>
<span class="normal">1529</span>
<span class="normal">1530</span>
<span class="normal">1531</span>
<span class="normal">1532</span>
<span class="normal">1533</span>
<span class="normal">1534</span>
<span class="normal">1535</span>
<span class="normal">1536</span>
<span class="normal">1537</span>
<span class="normal">1538</span>
<span class="normal">1539</span>
<span class="normal">1540</span>
<span class="normal">1541</span>
<span class="normal">1542</span>
<span class="normal">1543</span>
<span class="normal">1544</span>
<span class="normal">1545</span>
<span class="normal">1546</span>
<span class="normal">1547</span>
<span class="normal">1548</span>
<span class="normal">1549</span>
<span class="normal">1550</span>
<span class="normal">1551</span>
<span class="normal">1552</span>
<span class="normal">1553</span>
<span class="normal">1554</span>
<span class="normal">1555</span>
<span class="normal">1556</span>
<span class="normal">1557</span>
<span class="normal">1558</span>
<span class="normal">1559</span>
<span class="normal">1560</span>
<span class="normal">1561</span>
<span class="normal">1562</span>
<span class="normal">1563</span>
<span class="normal">1564</span>
<span class="normal">1565</span>
<span class="normal">1566</span>
<span class="normal">1567</span>
<span class="normal">1568</span>
<span class="normal">1569</span>
<span class="normal">1570</span>
<span class="normal">1571</span>
<span class="normal">1572</span>
<span class="normal">1573</span>
<span class="normal">1574</span>
<span class="normal">1575</span>
<span class="normal">1576</span>
<span class="normal">1577</span>
<span class="normal">1578</span>
<span class="normal">1579</span>
<span class="normal">1580</span>
<span class="normal">1581</span>
<span class="normal">1582</span>
<span class="normal">1583</span>
<span class="normal">1584</span>
<span class="normal">1585</span>
<span class="normal">1586</span>
<span class="normal">1587</span>
<span class="normal">1588</span>
<span class="normal">1589</span>
<span class="normal">1590</span>
<span class="normal">1591</span>
<span class="normal">1592</span>
<span class="normal">1593</span>
<span class="normal">1594</span>
<span class="normal">1595</span>
<span class="normal">1596</span>
<span class="normal">1597</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="c1"># https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam</span>
    <span class="c1"># not working because of poor weight decay implem</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim</span> <span class="o">==</span> <span class="s2">&quot;adam&quot;</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
            <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span>
            <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
            <span class="n">eps</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span>  <span class="c1"># 1e-5 to 1e-8</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
            <span class="n">amsgrad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">fused</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fused_adam</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim</span> <span class="o">==</span> <span class="s2">&quot;adamW&quot;</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
            <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span>
            <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
            <span class="n">eps</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span>  <span class="c1"># 1e-5 to 1e-8</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
            <span class="n">amsgrad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">fused</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fused_adam</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim</span> <span class="o">==</span> <span class="s2">&quot;galore&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Galore optimizer not implemented&quot;</span><span class="p">)</span>
        <span class="c1"># param_groups = [</span>
        <span class="c1">#    {</span>
        <span class="c1">#        &quot;params&quot;: [</span>
        <span class="c1">#            v for k, v in self.named_parameters() if &quot;transformer&quot; not in k</span>
        <span class="c1">#        ]</span>
        <span class="c1">#    },</span>
        <span class="c1">#    {</span>
        <span class="c1">#        &quot;params&quot;: [</span>
        <span class="c1">#            v for k, v in self.named_parameters() if &quot;transformer&quot; in k</span>
        <span class="c1">#        ],</span>
        <span class="c1">#        &quot;rank&quot;: 128,</span>
        <span class="c1">#        &quot;update_proj_gap&quot;: 200,</span>
        <span class="c1">#        &quot;scale&quot;: 0.25,</span>
        <span class="c1">#        &quot;proj_type&quot;: &quot;std&quot;,</span>
        <span class="c1">#    },</span>
        <span class="c1"># ]</span>
        <span class="c1"># optimizer = GaLoreAdamW(param_groups, lr=self.hparams.lr)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown optimizer: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_monitor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;no lr reduce factor&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">optimizer</span><span class="p">]</span>
    <span class="c1"># lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(</span>
    <span class="c1">#    optimizer,</span>
    <span class="c1">#    T_0=20000,</span>
    <span class="c1">#    T_mult=2,</span>
    <span class="c1">#    eta_min=1e-8,</span>
    <span class="c1"># )</span>
    <span class="c1"># interval = &quot;step&quot;</span>
    <span class="c1"># frequency = 10</span>
    <span class="c1"># lr_scheduler = optim.lr_scheduler.ExponentialLR(</span>
    <span class="c1">#    optimizer,</span>
    <span class="c1">#    gamma=0.85,</span>
    <span class="c1"># )</span>
    <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
        <span class="n">patience</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_patience</span><span class="p">,</span>
        <span class="n">factor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_factor</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">interval</span> <span class="o">=</span> <span class="s2">&quot;epoch&quot;</span>
    <span class="n">frequency</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="c1"># lr_scheduler = StepwiseCAWRWithWD(</span>
    <span class="c1">#     optimizer,</span>
    <span class="c1">#     T_0=20_000,</span>
    <span class="c1">#     T_mult=2,</span>
    <span class="c1">#     eta_min=1e-8,</span>
    <span class="c1">#     wd_decay=0.9</span>
    <span class="c1"># )</span>
    <span class="n">lr_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
        <span class="c1"># The unit of the scheduler&#39;s step size, could also be &#39;step&#39;.</span>
        <span class="c1"># &#39;epoch&#39; updates the scheduler on epoch end whereas &#39;step&#39;</span>
        <span class="c1"># updates it after a optimizer update.</span>
        <span class="s2">&quot;interval&quot;</span><span class="p">:</span> <span class="n">interval</span><span class="p">,</span>
        <span class="c1"># How many epochs/steps should pass between calls to</span>
        <span class="c1"># `scheduler.step()`. 1 corresponds to updating the learning</span>
        <span class="c1"># rate after every epoch/step.</span>
        <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="n">frequency</span><span class="p">,</span>
        <span class="c1"># Metric to to monitor for schedulers like `ReduceLROnPlateau`</span>
        <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_monitor</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">callbacks</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="n">_LRCallback</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">=</span> <span class="n">val</span><span class="o">.</span><span class="n">num_training</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="n">LearningRateFinder</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">=</span> <span class="n">val</span><span class="o">.</span><span class="n">_num_training_steps</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">optimizer</span><span class="p">],</span> <span class="p">[</span><span class="n">lr_dict</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.model.scPRINT2.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Complete forward pass through the scPRINT-2</p>
<p>Encodes input expression data, processes through transformer(s), and
decodes into expression predictions and cell classifications.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>gene_pos</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Gene indices of shape (batch, seq_len) mapping to
positions in the model's gene vocabulary.</p>
              </div>
            </li>
            <li>
              <b><code>expression</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Expression values of shape (batch, seq_len).
Can be raw counts or normalized depending on model config.
Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>neighbors</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>KNN neighbor expressions of shape
(batch, n_neighbors, seq_len) for metacell-style encoding.
Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>neighbors_info</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Neighbor weights of shape
(batch, n_neighbors). Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>mask</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Boolean mask of shape (batch, seq_len) where
True indicates positions to mask (set to zero). Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>req_depth</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Target sequencing depth of shape (batch,)
for depth-conditional generation. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>get_gene_emb</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Return gene embeddings from transformer.
Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>metacell_token</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Binary metacell indicators of shape
(batch,). Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>depth_mult</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Expression depth multiplier. If None,
uses sum of expression values. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>do_sample</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Sample from predicted distribution.
Currently unused. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>do_mvc</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Compute multi-view coding predictions.
Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>do_class</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Compute classification predictions.
Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>get_attention_layer</code></b>
                  (<code><span title="list">list</span>[<span title="int">int</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Layer indices to extract
attention weights from. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>mask_zeros</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Boolean mask for zero-expression genes
of shape (batch, seq_len + num_special_tokens). Defaults to None.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="typing.Dict">Dict</span>[<span title="str">str</span>, <span title="torch.Tensor">Tensor</span>] | <span title="tuple">tuple</span>[<span title="typing.Dict">Dict</span>[<span title="str">str</span>, <span title="torch.Tensor">Tensor</span>], <span title="list">list</span>]</code>
              –
              <div class="doc-md-description">
                <p>dict[str, Tensor] | tuple[dict, list]: Model outputs containing:
- "mean": Predicted expression (batch, seq_len)
- "disp": Dispersion parameters (batch, seq_len) [if ZINB]
- "zero_logits": Zero-inflation logits (batch, seq_len) [if ZINB]
- "input_cell_embs": Cell embeddings (batch, n_classes+1, d_model)
- "input_cell_emb": Mean cell embedding (batch, d_model)
- "output_cell_embs": Processed cell embeddings
- "output_cell_emb": Final cell embedding
- "cls_output_{class}": Classification logits for each class
- "gene_embedding": Gene embeddings [if get_gene_emb]
- "mvc_*": MVC predictions [if do_mvc]</p>
<p>If get_attention_layer is not None, returns (outputs_dict, attention_list)
where attention_list contains QKV tensors from specified layers.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="example" open>
  <summary>Example</summary>
  <blockquote>
<blockquote>
<blockquote>
<p>output = model(
...     gene_pos=batch["genes"],
...     expression=batch["x"],
...     req_depth=batch["depth"],
...     do_class=True,
... )
predictions = output["mean"]
cell_types = output["cls_output_cell_type_ontology_term_id"].argmax(-1)</p>
</blockquote>
</blockquote>
</blockquote>
</details>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span>
<span class="normal">1288</span>
<span class="normal">1289</span>
<span class="normal">1290</span>
<span class="normal">1291</span>
<span class="normal">1292</span>
<span class="normal">1293</span>
<span class="normal">1294</span>
<span class="normal">1295</span>
<span class="normal">1296</span>
<span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span>
<span class="normal">1354</span>
<span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span>
<span class="normal">1381</span>
<span class="normal">1382</span>
<span class="normal">1383</span>
<span class="normal">1384</span>
<span class="normal">1385</span>
<span class="normal">1386</span>
<span class="normal">1387</span>
<span class="normal">1388</span>
<span class="normal">1389</span>
<span class="normal">1390</span>
<span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">gene_pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">expression</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">neighbors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">neighbors_info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">req_depth</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">get_gene_emb</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metacell_token</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># (minibatch, 1)</span>
    <span class="n">depth_mult</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">do_mvc</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">do_class</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">get_attention_layer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mask_zeros</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]</span> <span class="o">|</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="nb">list</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Complete forward pass through the scPRINT-2</span>

<span class="sd">    Encodes input expression data, processes through transformer(s), and</span>
<span class="sd">    decodes into expression predictions and cell classifications.</span>

<span class="sd">    Args:</span>
<span class="sd">        gene_pos (Tensor): Gene indices of shape (batch, seq_len) mapping to</span>
<span class="sd">            positions in the model&#39;s gene vocabulary.</span>
<span class="sd">        expression (Tensor, optional): Expression values of shape (batch, seq_len).</span>
<span class="sd">            Can be raw counts or normalized depending on model config.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        neighbors (Tensor, optional): KNN neighbor expressions of shape</span>
<span class="sd">            (batch, n_neighbors, seq_len) for metacell-style encoding.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">        neighbors_info (Tensor, optional): Neighbor weights of shape</span>
<span class="sd">            (batch, n_neighbors). Defaults to None.</span>
<span class="sd">        mask (Tensor, optional): Boolean mask of shape (batch, seq_len) where</span>
<span class="sd">            True indicates positions to mask (set to zero). Defaults to None.</span>
<span class="sd">        req_depth (Tensor, optional): Target sequencing depth of shape (batch,)</span>
<span class="sd">            for depth-conditional generation. Defaults to None.</span>
<span class="sd">        get_gene_emb (bool, optional): Return gene embeddings from transformer.</span>
<span class="sd">            Defaults to False.</span>
<span class="sd">        metacell_token (Tensor, optional): Binary metacell indicators of shape</span>
<span class="sd">            (batch,). Defaults to None.</span>
<span class="sd">        depth_mult (Tensor, optional): Expression depth multiplier. If None,</span>
<span class="sd">            uses sum of expression values. Defaults to None.</span>
<span class="sd">        do_sample (bool, optional): Sample from predicted distribution.</span>
<span class="sd">            Currently unused. Defaults to False.</span>
<span class="sd">        do_mvc (bool, optional): Compute multi-view coding predictions.</span>
<span class="sd">            Defaults to False.</span>
<span class="sd">        do_class (bool, optional): Compute classification predictions.</span>
<span class="sd">            Defaults to False.</span>
<span class="sd">        get_attention_layer (list[int], optional): Layer indices to extract</span>
<span class="sd">            attention weights from. Defaults to None.</span>
<span class="sd">        mask_zeros (Tensor, optional): Boolean mask for zero-expression genes</span>
<span class="sd">            of shape (batch, seq_len + num_special_tokens). Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict[str, Tensor] | tuple[dict, list]: Model outputs containing:</span>
<span class="sd">            - &quot;mean&quot;: Predicted expression (batch, seq_len)</span>
<span class="sd">            - &quot;disp&quot;: Dispersion parameters (batch, seq_len) [if ZINB]</span>
<span class="sd">            - &quot;zero_logits&quot;: Zero-inflation logits (batch, seq_len) [if ZINB]</span>
<span class="sd">            - &quot;input_cell_embs&quot;: Cell embeddings (batch, n_classes+1, d_model)</span>
<span class="sd">            - &quot;input_cell_emb&quot;: Mean cell embedding (batch, d_model)</span>
<span class="sd">            - &quot;output_cell_embs&quot;: Processed cell embeddings</span>
<span class="sd">            - &quot;output_cell_emb&quot;: Final cell embedding</span>
<span class="sd">            - &quot;cls_output_{class}&quot;: Classification logits for each class</span>
<span class="sd">            - &quot;gene_embedding&quot;: Gene embeddings [if get_gene_emb]</span>
<span class="sd">            - &quot;mvc_*&quot;: MVC predictions [if do_mvc]</span>

<span class="sd">            If get_attention_layer is not None, returns (outputs_dict, attention_list)</span>
<span class="sd">            where attention_list contains QKV tensors from specified layers.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; output = model(</span>
<span class="sd">        ...     gene_pos=batch[&quot;genes&quot;],</span>
<span class="sd">        ...     expression=batch[&quot;x&quot;],</span>
<span class="sd">        ...     req_depth=batch[&quot;depth&quot;],</span>
<span class="sd">        ...     do_class=True,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; predictions = output[&quot;mean&quot;]</span>
<span class="sd">        &gt;&gt;&gt; cell_types = output[&quot;cls_output_cell_type_ontology_term_id&quot;].argmax(-1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cell_embs</span><span class="p">,</span> <span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span><span class="p">(</span>
        <span class="n">gene_pos</span><span class="p">,</span>
        <span class="n">expression</span><span class="p">,</span>
        <span class="n">neighbors</span><span class="p">,</span>
        <span class="n">neighbors_info</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">,</span>
        <span class="n">metacell_token</span><span class="o">=</span><span class="n">metacell_token</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># attention_bias</span>
    <span class="n">num</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_metacell_token</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span>
        <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_transformer</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;nbias_sparse&quot;</span><span class="p">):</span>
            <span class="n">bias_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attn_bias</span><span class="p">)</span>
            <span class="c1"># Keep as sparse matrix - much more memory efficient</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nbias_sparse</span> <span class="o">=</span> <span class="n">load_npz</span><span class="p">(</span><span class="n">bias_path</span><span class="p">)</span>

        <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">gene_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">gene_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">num</span><span class="p">,</span>
                <span class="n">gene_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">num</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">device</span><span class="o">=</span><span class="n">gene_pos</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">fade_factor</span> <span class="o">=</span> <span class="mi">100</span>

        <span class="c1"># Extract only the needed values from sparse matrix</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">gene_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Vectorized extraction from sparse matrix</span>
        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">gene_pos</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="c1"># Get submatrix for this batch&#39;s genes</span>
            <span class="n">submatrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nbias_sparse</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">indices</span><span class="p">)]</span>
            <span class="n">bias</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">num</span><span class="p">:,</span> <span class="n">num</span><span class="p">:]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                    <span class="n">submatrix</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">device</span><span class="o">=</span><span class="n">gene_pos</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
                <span class="p">)</span>
                <span class="o">*</span> <span class="n">fade_factor</span>
            <span class="p">)</span>

        <span class="n">bias</span><span class="p">[:,</span> <span class="n">num</span><span class="p">:,</span> <span class="p">:</span><span class="n">num</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10_000</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_transformer</span><span class="p">:</span>
        <span class="n">encoding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cell_embs</span><span class="p">,</span> <span class="n">encoding</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">)</span> <span class="ow">is</span> <span class="n">FlashTransformer</span><span class="p">:</span>
        <span class="n">transformer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span>
            <span class="n">encoding</span><span class="p">,</span>
            <span class="n">return_qkv</span><span class="o">=</span><span class="n">get_attention_layer</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">bias_layer</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nlayers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)),</span>
            <span class="n">mask_zeros</span><span class="o">=</span><span class="n">mask_zeros</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">)</span> <span class="ow">is</span> <span class="n">Performer</span><span class="p">:</span>
        <span class="n">transformer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span><span class="n">encoding</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown transformer: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">get_attention_layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">transformer_output</span><span class="p">,</span> <span class="n">qkvs</span> <span class="o">=</span> <span class="n">transformer_output</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_transformer</span><span class="p">:</span>
        <span class="n">cell_embs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_transformer</span><span class="p">(</span><span class="n">cell_embs</span><span class="p">,</span> <span class="n">x_kv</span><span class="o">=</span><span class="n">transformer_output</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">cell_embs</span><span class="p">,</span> <span class="n">transformer_output</span> <span class="o">=</span> <span class="n">transformer_output</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                <span class="n">transformer_output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
            <span class="p">],</span>
            <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="c1"># if not provided we will mult by the current expression sum</span>
    <span class="n">depth_mult</span> <span class="o">=</span> <span class="n">expression</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">depth_mult</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">depth_mult</span>
    <span class="n">req_depth</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">req_depth</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expr_decoder</span><span class="p">(</span>
        <span class="n">transformer_output</span><span class="p">[:,</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_metacell_token</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span> <span class="p">:,</span> <span class="p">:],</span>
        <span class="n">depth_mult</span><span class="p">,</span>
        <span class="n">req_depth</span><span class="p">,</span>
        <span class="n">get_gene_emb</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">res</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cell_decoder</span><span class="p">(</span>
            <span class="n">cell_embs</span><span class="p">,</span>
            <span class="n">do_mvc</span><span class="p">,</span>
            <span class="n">do_class</span><span class="p">,</span>
            <span class="n">depth_mult</span><span class="p">,</span>
            <span class="n">req_depth</span><span class="p">,</span>
            <span class="n">gene_pos</span> <span class="k">if</span> <span class="n">do_mvc</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">qkvs</span><span class="p">)</span> <span class="k">if</span> <span class="n">get_attention_layer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">res</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.model.scPRINT2.log_adata" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">log_adata</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>log_adata will log an adata from predictions.
It will log to tensorboard and wandb if available</p>
<p>see @utils.log_adata</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2642</span>
<span class="normal">2643</span>
<span class="normal">2644</span>
<span class="normal">2645</span>
<span class="normal">2646</span>
<span class="normal">2647</span>
<span class="normal">2648</span>
<span class="normal">2649</span>
<span class="normal">2650</span>
<span class="normal">2651</span>
<span class="normal">2652</span>
<span class="normal">2653</span>
<span class="normal">2654</span>
<span class="normal">2655</span>
<span class="normal">2656</span>
<span class="normal">2657</span>
<span class="normal">2658</span>
<span class="normal">2659</span>
<span class="normal">2660</span>
<span class="normal">2661</span>
<span class="normal">2662</span>
<span class="normal">2663</span>
<span class="normal">2664</span>
<span class="normal">2665</span>
<span class="normal">2666</span>
<span class="normal">2667</span>
<span class="normal">2668</span>
<span class="normal">2669</span>
<span class="normal">2670</span>
<span class="normal">2671</span>
<span class="normal">2672</span>
<span class="normal">2673</span>
<span class="normal">2674</span>
<span class="normal">2675</span>
<span class="normal">2676</span>
<span class="normal">2677</span>
<span class="normal">2678</span>
<span class="normal">2679</span>
<span class="normal">2680</span>
<span class="normal">2681</span>
<span class="normal">2682</span>
<span class="normal">2683</span>
<span class="normal">2684</span>
<span class="normal">2685</span>
<span class="normal">2686</span>
<span class="normal">2687</span>
<span class="normal">2688</span>
<span class="normal">2689</span>
<span class="normal">2690</span>
<span class="normal">2691</span>
<span class="normal">2692</span>
<span class="normal">2693</span>
<span class="normal">2694</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">log_adata</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gtclass</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    log_adata will log an adata from predictions.</span>
<span class="sd">    It will log to tensorboard and wandb if available</span>

<span class="sd">    see @utils.log_adata</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">mdir</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">save_dir</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">save_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;/tmp&quot;</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">mdir</span> <span class="o">=</span> <span class="s2">&quot;data/&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">mdir</span><span class="p">):</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">mdir</span><span class="p">)</span>
    <span class="n">adata</span><span class="p">,</span> <span class="n">fig</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">make_adata</span><span class="p">(</span>
        <span class="n">genes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">,</span>
        <span class="n">embs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embs</span><span class="p">,</span>
        <span class="n">pos</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pos</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_expr</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">expr_pred</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">expr_pred</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_expr</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">,</span>
        <span class="n">pred</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pred</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_all_labels_pred</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">label_decoders</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">label_decoders</span><span class="p">,</span>
        <span class="n">labels_hierarchy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">labels_hierarchy</span><span class="p">,</span>
        <span class="n">gtclass</span><span class="o">=</span><span class="n">gtclass</span><span class="p">,</span>
        <span class="n">doplot</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">doplot</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">adata</span><span class="o">.</span><span class="n">write</span><span class="p">(</span>
        <span class="nb">str</span><span class="p">(</span><span class="n">mdir</span><span class="p">)</span>
        <span class="o">+</span> <span class="s2">&quot;/step_&quot;</span>
        <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">global_step</span><span class="p">)</span>
        <span class="o">+</span> <span class="s2">&quot;_&quot;</span>
        <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="o">+</span> <span class="s2">&quot;_&quot;</span>
        <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="o">+</span> <span class="s2">&quot;_&quot;</span>
        <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span><span class="p">)</span>
        <span class="o">+</span> <span class="s2">&quot;.h5ad&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">doplot</span> <span class="ow">and</span> <span class="n">fig</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logged</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_figure</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
            <span class="n">logged</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;couldn&#39;t log to tensorboard&quot;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">log_image</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s2">&quot;umaps&quot;</span><span class="p">,</span> <span class="n">images</span><span class="o">=</span><span class="p">[</span><span class="n">fig</span><span class="p">],</span> <span class="n">step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">global_step</span><span class="p">)</span>
            <span class="n">logged</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;couldn&#39;t log to wandb&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">logged</span><span class="p">:</span>
            <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">mdir</span> <span class="o">+</span> <span class="s2">&quot;/umap_&quot;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.png&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">adata</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.model.scPRINT2.on_fit_start" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">on_fit_start</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>@see pl.LightningModule</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1599</span>
<span class="normal">1600</span>
<span class="normal">1601</span>
<span class="normal">1602</span>
<span class="normal">1603</span>
<span class="normal">1604</span>
<span class="normal">1605</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">on_fit_start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">)</span> <span class="ow">is</span> <span class="n">FlashTransformer</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">encoder_layers</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
            <span class="n">encoder_layers</span><span class="o">.</span><span class="n">set_seq_parallel</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mat_labels_hierarchy</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mat_labels_hierarchy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.model.scPRINT2.on_load_checkpoint" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">on_load_checkpoint</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Handle checkpoint loading with backward compatibility.</p>
<p>Automatically handles:
- Different class configurations between checkpoint and current model
- Legacy parameter names and structures
- Encoder/decoder mismatches with datamodule
- Gene vocabulary differences
- Early stopping callback state</p>
<p>Called automatically by PyTorch Lightning during checkpoint loading.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>checkpoints</code></b>
                  (<code><span title="dict">dict</span></code>)
              –
              <div class="doc-md-description">
                <p>Checkpoint dictionary from torch.load().</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="note" open>
  <summary>Note</summary>
  <p>Prints warnings when configurations differ between checkpoint and
current model. These should be reviewed to ensure expected behavior.</p>
</details>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span>
<span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span>
<span class="normal">911</span>
<span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span>
<span class="normal">918</span>
<span class="normal">919</span>
<span class="normal">920</span>
<span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span>
<span class="normal">944</span>
<span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoints</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Handle checkpoint loading with backward compatibility.</span>

<span class="sd">    Automatically handles:</span>
<span class="sd">    - Different class configurations between checkpoint and current model</span>
<span class="sd">    - Legacy parameter names and structures</span>
<span class="sd">    - Encoder/decoder mismatches with datamodule</span>
<span class="sd">    - Gene vocabulary differences</span>
<span class="sd">    - Early stopping callback state</span>

<span class="sd">    Called automatically by PyTorch Lightning during checkpoint loading.</span>

<span class="sd">    Args:</span>
<span class="sd">        checkpoints (dict): Checkpoint dictionary from torch.load().</span>

<span class="sd">    Note:</span>
<span class="sd">        Prints warnings when configurations differ between checkpoint and</span>
<span class="sd">        current model. These should be reviewed to ensure expected behavior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if not the same number of labels (due to diff datasets)</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">clss</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_decoders</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">size</span> <span class="o">=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">][</span>
            <span class="s2">&quot;cls_decoders.&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.out_layer.bias&quot;</span>
        <span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">size</span> <span class="o">!=</span> <span class="n">clss</span><span class="o">.</span><span class="n">out_layer</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cls_decoders</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
                <span class="n">clss</span><span class="o">.</span><span class="n">out_layer</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span>
            <span class="p">)</span>
    <span class="c1"># from older model versions</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;normalization&quot;</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;gene_encoder.0.embedding.weight&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">):</span>
        <span class="c1"># replace it with the new one gene_encoder.0.embeddings.weight in the state_dict</span>
        <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">][</span><span class="s2">&quot;gene_encoder.0.embeddings.weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">checkpoints</span><span class="p">[</span>
            <span class="s2">&quot;state_dict&quot;</span>
        <span class="p">][</span><span class="s2">&quot;gene_encoder.0.embedding.weight&quot;</span><span class="p">]</span>
        <span class="k">del</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">][</span><span class="s2">&quot;gene_encoder.0.embedding.weight&quot;</span><span class="p">]</span>
    <span class="c1"># same</span>
    <span class="c1"># when doing batch effect correction and input dataset is not the same</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="s2">&quot;grad_reverse_discriminator_loss.out_layer.bias&quot;</span>
        <span class="ow">in</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="k">if</span> <span class="s2">&quot;grad_reverse_discriminator_loss&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">:</span>
                <span class="k">del</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="s2">&quot;the discriminator for batch effect correction has been removed. &quot;</span>
            <span class="s2">&quot;dropping the legacy key.&quot;</span>
        <span class="p">)</span>
    <span class="c1"># same</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;gene_encoder.embedding.weight&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">):</span>
        <span class="c1"># replace it with the new one gene_encoder.embeddings.weight in the state_dict</span>
        <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">][</span><span class="s2">&quot;gene_encoder.embeddings.weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">checkpoints</span><span class="p">[</span>
            <span class="s2">&quot;state_dict&quot;</span>
        <span class="p">][</span><span class="s2">&quot;gene_encoder.embedding.weight&quot;</span><span class="p">]</span>
        <span class="k">del</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">][</span><span class="s2">&quot;gene_encoder.embedding.weight&quot;</span><span class="p">]</span>

    <span class="k">if</span> <span class="s2">&quot;classes&quot;</span> <span class="ow">in</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">]:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_counts</span> <span class="o">!=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;classes&quot;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="s2">&quot;label_counts&quot;</span> <span class="ow">in</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="nb">set</span><span class="p">(</span>
                <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;label_counts&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
            <span class="p">)</span> <span class="o">==</span> <span class="nb">set</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;classes&quot;</span><span class="p">]):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes</span> <span class="o">!=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;classes&quot;</span><span class="p">]:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;classes have changed, be careful&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">classes</span> <span class="o">=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;classes&quot;</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">label_counts</span> <span class="o">=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;label_counts&quot;</span><span class="p">]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_counts</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;classes and label_counts are the same, this is not allowed, please use another checkpoint&quot;</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">label_counts</span> <span class="o">=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;classes&quot;</span><span class="p">]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes</span> <span class="o">!=</span> <span class="nb">list</span><span class="p">(</span>
                    <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;classes&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                <span class="p">):</span>
                    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;classes have changed, be careful&quot;</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">classes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span>
                        <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;classes&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
                    <span class="p">)</span>
        <span class="c1"># else it is all good as expected</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;no classes in the checkpoint, be careful&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;pos_encoder.pe&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">PositionalEncoding</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">gene_pos_enc</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span><span class="o">.</span><span class="n">pe</span> <span class="o">=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">][</span><span class="s2">&quot;pos_encoder.pe&quot;</span><span class="p">]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_decoders</span> <span class="o">!=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span>
        <span class="s2">&quot;label_decoders&quot;</span>
    <span class="p">]</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels_hierarchy</span> <span class="o">!=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
        <span class="s2">&quot;labels_hierarchy&quot;</span><span class="p">,</span> <span class="p">{}</span>
    <span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;label decoders have changed, be careful&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_decoders</span> <span class="o">=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;label_decoders&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels_hierarchy</span> <span class="o">=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;labels_hierarchy&quot;</span><span class="p">,</span> <span class="p">{}</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels_hierarchy</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">tens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_counts</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
            <span class="k">for</span> <span class="n">k2</span><span class="p">,</span> <span class="n">v2</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">tens</span><span class="p">[</span><span class="n">k2</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_counts</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">v2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mat_labels_hierarchy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">tens</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="s2">&quot;gene_pos_enc&quot;</span> <span class="ow">in</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">]</span>
        <span class="ow">and</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;gene_pos_enc&quot;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span><span class="o">.</span><span class="n">gene_pos_enc</span>
            <span class="o">!=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;gene_pos_enc&quot;</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="s2">&quot;Gene position encoding has changed in the dataloader compared to last time, trying to revert&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">PositionalEncoding</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
                <span class="n">gene_pos_enc</span><span class="o">=</span><span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;gene_pos_enc&quot;</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;gene_pos_enc&quot;</span><span class="p">)</span>
    <span class="n">mencoders</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;genes&quot;</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">list</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;converting a gene list-based model&quot;</span><span class="p">)</span>
        <span class="n">org</span> <span class="o">=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;organisms&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">organisms</span><span class="p">)</span>
        <span class="n">genedf</span> <span class="o">=</span> <span class="n">load_genes</span><span class="p">(</span><span class="n">org</span><span class="p">)</span>
        <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;genes&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">i</span><span class="p">:</span> <span class="n">genedf</span><span class="o">.</span><span class="n">index</span><span class="p">[</span>
                <span class="p">(</span><span class="n">genedf</span><span class="o">.</span><span class="n">organism</span> <span class="o">==</span> <span class="n">i</span><span class="p">)</span>
                <span class="o">&amp;</span> <span class="n">genedf</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;genes&quot;</span><span class="p">])</span>
            <span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">org</span>
        <span class="p">}</span>
    <span class="k">if</span> <span class="s2">&quot;precpt_gene_emb&quot;</span> <span class="ow">in</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">]:</span>
        <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;precpt_gene_emb&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;gene_pos_file&quot;</span> <span class="ow">in</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">]:</span>
        <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;gene_pos_file&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;transformer&quot;</span> <span class="ow">in</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">]:</span>
        <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;attention&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">checkpoints</span><span class="p">[</span>
            <span class="s2">&quot;hyper_parameters&quot;</span>
        <span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;transformer&quot;</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">datamodule</span><span class="o">.</span><span class="n">decoders</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_decoders</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;label decoders have changed, be careful&quot;</span><span class="p">)</span>
            <span class="c1"># if we don&#39;t have the same decoders, we need to update the one on the datamodule side</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_decoders</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">mencoders</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="n">va</span><span class="p">:</span> <span class="n">ke</span> <span class="k">for</span> <span class="n">ke</span><span class="p">,</span> <span class="n">va</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">datamodule</span><span class="o">.</span><span class="n">encoders</span> <span class="o">=</span> <span class="n">mencoders</span>

        <span class="n">es</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">callbacks</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">EarlyStopping</span><span class="p">):</span>
                <span class="n">es</span> <span class="o">=</span> <span class="n">k</span>
        <span class="k">if</span> <span class="n">es</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">prev</span> <span class="o">=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;callbacks&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="s2">&quot;EarlyStopping{&#39;monitor&#39;: &#39;val_loss&#39;, &#39;mode&#39;: &#39;min&#39;}&quot;</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">prev</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">prev</span> <span class="o">=</span> <span class="n">prev</span><span class="p">[</span><span class="s2">&quot;patience&quot;</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">prev</span> <span class="o">!=</span> <span class="n">es</span><span class="o">.</span><span class="n">patience</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="s2">&quot;updating the early stopping parameter to </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">es</span><span class="o">.</span><span class="n">patience</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;callbacks&quot;</span><span class="p">][</span>
                    <span class="s2">&quot;EarlyStopping{&#39;monitor&#39;: &#39;val_loss&#39;, &#39;mode&#39;: &#39;min&#39;}&quot;</span>
                <span class="p">][</span><span class="s2">&quot;patience&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">es</span><span class="o">.</span><span class="n">patience</span>
                <span class="k">if</span> <span class="n">prev</span> <span class="o">&lt;</span> <span class="n">es</span><span class="o">.</span><span class="n">patience</span><span class="p">:</span>
                    <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;callbacks&quot;</span><span class="p">][</span>
                        <span class="s2">&quot;EarlyStopping{&#39;monitor&#39;: &#39;val_loss&#39;, &#39;mode&#39;: &#39;min&#39;}&quot;</span>
                    <span class="p">][</span><span class="s2">&quot;stopped_epoch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;scPRINT2 is not attached to a `Trainer`.&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FYI: scPRINT2 is not attached to a `Trainer`.&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">e</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mvc_decoder</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="ow">and</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;mvc_decoder.gene2query.weight&quot;</span><span class="p">)</span>
        <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="s2">&quot;mvc_decoder.gene2query.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;mvc_decoder.gene2query.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;mvc_decoder.norm.weight&quot;</span><span class="p">,</span>
            <span class="s2">&quot;mvc_decoder.norm.bias&quot;</span><span class="p">,</span>
            <span class="s2">&quot;mvc_decoder.pred_var_zero.weight&quot;</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">]:</span>
                <span class="k">del</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
    <span class="n">org</span> <span class="o">=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;organisms&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">organisms</span> <span class="o">!=</span> <span class="n">org</span> <span class="ow">and</span> <span class="n">org</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">organisms</span> <span class="o">=</span> <span class="n">org</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">datamodule</span><span class="o">.</span><span class="n">organisms</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">organisms</span>
        <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;scPRINT2 is not attached to a `Trainer`.&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>
                <span class="k">raise</span> <span class="n">e</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_genes</span> <span class="o">!=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;genes&quot;</span><span class="p">]:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_genes</span> <span class="o">=</span> <span class="n">checkpoints</span><span class="p">[</span><span class="s2">&quot;hyper_parameters&quot;</span><span class="p">][</span><span class="s2">&quot;genes&quot;</span><span class="p">]</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">datamodule</span><span class="o">.</span><span class="n">set_valid_genes_collator</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;scPRINT2 is not attached to a `Trainer`.&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">e</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_interactive</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.model.scPRINT2.on_predict_epoch_end" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">on_predict_epoch_end</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>@see pl.LightningModule will</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2634</span>
<span class="normal">2635</span>
<span class="normal">2636</span>
<span class="normal">2637</span>
<span class="normal">2638</span>
<span class="normal">2639</span>
<span class="normal">2640</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">on_predict_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule will&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">100</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_log_adata</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;adding on disk&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_adata</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;predict_part_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">counter</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.model.scPRINT2.on_predict_epoch_start" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">on_predict_epoch_start</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>@see pl.LightningModule</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2365</span>
<span class="normal">2366</span>
<span class="normal">2367</span>
<span class="normal">2368</span>
<span class="normal">2369</span>
<span class="normal">2370</span>
<span class="normal">2371</span>
<span class="normal">2372</span>
<span class="normal">2373</span>
<span class="normal">2374</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">on_predict_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;predict epoch start&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embs</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">)</span> <span class="ow">is</span> <span class="n">FlashTransformer</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">encoder_layers</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
            <span class="n">encoder_layers</span><span class="o">.</span><span class="n">set_seq_parallel</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.model.scPRINT2.on_test_start" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">on_test_start</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>@see pl.LightningModule</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2324</span>
<span class="normal">2325</span>
<span class="normal">2326</span>
<span class="normal">2327</span>
<span class="normal">2328</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">on_test_start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;test start&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mat_labels_hierarchy</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mat_labels_hierarchy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.model.scPRINT2.on_validation_epoch_end" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">on_validation_epoch_end</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>@see pl.LightningModule</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2277</span>
<span class="normal">2278</span>
<span class="normal">2279</span>
<span class="normal">2280</span>
<span class="normal">2281</span>
<span class="normal">2282</span>
<span class="normal">2283</span>
<span class="normal">2284</span>
<span class="normal">2285</span>
<span class="normal">2286</span>
<span class="normal">2287</span>
<span class="normal">2288</span>
<span class="normal">2289</span>
<span class="normal">2290</span>
<span class="normal">2291</span>
<span class="normal">2292</span>
<span class="normal">2293</span>
<span class="normal">2294</span>
<span class="normal">2295</span>
<span class="normal">2296</span>
<span class="normal">2297</span>
<span class="normal">2298</span>
<span class="normal">2299</span>
<span class="normal">2300</span>
<span class="normal">2301</span>
<span class="normal">2302</span>
<span class="normal">2303</span>
<span class="normal">2304</span>
<span class="normal">2305</span>
<span class="normal">2306</span>
<span class="normal">2307</span>
<span class="normal">2308</span>
<span class="normal">2309</span>
<span class="normal">2310</span>
<span class="normal">2311</span>
<span class="normal">2312</span>
<span class="normal">2313</span>
<span class="normal">2314</span>
<span class="normal">2315</span>
<span class="normal">2316</span>
<span class="normal">2317</span>
<span class="normal">2318</span>
<span class="normal">2319</span>
<span class="normal">2320</span>
<span class="normal">2321</span>
<span class="normal">2322</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">on_validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pos</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">expr_pred</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_adv_cls</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_store_adv_cls</span>
    <span class="n">gathered_embs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embs</span><span class="p">)</span>
    <span class="c1"># Merge the dictionaries from all processes</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">embs</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">gathered_embs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">gathered_embs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">info</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pred</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="kc">None</span>
    <span class="p">)</span>
    <span class="c1"># self.pos = self.all_gather(self.pos).view(-1, self.pos.shape[-1])</span>
    <span class="c1"># self.expr_pred[0] = self.all_gather(self.expr_pred[0]).view(</span>
    <span class="c1">#     -1, self.expr_pred[0].shape[-1]</span>
    <span class="c1"># )</span>
    <span class="c1"># if len(self.expr_pred) &gt; 1:</span>
    <span class="c1">#     self.expr_pred[1] = self.all_gather(self.expr_pred[1]).view(</span>
    <span class="c1">#         -1, self.expr_pred[1].shape[-1]</span>
    <span class="c1">#     )</span>
    <span class="c1"># self.expr_pred[2] = self.all_gather(self.expr_pred[2]).view(</span>
    <span class="c1">#     -1, self.expr_pred[2].shape[-1]</span>
    <span class="c1"># )</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">stage</span> <span class="o">!=</span> <span class="s2">&quot;sanity_check&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">is_global_zero</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;logging anndata&quot;</span><span class="p">)</span>
            <span class="n">sch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_schedulers</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">sch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">sch</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">callback_metrics</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">])</span>
            <span class="c1"># run the test function on specific dataset</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">embs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log_adata</span><span class="p">(</span>
                    <span class="n">gtclass</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">info</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;validation_part_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">counter</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">on_test_epoch_end</span><span class="p">()</span>
            <span class="c1"># Synchronize all processes with a timeout</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="c1"># Set a timeout that&#39;s longer than your test typically takes</span>
            <span class="c1"># Write rank to file for debugging</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">strategy</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.model.scPRINT2.optimizer_step" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">optimizer_step</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>@see pl.LightningModule</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2161</span>
<span class="normal">2162</span>
<span class="normal">2163</span>
<span class="normal">2164</span>
<span class="normal">2165</span>
<span class="normal">2166</span>
<span class="normal">2167</span>
<span class="normal">2168</span>
<span class="normal">2169</span>
<span class="normal">2170</span>
<span class="normal">2171</span>
<span class="normal">2172</span>
<span class="normal">2173</span>
<span class="normal">2174</span>
<span class="normal">2175</span>
<span class="normal">2176</span>
<span class="normal">2177</span>
<span class="normal">2178</span>
<span class="normal">2179</span>
<span class="normal">2180</span>
<span class="normal">2181</span>
<span class="normal">2182</span>
<span class="normal">2183</span>
<span class="normal">2184</span>
<span class="normal">2185</span>
<span class="normal">2186</span>
<span class="normal">2187</span>
<span class="normal">2188</span>
<span class="normal">2189</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_closure</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="c1"># update params</span>
    <span class="c1"># manually warm up lr without a scheduler</span>
    <span class="c1"># making sure that we don&#39;t do this during lrfinder</span>
    <span class="n">lr_scale</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">prev_lr</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_duration</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span>
    <span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
            <span class="n">lr_scale</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span>
                <span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_duration</span>
            <span class="p">)</span>
            <span class="n">prev_lr</span> <span class="o">=</span> <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span>
            <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
        <span class="c1"># if pg[&quot;lr&quot;] &lt; 2e-5:</span>
        <span class="c1">#    pg[&quot;lr&quot;] = 2e-5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;lr_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">prev_lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev_lr</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;OPTIMIZER HAS INCREASED LR. WHYY?&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.model.scPRINT2.predict_step" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">predict_step</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>embed given gene expression, encode the gene embedding and cell embedding.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>batch</code></b>
                  (<code><span title="typing.Dict">Dict</span>[<span title="str">str</span>, <span title="torch.Tensor">Tensor</span>]</code>)
              –
              <div class="doc-md-description">
                <p>Dictionary containing 'genes', 'x', 'depth', and optionally 'knn_cells'.</p>
              </div>
            </li>
            <li>
              <b><code>batch_idx</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>Index of the batch.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="typing.Dict">Dict</span>[<span title="str">str</span>, <span title="torch.Tensor">Tensor</span>]</code>
              –
              <div class="doc-md-description">
                <p>Dict[str, Tensor]: Dictionary containing model predictions.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2376</span>
<span class="normal">2377</span>
<span class="normal">2378</span>
<span class="normal">2379</span>
<span class="normal">2380</span>
<span class="normal">2381</span>
<span class="normal">2382</span>
<span class="normal">2383</span>
<span class="normal">2384</span>
<span class="normal">2385</span>
<span class="normal">2386</span>
<span class="normal">2387</span>
<span class="normal">2388</span>
<span class="normal">2389</span>
<span class="normal">2390</span>
<span class="normal">2391</span>
<span class="normal">2392</span>
<span class="normal">2393</span>
<span class="normal">2394</span>
<span class="normal">2395</span>
<span class="normal">2396</span>
<span class="normal">2397</span>
<span class="normal">2398</span>
<span class="normal">2399</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">predict_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    embed given gene expression, encode the gene embedding and cell embedding.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (Dict[str, Tensor]): Dictionary containing &#39;genes&#39;, &#39;x&#39;, &#39;depth&#39;, and optionally &#39;knn_cells&#39;.</span>
<span class="sd">        batch_idx: Index of the batch.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict[str, Tensor]: Dictionary containing model predictions.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span>
        <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;genes&quot;</span><span class="p">],</span>
        <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span>
        <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;depth&quot;</span><span class="p">],</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;knn_cells&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;knn_cells_info&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">predict_mode</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred_embedding</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_attention_layer</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">predict_depth_mult</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.model.scPRINT2.training_step" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">training_step</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>training_step defines the train loop. It is independent of forward</p>
<p>@see pl.LightningModule</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>Total loss value for the training step.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1607</span>
<span class="normal">1608</span>
<span class="normal">1609</span>
<span class="normal">1610</span>
<span class="normal">1611</span>
<span class="normal">1612</span>
<span class="normal">1613</span>
<span class="normal">1614</span>
<span class="normal">1615</span>
<span class="normal">1616</span>
<span class="normal">1617</span>
<span class="normal">1618</span>
<span class="normal">1619</span>
<span class="normal">1620</span>
<span class="normal">1621</span>
<span class="normal">1622</span>
<span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span>
<span class="normal">1630</span>
<span class="normal">1631</span>
<span class="normal">1632</span>
<span class="normal">1633</span>
<span class="normal">1634</span>
<span class="normal">1635</span>
<span class="normal">1636</span>
<span class="normal">1637</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
    <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    training_step defines the train loop. It is independent of forward</span>

<span class="sd">    @see pl.LightningModule</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: Total loss value for the training step.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">total_loss</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_training</span><span class="p">(</span>
        <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
        <span class="n">noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">,</span>
        <span class="n">do_next_tp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_next_tp</span><span class="p">,</span>
        <span class="n">cce_temp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cce_temp</span><span class="p">,</span>
        <span class="n">do_generate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_generate</span><span class="p">,</span>
        <span class="n">run_full_forward</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">run_full_forward</span><span class="p">,</span>
        <span class="n">mask_ratio</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mask_ratio</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">total_loss</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">total_loss</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Loss is NaN&quot;</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;train_loss&quot;</span><span class="p">,</span> <span class="n">total_loss</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total_loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.model.scPRINT2.validation_step" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">validation_step</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>validation_step defines the validation loop. It is independent of forward
@see pl.LightningModule</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>batch</code></b>
                  (<code><span title="list">list</span>[<span title="torch.Tensor">Tensor</span>]</code>)
              –
              <div class="doc-md-description">
                <p>@see training_step</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">2202</span>
<span class="normal">2203</span>
<span class="normal">2204</span>
<span class="normal">2205</span>
<span class="normal">2206</span>
<span class="normal">2207</span>
<span class="normal">2208</span>
<span class="normal">2209</span>
<span class="normal">2210</span>
<span class="normal">2211</span>
<span class="normal">2212</span>
<span class="normal">2213</span>
<span class="normal">2214</span>
<span class="normal">2215</span>
<span class="normal">2216</span>
<span class="normal">2217</span>
<span class="normal">2218</span>
<span class="normal">2219</span>
<span class="normal">2220</span>
<span class="normal">2221</span>
<span class="normal">2222</span>
<span class="normal">2223</span>
<span class="normal">2224</span>
<span class="normal">2225</span>
<span class="normal">2226</span>
<span class="normal">2227</span>
<span class="normal">2228</span>
<span class="normal">2229</span>
<span class="normal">2230</span>
<span class="normal">2231</span>
<span class="normal">2232</span>
<span class="normal">2233</span>
<span class="normal">2234</span>
<span class="normal">2235</span>
<span class="normal">2236</span>
<span class="normal">2237</span>
<span class="normal">2238</span>
<span class="normal">2239</span>
<span class="normal">2240</span>
<span class="normal">2241</span>
<span class="normal">2242</span>
<span class="normal">2243</span>
<span class="normal">2244</span>
<span class="normal">2245</span>
<span class="normal">2246</span>
<span class="normal">2247</span>
<span class="normal">2248</span>
<span class="normal">2249</span>
<span class="normal">2250</span>
<span class="normal">2251</span>
<span class="normal">2252</span>
<span class="normal">2253</span>
<span class="normal">2254</span>
<span class="normal">2255</span>
<span class="normal">2256</span>
<span class="normal">2257</span>
<span class="normal">2258</span>
<span class="normal">2259</span>
<span class="normal">2260</span>
<span class="normal">2261</span>
<span class="normal">2262</span>
<span class="normal">2263</span>
<span class="normal">2264</span>
<span class="normal">2265</span>
<span class="normal">2266</span>
<span class="normal">2267</span>
<span class="normal">2268</span>
<span class="normal">2269</span>
<span class="normal">2270</span>
<span class="normal">2271</span>
<span class="normal">2272</span>
<span class="normal">2273</span>
<span class="normal">2274</span>
<span class="normal">2275</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    validation_step defines the validation loop. It is independent of forward</span>
<span class="sd">    @see pl.LightningModule</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (list[Tensor]): @see training_step</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">val_loss</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_training</span><span class="p">(</span>
        <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
        <span class="n">noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">,</span>
        <span class="n">do_next_tp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_next_tp</span><span class="p">,</span>
        <span class="n">cce_temp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cce_temp</span><span class="p">,</span>
        <span class="n">do_vae_kl</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">do_generate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_generate</span><span class="p">,</span>
        <span class="n">run_full_forward</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">run_full_forward</span><span class="p">,</span>
        <span class="n">mask_ratio</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mask_ratio</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">expression</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">]</span>
    <span class="n">gene_pos</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;genes&quot;</span><span class="p">]</span>
    <span class="n">depth</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;depth&quot;</span><span class="p">]</span>
    <span class="n">metacell_token</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;is_meta&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">knn_cells</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;knn_cells&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">knn_cells_info</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;knn_cells_info&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="c1"># TODO: make this faster by only calling val loss</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">embs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">100_000</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">world_size</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">info</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">info</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span>
                <span class="n">gene_pos</span><span class="p">,</span>
                <span class="n">expression</span><span class="p">,</span>
                <span class="n">depth</span><span class="p">,</span>
                <span class="n">knn_cells</span><span class="o">=</span><span class="n">knn_cells</span><span class="p">,</span>
                <span class="n">knn_cells_info</span><span class="o">=</span><span class="n">knn_cells_info</span><span class="p">,</span>
                <span class="n">pred_embedding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pred_embedding</span><span class="p">,</span>
                <span class="n">max_size_in_mem</span><span class="o">=</span><span class="mi">120_000</span><span class="p">,</span>
                <span class="n">metacell_token</span><span class="o">=</span><span class="n">metacell_token</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">info</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span>
            <span class="n">gene_pos</span><span class="p">,</span>
            <span class="n">expression</span><span class="p">,</span>
            <span class="n">depth</span><span class="p">,</span>
            <span class="n">knn_cells</span><span class="o">=</span><span class="n">knn_cells</span><span class="p">,</span>
            <span class="n">knn_cells_info</span><span class="o">=</span><span class="n">knn_cells_info</span><span class="p">,</span>
            <span class="n">pred_embedding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pred_embedding</span><span class="p">,</span>
            <span class="n">max_size_in_mem</span><span class="o">=</span><span class="mi">120_000</span><span class="p">,</span>
            <span class="n">metacell_token</span><span class="o">=</span><span class="n">metacell_token</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">expr_loss</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">v</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="ow">is</span> <span class="n">Tensor</span> <span class="k">else</span> <span class="n">v</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">losses</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="k">if</span> <span class="s2">&quot;expr&quot;</span> <span class="ow">in</span> <span class="n">k</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss_expr&quot;</span><span class="p">,</span> <span class="n">expr_loss</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">cls_loss</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">v</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="ow">is</span> <span class="n">Tensor</span> <span class="k">else</span> <span class="n">v</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">losses</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="k">if</span> <span class="s2">&quot;cls&quot;</span> <span class="ow">in</span> <span class="n">k</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss_cls&quot;</span><span class="p">,</span> <span class="n">cls_loss</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># self.log_dict(losses, sync_dist=True)</span>
    <span class="k">return</span> <span class="n">val_loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>




  </div>

    </div>

</div><h2 id="losses">losses</h2>


<div class="doc doc-object doc-module">



<h2 id="scprint2.model.loss" class="doc doc-heading">
            <code>scprint2.model.loss</code>


</h2>

    <div class="doc doc-contents first">










<p><span class="doc-section-title">Classes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="AdversarialDiscriminatorLoss (scprint2.model.loss.AdversarialDiscriminatorLoss)" href="#scprint2.model.loss.AdversarialDiscriminatorLoss">AdversarialDiscriminatorLoss</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
      </tbody>
    </table>




<p><span class="doc-section-title">Functions:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="contrastive_loss (scprint2.model.loss.contrastive_loss)" href="#scprint2.model.loss.contrastive_loss">contrastive_loss</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Computes NT-Xent loss (InfoNCE) between two sets of vectors.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="criterion_neg_log_bernoulli (scprint2.model.loss.criterion_neg_log_bernoulli)" href="#scprint2.model.loss.criterion_neg_log_bernoulli">criterion_neg_log_bernoulli</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Compute the negative log-likelihood of Bernoulli distribution</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="ecs (scprint2.model.loss.ecs)" href="#scprint2.model.loss.ecs">ecs</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>ecs Computes the similarity of cell embeddings based on a threshold.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="grad_reverse (scprint2.model.loss.grad_reverse)" href="#scprint2.model.loss.grad_reverse">grad_reverse</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>grad_reverse Reverses the gradient of the input tensor.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="hierarchical_classification (scprint2.model.loss.hierarchical_classification)" href="#scprint2.model.loss.hierarchical_classification">hierarchical_classification</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Computes the classification loss for a given batch of predictions and ground truth labels.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="masked_mae (scprint2.model.loss.masked_mae)" href="#scprint2.model.loss.masked_mae">masked_mae</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Compute the masked MAE loss between input and target.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="masked_mse (scprint2.model.loss.masked_mse)" href="#scprint2.model.loss.masked_mse">masked_mse</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Compute the masked MSE loss between input and target.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="masked_nb (scprint2.model.loss.masked_nb)" href="#scprint2.model.loss.masked_nb">masked_nb</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Compute the masked negative binomial loss between input and target.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="masked_relative_error (scprint2.model.loss.masked_relative_error)" href="#scprint2.model.loss.masked_relative_error">masked_relative_error</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Compute the masked relative error between input and target.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mse (scprint2.model.loss.mse)" href="#scprint2.model.loss.mse">mse</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Compute the MSE loss between input and target.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="nb (scprint2.model.loss.nb)" href="#scprint2.model.loss.nb">nb</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Computes the negative binomial (NB) loss.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="within_sample (scprint2.model.loss.within_sample)" href="#scprint2.model.loss.within_sample">within_sample</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Compute dissimilarity between embeddings within each sample</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="zinb (scprint2.model.loss.zinb)" href="#scprint2.model.loss.zinb">zinb</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Computes zero-inflated negative binomial (ZINB) loss.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>





<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="scprint2.model.loss.AdversarialDiscriminatorLoss" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">AdversarialDiscriminatorLoss</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>



        <p>Discriminator for the adversarial training for batch correction.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The size of the input tensor.</p>
              </div>
            </li>
            <li>
              <b><code>n_cls</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The number of classes.</p>
              </div>
            </li>
            <li>
              <b><code>nlayers</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>3</code>
)
              –
              <div class="doc-md-description">
                <p>The number of layers in the discriminator. Defaults to 3.</p>
              </div>
            </li>
            <li>
              <b><code>activation</code></b>
                  (<code><span title="callable">callable</span></code>, default:
                      <code><span title="torch.nn.LeakyReLU">LeakyReLU</span></code>
)
              –
              <div class="doc-md-description">
                <p>The activation function. Defaults to nn.LeakyReLU.</p>
              </div>
            </li>
            <li>
              <b><code>reverse_grad</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to reverse the gradient. Defaults</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>










<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint2.model.loss.AdversarialDiscriminatorLoss.forward)" href="#scprint2.model.loss.AdversarialDiscriminatorLoss.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Args:</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/loss.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_cls</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">nlayers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">,</span>
    <span class="n">reverse_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Discriminator for the adversarial training for batch correction.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The size of the input tensor.</span>
<span class="sd">        n_cls (int): The number of classes.</span>
<span class="sd">        nlayers (int, optional): The number of layers in the discriminator. Defaults to 3.</span>
<span class="sd">        activation (callable, optional): The activation function. Defaults to nn.LeakyReLU.</span>
<span class="sd">        reverse_grad (bool, optional): Whether to reverse the gradient. Defaults</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="c1"># module list</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nlayers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_cls</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reverse_grad</span> <span class="o">=</span> <span class="n">reverse_grad</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="scprint2.model.loss.AdversarialDiscriminatorLoss.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape [batch_size, embsize]</p>
              </div>
            </li>
            <li>
              <b><code>batch_labels</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape [batch_size]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [batch_size, embsize]</span>
<span class="sd">        batch_labels: Tensor, shape [batch_size]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reverse_grad</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">grad_reverse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h3 id="scprint2.model.loss.contrastive_loss" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">contrastive_loss</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Computes NT-Xent loss (InfoNCE) between two sets of vectors.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor of shape [batch_size, feature_dim]</p>
              </div>
            </li>
            <li>
              <b><code>y</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor of shape [batch_size, feature_dim]</p>
              </div>
            </li>
            <li>
              <b><code>temperature</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>Temperature parameter to scale the similarities.
Lower values make the model more confident/selective.
Typical values are between 0.1 and 0.5.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>NT-Xent loss value</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="note" open>
  <summary>Note</summary>
  <ul>
<li>Assumes x[i] and y[i] are positive pairs</li>
<li>All other combinations are considered negative pairs</li>
<li>Uses cosine similarity scaled by temperature</li>
</ul>
</details>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">contrastive_loss</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes NT-Xent loss (InfoNCE) between two sets of vectors.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Tensor of shape [batch_size, feature_dim]</span>
<span class="sd">        y: Tensor of shape [batch_size, feature_dim]</span>
<span class="sd">        temperature: Temperature parameter to scale the similarities.</span>
<span class="sd">            Lower values make the model more confident/selective.</span>
<span class="sd">            Typical values are between 0.1 and 0.5.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: NT-Xent loss value</span>

<span class="sd">    Note:</span>
<span class="sd">        - Assumes x[i] and y[i] are positive pairs</span>
<span class="sd">        - All other combinations are considered negative pairs</span>
<span class="sd">        - Uses cosine similarity scaled by temperature</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check input dimensions</span>
    <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;Input tensors must have the same shape&quot;</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Compute cosine similarity matrix</span>
    <span class="c1"># x_unsqueeze: [batch_size, 1, feature_dim]</span>
    <span class="c1"># y_unsqueeze: [1, batch_size, feature_dim]</span>
    <span class="c1"># -&gt; similarities: [batch_size, batch_size]</span>
    <span class="n">similarities</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">F</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span>
    <span class="p">)</span>

    <span class="c1"># The positive pairs are on the diagonal</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Cross entropy loss</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">similarities</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint2.model.loss.criterion_neg_log_bernoulli" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">criterion_neg_log_bernoulli</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute the negative log-likelihood of Bernoulli distribution</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">criterion_neg_log_bernoulli</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the negative log-likelihood of Bernoulli distribution</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">bernoulli</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">masked_log_probs</span> <span class="o">=</span> <span class="n">bernoulli</span><span class="o">.</span><span class="n">log_prob</span><span class="p">((</span><span class="n">target</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span> <span class="o">*</span> <span class="n">mask</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">masked_log_probs</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint2.model.loss.ecs" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">ecs</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>ecs Computes the similarity of cell embeddings based on a threshold.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>cell_emb</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>A tensor representing cell embeddings.</p>
              </div>
            </li>
            <li>
              <b><code>ecs_threshold</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.5</code>
)
              –
              <div class="doc-md-description">
                <p>A threshold for determining similarity. Defaults to 0.5.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>A tensor representing the mean of 1 minus the square of the difference between the cosine similarity and the threshold.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">ecs</span><span class="p">(</span><span class="n">cell_emb</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ecs_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ecs Computes the similarity of cell embeddings based on a threshold.</span>

<span class="sd">    Args:</span>
<span class="sd">        cell_emb (Tensor): A tensor representing cell embeddings.</span>
<span class="sd">        ecs_threshold (float, optional): A threshold for determining similarity. Defaults to 0.5.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: A tensor representing the mean of 1 minus the square of the difference between the cosine similarity and the threshold.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Here using customized cosine similarity instead of F.cosine_similarity</span>
    <span class="c1"># to avoid the pytorch issue of similarity larger than 1.0, pytorch # 78064</span>
    <span class="c1"># normalize the embedding</span>
    <span class="n">cell_emb_normed</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">cell_emb</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">cell_emb_normed</span><span class="p">,</span> <span class="n">cell_emb_normed</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>

    <span class="c1"># mask out diagnal elements</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">cos_sim</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cos_sim</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">cos_sim</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="c1"># only optimize positive similarities</span>
    <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">cos_sim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">cos_sim</span> <span class="o">-</span> <span class="n">ecs_threshold</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint2.model.loss.grad_reverse" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">grad_reverse</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>grad_reverse Reverses the gradient of the input tensor.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The input tensor whose gradient is to be reversed.</p>
              </div>
            </li>
            <li>
              <b><code>lambd</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>1.0</code>
)
              –
              <div class="doc-md-description">
                <p>The scaling factor for the reversed gradient. Defaults to 1.0.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>The input tensor with its gradient reversed during the backward pass.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">grad_reverse</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">lambd</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    grad_reverse Reverses the gradient of the input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor whose gradient is to be reversed.</span>
<span class="sd">        lambd (float, optional): The scaling factor for the reversed gradient. Defaults to 1.0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: The input tensor with its gradient reversed during the backward pass.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">GradReverse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambd</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint2.model.loss.hierarchical_classification" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">hierarchical_classification</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Computes the classification loss for a given batch of predictions and ground truth labels.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>pred</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The predicted logits for the batch. Shape: (batch_size, n_labels)</p>
              </div>
            </li>
            <li>
              <b><code>cl</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The ground truth labels for the batch. Shape: (batch_size,)</p>
              </div>
            </li>
            <li>
              <b><code>labels_hierarchy</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The hierarchical structure of the labels. Defaults to None.
A binary tensor of shape (number of parents, n_labels)
if not given, will act as a regular classification loss
see gist for more details of how one can compute it
https://gist.github.com/jkobject/5b36bc4807edb440b86644952a49781e</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="ValueError">ValueError</span></code>
              –
              <div class="doc-md-description">
                <p>If the labels_hierarchy is not found while the number of predicted
labels is smaller than the number of ground truth labels.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>The computed binary cross entropy loss for the given batch.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">hierarchical_classification</span><span class="p">(</span>
    <span class="n">pred</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">cl</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">labels_hierarchy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the classification loss for a given batch of predictions and ground truth labels.</span>

<span class="sd">    Args:</span>
<span class="sd">        pred (Tensor): The predicted logits for the batch. Shape: (batch_size, n_labels)</span>
<span class="sd">        cl (Tensor): The ground truth labels for the batch. Shape: (batch_size,)</span>
<span class="sd">        labels_hierarchy (Tensor, optional): The hierarchical structure of the labels. Defaults to None.</span>
<span class="sd">            A binary tensor of shape (number of parents, n_labels)</span>
<span class="sd">            if not given, will act as a regular classification loss</span>
<span class="sd">            see gist for more details of how one can compute it</span>
<span class="sd">            https://gist.github.com/jkobject/5b36bc4807edb440b86644952a49781e</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the labels_hierarchy is not found while the number of predicted</span>
<span class="sd">            labels is smaller than the number of ground truth labels.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: The computed binary cross entropy loss for the given batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">maxsize</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">newcl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">maxsize</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">cl</span><span class="o">.</span><span class="n">device</span>
    <span class="p">)</span>  <span class="c1"># batchsize * n_labels</span>
    <span class="c1"># if we don&#39;t know the label we set the weight to 0 else to 1</span>
    <span class="n">valid_indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">cl</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">cl</span> <span class="o">&lt;</span> <span class="n">maxsize</span><span class="p">)</span>
    <span class="n">valid_cl</span> <span class="o">=</span> <span class="n">cl</span><span class="p">[</span><span class="n">valid_indices</span><span class="p">]</span>
    <span class="n">newcl</span><span class="p">[</span><span class="n">valid_indices</span><span class="p">,</span> <span class="n">valid_cl</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">newcl</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cl</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># if we don&#39;t know the label we set the weight to 0 for all labels</span>
    <span class="n">weight</span><span class="p">[</span><span class="n">cl</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># if we have non leaf values, we don&#39;t know so we don&#39;t compute grad and set weight to 0</span>
    <span class="c1"># and add labels that won&#39;t be counted but so that we can still use them</span>
    <span class="k">if</span> <span class="n">labels_hierarchy</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">cl</span> <span class="o">&gt;=</span> <span class="n">maxsize</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
        <span class="n">is_parent</span> <span class="o">=</span> <span class="n">cl</span> <span class="o">&gt;=</span> <span class="n">maxsize</span>
        <span class="n">subset_parent_weight</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="n">is_parent</span><span class="p">]</span>
        <span class="c1"># we set the weight of the leaf elements for pred where we don&#39;t know the leaf, to 0</span>
        <span class="c1"># i.e. the elements where we will compute the max</span>
        <span class="c1"># in cl, parents are values past the maxsize</span>
        <span class="c1"># (if there is 10 leafs labels, the label 10,14, or 15 is a parent at position</span>
        <span class="c1"># row 0, 4, or 5 in the hierarchy matrix</span>
        <span class="n">subset_parent_weight</span><span class="p">[</span><span class="n">labels_hierarchy</span><span class="p">[</span><span class="n">cl</span><span class="p">[</span><span class="n">is_parent</span><span class="p">]</span> <span class="o">-</span> <span class="n">maxsize</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">weight</span><span class="p">[</span><span class="n">is_parent</span><span class="p">]</span> <span class="o">=</span> <span class="n">subset_parent_weight</span>

        <span class="c1"># we set their lead to 1 (since the weight will be zero, not really usefull..)</span>
        <span class="n">subset_parent_newcl</span> <span class="o">=</span> <span class="n">newcl</span><span class="p">[</span><span class="n">is_parent</span><span class="p">]</span>
        <span class="n">subset_parent_newcl</span><span class="p">[</span><span class="n">labels_hierarchy</span><span class="p">[</span><span class="n">cl</span><span class="p">[</span><span class="n">is_parent</span><span class="p">]</span> <span class="o">-</span> <span class="n">maxsize</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">newcl</span><span class="p">[</span><span class="n">is_parent</span><span class="p">]</span> <span class="o">=</span> <span class="n">subset_parent_newcl</span>

        <span class="c1"># all parental nodes that have a 1 in the labels_hierarchy matrix are set to 1</span>
        <span class="c1"># for each parent label / row in labels_hierarchy matrix, the addnewcl is</span>
        <span class="c1"># the max of the newcl values where the parent label is 1</span>
        <span class="n">newcl_expanded</span> <span class="o">=</span> <span class="n">newcl</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">labels_hierarchy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">addnewcl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">newcl_expanded</span> <span class="o">*</span> <span class="n">labels_hierarchy</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># for their weight, it is decreasing based on number of children they have</span>
        <span class="c1"># it is the same here as for parental labels, we don&#39;t want to compute</span>
        <span class="c1"># gradients when they are 0 meaning not parents of the true leaf label.</span>
        <span class="c1"># for now we weight related to how many labels they contain.</span>
        <span class="n">addweight</span> <span class="o">=</span> <span class="n">addnewcl</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">labels_hierarchy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>

        <span class="c1"># except if it is the cl label we know about?</span>
        <span class="n">subset_parent_weight</span> <span class="o">=</span> <span class="n">addweight</span><span class="p">[</span><span class="n">is_parent</span><span class="p">]</span>
        <span class="n">subset_parent_weight</span><span class="p">[:,</span> <span class="n">cl</span><span class="p">[</span><span class="n">is_parent</span><span class="p">]</span> <span class="o">-</span> <span class="n">maxsize</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">addweight</span><span class="p">[</span><span class="n">is_parent</span><span class="p">]</span> <span class="o">=</span> <span class="n">subset_parent_weight</span>

        <span class="c1"># we apply the same mask to the pred but now we want to compute</span>
        <span class="c1"># logsumexp instead of max since we want to keep the gradients</span>
        <span class="c1"># we also set to -inf since it is a more neutral element for logsumexp</span>
        <span class="n">pred_expanded</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">pred</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">labels_hierarchy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="p">)</span>
        <span class="n">pred_expanded</span> <span class="o">=</span> <span class="n">pred_expanded</span> <span class="o">*</span> <span class="n">labels_hierarchy</span><span class="o">.</span><span class="n">T</span>
        <span class="n">pred_expanded</span><span class="p">[</span><span class="n">pred_expanded</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span>
        <span class="n">addpred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">pred_expanded</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># we add the new labels to the cl</span>
        <span class="n">newcl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">newcl</span><span class="p">,</span> <span class="n">addnewcl</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">weight</span><span class="p">,</span> <span class="n">addweight</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pred</span><span class="p">,</span> <span class="n">addpred</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">labels_hierarchy</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">cl</span> <span class="o">&gt;=</span> <span class="n">maxsize</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;need to use labels_hierarchy for this usecase&quot;</span><span class="p">)</span>

    <span class="n">myloss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">newcl</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">myloss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint2.model.loss.masked_mae" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">masked_mae</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute the masked MAE loss between input and target.
MAE = mean absolute error</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">masked_mae</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the masked MAE loss between input and target.</span>
<span class="sd">    MAE = mean absolute error</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="nb">input</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">target</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint2.model.loss.masked_mse" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">masked_mse</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute the masked MSE loss between input and target.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">masked_mse</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the masked MSE loss between input and target.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="nb">input</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span> <span class="o">*</span> <span class="mi">10000</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="p">(</span><span class="n">target</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span> <span class="o">*</span> <span class="mi">10000</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="nb">input</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">target</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint2.model.loss.masked_nb" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">masked_nb</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute the masked negative binomial loss between input and target.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">masked_nb</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the masked negative binomial loss between input and target.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">nb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">NegativeBinomial</span><span class="p">(</span><span class="n">total_count</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">probs</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">masked_log_probs</span> <span class="o">=</span> <span class="n">nb</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">masked_log_probs</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint2.model.loss.masked_relative_error" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">masked_relative_error</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute the masked relative error between input and target.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">masked_relative_error</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the masked relative error between input and target.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">mask</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">-</span> <span class="n">target</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">target</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint2.model.loss.mse" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mse</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute the MSE loss between input and target.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">mse</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the MSE loss between input and target.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">mask</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">masked_mse</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="p">(</span><span class="n">target</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">))</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="nb">input</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span> <span class="o">*</span> <span class="mi">10000</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="p">(</span><span class="n">target</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span> <span class="o">*</span> <span class="mi">10000</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint2.model.loss.nb" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nb</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Computes the negative binomial (NB) loss.</p>
<p>This function was adapted from scvi-tools.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>target</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Ground truth data.</p>
              </div>
            </li>
            <li>
              <b><code>mu</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Means of the negative binomial distribution (must have positive support).</p>
              </div>
            </li>
            <li>
              <b><code>theta</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Inverse dispersion parameter (must have positive support).</p>
              </div>
            </li>
            <li>
              <b><code>eps</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.0001</code>
)
              –
              <div class="doc-md-description">
                <p>Numerical stability constant. Defaults to 1e-4.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>NB loss value.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nb</span><span class="p">(</span><span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mu</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the negative binomial (NB) loss.</span>

<span class="sd">    This function was adapted from scvi-tools.</span>

<span class="sd">    Args:</span>
<span class="sd">        target (Tensor): Ground truth data.</span>
<span class="sd">        mu (Tensor): Means of the negative binomial distribution (must have positive support).</span>
<span class="sd">        theta (Tensor): Inverse dispersion parameter (must have positive support).</span>
<span class="sd">        eps (float, optional): Numerical stability constant. Defaults to 1e-4.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: NB loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">theta</span><span class="o">.</span><span class="n">ndimension</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

    <span class="n">log_theta_mu_eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">theta</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_theta_mu_eps</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">target</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_theta_mu_eps</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="n">theta</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="o">-</span><span class="n">res</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint2.model.loss.within_sample" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">within_sample</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute dissimilarity between embeddings within each sample
using a combination of cosine and L2 distance</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>cell_embs</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>tensor of shape [batch_size, num_embeddings, embedding_dim]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">within_sample</span><span class="p">(</span><span class="n">cell_embs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute dissimilarity between embeddings within each sample</span>
<span class="sd">    using a combination of cosine and L2 distance</span>

<span class="sd">    Args:</span>
<span class="sd">        cell_embs: tensor of shape [batch_size, num_embeddings, embedding_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_embeddings</span><span class="p">,</span> <span class="n">emb_dim</span> <span class="o">=</span> <span class="n">cell_embs</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Normalize embeddings for cosine similarity</span>
    <span class="n">cell_embs_norm</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">cell_embs</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Compute pairwise cosine similarities</span>
    <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">cell_embs_norm</span><span class="p">,</span> <span class="n">cell_embs_norm</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

    <span class="c1"># Compute pairwise L2 distances (normalized by embedding dimension)</span>
    <span class="n">l2_dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="n">cell_embs</span><span class="p">,</span> <span class="n">cell_embs</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">emb_dim</span><span class="p">)</span>

    <span class="c1"># Create mask for pairs (excluding self-similarity)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cos_sim</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Combine losses:</span>
    <span class="c1"># - High cosine similarity should be penalized</span>
    <span class="c1"># - Small L2 distance should be penalized</span>
    <span class="n">cos_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">cos_sim</span> <span class="o">*</span> <span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">l2_loss</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">l2_dist</span> <span class="o">*</span> <span class="n">mask</span> <span class="o">+</span> <span class="mf">1e-3</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">cos_loss</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">l2_loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint2.model.loss.zinb" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">zinb</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Computes zero-inflated negative binomial (ZINB) loss.</p>
<p>This function was modified from scvi-tools.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>target</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of ground truth data.</p>
              </div>
            </li>
            <li>
              <b><code>mu</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of means of the negative binomial (must have positive support).</p>
              </div>
            </li>
            <li>
              <b><code>theta</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of inverse dispersion parameter (must have positive support).</p>
              </div>
            </li>
            <li>
              <b><code>pi</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of logits of the dropout parameter (real support).</p>
              </div>
            </li>
            <li>
              <b><code>eps</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.0001</code>
)
              –
              <div class="doc-md-description">
                <p>Numerical stability constant. Defaults to 1e-4.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>ZINB loss value.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">zinb</span><span class="p">(</span>
    <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">mu</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">pi</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
    <span class="n">mask</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes zero-inflated negative binomial (ZINB) loss.</span>

<span class="sd">    This function was modified from scvi-tools.</span>

<span class="sd">    Args:</span>
<span class="sd">        target (Tensor): Torch Tensor of ground truth data.</span>
<span class="sd">        mu (Tensor): Torch Tensor of means of the negative binomial (must have positive support).</span>
<span class="sd">        theta (Tensor): Torch Tensor of inverse dispersion parameter (must have positive support).</span>
<span class="sd">        pi (Tensor): Torch Tensor of logits of the dropout parameter (real support).</span>
<span class="sd">        eps (float, optional): Numerical stability constant. Defaults to 1e-4.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: ZINB loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">#  uses log(sigmoid(x)) = -softplus(-x)</span>
    <span class="n">softplus_pi</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="o">-</span><span class="n">pi</span><span class="p">)</span>
    <span class="c1"># eps to make it positive support and taking the log</span>
    <span class="n">log_theta_mu_eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">pi_theta_log</span> <span class="o">=</span> <span class="o">-</span><span class="n">pi</span> <span class="o">+</span> <span class="n">theta</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_theta_mu_eps</span><span class="p">)</span>

    <span class="n">case_zero</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">pi_theta_log</span><span class="p">)</span> <span class="o">-</span> <span class="n">softplus_pi</span>
    <span class="n">mul_case_zero</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">((</span><span class="n">target</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">case_zero</span><span class="p">)</span>

    <span class="n">case_non_zero</span> <span class="o">=</span> <span class="p">(</span>
        <span class="o">-</span><span class="n">softplus_pi</span>
        <span class="o">+</span> <span class="n">pi_theta_log</span>
        <span class="o">+</span> <span class="n">target</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_theta_mu_eps</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="n">theta</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">mul_case_non_zero</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">((</span><span class="n">target</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">case_non_zero</span><span class="p">)</span>

    <span class="n">res</span> <span class="o">=</span> <span class="n">mul_case_zero</span> <span class="o">+</span> <span class="n">mul_case_non_zero</span>
    <span class="c1"># we want to minize the loss but maximize the log likelyhood</span>
    <span class="k">if</span> <span class="n">mask</span><span class="p">:</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">target</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">res</span> <span class="o">*</span> <span class="n">mask</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">res</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">res</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h2 id="utils">utils</h2>


<div class="doc doc-object doc-module">



<h2 id="scprint2.model.utils" class="doc doc-heading">
            <code>scprint2.model.utils</code>


</h2>

    <div class="doc doc-contents first">










<p><span class="doc-section-title">Classes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="Attention (scprint2.model.utils.Attention)" href="#scprint2.model.utils.Attention">Attention</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="WeightedMasker (scprint2.model.utils.WeightedMasker)" href="#scprint2.model.utils.WeightedMasker">WeightedMasker</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
      </tbody>
    </table>




<p><span class="doc-section-title">Functions:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="downsample_profile (scprint2.model.utils.downsample_profile)" href="#scprint2.model.utils.downsample_profile">downsample_profile</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>This function downsamples the expression profile of a given single cell RNA matrix.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="make_adata (scprint2.model.utils.make_adata)" href="#scprint2.model.utils.make_adata">make_adata</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>This function creates an AnnData object from the given input parameters.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="simple_masker (scprint2.model.utils.simple_masker)" href="#scprint2.model.utils.simple_masker">simple_masker</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Randomly mask a batch of data.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="test (scprint2.model.utils.test)" href="#scprint2.model.utils.test">test</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Test the given model on the full set of benchmarks and save the results to JSON files.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="zinb_sample (scprint2.model.utils.zinb_sample)" href="#scprint2.model.utils.zinb_sample">zinb_sample</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>





<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="scprint2.model.utils.Attention" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">Attention</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Initialize the Attention class.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>gene_dim</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the gene.</p>
              </div>
            </li>
            <li>
              <b><code>additional_tokens</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>0</code>
)
              –
              <div class="doc-md-description">
                <p>The number of additional tokens to add.</p>
              </div>
            </li>
            <li>
              <b><code>precomp_attn</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to compute attention or it is precomputed</p>
              </div>
            </li>
            <li>
              <b><code>apply_softmax</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to apply softmax to the attention.</p>
              </div>
            </li>
            <li>
              <b><code>sum_heads</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to sum the heads.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>










<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="add_attn (scprint2.model.utils.Attention.add_attn)" href="#scprint2.model.utils.Attention.add_attn">add_attn</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Aggregate the attention or data based on the precomp_attn flag.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="add_qk (scprint2.model.utils.Attention.add_qk)" href="#scprint2.model.utils.Attention.add_qk">add_qk</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Add data to the internal storage.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="get (scprint2.model.utils.Attention.get)" href="#scprint2.model.utils.Attention.get">get</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Get the aggregated attention or data.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/utils.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">gene_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">precomp_attn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">apply_softmax</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">sum_heads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">additional_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initialize the Attention class.</span>

<span class="sd">    Args:</span>
<span class="sd">        gene_dim (int): The dimension of the gene.</span>
<span class="sd">        additional_tokens (int): The number of additional tokens to add.</span>
<span class="sd">        precomp_attn (bool): Whether to compute attention or it is precomputed</span>
<span class="sd">        apply_softmax (bool): Whether to apply softmax to the attention.</span>
<span class="sd">        sum_heads (bool): Whether to sum the heads.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">gene_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">additional_tokens</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">apply_softmax</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">apply_softmax</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sum_heads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">sum_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">precomp_attn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">precomp_attn</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">speciesloc</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="scprint2.model.utils.Attention.add_attn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">add_attn</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Aggregate the attention or data based on the precomp_attn flag.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="typing.List">List</span>[<span title="torch.Tensor">Tensor</span>]</code>)
              –
              <div class="doc-md-description">
                <p>List of tensors to aggregate. Tensor of size (batch, seq_len, 2, heads, emb)</p>
              </div>
            </li>
            <li>
              <b><code>pos</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Position tensor.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">add_attn</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">expr</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Aggregate the attention or data based on the precomp_attn flag.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (List[Tensor]): List of tensors to aggregate. Tensor of size (batch, seq_len, 2, heads, emb)</span>
<span class="sd">        pos (Tensor): Position tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span><span class="p">,</span>
            <span class="p">],</span>
            <span class="n">device</span><span class="o">=</span><span class="n">pos</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pos</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">elem</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_softmax</span><span class="p">:</span>
            <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
                <span class="n">elem</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
                <span class="o">@</span> <span class="n">elem</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">expr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">attn</span><span class="p">[:,</span> <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span> <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span> <span class="p">:]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">attn</span><span class="p">[:,</span> <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span> <span class="p">:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span> <span class="p">:]</span>
                    <span class="o">*</span> <span class="p">(</span><span class="n">expr</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="o">*</span> <span class="p">(</span><span class="n">expr</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">+=</span> <span class="n">attn</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="p">(</span>
                    <span class="n">elem</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
                    <span class="o">@</span> <span class="n">elem</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">div</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.utils.Attention.add_qk" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">add_qk</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Add data to the internal storage.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="typing.List">List</span>[<span title="torch.Tensor">Tensor</span>]</code>)
              –
              <div class="doc-md-description">
                <p>List of tensors to add.</p>
              </div>
            </li>
            <li>
              <b><code>pos</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Position tensor.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">add_qk</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">expr</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add data to the internal storage.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (List[Tensor]): List of tensors to add.</span>
<span class="sd">        pos (Tensor): Position tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># this is a debugger line</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]),</span>
            <span class="n">device</span><span class="o">=</span><span class="n">pos</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pos</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>  <span class="c1"># batch size</span>
        <span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pos</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
                <span class="n">pos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">speciesloc</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>  <span class="c1"># number of layers * heads</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="p">[</span><span class="n">loc</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.utils.Attention.get" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Get the aggregated attention or data.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="typing.Optional">Optional</span>[<span title="numpy.ndarray">ndarray</span>]</code>
              –
              <div class="doc-md-description">
                <p>Optional[np.ndarray]: The aggregated attention or data.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the aggregated attention or data.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Optional[np.ndarray]: The aggregated attention or data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">precomp_attn</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="c1"># shape is (layers, genes, qkv, heads, emb)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h3 id="scprint2.model.utils.WeightedMasker" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">WeightedMasker</span></code>

</h3>


    <div class="doc doc-contents ">



        <p>Randomly mask a batch of data.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>genes</code></b>
                  (<code><span title="typing.List">List</span>[<span title="str">str</span>]</code>)
              –
              <div class="doc-md-description">
                <p>The list of genes the model might see.</p>
              </div>
            </li>
            <li>
              <b><code>TFs</code></b>
                  (<code><span title="typing.List">List</span>[<span title="str">str</span>]</code>, default:
                      <code><a class="autorefs autorefs-internal" title="fileToList (scprint2.utils.fileToList)" href="../utils/#scprint2.utils.utils.fileToList">fileToList</a>(<span title="scprint2.model.utils.FILEDIR">FILEDIR</span> + &#39;/../../data/main/TFs.txt&#39;)</code>
)
              –
              <div class="doc-md-description">
                <p>The list of TFs the model can drop.</p>
              </div>
            </li>
            <li>
              <b><code>tf_weight</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>10</code>
)
              –
              <div class="doc-md-description">
                <p>How likely it is to drop a non TF compared to a TF.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>











                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/utils.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">genes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">TFs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">fileToList</span><span class="p">(</span><span class="n">FILEDIR</span> <span class="o">+</span> <span class="s2">&quot;/../../data/main/TFs.txt&quot;</span><span class="p">),</span>
    <span class="n">tf_weight</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly mask a batch of data.</span>

<span class="sd">    Args:</span>
<span class="sd">        genes (List[str]): The list of genes the model might see.</span>
<span class="sd">        TFs (List[str]): The list of TFs the model can drop.</span>
<span class="sd">        tf_weight (float): How likely it is to drop a non TF compared to a TF.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">TFs</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">TFs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">tf_weight</span> <span class="k">if</span> <span class="n">gene</span> <span class="ow">in</span> <span class="n">TFs</span> <span class="k">else</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">gene</span> <span class="ow">in</span> <span class="n">genes</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_to_drop</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">==</span> <span class="n">tf_weight</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tf_weight</span> <span class="o">=</span> <span class="n">tf_weight</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">












  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h3 id="scprint2.model.utils.downsample_profile" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">downsample_profile</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>This function downsamples the expression profile of a given single cell RNA matrix.</p>
<p>The noise is applied based on the renoise parameter,
the total counts of the matrix, and the number of genes. The function first calculates the noise
threshold (scaler) based on the renoise parameter. It then generates an initial matrix count by
applying a Poisson distribution to a random tensor scaled by the total counts and the number of genes.
The function then models the sampling zeros by applying a Poisson distribution to a random tensor
scaled by the noise threshold, the total counts, and the number of genes. The function also models
the technical zeros by generating a random tensor and comparing it to the noise threshold. The final
matrix count is calculated by subtracting the sampling zeros from the initial matrix count and
multiplying by the technical zeros. The function ensures that the final matrix count is not less
than zero by taking the maximum of the final matrix count and a tensor of zeros. The function
returns the final matrix count.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>mat</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The input matrix.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code><span title="float">float</span></code>)
              –
              <div class="doc-md-description">
                <p>The renoise parameter.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="torch.Tensor">Tensor</span></code>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: The matrix count after applying noise.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">downsample_profile</span><span class="p">(</span><span class="n">mat</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;new&quot;</span><span class="p">,</span> <span class="n">randsamp</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function downsamples the expression profile of a given single cell RNA matrix.</span>

<span class="sd">    The noise is applied based on the renoise parameter,</span>
<span class="sd">    the total counts of the matrix, and the number of genes. The function first calculates the noise</span>
<span class="sd">    threshold (scaler) based on the renoise parameter. It then generates an initial matrix count by</span>
<span class="sd">    applying a Poisson distribution to a random tensor scaled by the total counts and the number of genes.</span>
<span class="sd">    The function then models the sampling zeros by applying a Poisson distribution to a random tensor</span>
<span class="sd">    scaled by the noise threshold, the total counts, and the number of genes. The function also models</span>
<span class="sd">    the technical zeros by generating a random tensor and comparing it to the noise threshold. The final</span>
<span class="sd">    matrix count is calculated by subtracting the sampling zeros from the initial matrix count and</span>
<span class="sd">    multiplying by the technical zeros. The function ensures that the final matrix count is not less</span>
<span class="sd">    than zero by taking the maximum of the final matrix count and a tensor of zeros. The function</span>
<span class="sd">    returns the final matrix count.</span>

<span class="sd">    Args:</span>
<span class="sd">        mat (torch.Tensor): The input matrix.</span>
<span class="sd">        dropout (float): The renoise parameter.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The matrix count after applying noise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Randomly drop on average N counts to each element of expression using a heavy tail Gaussian distribution</span>
    <span class="c1"># here we try to get the scale of the distribution so as to remove the right number of counts from each gene</span>
    <span class="c1"># https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02601-5#:~:text=Zero%20measurements%20in%20scRNA%2Dseq,generation%20of%20scRNA%2Dseq%20data.</span>
    <span class="k">if</span> <span class="n">randsamp</span><span class="p">:</span>
        <span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="n">dropout</span>
        <span class="n">dropout</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">dropout</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
            <span class="k">else</span> <span class="n">dropout</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;old&quot;</span><span class="p">:</span>
        <span class="n">totcounts</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ngenes</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">tnoise</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># we model the sampling zeros (dropping 30% of the reads)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="o">*</span> <span class="p">((</span><span class="n">tnoise</span> <span class="o">*</span> <span class="n">totcounts</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">ngenes</span><span class="p">))</span>
        <span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="c1"># we model the technical zeros (dropping 50% of the genes)</span>
        <span class="n">drop</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">tnoise</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>

        <span class="n">mat</span> <span class="o">=</span> <span class="p">(</span><span class="n">mat</span> <span class="o">-</span> <span class="n">res</span><span class="p">)</span> <span class="o">*</span> <span class="n">drop</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
            <span class="n">mat</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                <span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;jules&quot;</span><span class="p">:</span>
        <span class="n">scaler</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">notdrop</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span>
                <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="o">&lt;</span> <span class="n">scaler</span>
        <span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="n">notdrop</span><span class="p">[</span><span class="n">mat</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># apply the dropout after the poisson, right?</span>
        <span class="k">return</span> <span class="n">notdrop</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">mat</span> <span class="o">*</span> <span class="n">scaler</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;new&quot;</span><span class="p">:</span>
        <span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span> <span class="o">*</span> <span class="mf">1.1</span>
        <span class="c1"># we model the sampling zeros (dropping 30% of the reads)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">poisson</span><span class="p">((</span><span class="n">mat</span> <span class="o">*</span> <span class="p">(</span><span class="n">dropout</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)))</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="c1"># we model the technical zeros (dropping 50% of the genes)</span>
        <span class="n">notdrop</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">dropout</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="n">mat</span> <span class="o">=</span> <span class="p">(</span><span class="n">mat</span> <span class="o">-</span> <span class="n">res</span><span class="p">)</span> <span class="o">*</span> <span class="n">notdrop</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
            <span class="n">mat</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                <span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;method </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2"> not recognized&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint2.model.utils.make_adata" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">make_adata</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>This function creates an AnnData object from the given input parameters.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>genes</code></b>
                  (<code><span title="list">list</span></code>)
              –
              <div class="doc-md-description">
                <p>List of genes that will be used as variable names.</p>
              </div>
            </li>
            <li>
              <b><code>embs</code></b>
                  (<code><span title="torch.Tensor">Tensor</span> | <span title="typing.Dict">Dict</span></code>)
              –
              <div class="doc-md-description">
                <p>Embeddings of the cells. The shape of the tensor is (n_cells, n_features).
if multiple, it is a dict of name -&gt; tensor</p>
              </div>
            </li>
            <li>
              <b><code>pos</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Positions of the cells. The shape of the tensor is (n_cells,).</p>
              </div>
            </li>
            <li>
              <b><code>expr_pred</code></b>
                  (<code><span title="typing.List">List</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Predicted expression. The shape of the tensors are (n_cells, n_genes).
the first is mu, the second theta, the third pi if present</p>
              </div>
            </li>
            <li>
              <b><code>classes</code></b>
                  (<code><span title="list">list</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>List of classes, the order should be the same as in the pred and gtclass tensors.</p>
              </div>
            </li>
            <li>
              <b><code>pred</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Predicted labels. The shape of the tensor is (n_cells, n_classes). Default is None.</p>
              </div>
            </li>
            <li>
              <b><code>label_decoders</code></b>
                  (<code><span title="dict">dict</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Dictionary to map class codes to class names. Default is None.</p>
              </div>
            </li>
            <li>
              <b><code>labels_hierarchy</code></b>
                  (<code><span title="dict">dict</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Dictionary representing the hierarchy of labels. Default is {}. see the model for defintion.</p>
              </div>
            </li>
            <li>
              <b><code>gtclass</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Ground truth class values. Default is None.</p>
              </div>
            </li>
            <li>
              <b><code>doplot</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to generate plots. Default is True.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="anndata.AnnData">AnnData</span></code>
              –
              <div class="doc-md-description">
                <p>anndata.AnnData: The created AnnData object.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">make_adata</span><span class="p">(</span>
    <span class="n">genes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">embs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]],</span>
    <span class="n">pos</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">expr_pred</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">classes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">pred</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">label_decoders</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">labels_hierarchy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">gtclass</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">doplot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AnnData</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function creates an AnnData object from the given input parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        genes (list): List of genes that will be used as variable names.</span>
<span class="sd">        embs (torch.Tensor|Dict): Embeddings of the cells. The shape of the tensor is (n_cells, n_features).</span>
<span class="sd">            if multiple, it is a dict of name -&gt; tensor</span>
<span class="sd">        pos (torch.Tensor): Positions of the cells. The shape of the tensor is (n_cells,).</span>
<span class="sd">        expr_pred (List[torch.Tensor]): Predicted expression. The shape of the tensors are (n_cells, n_genes).</span>
<span class="sd">            the first is mu, the second theta, the third pi if present</span>
<span class="sd">        classes (list): List of classes, the order should be the same as in the pred and gtclass tensors.</span>
<span class="sd">        pred (torch.Tensor, optional): Predicted labels. The shape of the tensor is (n_cells, n_classes). Default is None.</span>
<span class="sd">        label_decoders (dict, optional): Dictionary to map class codes to class names. Default is None.</span>
<span class="sd">        labels_hierarchy (dict, optional): Dictionary representing the hierarchy of labels. Default is {}. see the model for defintion.</span>
<span class="sd">        gtclass (torch.Tensor, optional): Ground truth class values. Default is None.</span>
<span class="sd">        doplot (bool, optional): Whether to generate plots. Default is True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        anndata.AnnData: The created AnnData object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;logging the anndata&quot;</span><span class="p">)</span>
    <span class="n">colname</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">pred</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
        <span class="c1"># label decoders is not cls_decoders. one is a dict to map class codes (ints)</span>
        <span class="c1"># to class names the other is the module the predict the class</span>
        <span class="k">if</span> <span class="n">label_decoders</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="p">[</span><span class="n">label_decoders</span><span class="p">[</span><span class="n">classes</span><span class="p">[</span><span class="n">i</span><span class="p">]][</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">name</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">obs</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
                <span class="p">]</span>
            <span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="k">if</span> <span class="n">gtclass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">colname</span> <span class="o">+=</span> <span class="n">classes</span>
            <span class="n">nobs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">gtclass</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">label_decoders</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">nobs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="p">[</span><span class="n">label_decoders</span><span class="p">[</span><span class="n">classes</span><span class="p">[</span><span class="n">i</span><span class="p">]][</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">name</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">nobs</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
                    <span class="p">]</span>
                <span class="p">)</span><span class="o">.</span><span class="n">T</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">obs</span><span class="p">,</span> <span class="n">nobs</span><span class="p">])</span>

    <span class="n">n_cells</span> <span class="o">=</span> <span class="n">embs</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">embs</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">genes</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">pos</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">minval</span> <span class="o">=</span> <span class="n">pos</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
        <span class="n">maxval</span> <span class="o">=</span> <span class="n">pos</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="n">genes</span> <span class="o">=</span> <span class="n">genes</span><span class="p">[</span><span class="n">minval</span> <span class="p">:</span> <span class="n">maxval</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">genes</span><span class="p">)</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">pos</span> <span class="o">-</span> <span class="n">minval</span>
        <span class="n">mu_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_cells</span><span class="p">,</span> <span class="n">size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">pos</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="c1"># Create empty array with same shape as expr_pred[0]</span>
        <span class="c1"># Fill array with values from expr_pred[0]</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_cells</span><span class="p">):</span>
            <span class="n">mu_array</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">pos</span><span class="p">[</span><span class="n">idx</span><span class="p">]]</span> <span class="o">=</span> <span class="n">expr_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">exist</span> <span class="o">=</span> <span class="n">mu_array</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span>
        <span class="n">mu_array</span> <span class="o">=</span> <span class="n">mu_array</span><span class="p">[:,</span> <span class="n">exist</span><span class="p">]</span>
        <span class="n">mu_array</span><span class="p">[</span><span class="n">mu_array</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;scprint_mu&quot;</span><span class="p">:</span> <span class="n">mu_array</span><span class="p">,</span>
            <span class="c1">#  &quot;used_scprint&quot;: csr_matrix(pos),</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">expr_pred</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">theta_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_cells</span><span class="p">,</span> <span class="n">size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="c1"># Fill array with values from expr_pred[0]</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_cells</span><span class="p">):</span>
                <span class="n">theta_array</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">pos</span><span class="p">[</span><span class="n">idx</span><span class="p">]]</span> <span class="o">=</span> <span class="n">expr_pred</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="n">layers</span><span class="p">[</span><span class="s2">&quot;scprint_theta&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">theta_array</span><span class="p">[:,</span> <span class="n">exist</span><span class="p">]</span>

            <span class="n">pi_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_cells</span><span class="p">,</span> <span class="n">size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="c1"># Fill array with values from expr_pred[0]</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_cells</span><span class="p">):</span>
                <span class="n">pi_array</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">pos</span><span class="p">[</span><span class="n">idx</span><span class="p">]]</span> <span class="o">=</span> <span class="n">expr_pred</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="n">layers</span><span class="p">[</span><span class="s2">&quot;scprint_pi&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pi_array</span><span class="p">[:,</span> <span class="n">exist</span><span class="p">]</span>
        <span class="n">genes</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">genes</span><span class="p">)</span> <span class="k">if</span> <span class="n">exist</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">genes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">adata</span> <span class="o">=</span> <span class="n">AnnData</span><span class="p">(</span>
        <span class="n">X</span><span class="o">=</span><span class="n">csr_matrix</span><span class="p">((</span><span class="n">n_cells</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">genes</span><span class="p">))),</span>
        <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span>
        <span class="n">obs</span><span class="o">=</span><span class="p">(</span>
            <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                <span class="n">obs</span><span class="p">,</span>
                <span class="n">columns</span><span class="o">=</span><span class="n">colname</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">pred</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="kc">None</span>
        <span class="p">),</span>
        <span class="n">var</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">genes</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">embs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">adata</span><span class="o">.</span><span class="n">obsm</span><span class="p">[</span><span class="s2">&quot;scprint_emb_&quot;</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">rep</span> <span class="o">=</span> <span class="s2">&quot;scprint_emb_&quot;</span> <span class="o">+</span> <span class="n">k</span>
    <span class="k">del</span> <span class="n">embs</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="n">labels_hierarchy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">labels_hierarchy</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="n">pred</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">clss</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">gtclass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">tr</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="n">clss</span><span class="p">]),</span> <span class="n">clss</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">tr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s2">&quot;conv_&quot;</span> <span class="o">+</span> <span class="n">clss</span><span class="p">]</span> <span class="o">=</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="n">clss</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">tr</span><span class="p">)</span>
            <span class="n">tr</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">clss</span><span class="p">]),</span> <span class="n">clss</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">tr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">clss</span><span class="p">]</span> <span class="o">=</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">clss</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">tr</span><span class="p">)</span>
            <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="n">label_decoders</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">gtclass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">class_topred</span> <span class="o">=</span> <span class="n">label_decoders</span><span class="p">[</span><span class="n">clss</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">clss</span> <span class="ow">in</span> <span class="n">labels_hierarchy</span><span class="p">:</span>
                    <span class="n">cur_labels_hierarchy</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="n">label_decoders</span><span class="p">[</span><span class="n">clss</span><span class="p">][</span><span class="n">k</span><span class="p">]:</span> <span class="p">[</span><span class="n">label_decoders</span><span class="p">[</span><span class="n">clss</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">v</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">labels_hierarchy</span><span class="p">[</span><span class="n">clss</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                    <span class="p">}</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">cur_labels_hierarchy</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">for</span> <span class="n">pred</span><span class="p">,</span> <span class="n">true</span> <span class="ow">in</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">clss</span><span class="p">,</span> <span class="n">clss</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">pred</span> <span class="o">==</span> <span class="n">true</span><span class="p">:</span>
                        <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                        <span class="k">continue</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels_hierarchy</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">true</span> <span class="ow">in</span> <span class="n">cur_labels_hierarchy</span><span class="p">:</span>
                            <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred</span> <span class="ow">in</span> <span class="n">cur_labels_hierarchy</span><span class="p">[</span><span class="n">true</span><span class="p">])</span>
                        <span class="k">elif</span> <span class="n">true</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">class_topred</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;true label </span><span class="si">{</span><span class="n">true</span><span class="si">}</span><span class="s2"> not in available classes&quot;</span>
                            <span class="p">)</span>
                        <span class="k">elif</span> <span class="n">true</span> <span class="o">!=</span> <span class="s2">&quot;unknown&quot;</span><span class="p">:</span>
                            <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">true</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">class_topred</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;true label </span><span class="si">{</span><span class="n">true</span><span class="si">}</span><span class="s2"> not in available classes&quot;</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">true</span> <span class="o">!=</span> <span class="s2">&quot;unknown&quot;</span><span class="p">:</span>
                        <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">pass</span>
                <span class="n">accuracy</span><span class="p">[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">clss</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">adata</span><span class="o">.</span><span class="n">obs</span> <span class="o">=</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;category&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">adata</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">doplot</span> <span class="ow">and</span> <span class="n">adata</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">100</span><span class="p">:</span>
        <span class="n">sc</span><span class="o">.</span><span class="n">pp</span><span class="o">.</span><span class="n">neighbors</span><span class="p">(</span><span class="n">adata</span><span class="p">,</span> <span class="n">use_rep</span><span class="o">=</span><span class="n">rep</span><span class="p">)</span>
        <span class="n">sc</span><span class="o">.</span><span class="n">tl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span><span class="n">adata</span><span class="p">)</span>
        <span class="n">sc</span><span class="o">.</span><span class="n">tl</span><span class="o">.</span><span class="n">leiden</span><span class="p">(</span><span class="n">adata</span><span class="p">,</span> <span class="n">key_added</span><span class="o">=</span><span class="s2">&quot;sprint_leiden&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">gtclass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">color</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">i</span>
                <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="s2">&quot;conv_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="k">if</span> <span class="s2">&quot;conv_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="o">.</span><span class="n">columns</span> <span class="k">else</span> <span class="n">i</span>
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">classes</span>
                    <span class="p">],</span>
                    <span class="p">[</span>
                        <span class="p">(</span>
                            <span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">i</span>
                            <span class="k">if</span> <span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="o">.</span><span class="n">columns</span>
                            <span class="k">else</span> <span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">i</span>
                        <span class="p">)</span>
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">classes</span>
                    <span class="p">],</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pair</span>
            <span class="p">]</span>
            <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span>
                <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">)</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">color</span><span class="p">):</span>
                    <span class="n">sc</span><span class="o">.</span><span class="n">pl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span>
                        <span class="n">adata</span><span class="p">,</span>
                        <span class="n">color</span><span class="o">=</span><span class="n">col</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">],</span>
                        <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">acc</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
                    <span class="k">if</span> <span class="s2">&quot;pred_&quot;</span> <span class="ow">in</span> <span class="n">col</span> <span class="ow">and</span> <span class="n">col</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;conv_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">in</span> <span class="n">accuracy</span><span class="p">:</span>
                        <span class="n">acc</span> <span class="o">=</span> <span class="s2">&quot; (accuracy: </span><span class="si">{:.2f}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="n">accuracy</span><span class="p">[</span><span class="n">col</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;conv_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
                        <span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">col</span> <span class="o">+</span> <span class="s2">&quot; UMAP&quot;</span> <span class="o">+</span> <span class="n">acc</span><span class="p">)</span>
                    <span class="k">if</span> <span class="s2">&quot;cell_type&quot;</span> <span class="ow">in</span> <span class="n">col</span><span class="p">:</span>
                        <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="s2">&quot;x-small&quot;</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;UMAP1&quot;</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;UMAP2&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">color</span><span class="p">):</span>
                    <span class="n">sc</span><span class="o">.</span><span class="n">pl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span>
                        <span class="n">adata</span><span class="p">,</span>
                        <span class="n">color</span><span class="o">=</span><span class="n">col</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">],</span>
                        <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">acc</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
                    <span class="k">if</span> <span class="s2">&quot;pred_&quot;</span> <span class="ow">in</span> <span class="n">col</span> <span class="ow">and</span> <span class="n">col</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;conv_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">in</span> <span class="n">accuracy</span><span class="p">:</span>
                        <span class="n">acc</span> <span class="o">=</span> <span class="s2">&quot; (accuracy: </span><span class="si">{:.2f}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="n">accuracy</span><span class="p">[</span><span class="n">col</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;conv_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
                        <span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">col</span> <span class="o">+</span> <span class="s2">&quot; UMAP&quot;</span> <span class="o">+</span> <span class="n">acc</span><span class="p">)</span>
                    <span class="k">if</span> <span class="s2">&quot;cell_type&quot;</span> <span class="ow">in</span> <span class="n">col</span><span class="p">:</span>
                        <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="s2">&quot;x-small&quot;</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;UMAP1&quot;</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;UMAP2&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">color</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">(</span>
                    <span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">i</span>
                    <span class="k">if</span> <span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="o">.</span><span class="n">columns</span>
                    <span class="k">else</span> <span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">i</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">classes</span>
            <span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">)</span> <span class="o">*</span> <span class="mi">8</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">color</span><span class="p">):</span>
                    <span class="n">sc</span><span class="o">.</span><span class="n">pl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span>
                        <span class="n">adata</span><span class="p">,</span>
                        <span class="n">color</span><span class="o">=</span><span class="n">col</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                        <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">acc</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
                    <span class="k">if</span> <span class="s2">&quot;pred_&quot;</span> <span class="ow">in</span> <span class="n">col</span> <span class="ow">and</span> <span class="n">col</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;conv_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">in</span> <span class="n">accuracy</span><span class="p">:</span>
                        <span class="n">acc</span> <span class="o">=</span> <span class="s2">&quot; (accuracy: </span><span class="si">{:.2f}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="n">accuracy</span><span class="p">[</span><span class="n">col</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;conv_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
                        <span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">col</span> <span class="o">+</span> <span class="s2">&quot; UMAP of &quot;</span> <span class="o">+</span> <span class="n">rep</span> <span class="o">+</span> <span class="s2">&quot; embedding &quot;</span> <span class="o">+</span> <span class="n">acc</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;UMAP1&quot;</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;UMAP2&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">fig</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">pl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span>
                    <span class="n">adata</span><span class="p">,</span>
                    <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span>
                    <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">return_fig</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">fig</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">adata</span><span class="p">,</span> <span class="n">fig</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint2.model.utils.simple_masker" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">simple_masker</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Randomly mask a batch of data.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>shape</code></b>
                  (<code><span title="typing.List">List</span>[<span title="int">int</span>]</code>)
              –
              <div class="doc-md-description">
                <p>The shape of the data.</p>
              </div>
            </li>
            <li>
              <b><code>mask_ratio</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.15</code>
)
              –
              <div class="doc-md-description">
                <p>The ratio of genes to mask, default to 0.15.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="torch.Tensor">Tensor</span></code>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: A tensor of masked data.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">simple_masker</span><span class="p">(</span>
    <span class="n">shape</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">mask_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly mask a batch of data.</span>

<span class="sd">    Args:</span>
<span class="sd">        shape (List[int]): The shape of the data.</span>
<span class="sd">        mask_ratio (float): The ratio of genes to mask, default to 0.15.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: A tensor of masked data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">mask_ratio</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint2.model.utils.test" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">test</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Test the given model on the full set of benchmarks and save the results to JSON files.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>model</code></b>
                  (<code><span title="torch.nn.Module">Module</span></code>)
              –
              <div class="doc-md-description">
                <p>The model to be tested.</p>
              </div>
            </li>
            <li>
              <b><code>filedir</code></b>
                  (<code><span title="str">str</span></code>)
              –
              <div class="doc-md-description">
                <p>The directory where the data files are located.</p>
              </div>
            </li>
            <li>
              <b><code>do_class</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to perform classification. Defaults to True.</p>
              </div>
            </li>
            <li>
              <b><code>maxcells_grn</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>1024</code>
)
              –
              <div class="doc-md-description">
                <p>Maximum cells for GRN analysis. Defaults to 1024.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>None</code>
              –
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">test</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">filedir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">do_class</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">maxcells_grn</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Test the given model on the full set of benchmarks and save the results to JSON files.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (torch.nn.Module): The model to be tested.</span>
<span class="sd">        filedir (str): The directory where the data files are located.</span>
<span class="sd">        do_class (bool): Whether to perform classification. Defaults to True.</span>
<span class="sd">        maxcells_grn (int): Maximum cells for GRN analysis. Defaults to 1024.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">tot</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">EMBEDDING_DATASETS</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">embbed_task</span><span class="o">.</span><span class="n">default_benchmark</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">dataset</span><span class="o">=</span><span class="n">path</span><span class="p">,</span>
            <span class="n">do_class</span><span class="o">=</span><span class="n">do_class</span><span class="p">,</span>
            <span class="n">coarse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">tot</span><span class="p">[</span><span class="s2">&quot;embed_&quot;</span> <span class="o">+</span> <span class="n">dataset</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span>
        <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;emb_&quot;</span> <span class="o">+</span> <span class="n">dataset</span> <span class="o">+</span> <span class="s2">&quot;/scib&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;scib&quot;</span><span class="p">][</span><span class="s2">&quot;Total&quot;</span><span class="p">]),</span>
                <span class="s2">&quot;emb_&quot;</span> <span class="o">+</span> <span class="n">dataset</span> <span class="o">+</span> <span class="s2">&quot;/scib_bio&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;scib&quot;</span><span class="p">][</span><span class="s2">&quot;Bio conservation&quot;</span><span class="p">]),</span>
                <span class="s2">&quot;emb_&quot;</span>
                <span class="o">+</span> <span class="n">dataset</span>
                <span class="o">+</span> <span class="s2">&quot;/scib_batch&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;scib&quot;</span><span class="p">][</span><span class="s2">&quot;Batch correction&quot;</span><span class="p">]),</span>
                <span class="s2">&quot;emb_&quot;</span>
                <span class="o">+</span> <span class="n">dataset</span>
                <span class="o">+</span> <span class="s2">&quot;/ct_class&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                    <span class="n">res</span><span class="p">[</span><span class="s2">&quot;classif&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;cell_type_ontology_term_id&quot;</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;macro&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">do_class</span>
                    <span class="k">else</span> <span class="mi">0</span>
                <span class="p">),</span>
                <span class="s2">&quot;emb_&quot;</span>
                <span class="o">+</span> <span class="n">dataset</span>
                <span class="o">+</span> <span class="s2">&quot;/ct_class_macro&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                    <span class="n">res</span><span class="p">[</span><span class="s2">&quot;classif&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;cell_type_ontology_term_id&quot;</span><span class="p">,</span> <span class="p">{})</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;macro&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">do_class</span>
                    <span class="k">else</span> <span class="mi">0</span>
                <span class="p">),</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
        <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">filepath</span> <span class="ow">in</span> <span class="n">DENOISE_DATASETS</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">denoise_task</span><span class="o">.</span><span class="n">default_benchmark</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="n">filepath</span><span class="p">)</span>
        <span class="n">tot</span><span class="p">[</span><span class="s2">&quot;denoise_&quot;</span> <span class="o">+</span> <span class="n">dataset</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span>
        <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;denoise_&quot;</span>
                <span class="o">+</span> <span class="n">dataset</span>
                <span class="o">+</span> <span class="s2">&quot;/reco2full_vs_noisy2full&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                    <span class="n">res</span><span class="p">[</span><span class="s2">&quot;reco2full&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;noisy2full&quot;</span><span class="p">]</span>
                <span class="p">),</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
        <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">grn_task</span><span class="o">.</span><span class="n">default_benchmark</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="s2">&quot;gwps&quot;</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span> <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">d_model</span> <span class="o">&lt;=</span> <span class="mi">512</span> <span class="k">else</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">maxcells</span><span class="o">=</span><span class="n">maxcells_grn</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">tot</span><span class="p">[</span><span class="s2">&quot;grn_gwps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span>
    <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;grn_gwps/auprc_self&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;self&quot;</span><span class="p">][</span><span class="s2">&quot;auprc&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;grn_gwps/epr_self&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;self&quot;</span><span class="p">][</span><span class="s2">&quot;epr&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;grn_gwps/auprc_omni&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;omni&quot;</span><span class="p">][</span><span class="s2">&quot;auprc&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;grn_gwps/epr_omni&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;omni&quot;</span><span class="p">][</span><span class="s2">&quot;epr&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;grn_gwps/auprc&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">][</span><span class="s2">&quot;auprc&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;grn_gwps/epr&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">][</span><span class="s2">&quot;epr&quot;</span><span class="p">]),</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">filepath</span> <span class="ow">in</span> <span class="p">{</span>
        <span class="s2">&quot;old_kidney&quot;</span><span class="p">:</span> <span class="s2">&quot;https://datasets.cellxgene.cziscience.com/ede85b09-454b-4374-bf60-5f675e989b64.h5ad&quot;</span><span class="p">,</span>
        <span class="c1"># &quot;kidney&quot;: &quot;https://datasets.cellxgene.cziscience.com/01bc7039-961f-4c24-b407-d535a2a7ba2c.h5ad&quot;,</span>
        <span class="s2">&quot;lung_smart&quot;</span><span class="p">:</span> <span class="s2">&quot;https://datasets.cellxgene.cziscience.com/6ebba0e0-a159-406f-8095-451115673a2c.h5ad&quot;</span><span class="p">,</span>
        <span class="c1"># filedir + &quot;/../../data/yBCKp6HmXuHa0cZptMo7.h5ad&quot;,</span>
    <span class="p">}</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">grn_task</span><span class="o">.</span><span class="n">default_benchmark</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">filepath</span><span class="p">,</span>
            <span class="c1"># kidney dataset (2.87, 1.27) (0.00147, 0.00133)</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span> <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">d_model</span> <span class="o">&lt;=</span> <span class="mi">512</span> <span class="k">else</span> <span class="mi">8</span><span class="p">,</span>
            <span class="n">maxcells</span><span class="o">=</span><span class="n">maxcells_grn</span><span class="p">,</span>
            <span class="n">maxgenes</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">tot</span><span class="p">[</span><span class="s2">&quot;grn_omni_&quot;</span> <span class="o">+</span> <span class="n">dataset</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span>
        <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="s2">&quot;grn_omni_&quot;</span>
                <span class="o">+</span> <span class="n">dataset</span>
                <span class="o">+</span> <span class="s2">&quot;/auprc_class&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="s2">&quot;auprc&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_class&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">])</span>
                <span class="p">),</span>
                <span class="s2">&quot;grn_omni_&quot;</span>
                <span class="o">+</span> <span class="n">dataset</span>
                <span class="o">+</span> <span class="s2">&quot;/or_class&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="s2">&quot;odd_ratio&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_class&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">])</span>
                <span class="p">),</span>
                <span class="s2">&quot;grn_omni_&quot;</span>
                <span class="o">+</span> <span class="n">dataset</span>
                <span class="o">+</span> <span class="s2">&quot;/tf_enr_class&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                        <span class="p">[</span>
                            <span class="n">i</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;TF_enr&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
                            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                            <span class="k">if</span> <span class="s2">&quot;_class&quot;</span> <span class="ow">in</span> <span class="n">k</span>
                        <span class="p">]</span>
                    <span class="p">)</span>
                <span class="p">),</span>
                <span class="s2">&quot;grn_omni_&quot;</span>
                <span class="o">+</span> <span class="n">dataset</span>
                <span class="o">+</span> <span class="s2">&quot;/tf_targ_enr_class&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                        <span class="p">[</span>
                            <span class="n">i</span><span class="p">[</span><span class="s2">&quot;significant_enriched_TFtargets&quot;</span><span class="p">]</span>
                            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                            <span class="k">if</span> <span class="s2">&quot;_class&quot;</span> <span class="ow">in</span> <span class="n">k</span>
                        <span class="p">]</span>
                    <span class="p">)</span>
                <span class="p">),</span>
                <span class="s2">&quot;grn_omni_&quot;</span>
                <span class="o">+</span> <span class="n">dataset</span>
                <span class="o">+</span> <span class="s2">&quot;/auprc&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="s2">&quot;auprc&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_mean&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">])</span>
                <span class="p">),</span>
                <span class="s2">&quot;grn_omni_&quot;</span>
                <span class="o">+</span> <span class="n">dataset</span>
                <span class="o">+</span> <span class="s2">&quot;/epr&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="s2">&quot;epr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_mean&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">])</span>
                <span class="p">),</span>
                <span class="s2">&quot;grn_omni_&quot;</span>
                <span class="o">+</span> <span class="n">dataset</span>
                <span class="o">+</span> <span class="s2">&quot;/or&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="s2">&quot;odd_ratio&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_mean&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">])</span>
                <span class="p">),</span>
                <span class="s2">&quot;grn_omni_&quot;</span>
                <span class="o">+</span> <span class="n">dataset</span>
                <span class="o">+</span> <span class="s2">&quot;/tf_enr&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                        <span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;TF_enr&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_mean&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">]</span>
                    <span class="p">)</span>
                <span class="p">),</span>
                <span class="s2">&quot;grn_omni_&quot;</span>
                <span class="o">+</span> <span class="n">dataset</span>
                <span class="o">+</span> <span class="s2">&quot;/tf_targ_enr&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                        <span class="p">[</span>
                            <span class="n">i</span><span class="p">[</span><span class="s2">&quot;significant_enriched_TFtargets&quot;</span><span class="p">]</span>
                            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                            <span class="k">if</span> <span class="s2">&quot;_mean&quot;</span> <span class="ow">in</span> <span class="n">k</span>
                        <span class="p">]</span>
                    <span class="p">)</span>
                <span class="p">),</span>
                <span class="c1"># &#39;grn_omni/ct&#39;: res[&#39;classif&#39;][&#39;cell_type_ontology_term_id&#39;][&#39;accuracy&#39;],</span>
            <span class="p">}</span>
        <span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
        <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">tot</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint2.model.utils.zinb_sample" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">zinb_sample</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>mu</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The mean of the Negative Binomial (NB) distribution.</p>
              </div>
            </li>
            <li>
              <b><code>theta</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The dispersion parameter of the NB distribution.</p>
              </div>
            </li>
            <li>
              <b><code>zi_probs</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The zero-inflation probabilities.</p>
              </div>
            </li>
            <li>
              <b><code>sample_shape</code></b>
                  (<code><span title="torch.Size">Size</span></code>, default:
                      <code><span title="torch.Size">Size</span>([])</code>
)
              –
              <div class="doc-md-description">
                <p>The output shape. Defaults to torch.Size([]).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="torch.Tensor">Tensor</span></code>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: A sample from the ZINB distribution.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">zinb_sample</span><span class="p">(</span>
    <span class="n">mu</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zi_probs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        mu (torch.Tensor): The mean of the Negative Binomial (NB) distribution.</span>
<span class="sd">        theta (torch.Tensor): The dispersion parameter of the NB distribution.</span>
<span class="sd">        zi_probs (torch.Tensor): The zero-inflation probabilities.</span>
<span class="sd">        sample_shape (torch.Size, optional): The output shape. Defaults to torch.Size([]).</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: A sample from the ZINB distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">concentration</span> <span class="o">=</span> <span class="n">theta</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">/</span> <span class="n">mu</span>
    <span class="c1"># Important remark: Gamma is parametrized by the rate = 1/scale!</span>
    <span class="n">gamma_d</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="n">concentration</span><span class="o">=</span><span class="n">concentration</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">rate</span><span class="p">)</span>
    <span class="n">p_means</span> <span class="o">=</span> <span class="n">gamma_d</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span>

    <span class="c1"># Clamping as distributions objects can have buggy behaviors when</span>
    <span class="c1"># their parameters are too high</span>
    <span class="n">l_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">p_means</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1e8</span><span class="p">)</span>
    <span class="n">samp</span> <span class="o">=</span> <span class="n">Poisson</span><span class="p">(</span><span class="n">l_train</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Shape : (n_samples, n_cells_batch, n_vars)</span>
    <span class="n">is_zero</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">samp</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">zi_probs</span>
    <span class="n">samp_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">is_zero</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">samp</span><span class="p">),</span> <span class="n">samp</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">samp_</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h2 id="encoder-and-decoder-modules">encoder and decoder modules</h2>


<div class="doc doc-object doc-module">



<h2 id="scprint2.model.encoders" class="doc doc-heading">
            <code>scprint2.model.encoders</code>


</h2>

    <div class="doc doc-contents first">










<p><span class="doc-section-title">Classes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="CategoryValueEncoder (scprint2.model.encoders.CategoryValueEncoder)" href="#scprint2.model.encoders.CategoryValueEncoder">CategoryValueEncoder</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="ContinuousValueEncoder (scprint2.model.encoders.ContinuousValueEncoder)" href="#scprint2.model.encoders.ContinuousValueEncoder">ContinuousValueEncoder</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="DPositionalEncoding (scprint2.model.encoders.DPositionalEncoding)" href="#scprint2.model.encoders.DPositionalEncoding">DPositionalEncoding</a></code></td>
            <td>
              <div class="doc-md-description">
                <p>The PositionalEncoding module applies a positional encoding to a sequence of vectors.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="EasyExprGNN (scprint2.model.encoders.EasyExprGNN)" href="#scprint2.model.encoders.EasyExprGNN">EasyExprGNN</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="ExprBasedFT (scprint2.model.encoders.ExprBasedFT)" href="#scprint2.model.encoders.ExprBasedFT">ExprBasedFT</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="GNN (scprint2.model.encoders.GNN)" href="#scprint2.model.encoders.GNN">GNN</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="GeneEncoder (scprint2.model.encoders.GeneEncoder)" href="#scprint2.model.encoders.GeneEncoder">GeneEncoder</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="PositionalEncoding (scprint2.model.encoders.PositionalEncoding)" href="#scprint2.model.encoders.PositionalEncoding">PositionalEncoding</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
      </tbody>
    </table>







<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="scprint2.model.encoders.CategoryValueEncoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">CategoryValueEncoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>



        <p>Encodes categorical values into a vector using an embedding layer and layer normalization.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>num_embeddings</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The number of possible values.</p>
              </div>
            </li>
            <li>
              <b><code>embedding_dim</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the output vectors.</p>
              </div>
            </li>
            <li>
              <b><code>padding_idx</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The index of the padding token. Defaults to None.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>        <p>Note: not used in the current version of scprint-2.</p>












                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encodes categorical values into a vector using an embedding layer and layer normalization.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_embeddings (int): The number of possible values.</span>
<span class="sd">        embedding_dim (int): The dimension of the output vectors.</span>
<span class="sd">        padding_idx (int, optional): The index of the padding token. Defaults to None.</span>

<span class="sd">    Note: not used in the current version of scprint-2.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CategoryValueEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
        <span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">












  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h3 id="scprint2.model.encoders.ContinuousValueEncoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">ContinuousValueEncoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>



        <p>Encode real number values to a vector using neural nets projection.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the input vectors.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the output of the positional encoding.</p>
              </div>
            </li>
            <li>
              <b><code>max_value</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>100000</code>
)
              –
              <div class="doc-md-description">
                <p>The maximum value of the input. Defaults to 100_000.</p>
              </div>
            </li>
            <li>
              <b><code>layers</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>The number of layers in the encoder. Defaults to 1.</p>
              </div>
            </li>
            <li>
              <b><code>size</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>The size of the input. Defaults to 1.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>










<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint2.model.encoders.ContinuousValueEncoder.forward)" href="#scprint2.model.encoders.ContinuousValueEncoder.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Args:</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">max_value</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100_000</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encode real number values to a vector using neural nets projection.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the input vectors.</span>
<span class="sd">        dropout (float, optional): The dropout rate to apply to the output of the positional encoding.</span>
<span class="sd">        max_value (int, optional): The maximum value of the input. Defaults to 100_000.</span>
<span class="sd">        layers (int, optional): The number of layers in the encoder. Defaults to 1.</span>
<span class="sd">        size (int, optional): The size of the input. Defaults to 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ContinuousValueEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_value</span> <span class="o">=</span> <span class="n">max_value</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="c1"># self.mask_value = nn.Embedding(1, d_model)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="scprint2.model.encoders.ContinuousValueEncoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape [batch_size, seq_len]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/encoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [batch_size, seq_len]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># expand last dimension</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># use the mask embedding when x=-1</span>
    <span class="c1"># mask = (x == -1).float()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_value</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">val</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># x = x.masked_fill_(mask.unsqueeze(-1), self.mask_value(0))</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h3 id="scprint2.model.encoders.DPositionalEncoding" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">DPositionalEncoding</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>



        <p>The PositionalEncoding module applies a positional encoding to a sequence of vectors.
This is necessary for the Transformer model, which does not have any inherent notion of
position in a sequence. The positional encoding is added to the input embeddings and
allows the model to attend to positions in the sequence.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the input vectors.</p>
              </div>
            </li>
            <li>
              <b><code>max_len_x</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The maximum length in the x dimension.</p>
              </div>
            </li>
            <li>
              <b><code>max_len_y</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The maximum length in the y dimension.</p>
              </div>
            </li>
            <li>
              <b><code>maxvalue_x</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>10000.0</code>
)
              –
              <div class="doc-md-description">
                <p>Maximum value for x dimension scaling. Defaults to 10000.0.</p>
              </div>
            </li>
            <li>
              <b><code>maxvalue_y</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>10000.0</code>
)
              –
              <div class="doc-md-description">
                <p>Maximum value for y dimension scaling. Defaults to 10000.0.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>        <p>Note: not used in the current version of scprint-2.</p>











<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint2.model.encoders.DPositionalEncoding.forward)" href="#scprint2.model.encoders.DPositionalEncoding.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Args:</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">max_len_x</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">max_len_y</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">maxvalue_x</span><span class="o">=</span><span class="mf">10000.0</span><span class="p">,</span>
    <span class="n">maxvalue_y</span><span class="o">=</span><span class="mf">10000.0</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">DPositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="n">position2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_len_y</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">position1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_len_x</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">half_n</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="mi">2</span>

    <span class="n">div_term2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">half_n</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">maxvalue_y</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">div_term1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">half_n</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">maxvalue_x</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pe1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">pe2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len_y</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">pe1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="n">half_n</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position1</span> <span class="o">*</span> <span class="n">div_term1</span><span class="p">)</span>
    <span class="n">pe1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="n">half_n</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position1</span> <span class="o">*</span> <span class="n">div_term1</span><span class="p">)</span>
    <span class="n">pe2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">half_n</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position2</span> <span class="o">*</span> <span class="n">div_term2</span><span class="p">)</span>
    <span class="n">pe2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">half_n</span> <span class="p">::</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position2</span> <span class="o">*</span> <span class="n">div_term2</span><span class="p">)</span>
    <span class="c1"># https://github.com/tatp22/multidim-positional-encoding/blob/master/positional_encodings/torch_encodings.py</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;pe1&quot;</span><span class="p">,</span> <span class="n">pe1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;pe2&quot;</span><span class="p">,</span> <span class="n">pe2</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="scprint2.model.encoders.DPositionalEncoding.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape [seq_len, batch_size, embedding_dim]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/encoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pos_x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pos_y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [seq_len, batch_size, embedding_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe1</span><span class="p">[</span><span class="n">pos_x</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe2</span><span class="p">[</span><span class="n">pos_y</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h3 id="scprint2.model.encoders.EasyExprGNN" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">EasyExprGNN</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>



        <p>Easy Expression Graph Neural Network</p>
<p>The main GNN used in scPRINT-2 for expression encoding.
It is inspired from the DeepSets architecture to aggregate neighbor information.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>self_dim</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>64</code>
)
              –
              <div class="doc-md-description">
                <p>Dimension of the self features</p>
              </div>
            </li>
            <li>
              <b><code>output_dim</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>32</code>
)
              –
              <div class="doc-md-description">
                <p>Output dimension</p>
              </div>
            </li>
            <li>
              <b><code>self_layers</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>2</code>
)
              –
              <div class="doc-md-description">
                <p>Number of layers for self features</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>Dropout rate</p>
              </div>
            </li>
            <li>
              <b><code>shared_layers</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>2</code>
)
              –
              <div class="doc-md-description">
                <p>Number of shared layers</p>
              </div>
            </li>
            <li>
              <b><code>neighbors_layers</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>2</code>
)
              –
              <div class="doc-md-description">
                <p>Number of layers for neighbors features</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>










<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint2.model.encoders.EasyExprGNN.forward)" href="#scprint2.model.encoders.EasyExprGNN.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Forward pass of the Easy Expression GNN</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">self_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">self_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">shared_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">neighbors_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Easy Expression Graph Neural Network</span>

<span class="sd">    The main GNN used in scPRINT-2 for expression encoding.</span>
<span class="sd">    It is inspired from the DeepSets architecture to aggregate neighbor information.</span>

<span class="sd">    Args:</span>
<span class="sd">        self_dim (int): Dimension of the self features</span>
<span class="sd">        output_dim (int): Output dimension</span>
<span class="sd">        self_layers (int): Number of layers for self features</span>
<span class="sd">        dropout (float): Dropout rate</span>
<span class="sd">        shared_layers (int): Number of shared layers</span>
<span class="sd">        neighbors_layers (int): Number of layers for neighbors features</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">EasyExprGNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">self_dim</span> <span class="o">=</span> <span class="n">self_dim</span>
    <span class="c1"># neighbors</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">neighbors_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">neighbors_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">self_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">neighbors_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neighbors_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">self_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neighbors_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neighbors_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neighbors_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">self_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">self_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>
    <span class="c1"># self</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">self_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">self_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">self_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">self_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">self_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">self_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">self_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>
    <span class="c1"># shared</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">shared_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">shared_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">self_dim</span><span class="p">,</span> <span class="n">self_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">self_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">shared_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">self_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="scprint2.model.encoders.EasyExprGNN.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Forward pass of the Easy Expression GNN</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>expr</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Tensor of shape (batch, seq_len) representing expression values</p>
              </div>
            </li>
            <li>
              <b><code>neighbors</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Tensor of shape (batch, seq_len, n_neighbors) representing neighbor indices</p>
              </div>
            </li>
            <li>
              <b><code>edge_info</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Tensor of shape (batch, seq_len, n_neighbors) representing edge information</p>
              </div>
            </li>
            <li>
              <b><code>mask</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Tensor of shape (batch, seq_len) representing mask for the input</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="torch.Tensor">Tensor</span></code>
              –
              <div class="doc-md-description">
                <p>Tensor of shape (batch, seq_len, output_dim) representing the output features</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/encoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">expr</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">neighbors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">edge_info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass of the Easy Expression GNN</span>

<span class="sd">    Args:</span>
<span class="sd">        expr: Tensor of shape (batch, seq_len) representing expression values</span>
<span class="sd">        neighbors: Tensor of shape (batch, seq_len, n_neighbors) representing neighbor indices</span>
<span class="sd">        edge_info: Tensor of shape (batch, seq_len, n_neighbors) representing edge information</span>
<span class="sd">        mask: Tensor of shape (batch, seq_len) representing mask for the input</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor of shape (batch, seq_len, output_dim) representing the output features</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># batch, seq_len, neighbs</span>
    <span class="k">if</span> <span class="n">neighbors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">neighbors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">expr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">expr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">expr</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">neighbors</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">neighbors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">neighbors</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">edge_info</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neighbors_layers</span><span class="p">):</span>
            <span class="c1"># batch, seq_len, neighbs, hidden_dim</span>
            <span class="n">neighbors</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">neighbors</span><span class="p">)</span>
        <span class="n">neighbors</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">expr</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">expr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">neighbors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">neighbors</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">expr</span> <span class="o">=</span> <span class="n">expr</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">self_layers</span><span class="p">):</span>
            <span class="n">expr</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">expr</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">expr</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared_layers</span><span class="p">:</span>
        <span class="c1"># batch, seq_len, neighbs, hidden_dim</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h3 id="scprint2.model.encoders.ExprBasedFT" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">ExprBasedFT</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>



        <p>Encode real number values to a vector using neural nets projection.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the input vectors.</p>
              </div>
            </li>
            <li>
              <b><code>gene_encoder</code></b>
                  (<code><span title="torch.nn.Module">Module</span></code>)
              –
              <div class="doc-md-description">
                <p>The gene name encoder module.</p>
              </div>
            </li>
            <li>
              <b><code>expr_encoder</code></b>
                  (<code><span title="torch.nn.Module">Module</span></code>, default:
                      <code><span title="torch.nn.Identity">Identity</span>()</code>
)
              –
              <div class="doc-md-description">
                <p>The expression encoder module. Defaults to nn.Identity.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the output of the positional encoding.</p>
              </div>
            </li>
            <li>
              <b><code>layers</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>2</code>
)
              –
              <div class="doc-md-description">
                <p>The number of layers in the encoder. Defaults to 2.</p>
              </div>
            </li>
            <li>
              <b><code>intermediary_d</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>256 + 64</code>
)
              –
              <div class="doc-md-description">
                <p>The dimension of the intermediary layers. Defaults to 256 + 64.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>










<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint2.model.encoders.ExprBasedFT.forward)" href="#scprint2.model.encoders.ExprBasedFT.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Args:</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">gene_encoder</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">expr_encoder</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">(),</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">intermediary_d</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span> <span class="o">+</span> <span class="mi">64</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encode real number values to a vector using neural nets projection.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the input vectors.</span>
<span class="sd">        gene_encoder (nn.Module): The gene name encoder module.</span>
<span class="sd">        expr_encoder (nn.Module, optional): The expression encoder module. Defaults to nn.Identity.</span>
<span class="sd">        dropout (float, optional): The dropout rate to apply to the output of the positional encoding.</span>
<span class="sd">        layers (int, optional): The number of layers in the encoder. Defaults to 2.</span>
<span class="sd">        intermediary_d (int, optional): The dimension of the intermediary layers. Defaults to 256 + 64.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ExprBasedFT</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="c1"># self.mask_value = nn.Embedding(1, d_model)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;gene_encoder&quot;</span><span class="p">,</span> <span class="n">gene_encoder</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span><span class="s2">&quot;expr_encoder&quot;</span><span class="p">,</span> <span class="n">expr_encoder</span><span class="p">)</span>
    <span class="n">expr_shape</span><span class="p">,</span> <span class="n">gene_shape</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene_encoder</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">expr_shape</span> <span class="o">+</span> <span class="n">gene_shape</span><span class="p">,</span> <span class="n">intermediary_d</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">intermediary_d</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">intermediary_d</span><span class="p">,</span> <span class="n">intermediary_d</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">layers</span> <span class="o">-</span> <span class="mi">2</span> <span class="k">else</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="scprint2.model.encoders.ExprBasedFT.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>gene_pos</code></b>
                  (<code><span title="torch.Tensor">Tensor</span>[<span title="batch_size">batch_size</span>, <span title="seq_len">seq_len</span>]</code>)
              –
              <div class="doc-md-description">
                <p>Gene position indices
input to the gene encoder</p>
              </div>
            </li>
            <li>
              <b><code>expr</code></b>
                  (<code>(<span title="torch.Tensor">Tensor</span>[<span title="batch_size">batch_size</span>, <span title="seq_len">seq_len</span>], <span title="typing.Optional">Optional</span>)</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Expression values
input to the expression encoder</p>
              </div>
            </li>
            <li>
              <b><code>mask</code></b>
                  (<code>(<span title="torch.Tensor">Tensor</span>[<span title="batch_size">batch_size</span>, <span title="seq_len">seq_len</span>], <span title="typing.Optional">Optional</span>)</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Mask for the input
input to the expression encoder</p>
              </div>
            </li>
            <li>
              <b><code>neighbors</code></b>
                  (<code>(<span title="torch.Tensor">Tensor</span>[<span title="batch_size">batch_size</span>, <span title="seq_len">seq_len</span>, <span title="n_neighbors">n_neighbors</span>], <span title="typing.Optional">Optional</span>)</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Neighbors indices
input to the expression encoder when it is a GNN</p>
              </div>
            </li>
            <li>
              <b><code>neighbors_info</code></b>
                  (<code>(<span title="torch.Tensor">Tensor</span>[<span title="batch_size">batch_size</span>, <span title="seq_len">seq_len</span>, <span title="n_neighbors">n_neighbors</span>], <span title="typing.Optional">Optional</span>)</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>optional additional information about the neighbors
input to the expression encoder when it is a GNN</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/encoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">gene_pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">expr</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">neighbors</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">neighbors_info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        gene_pos (Tensor[batch_size, seq_len]): Gene position indices</span>
<span class="sd">            input to the gene encoder</span>
<span class="sd">        expr (Tensor[batch_size, seq_len], Optional): Expression values</span>
<span class="sd">            input to the expression encoder</span>
<span class="sd">        mask (Tensor[batch_size, seq_len], Optional): Mask for the input</span>
<span class="sd">            input to the expression encoder</span>
<span class="sd">        neighbors (Tensor[batch_size, seq_len, n_neighbors], Optional): Neighbors indices</span>
<span class="sd">            input to the expression encoder when it is a GNN</span>
<span class="sd">        neighbors_info (Tensor[batch_size, seq_len, n_neighbors], Optional):</span>
<span class="sd">            optional additional information about the neighbors</span>
<span class="sd">            input to the expression encoder when it is a GNN</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># expand last dimension</span>
    <span class="k">if</span> <span class="n">neighbors</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">expr</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">expr</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span><span class="n">gene_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">gene_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span><span class="o">.</span><span class="n">output_dim</span><span class="p">),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">gene_pos</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># if no expr information: consider that it is all masked</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">expr</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span><span class="p">(</span><span class="n">expr</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">neighbors</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span><span class="p">(</span><span class="n">expr</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">,</span> <span class="n">neighbors_info</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="n">gene_pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gene_encoder</span><span class="p">(</span><span class="n">gene_pos</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">expr</span><span class="p">,</span> <span class="n">gene_pos</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">val</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h3 id="scprint2.model.encoders.GNN" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">GNN</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>



        <p>Graph Neural Network model</p>
<p>Another implementation of a GNN layer that can be used for expression encoding.
Supports GCN, GAT, GraphSAGE, and DeepSets architectures.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>input_dim</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>Dimension of input node features</p>
              </div>
            </li>
            <li>
              <b><code>output_dim</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>256</code>
)
              –
              <div class="doc-md-description">
                <p>Dimension of output node features</p>
              </div>
            </li>
            <li>
              <b><code>num_layers</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>2</code>
)
              –
              <div class="doc-md-description">
                <p>Number of GNN layers</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>Dropout probability</p>
              </div>
            </li>
            <li>
              <b><code>gnn_type</code></b>
                  (<code><span title="str">str</span></code>, default:
                      <code>&#39;deepset&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Type of GNN layer ('gcn', 'gat', 'sage', or 'deepset')</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>










<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint2.model.encoders.GNN.forward)" href="#scprint2.model.encoders.GNN.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Forward pass</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># here, 1 or 2</span>
    <span class="n">merge_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">gnn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;deepset&quot;</span><span class="p">,</span>
    <span class="n">add_connection_feature</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Graph Neural Network model</span>

<span class="sd">    Another implementation of a GNN layer that can be used for expression encoding.</span>
<span class="sd">    Supports GCN, GAT, GraphSAGE, and DeepSets architectures.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_dim: Dimension of input node features</span>
<span class="sd">        output_dim: Dimension of output node features</span>
<span class="sd">        num_layers: Number of GNN layers</span>
<span class="sd">        dropout: Dropout probability</span>
<span class="sd">        gnn_type: Type of GNN layer (&#39;gcn&#39;, &#39;gat&#39;, &#39;sage&#39;, or &#39;deepset&#39;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
    <span class="k">if</span> <span class="n">num_layers</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;num_layers must be greater than 1&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gnn_type</span> <span class="o">=</span> <span class="n">gnn_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_connection_feature</span> <span class="o">=</span> <span class="n">add_connection_feature</span>

    <span class="k">if</span> <span class="n">gnn_type</span> <span class="o">==</span> <span class="s2">&quot;deepset&quot;</span><span class="p">:</span>
        <span class="c1"># Local MLP (phi) for processing individual nodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_nn_layer</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">hidden_channels</span><span class="o">=</span><span class="n">merge_dim</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">merge_dim</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">act</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
            <span class="n">norm</span><span class="o">=</span><span class="s2">&quot;layer_norm&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_self_layer</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span>
            <span class="n">hidden_channels</span><span class="o">=</span><span class="n">merge_dim</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">merge_dim</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">act</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
            <span class="n">norm</span><span class="o">=</span><span class="s2">&quot;layer_norm&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Global MLP (rho) for processing aggregated features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="p">(</span>
                <span class="p">(</span><span class="n">merge_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">add_connection_feature</span> <span class="k">else</span> <span class="n">merge_dim</span> <span class="o">*</span> <span class="mi">2</span>
            <span class="p">),</span>
            <span class="n">hidden_channels</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">act</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span>
            <span class="n">norm</span><span class="o">=</span><span class="s2">&quot;layer_norm&quot;</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span>

    <span class="c1"># Select GNN layer type for other architectures</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">gnn_type</span> <span class="o">==</span> <span class="s2">&quot;gcn&quot;</span><span class="p">:</span>
            <span class="n">gnn_layer</span> <span class="o">=</span> <span class="n">GCNConv</span>
        <span class="k">elif</span> <span class="n">gnn_type</span> <span class="o">==</span> <span class="s2">&quot;gat&quot;</span><span class="p">:</span>
            <span class="n">gnn_layer</span> <span class="o">=</span> <span class="n">GATConv</span>
        <span class="k">elif</span> <span class="n">gnn_type</span> <span class="o">==</span> <span class="s2">&quot;sage&quot;</span><span class="p">:</span>
            <span class="n">gnn_layer</span> <span class="o">=</span> <span class="n">SAGEConv</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown GNN type: </span><span class="si">{</span><span class="n">gnn_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gnn_layer</span> <span class="o">=</span> <span class="n">gnn_layer</span><span class="p">(</span>
            <span class="n">output_dim</span><span class="p">,</span>
            <span class="n">output_dim</span><span class="p">,</span>
            <span class="n">add_self_loops</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">aggr</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="scprint2.model.encoders.GNN.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Forward pass</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Node features [minibatch_size, ngenes]</p>
              </div>
            </li>
            <li>
              <b><code>neighbors</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Neighbor nodes [minibatch_size, ngenes, n_neighbors] or [minibatch_size, ngenes, n_neighbors, 2]</p>
              </div>
            </li>
            <li>
              <b><code>edge_info</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Graph connectivity [2, num_edges] if gnn_type != deepset,
Edge features [num_edges, 1] if gnn_type == deepset,
or None if gnn_type == deepset and no edge features.</p>
              </div>
            </li>
            <li>
              <b><code>batch</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Batch assignment vector [num_nodes]</p>
              </div>
            </li>
            <li>
              <b><code>mask</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Mask tensor for the nodes.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>Node embeddings [num_nodes, hidden_dim]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/encoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">neighbors</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">edge_info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">batch</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Node features [minibatch_size, ngenes]</span>
<span class="sd">        neighbors (Tensor): Neighbor nodes [minibatch_size, ngenes, n_neighbors] or [minibatch_size, ngenes, n_neighbors, 2]</span>
<span class="sd">        edge_info (Tensor, optional): Graph connectivity [2, num_edges] if gnn_type != deepset,</span>
<span class="sd">            Edge features [num_edges, 1] if gnn_type == deepset,</span>
<span class="sd">            or None if gnn_type == deepset and no edge features.</span>
<span class="sd">        batch (Tensor, optional): Batch assignment vector [num_nodes]</span>
<span class="sd">        mask (Tensor, optional): Mask tensor for the nodes.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: Node embeddings [num_nodes, hidden_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Standard GNN forward pass</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">neighbors</span> <span class="o">=</span> <span class="n">neighbors</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gnn_type</span> <span class="o">==</span> <span class="s2">&quot;deepset&quot;</span><span class="p">:</span>
        <span class="n">neighbors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_nn_layer</span><span class="p">(</span><span class="n">neighbors</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_self_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">neighbors</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gnn_layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_info</span><span class="p">)</span>
        <span class="n">neighbors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gnn_layer</span><span class="p">(</span><span class="n">neighbors</span><span class="p">,</span> <span class="n">edge_info</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">edge_info</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">)</span>
        <span class="c1"># TODO: to finish</span>

    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h3 id="scprint2.model.encoders.GeneEncoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">GeneEncoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>



        <p>Encodes gene sequences into a continuous vector space using an embedding layer.
Uses memory mapping for efficient access to large embedding files.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>num_embeddings</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The number of possible values</p>
              </div>
            </li>
            <li>
              <b><code>embedding_dim</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the output vectors</p>
              </div>
            </li>
            <li>
              <b><code>padding_idx</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The index of the padding token</p>
              </div>
            </li>
            <li>
              <b><code>weights</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The initial weights for the embedding layer</p>
              </div>
            </li>
            <li>
              <b><code>weights_file</code></b>
                  (<code><span title="str">str</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Path to parquet file containing embeddings</p>
              </div>
            </li>
            <li>
              <b><code>freeze</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to freeze the weights of the embedding layer</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>










<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="__del__ (scprint2.model.encoders.GeneEncoder.__del__)" href="#scprint2.model.encoders.GeneEncoder.__del__">__del__</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Cleanup method to ensure proper handling of memory-mapped file.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint2.model.encoders.GeneEncoder.forward)" href="#scprint2.model.encoders.GeneEncoder.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Forward pass of the encoder.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">weights_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">freeze</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encodes gene sequences into a continuous vector space using an embedding layer.</span>
<span class="sd">    Uses memory mapping for efficient access to large embedding files.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_embeddings (int): The number of possible values</span>
<span class="sd">        embedding_dim (int): The dimension of the output vectors</span>
<span class="sd">        padding_idx (int, optional): The index of the padding token</span>
<span class="sd">        weights (Tensor, optional): The initial weights for the embedding layer</span>
<span class="sd">        weights_file (str, optional): Path to parquet file containing embeddings</span>
<span class="sd">        freeze (bool, optional): Whether to freeze the weights of the embedding layer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">GeneEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">embedding_dim</span>

    <span class="k">if</span> <span class="n">weights_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memmap</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">freeze</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;freeze must be True when using memory-mapped embeddings&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Load the parquet file and create memory-mapped array</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">os</span>

        <span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

        <span class="c1"># Create memory-mapped file path</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mmap_file</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">weights_file</span><span class="si">}</span><span class="s2">.mmap&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loc</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">enc</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Only create the memory-mapped file if it doesn&#39;t exist</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mmap_file</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Creating memory-mapped file for embeddings at </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">mmap_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">weights_file</span><span class="p">)</span>
            <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">)(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
            <span class="p">)</span>

            <span class="c1"># Create memory-mapped array</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">memmap</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mmap_file</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;w+&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span>
            <span class="p">)</span>
            <span class="c1"># Copy data to memory-mapped array</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
            <span class="c1">#</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>

            <span class="c1"># Clean up memory</span>
            <span class="k">del</span> <span class="n">df</span>
            <span class="k">del</span> <span class="n">embeddings</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Loading existing memory-mapped embeddings from </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">mmap_file</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="c1"># Load existing memory-mapped file</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">memmap</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">mmap_file</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">,</span>
                <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span>  <span class="c1"># Read-only mode since we don&#39;t need to modify</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">),</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memmap</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
            <span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span><span class="p">,</span> <span class="n">_freeze</span><span class="o">=</span><span class="n">freeze</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="scprint2.model.encoders.GeneEncoder.__del__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__del__</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Cleanup method to ensure proper handling of memory-mapped file.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/encoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Cleanup method to ensure proper handling of memory-mapped file.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;embeddings&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">_mmap</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">pass</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.encoders.GeneEncoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Forward pass of the encoder.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Input tensor of indices [batch_size, seq_len]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>Embedded vectors [batch_size, seq_len, embedding_dim]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/encoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass of the encoder.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor of indices [batch_size, seq_len]</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: Embedded vectors [batch_size, seq_len, embedding_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">memmap</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loc</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">loc</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">enc</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,))</span>
                <span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loc</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h3 id="scprint2.model.encoders.PositionalEncoding" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">PositionalEncoding</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>



        <p>The PositionalEncoding module applies a positional encoding to a sequence of vectors.
This is necessary for the Transformer model, which does not have any inherent notion of
position in a sequence. The positional encoding is added to the input embeddings and
allows the model to attend to positions in the sequence.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the input vectors.</p>
              </div>
            </li>
            <li>
              <b><code>gene_pos_enc</code></b>
                  (<code><span title="list">list</span>[<span title="str">str</span>]</code>, default:
                      <code>[]</code>
)
              –
              <div class="doc-md-description">
                <p>The gene position encoding to use.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>        <p>Note: not used in the current version of scprint-2.</p>











<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint2.model.encoders.PositionalEncoding.forward)" href="#scprint2.model.encoders.PositionalEncoding.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Args:</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">gene_pos_enc</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The PositionalEncoding module applies a positional encoding to a sequence of vectors.</span>
<span class="sd">    This is necessary for the Transformer model, which does not have any inherent notion of</span>
<span class="sd">    position in a sequence. The positional encoding is added to the input embeddings and</span>
<span class="sd">    allows the model to attend to positions in the sequence.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the input vectors.</span>
<span class="sd">        gene_pos_enc (list[str], optional): The gene position encoding to use.</span>

<span class="sd">    Note: not used in the current version of scprint-2.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gene_pos_enc</span> <span class="o">=</span> <span class="n">gene_pos_enc</span>
    <span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">gene_pos_enc</span><span class="p">)</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">token_to_pos</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">pos</span> <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">gene_pos_enc</span><span class="p">)}</span>

    <span class="c1"># Create a dictionary to convert token to position</span>

    <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="mi">10_000</span><span class="p">))</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="c1"># we reorder them and map them to gene_id (position)</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">token_to_pos</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">arr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pe</span><span class="p">[</span><span class="n">v</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">arr</span><span class="p">))</span>
    <span class="c1"># Remove the unnecessary middle dimension since pe should be [m, d]</span>
    <span class="c1"># pe = pe.squeeze(1)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;pe&quot;</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="scprint2.model.encoders.PositionalEncoding.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>gene_pos</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Gene position indices, shape [seq_len, batch_size] or [seq_len]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>Positional encodings, shape [*gene_pos.shape, embedding_dim]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/encoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gene_pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        gene_pos (Tensor): Gene position indices, shape [seq_len, batch_size] or [seq_len]</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: Positional encodings, shape [*gene_pos.shape, embedding_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">gene_pos</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">gene_pos</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="scprint2.model.decoders" class="doc doc-heading">
            <code>scprint2.model.decoders</code>


</h2>

    <div class="doc doc-contents first">










<p><span class="doc-section-title">Classes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="ClsDecoder (scprint2.model.decoders.ClsDecoder)" href="#scprint2.model.decoders.ClsDecoder">ClsDecoder</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="ExprDecoder (scprint2.model.decoders.ExprDecoder)" href="#scprint2.model.decoders.ExprDecoder">ExprDecoder</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="GraphSDEExprDecoder (scprint2.model.decoders.GraphSDEExprDecoder)" href="#scprint2.model.decoders.GraphSDEExprDecoder">GraphSDEExprDecoder</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="MVCDecoder (scprint2.model.decoders.MVCDecoder)" href="#scprint2.model.decoders.MVCDecoder">MVCDecoder</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="VAEDecoder (scprint2.model.decoders.VAEDecoder)" href="#scprint2.model.decoders.VAEDecoder">VAEDecoder</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
      </tbody>
    </table>







<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="scprint2.model.decoders.ClsDecoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">ClsDecoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>



        <p>ClsDecoder Decoder for classification task.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>Dimension of the input.</p>
              </div>
            </li>
            <li>
              <b><code>n_cls</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>Number of classes.</p>
              </div>
            </li>
            <li>
              <b><code>layers</code></b>
                  (<code><span title="typing.List">List</span>[<span title="int">int</span>]</code>, default:
                      <code>[256, 128]</code>
)
              –
              <div class="doc-md-description">
                <p>List of hidden layers.</p>
              </div>
            </li>
            <li>
              <b><code>activation</code></b>
                  (<code><span title="typing.Callable">Callable</span></code>, default:
                      <code><span title="torch.nn.ReLU">ReLU</span></code>
)
              –
              <div class="doc-md-description">
                <p>Activation function.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>Dropout rate.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>










<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint2.model.decoders.ClsDecoder.forward)" href="#scprint2.model.decoders.ClsDecoder.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Args:</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/decoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_cls</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ClsDecoder Decoder for classification task.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): Dimension of the input.</span>
<span class="sd">        n_cls (int): Number of classes.</span>
<span class="sd">        layers (List[int]): List of hidden layers.</span>
<span class="sd">        activation (Callable): Activation function.</span>
<span class="sd">        dropout (float): Dropout rate.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ClsDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="c1"># module List</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">d_model</span><span class="p">]</span> <span class="o">+</span> <span class="n">layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_cls</span> <span class="o">=</span> <span class="n">n_cls</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">l</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">l</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_cls</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="scprint2.model.decoders.ClsDecoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape [batch_size, embsize]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/decoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [batch_size, embsize]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h3 id="scprint2.model.decoders.ExprDecoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">ExprDecoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>



        <p>ExprDecoder Decoder for the gene expression prediction.</p>
<p>Will output the mean, variance and zero logits, parameters of a zero inflated negative binomial distribution.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the model. This is the size of the input feature vector.</p>
              </div>
            </li>
            <li>
              <b><code>nfirst_tokens_to_skip</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>0</code>
)
              –
              <div class="doc-md-description">
                <p>The number of initial labels to skip in the sequence. Defaults to 0.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>The dropout rate applied during training to prevent overfitting. Defaults to 0.1.</p>
              </div>
            </li>
            <li>
              <b><code>zinb</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use a zero inflated negative binomial distribution. Defaults to True.</p>
              </div>
            </li>
            <li>
              <b><code>use_depth</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use depth as an additional feature. Defaults to False.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>










<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint2.model.decoders.ExprDecoder.forward)" href="#scprint2.model.decoders.ExprDecoder.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>x is the output of the transformer, (batch, seq_len, d_model)</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/decoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">nfirst_tokens_to_skip</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">zinb</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">use_depth</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ExprDecoder Decoder for the gene expression prediction.</span>

<span class="sd">    Will output the mean, variance and zero logits, parameters of a zero inflated negative binomial distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the model. This is the size of the input feature vector.</span>
<span class="sd">        nfirst_tokens_to_skip (int, optional): The number of initial labels to skip in the sequence. Defaults to 0.</span>
<span class="sd">        dropout (float, optional): The dropout rate applied during training to prevent overfitting. Defaults to 0.1.</span>
<span class="sd">        zinb (bool, optional): Whether to use a zero inflated negative binomial distribution. Defaults to True.</span>
<span class="sd">        use_depth (bool, optional): Whether to use depth as an additional feature. Defaults to False.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ExprDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">use_depth</span> <span class="k">else</span> <span class="n">d_model</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">3</span> <span class="k">if</span> <span class="n">zinb</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span> <span class="o">=</span> <span class="n">zinb</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="scprint2.model.decoders.ExprDecoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>x is the output of the transformer, (batch, seq_len, d_model)</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/decoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">req_depth</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;x is the output of the transformer, (batch, seq_len, d_model)&quot;&quot;&quot;</span>
    <span class="c1"># we don&#39;t do it on the labels</span>
    <span class="k">if</span> <span class="n">req_depth</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">req_depth</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)],</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span><span class="p">:</span>
        <span class="n">pred_value</span><span class="p">,</span> <span class="n">var_value</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># (batch, seq_len)</span>
        <span class="c1"># The sigmoid function is used to map the zero_logits to a probability between 0 and 1.</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">mean</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred_value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">disp</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">var_value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="nb">max</span><span class="o">=</span><span class="mi">15</span><span class="p">)),</span>
            <span class="n">zero_logits</span><span class="o">=</span><span class="n">zero_logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pred_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred_value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h3 id="scprint2.model.decoders.GraphSDEExprDecoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">GraphSDEExprDecoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>



        <p>Initialize the ExprNeuralSDEDecoder module.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the model.</p>
              </div>
            </li>
            <li>
              <b><code>drift</code></b>
                  (<code><span title="torch.nn.Module">Module</span></code>)
              –
              <div class="doc-md-description">
                <p>The drift component of the SDE.</p>
              </div>
            </li>
            <li>
              <b><code>diffusion</code></b>
                  (<code><span title="torch.nn.Module">Module</span></code>)
              –
              <div class="doc-md-description">
                <p>The diffusion component of the SDE.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>











                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/decoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">drift</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">diffusion</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initialize the ExprNeuralSDEDecoder module.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the model.</span>
<span class="sd">        drift (nn.Module): The drift component of the SDE.</span>
<span class="sd">        diffusion (nn.Module): The diffusion component of the SDE.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drift</span> <span class="o">=</span> <span class="n">drift</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">diffusion</span> <span class="o">=</span> <span class="n">diffusion</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">












  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h3 id="scprint2.model.decoders.MVCDecoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">MVCDecoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>



        <p>MVCDecoder Decoder for masked value prediction of cell embeddings.</p>
<p>Uses gene embeddings with cell embeddings to predict mean, variance, and zero logits
parameters of a zero-inflated negative binomial distribution.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>Dimension of the gene embedding.</p>
              </div>
            </li>
            <li>
              <b><code>arch_style</code></b>
                  (<code><span title="str">str</span></code>, default:
                      <code>&#39;inner product&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Architecture style of the decoder. Options:
"inner product": Uses inner product between cell and gene embeddings,
"concat query": Concatenates cell and gene embeddings,
"sum query": Sums cell and gene embeddings.
Defaults to "inner product".</p>
              </div>
            </li>
            <li>
              <b><code>tot_labels</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>Total number of labels in the input. Defaults to 1.</p>
              </div>
            </li>
            <li>
              <b><code>query_activation</code></b>
                  (<code><span title="torch.nn.Module">Module</span></code>, default:
                      <code><span title="torch.nn.Sigmoid">Sigmoid</span></code>
)
              –
              <div class="doc-md-description">
                <p>Activation function for query vectors. Defaults to nn.Sigmoid.</p>
              </div>
            </li>
            <li>
              <b><code>hidden_activation</code></b>
                  (<code><span title="torch.nn.Module">Module</span></code>, default:
                      <code><span title="torch.nn.PReLU">PReLU</span></code>
)
              –
              <div class="doc-md-description">
                <p>Activation function for hidden layers. Defaults to nn.PReLU.</p>
              </div>
            </li>
            <li>
              <b><code>use_depth</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use depth as an additional feature. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>zinb</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use a zero-inflated negative binomial distribution. Defaults to True.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>










<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint2.model.decoders.MVCDecoder.forward)" href="#scprint2.model.decoders.MVCDecoder.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Args:</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/decoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">arch_style</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;inner product&quot;</span><span class="p">,</span>
    <span class="n">tot_labels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">query_activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">,</span>
    <span class="n">hidden_activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">,</span>
    <span class="n">use_depth</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">zinb</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MVCDecoder Decoder for masked value prediction of cell embeddings.</span>

<span class="sd">    Uses gene embeddings with cell embeddings to predict mean, variance, and zero logits</span>
<span class="sd">    parameters of a zero-inflated negative binomial distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): Dimension of the gene embedding.</span>
<span class="sd">        arch_style (str, optional): Architecture style of the decoder. Options:</span>
<span class="sd">            &quot;inner product&quot;: Uses inner product between cell and gene embeddings,</span>
<span class="sd">            &quot;concat query&quot;: Concatenates cell and gene embeddings,</span>
<span class="sd">            &quot;sum query&quot;: Sums cell and gene embeddings.</span>
<span class="sd">            Defaults to &quot;inner product&quot;.</span>
<span class="sd">        tot_labels (int, optional): Total number of labels in the input. Defaults to 1.</span>
<span class="sd">        query_activation (nn.Module, optional): Activation function for query vectors. Defaults to nn.Sigmoid.</span>
<span class="sd">        hidden_activation (nn.Module, optional): Activation function for hidden layers. Defaults to nn.PReLU.</span>
<span class="sd">        use_depth (bool, optional): Whether to use depth as an additional feature. Defaults to False.</span>
<span class="sd">        zinb (bool, optional): Whether to use a zero-inflated negative binomial distribution. Defaults to True.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MVCDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;inner product&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">d_model</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">use_depth</span> <span class="k">else</span> <span class="n">d_model</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span> <span class="o">=</span> <span class="n">query_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">*</span> <span class="p">(</span><span class="mi">3</span> <span class="k">if</span> <span class="n">zinb</span> <span class="k">else</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;concat query&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">d_model</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">use_depth</span> <span class="k">else</span> <span class="n">d_model</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span> <span class="o">=</span> <span class="n">query_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">tot_labels</span><span class="p">),</span> <span class="n">d_model</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span> <span class="o">=</span> <span class="n">hidden_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span> <span class="k">if</span> <span class="n">zinb</span> <span class="k">else</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;sum query&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">d_model</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">use_depth</span> <span class="k">else</span> <span class="n">d_model</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span> <span class="o">=</span> <span class="n">query_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span> <span class="o">=</span> <span class="n">hidden_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span> <span class="k">if</span> <span class="n">zinb</span> <span class="k">else</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown arch_style: </span><span class="si">{</span><span class="n">arch_style</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">arch_style</span> <span class="o">=</span> <span class="n">arch_style</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_detach</span> <span class="o">=</span> <span class="n">arch_style</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;detach&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span> <span class="o">=</span> <span class="n">zinb</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="scprint2.model.decoders.MVCDecoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>cell_emb</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape (batch, embsize=d_model)</p>
              </div>
            </li>
            <li>
              <b><code>gene_embs</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape (batch, seq_len, embsize=d_model)</p>
              </div>
            </li>
            <li>
              <b><code>req_depth</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Tensor, shape (batch,), optional depth information.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="typing.Dict">Dict</span>[<span title="str">str</span>, <span title="torch.Tensor">Tensor</span>]]</code>
              –
              <div class="doc-md-description">
                <p>Dict[str, Tensor]: A dictionary containing the predicted mean, variance, and zero logits (if zinb is True).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/decoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">cell_emb</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">gene_embs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">req_depth</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        cell_emb: Tensor, shape (batch, embsize=d_model)</span>
<span class="sd">        gene_embs: Tensor, shape (batch, seq_len, embsize=d_model)</span>
<span class="sd">        req_depth: Tensor, shape (batch,), optional depth information.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict[str, Tensor]: A dictionary containing the predicted mean, variance, and zero logits (if zinb is True).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">req_depth</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">gene_embs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">gene_embs</span><span class="p">,</span>
                <span class="n">req_depth</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">gene_embs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
            <span class="p">],</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;inner product&quot;</span><span class="p">:</span>
        <span class="n">query_vecs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span><span class="p">(</span><span class="n">gene_embs</span><span class="p">)))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span><span class="p">:</span>
            <span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span><span class="p">(</span><span class="n">query_vecs</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span><span class="p">(</span><span class="n">query_vecs</span><span class="p">)</span>
        <span class="n">cell_emb</span> <span class="o">=</span> <span class="n">cell_emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span><span class="p">:</span>
            <span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">cell_emb</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">cell_emb</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">zero_logits</span><span class="p">,</span> <span class="n">cell_emb</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">cell_emb</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="c1"># zero logits need to based on the cell_emb, because of input exprs</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;concat query&quot;</span><span class="p">:</span>
        <span class="n">query_vecs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span><span class="p">(</span><span class="n">gene_embs</span><span class="p">))</span>
        <span class="c1"># expand cell_emb to (batch, seq_len, embsize)</span>
        <span class="n">cell_emb</span> <span class="o">=</span> <span class="n">cell_emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">gene_embs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cell_emb</span><span class="p">,</span> <span class="n">query_vecs</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span><span class="p">:</span>
            <span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;sum query&quot;</span><span class="p">:</span>
        <span class="n">query_vecs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span><span class="p">(</span><span class="n">gene_embs</span><span class="p">))</span>
        <span class="n">cell_emb</span> <span class="o">=</span> <span class="n">cell_emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">cell_emb</span> <span class="o">+</span> <span class="n">query_vecs</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span><span class="p">:</span>
            <span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">mvc_mean</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">mvc_disp</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">15</span><span class="p">)),</span>
            <span class="n">mvc_zero_logits</span><span class="o">=</span><span class="n">zero_logits</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mvc_mean</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>

<div class="doc doc-object doc-class">



<h3 id="scprint2.model.decoders.VAEDecoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">VAEDecoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>



        <p>VAEDecoder for variational autoencoding of cell embeddings.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>Input dimension (original embedding size)</p>
              </div>
            </li>
            <li>
              <b><code>layers</code></b>
                  (<code><span title="typing.List">List</span>[<span title="int">int</span>]</code>, default:
                      <code>[64, 64]</code>
)
              –
              <div class="doc-md-description">
                <p>List of hidden layer sizes for encoder and decoder</p>
              </div>
            </li>
            <li>
              <b><code>activation</code></b>
                  (<code><span title="typing.Callable">Callable</span></code>, default:
                      <code><span title="torch.nn.ReLU">ReLU</span></code>
)
              –
              <div class="doc-md-description">
                <p>Activation function to use</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>Dropout rate</p>
              </div>
            </li>
            <li>
              <b><code>return_latent</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to return the latent vectors</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>










<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint2.model.decoders.VAEDecoder.forward)" href="#scprint2.model.decoders.VAEDecoder.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Forward pass through VAE.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="kl_divergence (scprint2.model.decoders.VAEDecoder.kl_divergence)" href="#scprint2.model.decoders.VAEDecoder.kl_divergence">kl_divergence</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Compute KL divergence between N(mu, var) and N(0, 1).</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="reparameterize (scprint2.model.decoders.VAEDecoder.reparameterize)" href="#scprint2.model.decoders.VAEDecoder.reparameterize">reparameterize</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Reparameterization trick to sample from N(mu, var) from N(0,1).</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/decoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">],</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">return_latent</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    VAEDecoder for variational autoencoding of cell embeddings.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): Input dimension (original embedding size)</span>
<span class="sd">        layers (List[int]): List of hidden layer sizes for encoder and decoder</span>
<span class="sd">        activation (Callable): Activation function to use</span>
<span class="sd">        dropout (float): Dropout rate</span>
<span class="sd">        return_latent (bool): Whether to return the latent vectors</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">VAEDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="c1"># Encoder layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">return_latent</span> <span class="o">=</span> <span class="n">return_latent</span>
    <span class="n">encoder_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">d_model</span><span class="p">]</span> <span class="o">+</span> <span class="n">layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
        <span class="nb">zip</span><span class="p">(</span><span class="n">encoder_layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">encoder_layers</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">out_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">))</span>

    <span class="c1"># VAE latent parameters</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc_mu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">encoder_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">encoder_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc_var</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">encoder_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">encoder_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Decoder layers</span>
    <span class="n">decoder_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">encoder_layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span> <span class="o">+</span> <span class="p">[</span><span class="n">d_model</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
        <span class="nb">zip</span><span class="p">(</span>
            <span class="n">decoder_layers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">decoder_layers</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="p">)</span>  <span class="c1"># Changed to include final layer</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">))</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">decoder_layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span>
        <span class="p">):</span>  <span class="c1"># Don&#39;t apply activation/norm to final layer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">out_size</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="scprint2.model.decoders.VAEDecoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Forward pass through VAE.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Input tensor of shape [batch_size, d_model]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>, <span title="torch.Tensor">Tensor</span>, <span title="torch.Tensor">Tensor</span>, <span title="torch.Tensor">Tensor</span>, <span title="torch.Tensor">Tensor</span>]]</code>
              –
              <div class="doc-md-description">
                <p>If self.return_latent is True:
Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
    - reconstructed_x (Tensor): Reconstructed input, shape [batch_size, d_model]
    - mu (Tensor): Mean of the latent Gaussian, shape [batch_size, latent_dim]
    - log_var (Tensor): Log variance of the latent Gaussian, shape [batch_size, latent_dim]
    - kl_loss (Tensor): KL divergence loss (scalar tensor)</p>
              </div>
            </li>
            <li>
<b><code>Else</code></b>(                  <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>, <span title="torch.Tensor">Tensor</span>, <span title="torch.Tensor">Tensor</span>, <span title="torch.Tensor">Tensor</span>, <span title="torch.Tensor">Tensor</span>]]</code>
)              –
              <div class="doc-md-description">
                <p>Tensor: reconstructed_x of shape [batch_size, d_model]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/decoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass through VAE.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Input tensor of shape [batch_size, d_model]</span>

<span class="sd">    Returns:</span>
<span class="sd">        If self.return_latent is True:</span>
<span class="sd">            Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:</span>
<span class="sd">                - reconstructed_x (Tensor): Reconstructed input, shape [batch_size, d_model]</span>
<span class="sd">                - mu (Tensor): Mean of the latent Gaussian, shape [batch_size, latent_dim]</span>
<span class="sd">                - log_var (Tensor): Log variance of the latent Gaussian, shape [batch_size, latent_dim]</span>
<span class="sd">                - kl_loss (Tensor): KL divergence loss (scalar tensor)</span>
<span class="sd">        Else:</span>
<span class="sd">            Tensor: reconstructed_x of shape [batch_size, d_model]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Encode</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Get latent parameters</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_mu</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
    <span class="n">log_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_var</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
    <span class="n">log_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">log_var</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mi">10</span><span class="p">)</span>

    <span class="c1"># Sample latent vector</span>
    <span class="n">kl_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">)</span>
    <span class="c1"># free_bits = 2.0  # per latent dim</span>
    <span class="c1"># kl_loss = torch.clamp(kl_loss / mu.size(-1), min=free_bits) * mu.size(-1)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">)</span>

    <span class="c1"># Decode</span>
    <span class="n">decoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_latent</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">decoded</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">,</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">kl_loss</span>
    <span class="k">return</span> <span class="n">decoded</span><span class="p">,</span> <span class="n">kl_loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.decoders.VAEDecoder.kl_divergence" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">kl_divergence</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Compute KL divergence between N(mu, var) and N(0, 1).</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>mu</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Mean of the latent Gaussian</p>
              </div>
            </li>
            <li>
              <b><code>log_var</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Log variance of the latent Gaussian</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>KL divergence loss</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/decoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">kl_divergence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">log_var</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute KL divergence between N(mu, var) and N(0, 1).</span>

<span class="sd">    Args:</span>
<span class="sd">        mu (Tensor): Mean of the latent Gaussian</span>
<span class="sd">        log_var (Tensor): Log variance of the latent Gaussian</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: KL divergence loss</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># KL(N(mu, var) || N(0, 1)) = -0.5 * sum(1 + log(var) - mu^2 - var)</span>
    <span class="n">kl_loss</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">log_var</span> <span class="o">-</span> <span class="n">mu</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_var</span><span class="o">.</span><span class="n">exp</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">kl_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.decoders.VAEDecoder.reparameterize" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">reparameterize</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Reparameterization trick to sample from N(mu, var) from N(0,1).</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>mu</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Mean of the latent Gaussian</p>
              </div>
            </li>
            <li>
              <b><code>log_var</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Log variance of the latent Gaussian</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>Sampled latent vector</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/decoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">reparameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">log_var</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reparameterization trick to sample from N(mu, var) from N(0,1).</span>

<span class="sd">    Args:</span>
<span class="sd">        mu (Tensor): Mean of the latent Gaussian</span>
<span class="sd">        log_var (Tensor): Log variance of the latent Gaussian</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: Sampled latent vector</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">log_var</span><span class="p">)</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">std</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="scprint2.model.fsq" class="doc doc-heading">
            <code>scprint2.model.fsq</code>


</h2>

    <div class="doc doc-contents first">

        <p>Finite Scalar Quantization: VQ-VAE Made Simple - https://arxiv.org/abs/2309.15505
Code adapted from Jax version in Appendix A.1</p>










<p><span class="doc-section-title">Classes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="FSQ (scprint2.model.fsq.FSQ)" href="#scprint2.model.fsq.FSQ">FSQ</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
      </tbody>
    </table>




<p><span class="doc-section-title">Functions:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="round_ste (scprint2.model.fsq.round_ste)" href="#scprint2.model.fsq.round_ste">round_ste</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Round with straight through gradients.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>





<div class="doc doc-children">









<div class="doc doc-object doc-class">



<h3 id="scprint2.model.fsq.FSQ" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">FSQ</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>













<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="bound (scprint2.model.fsq.FSQ.bound)" href="#scprint2.model.fsq.FSQ.bound">bound</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Bound <code>z</code>, an array of shape (..., d).</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="codes_to_indices (scprint2.model.fsq.FSQ.codes_to_indices)" href="#scprint2.model.fsq.FSQ.codes_to_indices">codes_to_indices</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Converts a <code>code</code> to an index in the codebook.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint2.model.fsq.FSQ.forward)" href="#scprint2.model.fsq.FSQ.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>einstein notation</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="indices_to_codes (scprint2.model.fsq.FSQ.indices_to_codes)" href="#scprint2.model.fsq.FSQ.indices_to_codes">indices_to_codes</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Inverse of <code>codes_to_indices</code>.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="quantize (scprint2.model.fsq.FSQ.quantize)" href="#scprint2.model.fsq.FSQ.quantize">quantize</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Quantizes z, returns quantized zhat, same shape as z.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint2/model/fsq.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span>
<span class="normal">97</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">levels</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_codebooks</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">keep_num_codebooks_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="n">_levels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">levels</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int32</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_levels&quot;</span><span class="p">,</span> <span class="n">_levels</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">_basis</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">levels</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int32</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;_basis&quot;</span><span class="p">,</span> <span class="n">_basis</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>

    <span class="n">codebook_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">levels</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">codebook_dim</span> <span class="o">=</span> <span class="n">codebook_dim</span>

    <span class="n">effective_codebook_dim</span> <span class="o">=</span> <span class="n">codebook_dim</span> <span class="o">*</span> <span class="n">num_codebooks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_codebooks</span> <span class="o">=</span> <span class="n">num_codebooks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">effective_codebook_dim</span> <span class="o">=</span> <span class="n">effective_codebook_dim</span>

    <span class="n">keep_num_codebooks_dim</span> <span class="o">=</span> <span class="n">default</span><span class="p">(</span><span class="n">keep_num_codebooks_dim</span><span class="p">,</span> <span class="n">num_codebooks</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">num_codebooks</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">keep_num_codebooks_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">keep_num_codebooks_dim</span> <span class="o">=</span> <span class="n">keep_num_codebooks_dim</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">default</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">_levels</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_codebooks</span><span class="p">)</span>

    <span class="n">has_projections</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">!=</span> <span class="n">effective_codebook_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">project_in</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">effective_codebook_dim</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">has_projections</span>
        <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">project_out</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">effective_codebook_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">has_projections</span>
        <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">has_projections</span> <span class="o">=</span> <span class="n">has_projections</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">codebook_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_levels</span><span class="o">.</span><span class="n">prod</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">implicit_codebook</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices_to_codes</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">codebook_size</span><span class="p">),</span> <span class="n">project_out</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;implicit_codebook&quot;</span><span class="p">,</span> <span class="n">implicit_codebook</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



<div class="doc doc-children">










<div class="doc doc-object doc-function">


<h4 id="scprint2.model.fsq.FSQ.bound" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">bound</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Bound <code>z</code>, an array of shape (..., d).</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/fsq.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">bound</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Bound `z`, an array of shape (..., d).&quot;&quot;&quot;</span>
    <span class="n">half_l</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_levels</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">eps</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_levels</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="n">shift</span> <span class="o">=</span> <span class="p">(</span><span class="n">offset</span> <span class="o">/</span> <span class="n">half_l</span><span class="p">)</span><span class="o">.</span><span class="n">tan</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">z</span> <span class="o">+</span> <span class="n">shift</span><span class="p">)</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span> <span class="o">*</span> <span class="n">half_l</span> <span class="o">-</span> <span class="n">offset</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.fsq.FSQ.codes_to_indices" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">codes_to_indices</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Converts a <code>code</code> to an index in the codebook.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/fsq.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">codes_to_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">zhat</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Converts a `code` to an index in the codebook.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">zhat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">codebook_dim</span>
    <span class="n">zhat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale_and_shift</span><span class="p">(</span><span class="n">zhat</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">zhat</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_basis</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">int32</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.fsq.FSQ.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>einstein notation
b - batch
n - sequence (or flattened spatial dimensions)
d - feature dimension, which is also log2(codebook size)
c - number of codebook dim</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/fsq.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    einstein notation</span>
<span class="sd">    b - batch</span>
<span class="sd">    n - sequence (or flattened spatial dimensions)</span>
<span class="sd">    d - feature dimension, which is also log2(codebook size)</span>
<span class="sd">    c - number of codebook dim</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;expected dimension of </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="si">}</span><span class="s2"> but found dimension of </span><span class="si">{</span><span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
    <span class="n">small</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">project_in</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">small</span><span class="p">,</span> <span class="s2">&quot;b (c d) -&gt; b c d&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_codebooks</span><span class="p">)</span>

    <span class="n">codes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">codes_to_indices</span><span class="p">(</span><span class="n">codes</span><span class="p">)</span>

    <span class="n">codes</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">codes</span><span class="p">,</span> <span class="s2">&quot;b c d -&gt; b (c d)&quot;</span><span class="p">)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">project_out</span><span class="p">(</span><span class="n">codes</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_num_codebooks_dim</span><span class="p">:</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="s2">&quot;... 1 -&gt; ...&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">small</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.fsq.FSQ.indices_to_codes" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">indices_to_codes</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Inverse of <code>codes_to_indices</code>.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/fsq.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">indices_to_codes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">project_out</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Inverse of `codes_to_indices`.&quot;&quot;&quot;</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="s2">&quot;... -&gt; ... 1&quot;</span><span class="p">)</span>
    <span class="n">codes_non_centered</span> <span class="o">=</span> <span class="p">(</span><span class="n">indices</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">_basis</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_levels</span>
    <span class="n">codes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scale_and_shift_inverse</span><span class="p">(</span><span class="n">codes_non_centered</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_num_codebooks_dim</span><span class="p">:</span>
        <span class="n">codes</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">codes</span><span class="p">,</span> <span class="s2">&quot;... c d -&gt; ... (c d)&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">project_out</span><span class="p">:</span>
        <span class="n">codes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">project_out</span><span class="p">(</span><span class="n">codes</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">codes</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint2.model.fsq.FSQ.quantize" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">quantize</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Quantizes z, returns quantized zhat, same shape as z.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/fsq.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">quantize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Quantizes z, returns quantized zhat, same shape as z.&quot;&quot;&quot;</span>
    <span class="n">quantized</span> <span class="o">=</span> <span class="n">round_ste</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bound</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
    <span class="n">half_width</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_levels</span> <span class="o">//</span> <span class="mi">2</span>  <span class="c1"># Renormalize to [-1, 1].</span>
    <span class="k">return</span> <span class="n">quantized</span> <span class="o">/</span> <span class="n">half_width</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>


</div>


<div class="doc doc-object doc-function">


<h3 id="scprint2.model.fsq.round_ste" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">round_ste</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Round with straight through gradients.</p>


            <details class="mkdocstrings-source">
              <summary>Source code in <code>scprint2/model/fsq.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">round_ste</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Round with straight through gradients.&quot;&quot;&quot;</span>
    <span class="n">zhat</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">round</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">z</span> <span class="o">+</span> <span class="p">(</span><span class="n">zhat</span> <span class="o">-</span> <span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../umap/nice_umap_scprint2.html" class="btn btn-neutral float-left" title="350M subset umap"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../tokenizers/" class="btn btn-neutral float-right" title="tokenizers">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../umap/nice_umap_scprint2.html" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../tokenizers/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
