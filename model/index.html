<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://www.jkobject.com/scPRINT/model/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>model - scprint</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "model";
        var mkdocs_page_input_path = "model.md";
        var mkdocs_page_url = "/scPRINT/model/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> scprint
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../structure/">structure</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../pretrain/">pre-training</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../usage/">usage example</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">example notebooks</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../notebooks/cancer_usecase/">scPRINT use case on BPH</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../notebooks/cancer_usecase_part2/">scPRINT use case on BPH (part 2, GN analysis)</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">documentation</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">model</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#model-description">model description</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.model">model</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.model.scPrint">scPrint</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.configure_optimizers">configure_optimizers</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.forward">forward</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.log_adata">log_adata</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.on_fit_start">on_fit_start</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.on_predict_epoch_end">on_predict_epoch_end</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.on_predict_epoch_start">on_predict_epoch_start</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.on_validation_epoch_end">on_validation_epoch_end</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.optimizer_step">optimizer_step</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.predict_step">predict_step</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.training_step">training_step</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.validation_step">validation_step</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#losses">losses</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.loss">loss</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.AdversarialDiscriminatorLoss">AdversarialDiscriminatorLoss</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.loss.AdversarialDiscriminatorLoss.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.classification">classification</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.contrastive_loss">contrastive_loss</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.criterion_neg_log_bernoulli">criterion_neg_log_bernoulli</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.ecs">ecs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.grad_reverse">grad_reverse</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.masked_mae">masked_mae</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.masked_mse">masked_mse</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.masked_nb">masked_nb</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.masked_relative_error">masked_relative_error</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.mse">mse</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.nb">nb</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.nb_dist">nb_dist</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.within_sample">within_sample</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.zinb">zinb</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#utils">utils</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.utils">utils</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.Attention">Attention</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.utils.Attention.add_attn">add_attn</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.utils.Attention.add_qk">add_qk</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.utils.Attention.get">get</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.WeightedMasker">WeightedMasker</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.downsample_profile">downsample_profile</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.make_adata">make_adata</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.simple_masker">simple_masker</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.test">test</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.zinb_sample">zinb_sample</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#encoder-and-decoder-modules">encoder and decoder modules</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.encoders">encoders</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.encoders.CategoryValueEncoder">CategoryValueEncoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.encoders.ContinuousValueEncoder">ContinuousValueEncoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.encoders.ContinuousValueEncoder.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.encoders.DPositionalEncoding">DPositionalEncoding</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.encoders.DPositionalEncoding.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.encoders.GeneEncoder">GeneEncoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.encoders.PositionalEncoding">PositionalEncoding</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.encoders.PositionalEncoding.forward">forward</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.decoders">decoders</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.decoders.ClsDecoder">ClsDecoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.decoders.ClsDecoder.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.decoders.ExprDecoder">ExprDecoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.decoders.ExprDecoder.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.decoders.GraphSDEExprDecoder">GraphSDEExprDecoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.decoders.MVCDecoder">MVCDecoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.decoders.MVCDecoder.forward">forward</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tasks/">tasks</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../cli/">cli</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../embedder/">embedders</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../utils/">utils</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">scprint</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">documentation</li>
      <li class="breadcrumb-item active">model</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="documentation-for-the-model">Documentation for the <code>model</code></h1>
<h2 id="model-description">model description</h2>


<div class="doc doc-object doc-module">



<h2 id="scprint.model.model" class="doc doc-heading">
            <code>scprint.model.model</code>


</h2>

    <div class="doc doc-contents first">








<p><span class="doc-section-title">Classes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="scPrint (scprint.model.model.scPrint)" href="#scprint.model.model.scPrint">scPrint</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
      </tbody>
    </table>







  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="scprint.model.model.scPrint" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">scPrint</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="lightning.LightningModule">LightningModule</span></code>, <code><span title="huggingface_hub.PyTorchModelHubMixin">PyTorchModelHubMixin</span></code></p>


        <p>scPRINT transformer for single cell biology and the inference of Gene Regulatory networks</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>genes</code></b>
                  (<code><span title="list">list</span></code>)
              –
              <div class="doc-md-description">
                <p>List of gene names the model will work with.</p>
              </div>
            </li>
            <li>
              <b><code>precpt_gene_emb</code></b>
                  (<code><span title="numpy.array">array</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Gene embeddings of size (len(genes), d_model). Should be in the same order as the genes. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>gene_pos_enc</code></b>
                  (<code><span title="list">list</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Gene position encoding of the same size as genes. Provides a location value for each gene in genes. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>256</code>
)
              –
              <div class="doc-md-description">
                <p>Dimension of the model. Defaults to 512.</p>
              </div>
            </li>
            <li>
              <b><code>nhead</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>4</code>
)
              –
              <div class="doc-md-description">
                <p>Number of heads in the multihead attention models. Defaults to 8.</p>
              </div>
            </li>
            <li>
              <b><code>d_hid</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>Dimension of the feedforward network model. Defaults to 512.</p>
              </div>
            </li>
            <li>
              <b><code>nlayers</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>8</code>
)
              –
              <div class="doc-md-description">
                <p>Number of layers in the transformer model. Defaults to 6.</p>
              </div>
            </li>
            <li>
              <b><code>expr_encoder_layers</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>2</code>
)
              –
              <div class="doc-md-description">
                <p>Number of layers in the expression encoder. Defaults to 2.</p>
              </div>
            </li>
            <li>
              <b><code>layers_cls</code></b>
                  (<code><span title="list">list</span>[<span title="int">int</span>]</code>, default:
                      <code>[]</code>
)
              –
              <div class="doc-md-description">
                <p>List specifying the number of layers in the classifier. Defaults to [].</p>
              </div>
            </li>
            <li>
              <b><code>classes</code></b>
                  (<code><span title="typing.Dict">Dict</span>[<span title="str">str</span>, <span title="int">int</span>]</code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>Classes to predict with the number of classes for each. Defaults to {}.</p>
              </div>
            </li>
            <li>
              <b><code>labels_hierarchy</code></b>
                  (<code><span title="typing.Dict">Dict</span>[<span title="str">str</span>, <span title="typing.Dict">Dict</span>[<span title="int">int</span>, <span title="list">list</span>[<span title="int">int</span>]]]</code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>Class hierarchy for classes with hierarchical classes. Defaults to {}.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>Dropout value. Defaults to 0.2.</p>
              </div>
            </li>
            <li>
              <b><code>transformer</code></b>
                  (<code><span title="str">str</span></code>, default:
                      <code>&#39;flash&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Transformer type to use. One of "linear", "flash", "flashsparse", "scprint". Defaults to "fast".</p>
              </div>
            </li>
            <li>
              <b><code>domain_spec_batchnorm</code></b>
                  (<code><span title="str">str</span></code>, default:
                      <code>&#39;None&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to apply domain-specific batch normalization. Defaults to "None".</p>
              </div>
            </li>
            <li>
              <b><code>expr_emb_style</code></b>
                  (<code><span title="str">str</span></code>, default:
                      <code>&#39;continuous&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Style of input embedding. One of "continuous", "binned_pos", "cont_pos". Defaults to "continuous".</p>
              </div>
            </li>
            <li>
              <b><code>mvc_decoder</code></b>
                  (<code><span title="str">str</span></code>, default:
                      <code>&#39;None&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Style of MVC decoder. One of "None", "inner product", "concat query", "sum query". Defaults to "None".</p>
              </div>
            </li>
            <li>
              <b><code>pred_embedding</code></b>
                  (<code><span title="list">list</span>[<span title="str">str</span>]</code>, default:
                      <code>[]</code>
)
              –
              <div class="doc-md-description">
                <p>List of classes to use for plotting embeddings. Defaults to [].</p>
              </div>
            </li>
            <li>
              <b><code>cell_emb_style</code></b>
                  (<code><span title="str">str</span></code>, default:
                      <code>&#39;cls&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Style of cell embedding. One of "cls", "avg-pool", "w-pool". Defaults to "cls".</p>
              </div>
            </li>
            <li>
              <b><code>freeze_embeddings</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to freeze the embeddings during training. Defaults to True.</p>
              </div>
            </li>
            <li>
              <b><code>label_decoders</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[<span title="str">str</span>, <span title="typing.Dict">Dict</span>[<span title="int">int</span>, <span title="str">str</span>]]]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Label decoders to use for plotting the UMAP during validations. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>zinb</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use Zero-Inflated Negative Binomial distribution. Defaults to True.</p>
              </div>
            </li>
            <li>
              <b><code>use_metacell_token</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use a metacell token. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>**flash_attention_kwargs</code></b>
                  (<code><span title="dict">dict</span></code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>Additional keyword arguments for the model. see @flashformer.py</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="notes" open>
  <summary>Notes</summary>
  <p>for other parameters of the model that are not part of its class definition, see @trainer.trainer.py</p>
</details>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="ValueError">ValueError</span></code>
              –
              <div class="doc-md-description">
                <p>If the expr_emb_style is not one of "continuous", "binned_pos", "cont_pos".</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>









<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="configure_optimizers (scprint.model.model.scPrint.configure_optimizers)" href="#scprint.model.model.scPrint.configure_optimizers">configure_optimizers</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>@see pl.LightningModule</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint.model.model.scPrint.forward)" href="#scprint.model.model.scPrint.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>forward also called on self(), a full forward pass on the model</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="log_adata (scprint.model.model.scPrint.log_adata)" href="#scprint.model.model.scPrint.log_adata">log_adata</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>log_adata will log an adata from predictions.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="on_fit_start (scprint.model.model.scPrint.on_fit_start)" href="#scprint.model.model.scPrint.on_fit_start">on_fit_start</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>@see pl.LightningModule</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="on_predict_epoch_end (scprint.model.model.scPrint.on_predict_epoch_end)" href="#scprint.model.model.scPrint.on_predict_epoch_end">on_predict_epoch_end</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>@see pl.LightningModule will</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="on_predict_epoch_start (scprint.model.model.scPrint.on_predict_epoch_start)" href="#scprint.model.model.scPrint.on_predict_epoch_start">on_predict_epoch_start</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>@see pl.LightningModule</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="on_validation_epoch_end (scprint.model.model.scPrint.on_validation_epoch_end)" href="#scprint.model.model.scPrint.on_validation_epoch_end">on_validation_epoch_end</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>@see pl.LightningModule</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="optimizer_step (scprint.model.model.scPrint.optimizer_step)" href="#scprint.model.model.scPrint.optimizer_step">optimizer_step</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>@see pl.LightningModule</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="predict_step (scprint.model.model.scPrint.predict_step)" href="#scprint.model.model.scPrint.predict_step">predict_step</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>embed given gene expression, encode the gene embedding and cell embedding.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="training_step (scprint.model.model.scPrint.training_step)" href="#scprint.model.model.scPrint.training_step">training_step</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>training_step defines the train loop. It is independent of forward</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="validation_step (scprint.model.model.scPrint.validation_step)" href="#scprint.model.model.scPrint.validation_step">validation_step</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>validation_step defines the validation loop. It is independent of forward</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint/model/model.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">genes</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
    <span class="n">organisms</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;NCBITaxon:9606&quot;</span><span class="p">],</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
    <span class="n">nhead</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">nlayers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">precpt_gene_emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">gene_pos_enc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">normalization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span>
    <span class="n">attn_bias</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span>
    <span class="n">expr_encoder_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">transformer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;flash&quot;</span><span class="p">,</span>  <span class="c1"># &quot;performer&quot;, &quot;flash&quot;, &quot;normal&quot;, &quot;crisscross&quot;</span>
    <span class="n">expr_emb_style</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;continuous&quot;</span><span class="p">,</span>  <span class="c1"># &quot;binned_pos&quot;, &quot;cont_pos&quot;</span>
    <span class="n">domain_spec_batchnorm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
    <span class="n">n_input_bins</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">num_batch_labels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">label_counts</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">mvc_decoder</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
    <span class="n">pred_embedding</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">layers_cls</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">classes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">labels_hierarchy</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">label_decoders</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">compress_class_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cell_emb_style</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cls&quot;</span><span class="p">,</span>
    <span class="n">cell_specific_blocks</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">depth_atinput</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">freeze_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">zinb</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">use_metacell_token</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0001</span><span class="p">,</span>
    <span class="o">**</span><span class="n">flash_attention_kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    scPRINT transformer for single cell biology and the inference of Gene Regulatory networks</span>

<span class="sd">    Args:</span>
<span class="sd">        genes (list): List of gene names the model will work with.</span>
<span class="sd">        precpt_gene_emb (np.array, optional): Gene embeddings of size (len(genes), d_model). Should be in the same order as the genes. Defaults to None.</span>
<span class="sd">        gene_pos_enc (list, optional): Gene position encoding of the same size as genes. Provides a location value for each gene in genes. Defaults to None.</span>
<span class="sd">        d_model (int, optional): Dimension of the model. Defaults to 512.</span>
<span class="sd">        nhead (int, optional): Number of heads in the multihead attention models. Defaults to 8.</span>
<span class="sd">        d_hid (int, optional): Dimension of the feedforward network model. Defaults to 512.</span>
<span class="sd">        nlayers (int, optional): Number of layers in the transformer model. Defaults to 6.</span>
<span class="sd">        expr_encoder_layers (int, optional): Number of layers in the expression encoder. Defaults to 2.</span>
<span class="sd">        layers_cls (list[int], optional): List specifying the number of layers in the classifier. Defaults to [].</span>
<span class="sd">        classes (Dict[str, int], optional): Classes to predict with the number of classes for each. Defaults to {}.</span>
<span class="sd">        labels_hierarchy (Dict[str, Dict[int, list[int]]], optional): Class hierarchy for classes with hierarchical classes. Defaults to {}.</span>
<span class="sd">        dropout (float, optional): Dropout value. Defaults to 0.2.</span>
<span class="sd">        transformer (str, optional): Transformer type to use. One of &quot;linear&quot;, &quot;flash&quot;, &quot;flashsparse&quot;, &quot;scprint&quot;. Defaults to &quot;fast&quot;.</span>
<span class="sd">        domain_spec_batchnorm (str, optional): Whether to apply domain-specific batch normalization. Defaults to &quot;None&quot;.</span>
<span class="sd">        expr_emb_style (str, optional): Style of input embedding. One of &quot;continuous&quot;, &quot;binned_pos&quot;, &quot;cont_pos&quot;. Defaults to &quot;continuous&quot;.</span>
<span class="sd">        mvc_decoder (str, optional): Style of MVC decoder. One of &quot;None&quot;, &quot;inner product&quot;, &quot;concat query&quot;, &quot;sum query&quot;. Defaults to &quot;None&quot;.</span>
<span class="sd">        pred_embedding (list[str], optional): List of classes to use for plotting embeddings. Defaults to [].</span>
<span class="sd">        cell_emb_style (str, optional): Style of cell embedding. One of &quot;cls&quot;, &quot;avg-pool&quot;, &quot;w-pool&quot;. Defaults to &quot;cls&quot;.</span>
<span class="sd">        freeze_embeddings (bool, optional): Whether to freeze the embeddings during training. Defaults to True.</span>
<span class="sd">        label_decoders (Optional[Dict[str, Dict[int, str]]], optional): Label decoders to use for plotting the UMAP during validations. Defaults to None.</span>
<span class="sd">        zinb (bool, optional): Whether to use Zero-Inflated Negative Binomial distribution. Defaults to True.</span>
<span class="sd">        use_metacell_token (bool, optional): Whether to use a metacell token. Defaults to False.</span>
<span class="sd">        **flash_attention_kwargs (dict): Additional keyword arguments for the model. see @flashformer.py</span>

<span class="sd">    Notes:</span>
<span class="sd">        for other parameters of the model that are not part of its class definition, see @trainer.trainer.py</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the expr_emb_style is not one of &quot;continuous&quot;, &quot;binned_pos&quot;, &quot;cont_pos&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
    <span class="c1"># training flags</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_denoise</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.6</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_cce</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cce_temp</span> <span class="o">=</span> <span class="mf">0.2</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cce_scale</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_ecs</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ecs_threshold</span> <span class="o">=</span> <span class="mf">0.4</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ecs_scale</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_mvc</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mvc_scale</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">class_embd_diss_scale</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_adv_cls</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">adv_class_scale</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_cls</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mean_attn_tot</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mean_attn_tot_c</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_adv_batch</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">run_full_forward</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">class_scale</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">zinb_and_mse</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_next_tp</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_generate</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">var_context_length</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mask_ratio</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">warmup_duration</span> <span class="o">=</span> <span class="mi">500</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">0.01</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optim</span> <span class="o">=</span> <span class="s2">&quot;adamW&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fused_adam</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_patience</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_factor</span> <span class="o">=</span> <span class="mf">0.6</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">test_every</span> <span class="o">=</span> <span class="mi">20</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_monitor</span> <span class="o">=</span> <span class="s2">&quot;val_loss&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">set_step</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">doplot</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">get_attention_layer</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embs</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred_log_adata</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">predict_depth_mult</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">predict_mode</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">keep_all_cls_pred</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cell_separation</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">depth_atinput</span> <span class="o">=</span> <span class="n">depth_atinput</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">Attention</span><span class="p">(</span>
        <span class="nb">len</span><span class="p">(</span><span class="n">genes</span><span class="p">),</span>
        <span class="n">additional_tokens</span><span class="o">=</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">2</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth_atinput</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">cell_specific_blocks</span>
            <span class="k">else</span> <span class="mi">0</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tf_masker</span> <span class="o">=</span> <span class="n">WeightedMasker</span><span class="p">(</span><span class="n">genes</span><span class="p">,</span> <span class="n">inv_weight</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="c1"># should be stored somehow</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">normalization</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">organisms</span> <span class="o">=</span> <span class="n">organisms</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_bias</span> <span class="o">=</span> <span class="n">attn_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nlayers</span> <span class="o">=</span> <span class="n">nlayers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gene_pos_enc</span> <span class="o">=</span> <span class="n">gene_pos_enc</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_metacell_token</span> <span class="o">=</span> <span class="n">use_metacell_token</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mvc_decoder</span> <span class="o">=</span> <span class="n">mvc_decoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">domain_spec_batchnorm</span> <span class="o">=</span> <span class="n">domain_spec_batchnorm</span>
    <span class="c1"># need to store</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_input_bins</span> <span class="o">=</span> <span class="n">n_input_bins</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">transformer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">label_counts</span> <span class="o">=</span> <span class="n">classes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">classes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">classes</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="k">if</span> <span class="n">cell_emb_style</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;cls&quot;</span><span class="p">,</span> <span class="s2">&quot;avg-pool&quot;</span><span class="p">,</span> <span class="s2">&quot;w-pool&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown cell_emb_style: </span><span class="si">{</span><span class="n">cell_emb_style</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cell_emb_style</span> <span class="o">=</span> <span class="n">cell_emb_style</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">label_decoders</span> <span class="o">=</span> <span class="n">label_decoders</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred_embedding</span> <span class="o">=</span> <span class="n">pred_embedding</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">genes</span> <span class="o">=</span> <span class="n">genes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">n</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">genes</span><span class="p">)}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">expr_emb_style</span> <span class="o">=</span> <span class="n">expr_emb_style</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">expr_emb_style</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;category&quot;</span><span class="p">,</span> <span class="s2">&quot;continuous&quot;</span><span class="p">,</span> <span class="s2">&quot;none&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;expr_emb_style should be one of category, continuous, scaling, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;got </span><span class="si">{</span><span class="n">expr_emb_style</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">labels_hierarchy</span> <span class="o">=</span> <span class="n">labels_hierarchy</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="p">[</span><span class="s2">&quot;labels_hierarchy&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels_hierarchy</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="p">[</span><span class="s2">&quot;classes&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="p">[</span><span class="s2">&quot;label_decoders&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_decoders</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="p">[</span><span class="s2">&quot;label_counts&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_counts</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="p">[</span><span class="s2">&quot;gene_pos_enc&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gene_pos_enc</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="p">[</span><span class="s2">&quot;genes&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">genes</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mat_labels_hierarchy</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">labels_hierarchy</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">tens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">classes</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
        <span class="k">for</span> <span class="n">k2</span><span class="p">,</span> <span class="n">v2</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">tens</span><span class="p">[</span><span class="n">k2</span> <span class="o">-</span> <span class="n">classes</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">v2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mat_labels_hierarchy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">tens</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>

    <span class="c1"># encoder</span>
    <span class="c1"># gene encoder</span>
    <span class="k">if</span> <span class="n">precpt_gene_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">precpt_gene_emb</span><span class="p">)</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;the gene embeddings file </span><span class="si">{</span><span class="n">precpt_gene_emb</span><span class="si">}</span><span class="s2"> does not contain any of the genes given to the model&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="s2">&quot;Warning: only a subset of the genes available in the embeddings file.&quot;</span>
            <span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;number of genes: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">))</span>
        <span class="n">sembeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="n">d_model</span><span class="p">)(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gene_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">GeneEncoder</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="c1"># weights_file=precpt_gene_emb,</span>
            <span class="n">weights</span><span class="o">=</span><span class="n">sembeddings</span><span class="p">,</span>
            <span class="n">freeze</span><span class="o">=</span><span class="n">freeze_embeddings</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">GeneEncoder</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="c1"># Value Encoder, NOTE: the scaling style is also handled in _encode method</span>
    <span class="k">if</span> <span class="n">expr_emb_style</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;continuous&quot;</span><span class="p">,</span> <span class="s2">&quot;full_pos&quot;</span><span class="p">]:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">ContinuousValueEncoder</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">expr_encoder_layers</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">expr_emb_style</span> <span class="o">==</span> <span class="s2">&quot;binned_pos&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">n_input_bins</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">CategoryValueEncoder</span><span class="p">(</span><span class="n">n_input_bins</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>

    <span class="c1"># Positional Encoding</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gene_pos_enc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">gene_pos_enc</span><span class="p">)</span>
        <span class="n">token_to_pos</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">pos</span> <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene_pos_enc</span><span class="p">)}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">PositionalEncoding</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="n">max_len</span><span class="p">,</span> <span class="n">token_to_pos</span><span class="o">=</span><span class="n">token_to_pos</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">cell_embs_count</span> <span class="o">=</span> <span class="p">(</span>
        <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">)</span>
        <span class="o">+</span> <span class="p">(</span><span class="mi">2</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth_atinput</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span>
        <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_metacell_token</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="c1"># Class Encoder</span>
    <span class="c1"># always have [base_cell_emb, time_embedding, depth_embedding] + any other class info</span>
    <span class="c1"># base cell embedding will store other cell specific information</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">class_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">CategoryValueEncoder</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_embs_count</span>
        <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth_atinput</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
        <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_metacell_token</span> <span class="k">else</span> <span class="mi">0</span><span class="p">),</span>
        <span class="n">d_model</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># self.time_encoder = encoders.ContinuousValueEncoder(d_model, dropout)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth_atinput</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">depth_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">ContinuousValueEncoder</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">expr_encoder_layers</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_metacell_token</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metacell_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">CategoryValueEncoder</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="c1"># compute tensor for mat_labels_hierarchy</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="s2">&quot;strict_loading&quot;</span><span class="p">,</span>
        <span class="s2">&quot;optim&quot;</span><span class="p">,</span>
        <span class="s2">&quot;weight_decay&quot;</span><span class="p">,</span>
        <span class="s2">&quot;d_hid&quot;</span><span class="p">,</span>
        <span class="s2">&quot;edge_dim&quot;</span><span class="p">,</span>
        <span class="s2">&quot;prenorm&quot;</span><span class="p">,</span>
        <span class="s2">&quot;use_flash_attn&quot;</span><span class="p">,</span>
    <span class="p">]:</span>
        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">flash_attention_kwargs</span><span class="p">:</span>
            <span class="n">flash_attention_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="c1"># Transformer</span>
    <span class="c1"># Linear</span>
    <span class="k">if</span> <span class="n">transformer</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
        <span class="c1"># linear transformer using the fast transformer package</span>
        <span class="c1"># self.transformer = FastTransformerEncoder(</span>
        <span class="c1">#    d_model, nhead, d_hid, nlayers, dropout, &quot;linear&quot;</span>
        <span class="c1"># )</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Linear transformer is not implemented&quot;</span><span class="p">)</span>
    <span class="c1"># regular or flash</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">FlashTransformer</span><span class="p">(</span>
            <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
            <span class="n">nhead</span><span class="o">=</span><span class="n">nhead</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">nlayers</span><span class="o">=</span><span class="n">nlayers</span><span class="p">,</span>
            <span class="n">cross_attn</span><span class="o">=</span><span class="n">cell_specific_blocks</span><span class="p">,</span>
            <span class="n">use_flash_attn</span><span class="o">=</span><span class="p">(</span><span class="n">transformer</span> <span class="o">==</span> <span class="s2">&quot;flash&quot;</span><span class="p">),</span>
            <span class="o">**</span><span class="n">flash_attention_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">cell_specific_blocks</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_transformer</span> <span class="o">=</span> <span class="n">FlashTransformer</span><span class="p">(</span>
            <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
            <span class="n">nhead</span><span class="o">=</span><span class="n">nhead</span><span class="p">,</span>
            <span class="n">nlayers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">cross_attn</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">use_flash_attn</span><span class="o">=</span><span class="p">(</span><span class="n">transformer</span> <span class="o">==</span> <span class="s2">&quot;flash&quot;</span><span class="p">),</span>
            <span class="o">**</span><span class="n">flash_attention_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_transformer</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># decoders</span>
    <span class="c1"># expression</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">expr_decoder</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">ExprDecoder</span><span class="p">(</span>
        <span class="n">d_model</span><span class="p">,</span>
        <span class="n">nfirst_tokens_to_skip</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cell_embs_count</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">zinb</span><span class="o">=</span><span class="n">zinb</span><span class="p">,</span>
        <span class="n">use_depth</span><span class="o">=</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth_atinput</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># cls decoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cls_decoders</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
    <span class="c1"># should be a very simple classifier for most things</span>
    <span class="c1"># (maybe scale with the number of classes) should be 1 layer...</span>
    <span class="k">for</span> <span class="n">clss</span><span class="p">,</span> <span class="n">n_cls</span> <span class="ow">in</span> <span class="n">classes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cls_decoders</span><span class="p">[</span><span class="n">clss</span><span class="p">]</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">ClsDecoder</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="n">n_cls</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">layers_cls</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span>
        <span class="p">)</span>

    <span class="c1"># Batch effect correction via adversarial training on batch classes</span>
    <span class="k">if</span> <span class="n">num_batch_labels</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_reverse_discriminator_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">AdversarialDiscriminatorLoss</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="n">n_cls</span><span class="o">=</span><span class="n">num_batch_labels</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_reverse_discriminator_loss</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># expression decoder from batch embbedding</span>
    <span class="k">if</span> <span class="n">mvc_decoder</span> <span class="o">!=</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mvc_decoder</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">MVCDecoder</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="n">arch_style</span><span class="o">=</span><span class="n">mvc_decoder</span><span class="p">,</span>
            <span class="n">zinb</span><span class="o">=</span><span class="n">zinb</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mvc_decoder</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span>
            <span class="n">utils</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">,</span>
            <span class="n">n_layer</span><span class="o">=</span><span class="n">nlayers</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dec</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_decoders</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">dec</span><span class="o">.</span><span class="n">out_layer</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.13</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">compress_class_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_mlps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">compress_class_dim</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_mlps</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">fsq</span><span class="o">.</span><span class="n">FSQ</span><span class="p">(</span><span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck_mlps</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.configure_optimizers" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">configure_optimizers</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>@see pl.LightningModule</p>


            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="c1"># https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam</span>
    <span class="c1"># not working because of poor weight decay implem</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim</span> <span class="o">==</span> <span class="s2">&quot;adam&quot;</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
            <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span>
            <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
            <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
            <span class="n">amsgrad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">fused</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fused_adam</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim</span> <span class="o">==</span> <span class="s2">&quot;adamW&quot;</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
            <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span>
            <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
            <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
            <span class="n">amsgrad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">fused</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fused_adam</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim</span> <span class="o">==</span> <span class="s2">&quot;galore&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Galore optimizer not implemented&quot;</span><span class="p">)</span>
        <span class="c1"># param_groups = [</span>
        <span class="c1">#    {</span>
        <span class="c1">#        &quot;params&quot;: [</span>
        <span class="c1">#            v for k, v in self.named_parameters() if &quot;transformer&quot; not in k</span>
        <span class="c1">#        ]</span>
        <span class="c1">#    },</span>
        <span class="c1">#    {</span>
        <span class="c1">#        &quot;params&quot;: [</span>
        <span class="c1">#            v for k, v in self.named_parameters() if &quot;transformer&quot; in k</span>
        <span class="c1">#        ],</span>
        <span class="c1">#        &quot;rank&quot;: 128,</span>
        <span class="c1">#        &quot;update_proj_gap&quot;: 200,</span>
        <span class="c1">#        &quot;scale&quot;: 0.25,</span>
        <span class="c1">#        &quot;proj_type&quot;: &quot;std&quot;,</span>
        <span class="c1">#    },</span>
        <span class="c1"># ]</span>
        <span class="c1"># optimizer = GaLoreAdamW(param_groups, lr=self.hparams.lr)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown optimizer: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_monitor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;no lr reduce factor&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">optimizer</span><span class="p">]</span>
    <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
        <span class="n">patience</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_patience</span><span class="p">,</span>
        <span class="n">factor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_factor</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">lr_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
        <span class="c1"># The unit of the scheduler&#39;s step size, could also be &#39;step&#39;.</span>
        <span class="c1"># &#39;epoch&#39; updates the scheduler on epoch end whereas &#39;step&#39;</span>
        <span class="c1"># updates it after a optimizer update.</span>
        <span class="s2">&quot;interval&quot;</span><span class="p">:</span> <span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
        <span class="c1"># How many epochs/steps should pass between calls to</span>
        <span class="c1"># `scheduler.step()`. 1 corresponds to updating the learning</span>
        <span class="c1"># rate after every epoch/step.</span>
        <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="c1"># Metric to to monitor for schedulers like `ReduceLROnPlateau`</span>
        <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_monitor</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">callbacks</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="n">_LRCallback</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">=</span> <span class="n">val</span><span class="o">.</span><span class="n">num_training</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="n">LearningRateFinder</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">=</span> <span class="n">val</span><span class="o">.</span><span class="n">_num_training_steps</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">optimizer</span><span class="p">],</span> <span class="p">[</span><span class="n">lr_dict</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>forward also called on self(), a full forward pass on the model</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>gene_pos</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>A tensor of shape (minibatch, seq_len)
representing the genes used for each cell in the minibatch.</p>
              </div>
            </li>
            <li>
              <b><code>expression</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>A tensor of shape (minibatch, seq_len)
representing the expression levels of genes in the minibatch. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>mask</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>A tensor of shape (minibatch, seq_len)
used to mask certain elements in the sequence during the forward pass. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>req_depth</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>A tensor of shape (minibatch,)
representing the full depth of each sequence in the minibatch. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>depth_mult</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>A tensor of shape (minibatch,)
representing the depth multiplier for each sequence in the minibatch. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>timepoint</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>A tensor of shape (minibatch,)
representing the timepoint associated with each sequence in the minibatch. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>get_gene_emb</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>A flag indicating whether to return the gene embeddings.
If True, the gene embeddings are included in the output. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>do_sample</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>A flag indicating whether to sample the expression levels.
If True, the expression levels are sampled during the forward pass. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>get_attention_layer</code></b>
                  (<code><span title="list">list</span></code>, default:
                      <code>[]</code>
)
              –
              <div class="doc-md-description">
                <p>A list indicating which attention layers to return.
If not empty, the specified attention layers are included in the output. Defaults to [].</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              –
              <div class="doc-md-description">
                <p>dict of output Tensors: A dictionary containing the output tensors from the forward pass.
The keys of the dictionary depend on the input flags (get_gene_emb, do_sample, get_attention_layer).
at minima, the dictionary codntains the following:
- "mean": the mean expression levels
- "zero_logits": the logits for zero-inflated expression levels
- "disp": the dispersion parameter
- "cell_embs": the cell embeddings per class
- "cell_emb": the main cell embedding
- "cls_output": the output of the classifier</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">gene_pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">expression</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">req_depth</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">timepoint</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># (new_minibatch_of_nxt_cells,)</span>
    <span class="n">get_gene_emb</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">metacell_token</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># (minibatch, 1)</span>
    <span class="n">depth_mult</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">do_mvc</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">do_class</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">get_attention_layer</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[],</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    forward also called on self(), a full forward pass on the model</span>

<span class="sd">    Args:</span>
<span class="sd">        gene_pos (Tensor): A tensor of shape (minibatch, seq_len)</span>
<span class="sd">            representing the genes used for each cell in the minibatch.</span>
<span class="sd">        expression (Tensor, optional): A tensor of shape (minibatch, seq_len)</span>
<span class="sd">            representing the expression levels of genes in the minibatch. Defaults to None.</span>
<span class="sd">        mask (Tensor, optional): A tensor of shape (minibatch, seq_len)</span>
<span class="sd">            used to mask certain elements in the sequence during the forward pass. Defaults to None.</span>
<span class="sd">        req_depth (Tensor, optional): A tensor of shape (minibatch,)</span>
<span class="sd">            representing the full depth of each sequence in the minibatch. Defaults to None.</span>
<span class="sd">        depth_mult (Tensor, optional): A tensor of shape (minibatch,)</span>
<span class="sd">            representing the depth multiplier for each sequence in the minibatch. Defaults to None.</span>
<span class="sd">        timepoint (Tensor, optional): A tensor of shape (minibatch,)</span>
<span class="sd">            representing the timepoint associated with each sequence in the minibatch. Defaults to None.</span>
<span class="sd">        get_gene_emb (bool, optional): A flag indicating whether to return the gene embeddings.</span>
<span class="sd">            If True, the gene embeddings are included in the output. Defaults to False.</span>
<span class="sd">        do_sample (bool, optional): A flag indicating whether to sample the expression levels.</span>
<span class="sd">            If True, the expression levels are sampled during the forward pass. Defaults to False.</span>
<span class="sd">        get_attention_layer (list, optional): A list indicating which attention layers to return.</span>
<span class="sd">            If not empty, the specified attention layers are included in the output. Defaults to [].</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict of output Tensors: A dictionary containing the output tensors from the forward pass.</span>
<span class="sd">            The keys of the dictionary depend on the input flags (get_gene_emb, do_sample, get_attention_layer).</span>
<span class="sd">            at minima, the dictionary codntains the following:</span>
<span class="sd">            - &quot;mean&quot;: the mean expression levels</span>
<span class="sd">            - &quot;zero_logits&quot;: the logits for zero-inflated expression levels</span>
<span class="sd">            - &quot;disp&quot;: the dispersion parameter</span>
<span class="sd">            - &quot;cell_embs&quot;: the cell embeddings per class</span>
<span class="sd">            - &quot;cell_emb&quot;: the main cell embedding</span>
<span class="sd">            - &quot;cls_output&quot;: the output of the classifier</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span><span class="p">(</span>
        <span class="n">gene_pos</span><span class="p">,</span>
        <span class="n">expression</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">,</span>
        <span class="n">req_depth</span><span class="o">=</span><span class="n">req_depth</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth_atinput</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timepoint</span><span class="o">=</span><span class="n">timepoint</span><span class="p">,</span>
        <span class="n">metacell_token</span><span class="o">=</span><span class="n">metacell_token</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_bias</span> <span class="o">!=</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;nbias&quot;</span><span class="p">):</span>
            <span class="n">bias_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                <span class="n">Path</span><span class="p">(</span><span class="n">FILEDIR</span><span class="p">)</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">parent</span><span class="p">,</span> <span class="s2">&quot;data&quot;</span><span class="p">,</span> <span class="s2">&quot;bias_sparse.npz&quot;</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nbias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">load_npz</span><span class="p">(</span><span class="n">bias_path</span><span class="p">)</span><span class="o">.</span><span class="n">todense</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                <span class="n">device</span><span class="o">=</span><span class="n">gene_pos</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
            <span class="p">)</span>
        <span class="n">num</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_embs_count</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_transformer</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">gene_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">gene_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">num</span><span class="p">,</span>
                <span class="n">gene_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">num</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">device</span><span class="o">=</span><span class="n">gene_pos</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># fade slowly through the iterations</span>
        <span class="n">fade_factor</span> <span class="o">=</span> <span class="mi">400</span> <span class="o">/</span> <span class="p">(</span><span class="mi">400</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span><span class="p">)</span>
        <span class="c1"># bias[:, num:, :num] = -10_000  # do not pay attention to the cls embeddings</span>
        <span class="n">bias</span><span class="p">[:,</span> <span class="n">num</span><span class="p">:,</span> <span class="n">num</span><span class="p">:]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nbias</span><span class="p">[</span><span class="n">gene_pos</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">gene_pos</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]]</span> <span class="o">*</span> <span class="n">fade_factor</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_transformer</span><span class="p">:</span>
        <span class="n">cell_encoding</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[:,</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_embs_count</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">encoding</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_embs_count</span> <span class="p">:,</span> <span class="p">:]</span>
    <span class="n">transformer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span>
        <span class="n">encoding</span><span class="p">,</span>
        <span class="n">return_qkv</span><span class="o">=</span><span class="n">get_attention_layer</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">bias</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_bias</span> <span class="o">!=</span> <span class="s2">&quot;none&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bias_layer</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nlayers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">get_attention_layer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">transformer_output</span><span class="p">,</span> <span class="n">qkvs</span> <span class="o">=</span> <span class="n">transformer_output</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_transformer</span><span class="p">:</span>
        <span class="n">cell_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_transformer</span><span class="p">(</span><span class="n">cell_encoding</span><span class="p">,</span> <span class="n">x_kv</span><span class="o">=</span><span class="n">transformer_output</span><span class="p">)</span>
        <span class="n">transformer_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cell_output</span><span class="p">,</span> <span class="n">transformer_output</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># if not provided we will mult by the current expression sum</span>
    <span class="n">depth_mult</span> <span class="o">=</span> <span class="n">expression</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">depth_mult</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">depth_mult</span>
    <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decoder</span><span class="p">(</span>
        <span class="n">transformer_output</span><span class="p">,</span>
        <span class="n">depth_mult</span><span class="p">,</span>
        <span class="n">get_gene_emb</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="p">,</span>
        <span class="n">do_mvc</span><span class="p">,</span>
        <span class="n">do_class</span><span class="p">,</span>
        <span class="n">req_depth</span><span class="o">=</span><span class="n">req_depth</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">depth_atinput</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">qkvs</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">get_attention_layer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">res</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.log_adata" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">log_adata</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>log_adata will log an adata from predictions.
It will log to tensorboard and wandb if available</p>
<p>see @utils.log_adata</p>


            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1675</span>
<span class="normal">1676</span>
<span class="normal">1677</span>
<span class="normal">1678</span>
<span class="normal">1679</span>
<span class="normal">1680</span>
<span class="normal">1681</span>
<span class="normal">1682</span>
<span class="normal">1683</span>
<span class="normal">1684</span>
<span class="normal">1685</span>
<span class="normal">1686</span>
<span class="normal">1687</span>
<span class="normal">1688</span>
<span class="normal">1689</span>
<span class="normal">1690</span>
<span class="normal">1691</span>
<span class="normal">1692</span>
<span class="normal">1693</span>
<span class="normal">1694</span>
<span class="normal">1695</span>
<span class="normal">1696</span>
<span class="normal">1697</span>
<span class="normal">1698</span>
<span class="normal">1699</span>
<span class="normal">1700</span>
<span class="normal">1701</span>
<span class="normal">1702</span>
<span class="normal">1703</span>
<span class="normal">1704</span>
<span class="normal">1705</span>
<span class="normal">1706</span>
<span class="normal">1707</span>
<span class="normal">1708</span>
<span class="normal">1709</span>
<span class="normal">1710</span>
<span class="normal">1711</span>
<span class="normal">1712</span>
<span class="normal">1713</span>
<span class="normal">1714</span>
<span class="normal">1715</span>
<span class="normal">1716</span>
<span class="normal">1717</span>
<span class="normal">1718</span>
<span class="normal">1719</span>
<span class="normal">1720</span>
<span class="normal">1721</span>
<span class="normal">1722</span>
<span class="normal">1723</span>
<span class="normal">1724</span>
<span class="normal">1725</span>
<span class="normal">1726</span>
<span class="normal">1727</span>
<span class="normal">1728</span>
<span class="normal">1729</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">log_adata</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gtclass</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    log_adata will log an adata from predictions.</span>
<span class="sd">    It will log to tensorboard and wandb if available</span>

<span class="sd">    see @utils.log_adata</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">mdir</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">save_dir</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">save_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;/tmp&quot;</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">mdir</span> <span class="o">=</span> <span class="s2">&quot;data/&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">mdir</span><span class="p">):</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">mdir</span><span class="p">)</span>
    <span class="n">adata</span><span class="p">,</span> <span class="n">fig</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">make_adata</span><span class="p">(</span>
        <span class="n">pos</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pos</span><span class="p">,</span>
        <span class="n">expr_pred</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">expr_pred</span><span class="p">,</span>
        <span class="n">genes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">,</span>
        <span class="n">embs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embs</span><span class="p">,</span>
        <span class="n">classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">,</span>
        <span class="n">pred</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pred</span><span class="p">,</span>
        <span class="n">attention</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
        <span class="n">label_decoders</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">label_decoders</span><span class="p">,</span>
        <span class="n">labels_hierarchy</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">labels_hierarchy</span><span class="p">,</span>
        <span class="n">gtclass</span><span class="o">=</span><span class="n">gtclass</span><span class="p">,</span>
        <span class="n">doplot</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">doplot</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">adata</span><span class="o">.</span><span class="n">write</span><span class="p">(</span>
        <span class="n">mdir</span>
        <span class="o">+</span> <span class="s2">&quot;/step_&quot;</span>
        <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">global_step</span><span class="p">)</span>
        <span class="o">+</span> <span class="s2">&quot;_&quot;</span>
        <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span>
        <span class="o">+</span> <span class="s2">&quot;_&quot;</span>
        <span class="o">+</span> <span class="n">name</span>
        <span class="o">+</span> <span class="s2">&quot;_&quot;</span>
        <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span><span class="p">)</span>
        <span class="o">+</span> <span class="s2">&quot;.h5ad&quot;</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">doplot</span><span class="p">:</span>
        <span class="n">logged</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_figure</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
            <span class="n">logged</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;couldn&#39;t log to tensorboard&quot;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">log_image</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s2">&quot;umaps&quot;</span><span class="p">,</span> <span class="n">images</span><span class="o">=</span><span class="p">[</span><span class="n">fig</span><span class="p">])</span>
            <span class="n">logged</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;couldn&#39;t log to wandb&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">logged</span><span class="p">:</span>
            <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;&#39;</span> <span class="o">+</span> <span class="n">mdir</span> <span class="o">+</span> <span class="s2">&quot;/umap_&quot;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span><span class="s2">&quot;_&quot;</span><span class="o">+</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.png&quot;</span><span class="p">)</span>


    <span class="k">return</span> <span class="n">adata</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.on_fit_start" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">on_fit_start</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>@see pl.LightningModule</p>


            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">on_fit_start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">)</span> <span class="ow">is</span> <span class="n">FlashTransformer</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">encoder_layers</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
            <span class="n">encoder_layers</span><span class="o">.</span><span class="n">set_seq_parallel</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mat_labels_hierarchy</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mat_labels_hierarchy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.on_predict_epoch_end" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">on_predict_epoch_end</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>@see pl.LightningModule will</p>


            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1623</span>
<span class="normal">1624</span>
<span class="normal">1625</span>
<span class="normal">1626</span>
<span class="normal">1627</span>
<span class="normal">1628</span>
<span class="normal">1629</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">on_predict_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule will&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">100</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_log_adata</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;adding on disk&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_adata</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;predict_part_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">counter</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.on_predict_epoch_start" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">on_predict_epoch_start</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>@see pl.LightningModule</p>


            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">on_predict_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embs</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">)</span> <span class="ow">is</span> <span class="n">FlashTransformer</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">encoder_layers</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
            <span class="n">encoder_layers</span><span class="o">.</span><span class="n">set_seq_parallel</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.on_validation_epoch_end" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">on_validation_epoch_end</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>@see pl.LightningModule</p>


            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1355</span>
<span class="normal">1356</span>
<span class="normal">1357</span>
<span class="normal">1358</span>
<span class="normal">1359</span>
<span class="normal">1360</span>
<span class="normal">1361</span>
<span class="normal">1362</span>
<span class="normal">1363</span>
<span class="normal">1364</span>
<span class="normal">1365</span>
<span class="normal">1366</span>
<span class="normal">1367</span>
<span class="normal">1368</span>
<span class="normal">1369</span>
<span class="normal">1370</span>
<span class="normal">1371</span>
<span class="normal">1372</span>
<span class="normal">1373</span>
<span class="normal">1374</span>
<span class="normal">1375</span>
<span class="normal">1376</span>
<span class="normal">1377</span>
<span class="normal">1378</span>
<span class="normal">1379</span>
<span class="normal">1380</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">on_validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embs</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">info</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pred</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="kc">None</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pos</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">stage</span> <span class="o">!=</span> <span class="s2">&quot;sanity_check&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">is_global_zero</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;logging anndata&quot;</span><span class="p">)</span>
            <span class="n">sch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_schedulers</span><span class="p">()</span>
            <span class="n">sch</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">callback_metrics</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">])</span>
            <span class="c1"># run the test function on specific dataset</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_adata</span><span class="p">(</span>
                <span class="n">gtclass</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">info</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;validation_part_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">counter</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">on_test_epoch_end</span><span class="p">()</span>
            <span class="c1"># Synchronize all processes with a timeout</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">():</span>
            <span class="c1"># Set a timeout that&#39;s longer than your test typically takes</span>
            <span class="c1"># Write rank to file for debugging</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">strategy</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.optimizer_step" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">optimizer_step</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>@see pl.LightningModule</p>


            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span>
<span class="normal">1273</span>
<span class="normal">1274</span>
<span class="normal">1275</span>
<span class="normal">1276</span>
<span class="normal">1277</span>
<span class="normal">1278</span>
<span class="normal">1279</span>
<span class="normal">1280</span>
<span class="normal">1281</span>
<span class="normal">1282</span>
<span class="normal">1283</span>
<span class="normal">1284</span>
<span class="normal">1285</span>
<span class="normal">1286</span>
<span class="normal">1287</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_closure</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="c1"># update params</span>
    <span class="c1"># manually warm up lr without a scheduler</span>
    <span class="c1"># making sure that we don&#39;t do this during lrfinder</span>
    <span class="n">lr_scale</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">prev_lr</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_duration</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span>
    <span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
            <span class="n">lr_scale</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span>
                <span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_duration</span>
            <span class="p">)</span>
            <span class="n">prev_lr</span> <span class="o">=</span> <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span>
            <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
        <span class="c1"># if pg[&quot;lr&quot;] &lt; 2e-5:</span>
        <span class="c1">#    pg[&quot;lr&quot;] = 2e-5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;lr_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">lr_scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_duration</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span><span class="p">,</span> <span class="n">prev_lr</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">prev_lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev_lr</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;OPTIMIZER HAS INCREASED LR. WHYY?&quot;</span><span class="p">)</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.predict_step" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">predict_step</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>embed given gene expression, encode the gene embedding and cell embedding.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>              –
              <div class="doc-md-description">
                <p><em>description</em></p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span>
<span class="normal">1428</span>
<span class="normal">1429</span>
<span class="normal">1430</span>
<span class="normal">1431</span>
<span class="normal">1432</span>
<span class="normal">1433</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    embed given gene expression, encode the gene embedding and cell embedding.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch @see training_step</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: _description_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span>
        <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;genes&quot;</span><span class="p">],</span>
        <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span>
        <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;depth&quot;</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">predict_mode</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred_embedding</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_attention_layer</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">predict_depth_mult</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.training_step" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">training_step</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>training_step defines the train loop. It is independent of forward</p>
<p>@see pl.LightningModule</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>_type_</code></b>              –
              <div class="doc-md-description">
                <p><em>description</em></p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">839</span>
<span class="normal">840</span>
<span class="normal">841</span>
<span class="normal">842</span>
<span class="normal">843</span>
<span class="normal">844</span>
<span class="normal">845</span>
<span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span>
<span class="normal">864</span>
<span class="normal">865</span>
<span class="normal">866</span>
<span class="normal">867</span>
<span class="normal">868</span>
<span class="normal">869</span>
<span class="normal">870</span>
<span class="normal">871</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
    <span class="n">batch_idx</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    training_step defines the train loop. It is independent of forward</span>

<span class="sd">    @see pl.LightningModule</span>

<span class="sd">    Returns:</span>
<span class="sd">        _type_: _description_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">total_loss</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_training</span><span class="p">(</span>
        <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
        <span class="n">do_denoise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_denoise</span><span class="p">,</span>
        <span class="n">noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">,</span>
        <span class="n">do_next_tp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_next_tp</span><span class="p">,</span>
        <span class="n">do_cce</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_cce</span><span class="p">,</span>
        <span class="n">cce_temp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cce_temp</span><span class="p">,</span>
        <span class="n">do_ecs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_ecs</span><span class="p">,</span>
        <span class="n">do_mvc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_mvc</span><span class="p">,</span>
        <span class="n">do_adv_cls</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_adv_cls</span><span class="p">,</span>
        <span class="n">do_adv_batch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_adv_batch</span><span class="p">,</span>
        <span class="n">do_cls</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_cls</span><span class="p">,</span>
        <span class="n">do_generate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_generate</span><span class="p">,</span>
        <span class="n">run_full_forward</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">run_full_forward</span><span class="p">,</span>
        <span class="n">mask_ratio</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mask_ratio</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;train_loss&quot;</span><span class="p">,</span> <span class="n">total_loss</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total_loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.validation_step" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">validation_step</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>validation_step defines the validation loop. It is independent of forward
@see pl.LightningModule</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>batch</code></b>
                  (<code><span title="list">list</span>[<span title="torch.Tensor">Tensor</span>]</code>)
              –
              <div class="doc-md-description">
                <p>@see training_step</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1297</span>
<span class="normal">1298</span>
<span class="normal">1299</span>
<span class="normal">1300</span>
<span class="normal">1301</span>
<span class="normal">1302</span>
<span class="normal">1303</span>
<span class="normal">1304</span>
<span class="normal">1305</span>
<span class="normal">1306</span>
<span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span>
<span class="normal">1350</span>
<span class="normal">1351</span>
<span class="normal">1352</span>
<span class="normal">1353</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    validation_step defines the validation loop. It is independent of forward</span>
<span class="sd">    @see pl.LightningModule</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (list[Tensor]): @see training_step</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">val_loss</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_training</span><span class="p">(</span>
        <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
        <span class="n">do_denoise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_denoise</span><span class="p">,</span>
        <span class="n">noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">,</span>
        <span class="n">do_next_tp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_next_tp</span><span class="p">,</span>
        <span class="n">do_cce</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_cce</span><span class="p">,</span>
        <span class="n">cce_temp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cce_temp</span><span class="p">,</span>
        <span class="n">do_ecs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_ecs</span><span class="p">,</span>
        <span class="n">do_mvc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_mvc</span><span class="p">,</span>
        <span class="n">do_adv_cls</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_adv_cls</span><span class="p">,</span>
        <span class="n">do_adv_batch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_adv_batch</span><span class="p">,</span>
        <span class="n">do_cls</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_cls</span><span class="p">,</span>
        <span class="n">do_generate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_generate</span><span class="p">,</span>
        <span class="n">run_full_forward</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">run_full_forward</span><span class="p">,</span>
        <span class="n">mask_ratio</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mask_ratio</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">expression</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">]</span>
    <span class="n">gene_pos</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;genes&quot;</span><span class="p">]</span>
    <span class="n">depth</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;depth&quot;</span><span class="p">]</span>
    <span class="n">metacell_token</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;is_meta&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
    <span class="c1"># TODO: make this faster by only calling val loss</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">embs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">embs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">100_000</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">info</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">info</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span>
                <span class="n">gene_pos</span><span class="p">,</span>
                <span class="n">expression</span><span class="p">,</span>
                <span class="n">depth</span><span class="p">,</span>
                <span class="n">pred_embedding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pred_embedding</span><span class="p">,</span>
                <span class="n">max_size_in_mem</span><span class="o">=</span><span class="mi">120_000</span><span class="p">,</span>
                <span class="n">metacell_token</span><span class="o">=</span><span class="n">metacell_token</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">info</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span>
            <span class="n">gene_pos</span><span class="p">,</span>
            <span class="n">expression</span><span class="p">,</span>
            <span class="n">depth</span><span class="p">,</span>
            <span class="n">pred_embedding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pred_embedding</span><span class="p">,</span>
            <span class="n">max_size_in_mem</span><span class="o">=</span><span class="mi">120_000</span><span class="p">,</span>
            <span class="n">metacell_token</span><span class="o">=</span><span class="n">metacell_token</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">val_loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div><h2 id="losses">losses</h2>


<div class="doc doc-object doc-module">



<h2 id="scprint.model.loss" class="doc doc-heading">
            <code>scprint.model.loss</code>


</h2>

    <div class="doc doc-contents first">








<p><span class="doc-section-title">Classes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="AdversarialDiscriminatorLoss (scprint.model.loss.AdversarialDiscriminatorLoss)" href="#scprint.model.loss.AdversarialDiscriminatorLoss">AdversarialDiscriminatorLoss</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
      </tbody>
    </table>




<p><span class="doc-section-title">Functions:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="classification (scprint.model.loss.classification)" href="#scprint.model.loss.classification">classification</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Computes the classification loss for a given batch of predictions and ground truth labels.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="contrastive_loss (scprint.model.loss.contrastive_loss)" href="#scprint.model.loss.contrastive_loss">contrastive_loss</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Computes NT-Xent loss (InfoNCE) between two sets of vectors.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="criterion_neg_log_bernoulli (scprint.model.loss.criterion_neg_log_bernoulli)" href="#scprint.model.loss.criterion_neg_log_bernoulli">criterion_neg_log_bernoulli</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Compute the negative log-likelihood of Bernoulli distribution</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="ecs (scprint.model.loss.ecs)" href="#scprint.model.loss.ecs">ecs</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>ecs Computes the similarity of cell embeddings based on a threshold.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="grad_reverse (scprint.model.loss.grad_reverse)" href="#scprint.model.loss.grad_reverse">grad_reverse</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>grad_reverse Reverses the gradient of the input tensor.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="masked_mae (scprint.model.loss.masked_mae)" href="#scprint.model.loss.masked_mae">masked_mae</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Compute the masked MAE loss between input and target.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="masked_mse (scprint.model.loss.masked_mse)" href="#scprint.model.loss.masked_mse">masked_mse</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Compute the masked MSE loss between input and target.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="masked_nb (scprint.model.loss.masked_nb)" href="#scprint.model.loss.masked_nb">masked_nb</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Compute the masked negative binomial loss between input and target.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="masked_relative_error (scprint.model.loss.masked_relative_error)" href="#scprint.model.loss.masked_relative_error">masked_relative_error</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Compute the masked relative error between input and target.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="mse (scprint.model.loss.mse)" href="#scprint.model.loss.mse">mse</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Compute the MSE loss between input and target.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="nb (scprint.model.loss.nb)" href="#scprint.model.loss.nb">nb</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Computes the negative binomial (NB) loss.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="nb_dist (scprint.model.loss.nb_dist)" href="#scprint.model.loss.nb_dist">nb_dist</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>nb_dist Computes the negative binomial distribution.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="within_sample (scprint.model.loss.within_sample)" href="#scprint.model.loss.within_sample">within_sample</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Compute dissimilarity between embeddings within each sample</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="zinb (scprint.model.loss.zinb)" href="#scprint.model.loss.zinb">zinb</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Computes zero-inflated negative binomial (ZINB) loss.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>





  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="scprint.model.loss.AdversarialDiscriminatorLoss" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">AdversarialDiscriminatorLoss</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Discriminator for the adversarial training for batch correction.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The size of the input tensor.</p>
              </div>
            </li>
            <li>
              <b><code>n_cls</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The number of classes.</p>
              </div>
            </li>
            <li>
              <b><code>nlayers</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>3</code>
)
              –
              <div class="doc-md-description">
                <p>The number of layers in the discriminator. Defaults to 3.</p>
              </div>
            </li>
            <li>
              <b><code>activation</code></b>
                  (<code><span title="callable">callable</span></code>, default:
                      <code><span title="torch.nn.LeakyReLU">LeakyReLU</span></code>
)
              –
              <div class="doc-md-description">
                <p>The activation function. Defaults to nn.LeakyReLU.</p>
              </div>
            </li>
            <li>
              <b><code>reverse_grad</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to reverse the gradient. Defaults</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>









<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint.model.loss.AdversarialDiscriminatorLoss.forward)" href="#scprint.model.loss.AdversarialDiscriminatorLoss.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Args:</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint/model/loss.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_cls</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">nlayers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">activation</span><span class="p">:</span> <span class="nb">callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">,</span>
    <span class="n">reverse_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Discriminator for the adversarial training for batch correction.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The size of the input tensor.</span>
<span class="sd">        n_cls (int): The number of classes.</span>
<span class="sd">        nlayers (int, optional): The number of layers in the discriminator. Defaults to 3.</span>
<span class="sd">        activation (callable, optional): The activation function. Defaults to nn.LeakyReLU.</span>
<span class="sd">        reverse_grad (bool, optional): Whether to reverse the gradient. Defaults</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="c1"># module list</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nlayers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_cls</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reverse_grad</span> <span class="o">=</span> <span class="n">reverse_grad</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.loss.AdversarialDiscriminatorLoss.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape [batch_size, embsize]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [batch_size, embsize]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reverse_grad</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">grad_reverse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.classification" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">classification</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Computes the classification loss for a given batch of predictions and ground truth labels.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>clsname</code></b>
                  (<code><span title="str">str</span></code>)
              –
              <div class="doc-md-description">
                <p>The name of the label.</p>
              </div>
            </li>
            <li>
              <b><code>pred</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The predicted logits for the batch.</p>
              </div>
            </li>
            <li>
              <b><code>cl</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The ground truth labels for the batch.</p>
              </div>
            </li>
            <li>
              <b><code>maxsize</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The number of possible labels.</p>
              </div>
            </li>
            <li>
              <b><code>labels_hierarchy</code></b>
                  (<code><span title="dict">dict</span></code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>The hierarchical structure of the labels. Defaults to {}.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="ValueError">ValueError</span></code>
              –
              <div class="doc-md-description">
                <p>If the clsname is not found in the labels_hierarchy dictionary.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>The computed binary cross entropy loss for the given batch.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">classification</span><span class="p">(</span>
    <span class="n">clsname</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">pred</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">cl</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">maxsize</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">labels_hierarchy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]]</span> <span class="o">=</span> <span class="p">{},</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the classification loss for a given batch of predictions and ground truth labels.</span>

<span class="sd">    Args:</span>
<span class="sd">        clsname (str): The name of the label.</span>
<span class="sd">        pred (Tensor): The predicted logits for the batch.</span>
<span class="sd">        cl (Tensor): The ground truth labels for the batch.</span>
<span class="sd">        maxsize (int): The number of possible labels.</span>
<span class="sd">        labels_hierarchy (dict, optional): The hierarchical structure of the labels. Defaults to {}.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the clsname is not found in the labels_hierarchy dictionary.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: The computed binary cross entropy loss for the given batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">newcl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="p">(</span><span class="n">cl</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">maxsize</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">cl</span><span class="o">.</span><span class="n">device</span>
    <span class="p">)</span>  <span class="c1"># batchsize * n_labels</span>
    <span class="c1"># if we don&#39;t know the label we set the weight to 0 else to 1</span>
    <span class="n">valid_indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">cl</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">cl</span> <span class="o">&lt;</span> <span class="n">maxsize</span><span class="p">)</span>
    <span class="n">valid_cl</span> <span class="o">=</span> <span class="n">cl</span><span class="p">[</span><span class="n">valid_indices</span><span class="p">]</span>
    <span class="n">newcl</span><span class="p">[</span><span class="n">valid_indices</span><span class="p">,</span> <span class="n">valid_cl</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">newcl</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cl</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">weight</span><span class="p">[</span><span class="n">cl</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">inv</span> <span class="o">=</span> <span class="n">cl</span> <span class="o">&gt;=</span> <span class="n">maxsize</span>
    <span class="c1"># if we have non leaf values, we don&#39;t know so we don&#39;t compute grad and set weight to 0</span>
    <span class="c1"># and add labels that won&#39;t be counted but so that we can still use them</span>
    <span class="k">if</span> <span class="n">inv</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">clsname</span> <span class="ow">in</span> <span class="n">labels_hierarchy</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">clhier</span> <span class="o">=</span> <span class="n">labels_hierarchy</span><span class="p">[</span><span class="n">clsname</span><span class="p">]</span>

            <span class="n">inv_weight</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="n">inv</span><span class="p">]</span>
            <span class="c1"># we set the weight of the elements that are not leaf to 0</span>
            <span class="c1"># i.e. the elements where we will compute the max</span>
            <span class="n">inv_weight</span><span class="p">[</span><span class="n">clhier</span><span class="p">[</span><span class="n">cl</span><span class="p">[</span><span class="n">inv</span><span class="p">]</span> <span class="o">-</span> <span class="n">maxsize</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">weight</span><span class="p">[</span><span class="n">inv</span><span class="p">]</span> <span class="o">=</span> <span class="n">inv_weight</span>

            <span class="n">addnewcl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
                <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">pred</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>  <span class="c1"># no need to set the other to 0 as the weight of the loss is set to 0</span>
            <span class="n">addweight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">pred</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">addweight</span><span class="p">[</span><span class="n">inv</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="c1"># computing hierarchical labels and adding them to cl</span>
            <span class="n">addpred</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="c1"># we only keep the elements where we need to compute the max,</span>
            <span class="c1"># for the rest we set them to -inf, so that they won&#39;t have any impact on the max()</span>
            <span class="n">inv_addpred</span> <span class="o">=</span> <span class="n">addpred</span><span class="p">[</span><span class="n">inv</span><span class="p">]</span>
            <span class="n">inv_addpred</span><span class="p">[</span><span class="n">inv_weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="nb">bool</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span>
            <span class="n">addpred</span><span class="p">[</span><span class="n">inv</span><span class="p">]</span> <span class="o">=</span> <span class="n">inv_addpred</span>

            <span class="c1"># differentiable max</span>
            <span class="n">addpred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">addpred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># we add the new labels to the cl</span>
            <span class="n">newcl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">newcl</span><span class="p">,</span> <span class="n">addnewcl</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pred</span><span class="p">,</span> <span class="n">addpred</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">weight</span><span class="p">,</span> <span class="n">addweight</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;need to use labels_hierarchy for this usecase&quot;</span><span class="p">)</span>

    <span class="n">myloss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">newcl</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">myloss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.contrastive_loss" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">contrastive_loss</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Computes NT-Xent loss (InfoNCE) between two sets of vectors.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor of shape [batch_size, feature_dim]</p>
              </div>
            </li>
            <li>
              <b><code>y</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor of shape [batch_size, feature_dim]</p>
              </div>
            </li>
            <li>
              <b><code>temperature</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>Temperature parameter to scale the similarities.
Lower values make the model more confident/selective.
Typical values are between 0.1 and 0.5.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>NT-Xent loss value</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="note" open>
  <summary>Note</summary>
  <ul>
<li>Assumes x[i] and y[i] are positive pairs</li>
<li>All other combinations are considered negative pairs</li>
<li>Uses cosine similarity scaled by temperature</li>
</ul>
</details>

            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">contrastive_loss</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes NT-Xent loss (InfoNCE) between two sets of vectors.</span>

<span class="sd">    Args:</span>
<span class="sd">        x: Tensor of shape [batch_size, feature_dim]</span>
<span class="sd">        y: Tensor of shape [batch_size, feature_dim]</span>
<span class="sd">        temperature: Temperature parameter to scale the similarities.</span>
<span class="sd">            Lower values make the model more confident/selective.</span>
<span class="sd">            Typical values are between 0.1 and 0.5.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: NT-Xent loss value</span>

<span class="sd">    Note:</span>
<span class="sd">        - Assumes x[i] and y[i] are positive pairs</span>
<span class="sd">        - All other combinations are considered negative pairs</span>
<span class="sd">        - Uses cosine similarity scaled by temperature</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check input dimensions</span>
    <span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;Input tensors must have the same shape&quot;</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Compute cosine similarity matrix</span>
    <span class="c1"># x_unsqueeze: [batch_size, 1, feature_dim]</span>
    <span class="c1"># y_unsqueeze: [1, batch_size, feature_dim]</span>
    <span class="c1"># -&gt; similarities: [batch_size, batch_size]</span>
    <span class="n">similarities</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">F</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span>
    <span class="p">)</span>

    <span class="c1"># The positive pairs are on the diagonal</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Cross entropy loss</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">similarities</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.criterion_neg_log_bernoulli" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">criterion_neg_log_bernoulli</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute the negative log-likelihood of Bernoulli distribution</p>


            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">criterion_neg_log_bernoulli</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the negative log-likelihood of Bernoulli distribution</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">bernoulli</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">masked_log_probs</span> <span class="o">=</span> <span class="n">bernoulli</span><span class="o">.</span><span class="n">log_prob</span><span class="p">((</span><span class="n">target</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span> <span class="o">*</span> <span class="n">mask</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">masked_log_probs</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.ecs" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">ecs</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>ecs Computes the similarity of cell embeddings based on a threshold.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>cell_emb</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>A tensor representing cell embeddings.</p>
              </div>
            </li>
            <li>
              <b><code>ecs_threshold</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.5</code>
)
              –
              <div class="doc-md-description">
                <p>A threshold for determining similarity. Defaults to 0.5.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>A tensor representing the mean of 1 minus the square of the difference between the cosine similarity and the threshold.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">ecs</span><span class="p">(</span><span class="n">cell_emb</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ecs_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ecs Computes the similarity of cell embeddings based on a threshold.</span>

<span class="sd">    Args:</span>
<span class="sd">        cell_emb (Tensor): A tensor representing cell embeddings.</span>
<span class="sd">        ecs_threshold (float, optional): A threshold for determining similarity. Defaults to 0.5.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: A tensor representing the mean of 1 minus the square of the difference between the cosine similarity and the threshold.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Here using customized cosine similarity instead of F.cosine_similarity</span>
    <span class="c1"># to avoid the pytorch issue of similarity larger than 1.0, pytorch # 78064</span>
    <span class="c1"># normalize the embedding</span>
    <span class="n">cell_emb_normed</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">cell_emb</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">cell_emb_normed</span><span class="p">,</span> <span class="n">cell_emb_normed</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>

    <span class="c1"># mask out diagnal elements</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">cos_sim</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cos_sim</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">cos_sim</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="c1"># only optimize positive similarities</span>
    <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">cos_sim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">cos_sim</span> <span class="o">-</span> <span class="n">ecs_threshold</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.grad_reverse" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">grad_reverse</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>grad_reverse Reverses the gradient of the input tensor.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The input tensor whose gradient is to be reversed.</p>
              </div>
            </li>
            <li>
              <b><code>lambd</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>1.0</code>
)
              –
              <div class="doc-md-description">
                <p>The scaling factor for the reversed gradient. Defaults to 1.0.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>The input tensor with its gradient reversed during the backward pass.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">grad_reverse</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">lambd</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    grad_reverse Reverses the gradient of the input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor whose gradient is to be reversed.</span>
<span class="sd">        lambd (float, optional): The scaling factor for the reversed gradient. Defaults to 1.0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: The input tensor with its gradient reversed during the backward pass.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">GradReverse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambd</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.masked_mae" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">masked_mae</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute the masked MAE loss between input and target.
MAE = mean absolute error</p>


            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">masked_mae</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the masked MAE loss between input and target.</span>
<span class="sd">    MAE = mean absolute error</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="nb">input</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">target</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.masked_mse" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">masked_mse</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute the masked MSE loss between input and target.</p>


            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">masked_mse</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the masked MSE loss between input and target.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="nb">input</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">target</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.masked_nb" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">masked_nb</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute the masked negative binomial loss between input and target.</p>


            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">masked_nb</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the masked negative binomial loss between input and target.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">nb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">NegativeBinomial</span><span class="p">(</span><span class="n">total_count</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">probs</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">masked_log_probs</span> <span class="o">=</span> <span class="n">nb</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">masked_log_probs</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.masked_relative_error" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">masked_relative_error</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute the masked relative error between input and target.</p>


            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">masked_relative_error</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the masked relative error between input and target.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">mask</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">-</span> <span class="n">target</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">target</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.mse" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mse</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute the MSE loss between input and target.</p>


            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">mse</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the MSE loss between input and target.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="nb">input</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span> <span class="o">*</span> <span class="mi">10000</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10000</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.nb" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nb</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Computes the negative binomial (NB) loss.</p>
<p>This function was adapted from scvi-tools.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>target</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Ground truth data.</p>
              </div>
            </li>
            <li>
              <b><code>mu</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Means of the negative binomial distribution (must have positive support).</p>
              </div>
            </li>
            <li>
              <b><code>theta</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Inverse dispersion parameter (must have positive support).</p>
              </div>
            </li>
            <li>
              <b><code>eps</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>1e-08</code>
)
              –
              <div class="doc-md-description">
                <p>Numerical stability constant. Defaults to 1e-8.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>              –
              <div class="doc-md-description">
                <p>NB loss value.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nb</span><span class="p">(</span><span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mu</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the negative binomial (NB) loss.</span>

<span class="sd">    This function was adapted from scvi-tools.</span>

<span class="sd">    Args:</span>
<span class="sd">        target (Tensor): Ground truth data.</span>
<span class="sd">        mu (Tensor): Means of the negative binomial distribution (must have positive support).</span>
<span class="sd">        theta (Tensor): Inverse dispersion parameter (must have positive support).</span>
<span class="sd">        eps (float, optional): Numerical stability constant. Defaults to 1e-8.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: NB loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">theta</span><span class="o">.</span><span class="n">ndimension</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

    <span class="n">log_theta_mu_eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">theta</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_theta_mu_eps</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">target</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_theta_mu_eps</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="n">theta</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="o">-</span><span class="n">res</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.nb_dist" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nb_dist</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>nb_dist Computes the negative binomial distribution.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of observed data.</p>
              </div>
            </li>
            <li>
              <b><code>mu</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of means of the negative binomial distribution (must have positive support).</p>
              </div>
            </li>
            <li>
              <b><code>theta</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of inverse dispersion parameter (must have positive support).</p>
              </div>
            </li>
            <li>
              <b><code>eps</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>1e-08</code>
)
              –
              <div class="doc-md-description">
                <p>Numerical stability constant. Defaults to 1e-8.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>              –
              <div class="doc-md-description">
                <p>Negative binomial loss value.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">nb_dist</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mu</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    nb_dist Computes the negative binomial distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Torch Tensor of observed data.</span>
<span class="sd">        mu (Tensor): Torch Tensor of means of the negative binomial distribution (must have positive support).</span>
<span class="sd">        theta (Tensor): Torch Tensor of inverse dispersion parameter (must have positive support).</span>
<span class="sd">        eps (float, optional): Numerical stability constant. Defaults to 1e-8.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: Negative binomial loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">NegativeBinomial</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="n">theta</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.within_sample" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">within_sample</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Compute dissimilarity between embeddings within each sample
using a combination of cosine and L2 distance</p>


            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">within_sample</span><span class="p">(</span><span class="n">cell_embs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute dissimilarity between embeddings within each sample</span>
<span class="sd">    using a combination of cosine and L2 distance</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_embeddings</span><span class="p">,</span> <span class="n">emb_dim</span> <span class="o">=</span> <span class="n">cell_embs</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Normalize embeddings for cosine similarity</span>
    <span class="n">cell_embs_norm</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">cell_embs</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Compute pairwise cosine similarities</span>
    <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">cell_embs_norm</span><span class="p">,</span> <span class="n">cell_embs_norm</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

    <span class="c1"># Compute pairwise L2 distances (normalized by embedding dimension)</span>
    <span class="n">l2_dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="n">cell_embs</span><span class="p">,</span> <span class="n">cell_embs</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">emb_dim</span><span class="p">)</span>

    <span class="c1"># Create mask for pairs (excluding self-similarity)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cos_sim</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Combine losses:</span>
    <span class="c1"># - High cosine similarity should be penalized</span>
    <span class="c1"># - Small L2 distance should be penalized</span>
    <span class="n">cos_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">cos_sim</span> <span class="o">*</span> <span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">l2_loss</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">l2_dist</span> <span class="o">*</span> <span class="n">mask</span> <span class="o">+</span> <span class="mf">1e-3</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">cos_loss</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">l2_loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.zinb" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">zinb</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Computes zero-inflated negative binomial (ZINB) loss.</p>
<p>This function was modified from scvi-tools.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>target</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of ground truth data.</p>
              </div>
            </li>
            <li>
              <b><code>mu</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of means of the negative binomial (must have positive support).</p>
              </div>
            </li>
            <li>
              <b><code>theta</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of inverse dispersion parameter (must have positive support).</p>
              </div>
            </li>
            <li>
              <b><code>pi</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of logits of the dropout parameter (real support).</p>
              </div>
            </li>
            <li>
              <b><code>eps</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>1e-08</code>
)
              –
              <div class="doc-md-description">
                <p>Numerical stability constant. Defaults to 1e-8.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>              –
              <div class="doc-md-description">
                <p>ZINB loss value.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">zinb</span><span class="p">(</span>
    <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">mu</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">pi</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes zero-inflated negative binomial (ZINB) loss.</span>

<span class="sd">    This function was modified from scvi-tools.</span>

<span class="sd">    Args:</span>
<span class="sd">        target (Tensor): Torch Tensor of ground truth data.</span>
<span class="sd">        mu (Tensor): Torch Tensor of means of the negative binomial (must have positive support).</span>
<span class="sd">        theta (Tensor): Torch Tensor of inverse dispersion parameter (must have positive support).</span>
<span class="sd">        pi (Tensor): Torch Tensor of logits of the dropout parameter (real support).</span>
<span class="sd">        eps (float, optional): Numerical stability constant. Defaults to 1e-8.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: ZINB loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">#  uses log(sigmoid(x)) = -softplus(-x)</span>
    <span class="n">softplus_pi</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="o">-</span><span class="n">pi</span><span class="p">)</span>
    <span class="c1"># eps to make it positive support and taking the log</span>
    <span class="n">log_theta_mu_eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">pi_theta_log</span> <span class="o">=</span> <span class="o">-</span><span class="n">pi</span> <span class="o">+</span> <span class="n">theta</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_theta_mu_eps</span><span class="p">)</span>

    <span class="n">case_zero</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">pi_theta_log</span><span class="p">)</span> <span class="o">-</span> <span class="n">softplus_pi</span>
    <span class="n">mul_case_zero</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">((</span><span class="n">target</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">case_zero</span><span class="p">)</span>

    <span class="n">case_non_zero</span> <span class="o">=</span> <span class="p">(</span>
        <span class="o">-</span><span class="n">softplus_pi</span>
        <span class="o">+</span> <span class="n">pi_theta_log</span>
        <span class="o">+</span> <span class="n">target</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_theta_mu_eps</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="n">theta</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">mul_case_non_zero</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">((</span><span class="n">target</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">case_non_zero</span><span class="p">)</span>

    <span class="n">res</span> <span class="o">=</span> <span class="n">mul_case_zero</span> <span class="o">+</span> <span class="n">mul_case_non_zero</span>
    <span class="c1"># we want to minize the loss but maximize the log likelyhood</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">res</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h2 id="utils">utils</h2>


<div class="doc doc-object doc-module">



<h2 id="scprint.model.utils" class="doc doc-heading">
            <code>scprint.model.utils</code>


</h2>

    <div class="doc doc-contents first">








<p><span class="doc-section-title">Classes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="Attention (scprint.model.utils.Attention)" href="#scprint.model.utils.Attention">Attention</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="WeightedMasker (scprint.model.utils.WeightedMasker)" href="#scprint.model.utils.WeightedMasker">WeightedMasker</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
      </tbody>
    </table>




<p><span class="doc-section-title">Functions:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="downsample_profile (scprint.model.utils.downsample_profile)" href="#scprint.model.utils.downsample_profile">downsample_profile</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>This function downsamples the expression profile of a given single cell RNA matrix.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="make_adata (scprint.model.utils.make_adata)" href="#scprint.model.utils.make_adata">make_adata</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>This function creates an AnnData object from the given input parameters.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="simple_masker (scprint.model.utils.simple_masker)" href="#scprint.model.utils.simple_masker">simple_masker</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Randomly mask a batch of data.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="test (scprint.model.utils.test)" href="#scprint.model.utils.test">test</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Test the given model on the full set of benchmarks and save the results to JSON files.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="zinb_sample (scprint.model.utils.zinb_sample)" href="#scprint.model.utils.zinb_sample">zinb_sample</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>





  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="scprint.model.utils.Attention" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">Attention</span></code>

</h3>


    <div class="doc doc-contents ">


        <p>Initialize the Attention class.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>gene_dim</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the gene.</p>
              </div>
            </li>
            <li>
              <b><code>additional_tokens</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>0</code>
)
              –
              <div class="doc-md-description">
                <p>The number of additional tokens to add.</p>
              </div>
            </li>
            <li>
              <b><code>comp_attn</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to compute attention or it is precomputed</p>
              </div>
            </li>
            <li>
              <b><code>apply_softmax</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to apply softmax to the attention.</p>
              </div>
            </li>
            <li>
              <b><code>sum_heads</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to sum the heads.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>









<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="add_attn (scprint.model.utils.Attention.add_attn)" href="#scprint.model.utils.Attention.add_attn">add_attn</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Aggregate the attention or data based on the comp_attn flag.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="add_qk (scprint.model.utils.Attention.add_qk)" href="#scprint.model.utils.Attention.add_qk">add_qk</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Add data to the internal storage.</p>
                </div>
              </td>
            </tr>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="get (scprint.model.utils.Attention.get)" href="#scprint.model.utils.Attention.get">get</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Get the aggregated attention or data.</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint/model/utils.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">gene_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">comp_attn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">apply_softmax</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sum_heads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">additional_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initialize the Attention class.</span>

<span class="sd">    Args:</span>
<span class="sd">        gene_dim (int): The dimension of the gene.</span>
<span class="sd">        additional_tokens (int): The number of additional tokens to add.</span>
<span class="sd">        comp_attn (bool): Whether to compute attention or it is precomputed</span>
<span class="sd">        apply_softmax (bool): Whether to apply softmax to the attention.</span>
<span class="sd">        sum_heads (bool): Whether to sum the heads.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">gene_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">additional_tokens</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">apply_softmax</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">apply_softmax</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sum_heads</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">sum_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">comp_attn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.utils.Attention.add_attn" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">add_attn</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Aggregate the attention or data based on the comp_attn flag.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="typing.List">List</span>[<span title="torch.Tensor">Tensor</span>]</code>)
              –
              <div class="doc-md-description">
                <p>List of tensors to aggregate. Tensor of size (batch, seq_len, 2, heads, emb)</p>
              </div>
            </li>
            <li>
              <b><code>pos</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Position tensor.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">add_attn</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">expr</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Aggregate the attention or data based on the comp_attn flag.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (List[Tensor]): List of tensors to aggregate. Tensor of size (batch, seq_len, 2, heads, emb)</span>
<span class="sd">        pos (Tensor): Position tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span><span class="p">,</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
            <span class="p">],</span>
            <span class="n">device</span><span class="o">=</span><span class="n">pos</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pos</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">elem</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">heads</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">elem</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_softmax</span><span class="p">:</span>
            <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
                <span class="n">elem</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
                <span class="o">@</span> <span class="n">elem</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">expr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">*</span> <span class="p">(</span><span class="n">expr</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">heads</span> <span class="o">*</span> <span class="n">i</span> <span class="p">:</span> <span class="n">heads</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="n">attn</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">heads</span> <span class="o">*</span> <span class="n">i</span> <span class="p">:</span> <span class="n">heads</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+=</span> <span class="p">(</span>
                <span class="n">elem</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
                <span class="o">@</span> <span class="n">elem</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">batch</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">div</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.utils.Attention.add_qk" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">add_qk</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Add data to the internal storage.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="typing.List">List</span>[<span title="torch.Tensor">Tensor</span>]</code>)
              –
              <div class="doc-md-description">
                <p>List of tensors to add.</p>
              </div>
            </li>
            <li>
              <b><code>pos</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Position tensor.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">add_qk</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">expr</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add data to the internal storage.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (List[Tensor]): List of tensors to add.</span>
<span class="sd">        pos (Tensor): Position tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]),</span>
            <span class="n">device</span><span class="o">=</span><span class="n">pos</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pos</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">pos</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
                <span class="n">pos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">additional_tokens</span><span class="p">,</span>
            <span class="p">]</span>
        <span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="p">[</span><span class="n">loc</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.utils.Attention.get" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>Get the aggregated attention or data.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="typing.Optional">Optional</span>[<span title="numpy.ndarray">ndarray</span>]</code>
              –
              <div class="doc-md-description">
                <p>Optional[np.ndarray]: The aggregated attention or data.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the aggregated attention or data.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Optional[np.ndarray]: The aggregated attention or data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comp_attn</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="c1"># shape is (layers, genes, qkv, heads, emb)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.utils.WeightedMasker" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">WeightedMasker</span></code>

</h3>


    <div class="doc doc-contents ">


        <p>Randomly mask a batch of data.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>genes</code></b>
                  (<code><span title="list">list</span>[<span title="str">str</span>]</code>)
              –
              <div class="doc-md-description">
                <p>The list of genes the model might see.</p>
              </div>
            </li>
            <li>
              <b><code>TFs</code></b>
                  (<code><span title="list">list</span>[<span title="str">str</span>]</code>, default:
                      <code><a class="autorefs autorefs-internal" title="fileToList (scprint.utils.fileToList)" href="../utils/#scprint.utils.utils.fileToList">fileToList</a>(<span title="scprint.model.utils.FILEDIR">FILEDIR</span> + &#39;/../../data/main/TFs.txt&#39;)</code>
)
              –
              <div class="doc-md-description">
                <p>The list of TFs the model can drop.</p>
              </div>
            </li>
            <li>
              <b><code>inv_weight</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.2</code>
)
              –
              <div class="doc-md-description">
                <p>How likely it is to drop a non TF compared to a TF.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: A tensor of masked data.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>










                  <details class="quote">
                    <summary>Source code in <code>scprint/model/utils.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">genes</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">TFs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">fileToList</span><span class="p">(</span><span class="n">FILEDIR</span> <span class="o">+</span> <span class="s2">&quot;/../../data/main/TFs.txt&quot;</span><span class="p">),</span>
    <span class="n">inv_weight</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly mask a batch of data.</span>

<span class="sd">    Args:</span>
<span class="sd">        genes (list[str]): The list of genes the model might see.</span>
<span class="sd">        TFs (list[str]): The list of TFs the model can drop.</span>
<span class="sd">        inv_weight (float): How likely it is to drop a non TF compared to a TF.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: A tensor of masked data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">TFs</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">TFs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
        <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">gene</span> <span class="ow">in</span> <span class="n">TFs</span> <span class="k">else</span> <span class="n">inv_weight</span> <span class="k">for</span> <span class="n">gene</span> <span class="ow">in</span> <span class="n">genes</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_to_drop</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">==</span> <span class="n">inv_weight</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inv_weight</span> <span class="o">=</span> <span class="n">inv_weight</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="scprint.model.utils.downsample_profile" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">downsample_profile</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>This function downsamples the expression profile of a given single cell RNA matrix.</p>
<p>The noise is applied based on the renoise parameter,
the total counts of the matrix, and the number of genes. The function first calculates the noise
threshold (scaler) based on the renoise parameter. It then generates an initial matrix count by
applying a Poisson distribution to a random tensor scaled by the total counts and the number of genes.
The function then models the sampling zeros by applying a Poisson distribution to a random tensor
scaled by the noise threshold, the total counts, and the number of genes. The function also models
the technical zeros by generating a random tensor and comparing it to the noise threshold. The final
matrix count is calculated by subtracting the sampling zeros from the initial matrix count and
multiplying by the technical zeros. The function ensures that the final matrix count is not less
than zero by taking the maximum of the final matrix count and a tensor of zeros. The function
returns the final matrix count.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>mat</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The input matrix.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code><span title="float">float</span></code>)
              –
              <div class="doc-md-description">
                <p>The renoise parameter.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: The matrix count after applying noise.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">downsample_profile</span><span class="p">(</span><span class="n">mat</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;new&quot;</span><span class="p">,</span> <span class="n">randsamp</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function downsamples the expression profile of a given single cell RNA matrix.</span>

<span class="sd">    The noise is applied based on the renoise parameter,</span>
<span class="sd">    the total counts of the matrix, and the number of genes. The function first calculates the noise</span>
<span class="sd">    threshold (scaler) based on the renoise parameter. It then generates an initial matrix count by</span>
<span class="sd">    applying a Poisson distribution to a random tensor scaled by the total counts and the number of genes.</span>
<span class="sd">    The function then models the sampling zeros by applying a Poisson distribution to a random tensor</span>
<span class="sd">    scaled by the noise threshold, the total counts, and the number of genes. The function also models</span>
<span class="sd">    the technical zeros by generating a random tensor and comparing it to the noise threshold. The final</span>
<span class="sd">    matrix count is calculated by subtracting the sampling zeros from the initial matrix count and</span>
<span class="sd">    multiplying by the technical zeros. The function ensures that the final matrix count is not less</span>
<span class="sd">    than zero by taking the maximum of the final matrix count and a tensor of zeros. The function</span>
<span class="sd">    returns the final matrix count.</span>

<span class="sd">    Args:</span>
<span class="sd">        mat (torch.Tensor): The input matrix.</span>
<span class="sd">        dropout (float): The renoise parameter.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The matrix count after applying noise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Randomly drop on average N counts to each element of expression using a heavy tail Gaussian distribution</span>
    <span class="c1"># here we try to get the scale of the distribution so as to remove the right number of counts from each gene</span>
    <span class="c1"># https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02601-5#:~:text=Zero%20measurements%20in%20scRNA%2Dseq,generation%20of%20scRNA%2Dseq%20data.</span>
    <span class="k">if</span> <span class="n">randsamp</span><span class="p">:</span>
        <span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="n">dropout</span>
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;old&quot;</span><span class="p">:</span>
        <span class="n">totcounts</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">ngenes</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">tnoise</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># we model the sampling zeros (dropping 30% of the reads)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">ngenes</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="o">*</span> <span class="p">((</span><span class="n">tnoise</span> <span class="o">*</span> <span class="n">totcounts</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">ngenes</span><span class="p">))</span>
        <span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="c1"># we model the technical zeros (dropping 50% of the genes)</span>
        <span class="n">drop</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">ngenes</span><span class="p">))</span> <span class="o">&gt;</span> <span class="n">tnoise</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">mat</span> <span class="o">=</span> <span class="p">(</span><span class="n">mat</span> <span class="o">-</span> <span class="n">res</span><span class="p">)</span> <span class="o">*</span> <span class="n">drop</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">]])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">))</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;jules&quot;</span><span class="p">:</span>
        <span class="n">scaler</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">notdrop</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span>
                <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="o">&lt;</span> <span class="n">scaler</span>
        <span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="n">notdrop</span><span class="p">[</span><span class="n">mat</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># apply the dropout after the poisson, right?</span>
        <span class="k">return</span> <span class="n">notdrop</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">mat</span> <span class="o">*</span> <span class="n">scaler</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;new&quot;</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">ngenes</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span> <span class="o">*</span> <span class="mf">1.1</span>
        <span class="c1"># we model the sampling zeros (dropping 30% of the reads)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">poisson</span><span class="p">((</span><span class="n">mat</span> <span class="o">*</span> <span class="p">(</span><span class="n">dropout</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)))</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="c1"># we model the technical zeros (dropping 50% of the genes)</span>
        <span class="n">notdrop</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">ngenes</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">dropout</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="n">mat</span> <span class="o">=</span> <span class="p">(</span><span class="n">mat</span> <span class="o">-</span> <span class="n">res</span><span class="p">)</span> <span class="o">*</span> <span class="n">notdrop</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
            <span class="n">mat</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;method </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2"> not recognized&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.utils.make_adata" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">make_adata</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>This function creates an AnnData object from the given input parameters.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>pos</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Positions of the cells. The shape of the tensor is (n_cells,).</p>
              </div>
            </li>
            <li>
              <b><code>expr_pred</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Predicted expression. The shape of the tensor is (n_cells, n_genes).</p>
              </div>
            </li>
            <li>
              <b><code>genes</code></b>
                  (<code><span title="list">list</span></code>)
              –
              <div class="doc-md-description">
                <p>List of genes.</p>
              </div>
            </li>
            <li>
              <b><code>embs</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Embeddings of the cells. The shape of the tensor is (n_cells, n_features).</p>
              </div>
            </li>
            <li>
              <b><code>classes</code></b>
                  (<code><span title="list">list</span></code>)
              –
              <div class="doc-md-description">
                <p>List of classes.</p>
              </div>
            </li>
            <li>
              <b><code>pred</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Predicted labels. The shape of the tensor is (n_cells, n_classes). Default is None.</p>
              </div>
            </li>
            <li>
              <b><code>attention</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Attention weights. Default is None.</p>
              </div>
            </li>
            <li>
              <b><code>label_decoders</code></b>
                  (<code><span title="dict">dict</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Dictionary to map class codes to class names. Default is None.</p>
              </div>
            </li>
            <li>
              <b><code>labels_hierarchy</code></b>
                  (<code><span title="dict">dict</span></code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>Dictionary representing the hierarchy of labels. Default is {}.</p>
              </div>
            </li>
            <li>
              <b><code>gtclass</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Ground truth class. Default is None.</p>
              </div>
            </li>
            <li>
              <b><code>doplot</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to generate plots. Default is True.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              –
              <div class="doc-md-description">
                <p>anndata.AnnData: The created AnnData object.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">make_adata</span><span class="p">(</span>
    <span class="n">pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">expr_pred</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">genes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">embs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">classes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">pred</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">label_decoders</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">labels_hierarchy</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">gtclass</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">doplot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function creates an AnnData object from the given input parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        pos (torch.Tensor): Positions of the cells. The shape of the tensor is (n_cells,).</span>
<span class="sd">        expr_pred (torch.Tensor): Predicted expression. The shape of the tensor is (n_cells, n_genes).</span>
<span class="sd">        genes (list): List of genes.</span>
<span class="sd">        embs (torch.Tensor): Embeddings of the cells. The shape of the tensor is (n_cells, n_features).</span>
<span class="sd">        classes (list): List of classes.</span>
<span class="sd">        pred (torch.Tensor, optional): Predicted labels. The shape of the tensor is (n_cells, n_classes). Default is None.</span>
<span class="sd">        attention (torch.Tensor, optional): Attention weights. Default is None.</span>
<span class="sd">        label_decoders (dict, optional): Dictionary to map class codes to class names. Default is None.</span>
<span class="sd">        labels_hierarchy (dict, optional): Dictionary representing the hierarchy of labels. Default is {}.</span>
<span class="sd">        gtclass (torch.Tensor, optional): Ground truth class. Default is None.</span>
<span class="sd">        doplot (bool, optional): Whether to generate plots. Default is True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        anndata.AnnData: The created AnnData object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">colname</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">pred</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
        <span class="c1"># label decoders is not cls_decoders. one is a dict to map class codes (ints)</span>
        <span class="c1"># to class names the other is the module the predict the class</span>
        <span class="k">if</span> <span class="n">label_decoders</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="p">[</span><span class="n">label_decoders</span><span class="p">[</span><span class="n">classes</span><span class="p">[</span><span class="n">i</span><span class="p">]][</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">name</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">obs</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
                <span class="p">]</span>
            <span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="k">if</span> <span class="n">gtclass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">colname</span> <span class="o">+=</span> <span class="n">classes</span>
            <span class="n">nobs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">gtclass</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">label_decoders</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">nobs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="p">[</span><span class="n">label_decoders</span><span class="p">[</span><span class="n">classes</span><span class="p">[</span><span class="n">i</span><span class="p">]][</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">name</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">nobs</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
                    <span class="p">]</span>
                <span class="p">)</span><span class="o">.</span><span class="n">T</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">obs</span><span class="p">,</span> <span class="n">nobs</span><span class="p">])</span>

    <span class="n">size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">genes</span><span class="p">)</span>
    <span class="n">n_cells</span> <span class="o">=</span> <span class="n">pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">pos</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># Create empty array with same shape as expr_pred[0]</span>
    <span class="n">mu_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_cells</span><span class="p">,</span> <span class="n">size</span><span class="p">))</span>
    <span class="c1"># Fill array with values from expr_pred[0]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_cells</span><span class="p">):</span>
        <span class="n">mu_array</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">pos</span><span class="p">[</span><span class="n">idx</span><span class="p">]]</span> <span class="o">=</span> <span class="n">expr_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;scprint_mu&quot;</span><span class="p">:</span> <span class="n">csr_matrix</span><span class="p">(</span><span class="n">mu_array</span><span class="p">),</span>
        <span class="c1">#  &quot;used_scprint&quot;: csr_matrix(pos),</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">expr_pred</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">theta_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_cells</span><span class="p">,</span> <span class="n">size</span><span class="p">))</span>
        <span class="c1"># Fill array with values from expr_pred[0]</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_cells</span><span class="p">):</span>
            <span class="n">theta_array</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">pos</span><span class="p">[</span><span class="n">idx</span><span class="p">]]</span> <span class="o">=</span> <span class="n">expr_pred</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">layers</span><span class="p">[</span><span class="s2">&quot;scprint_theta&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">csr_matrix</span><span class="p">(</span><span class="n">theta_array</span><span class="p">)</span>

        <span class="n">pi_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_cells</span><span class="p">,</span> <span class="n">size</span><span class="p">))</span>
        <span class="c1"># Fill array with values from expr_pred[0]</span>
        <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_cells</span><span class="p">):</span>
            <span class="n">pi_array</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">pos</span><span class="p">[</span><span class="n">idx</span><span class="p">]]</span> <span class="o">=</span> <span class="n">expr_pred</span><span class="p">[</span><span class="mi">2</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">layers</span><span class="p">[</span><span class="s2">&quot;scprint_pi&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">csr_matrix</span><span class="p">(</span><span class="n">pi_array</span><span class="p">)</span>

    <span class="n">adata</span> <span class="o">=</span> <span class="n">AnnData</span><span class="p">(</span>
        <span class="n">X</span><span class="o">=</span><span class="n">csr_matrix</span><span class="p">(</span><span class="n">mu_array</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
        <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span>
        <span class="n">obs</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
            <span class="n">obs</span><span class="p">,</span>
            <span class="n">columns</span><span class="o">=</span><span class="n">colname</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">pred</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">adata</span><span class="o">.</span><span class="n">obsm</span><span class="p">[</span><span class="s2">&quot;scprint_emb&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">embs</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">adata</span><span class="o">.</span><span class="n">var_names</span> <span class="o">=</span> <span class="n">genes</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="n">pred</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">clss</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">gtclass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">tr</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="n">clss</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">clss</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">tr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s2">&quot;conv_&quot;</span> <span class="o">+</span> <span class="n">clss</span><span class="p">]</span> <span class="o">=</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="n">clss</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">tr</span><span class="p">)</span>
            <span class="n">tr</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">clss</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">clss</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">tr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">clss</span><span class="p">]</span> <span class="o">=</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">clss</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">tr</span><span class="p">)</span>
            <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="n">label_decoders</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">gtclass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">class_topred</span> <span class="o">=</span> <span class="n">label_decoders</span><span class="p">[</span><span class="n">clss</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">clss</span> <span class="ow">in</span> <span class="n">labels_hierarchy</span><span class="p">:</span>
                    <span class="n">cur_labels_hierarchy</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="n">label_decoders</span><span class="p">[</span><span class="n">clss</span><span class="p">][</span><span class="n">k</span><span class="p">]:</span> <span class="p">[</span><span class="n">label_decoders</span><span class="p">[</span><span class="n">clss</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">v</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">labels_hierarchy</span><span class="p">[</span><span class="n">clss</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                    <span class="p">}</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">cur_labels_hierarchy</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">for</span> <span class="n">pred</span><span class="p">,</span> <span class="n">true</span> <span class="ow">in</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">clss</span><span class="p">,</span> <span class="n">clss</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">pred</span> <span class="o">==</span> <span class="n">true</span><span class="p">:</span>
                        <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                        <span class="k">continue</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels_hierarchy</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">true</span> <span class="ow">in</span> <span class="n">cur_labels_hierarchy</span><span class="p">:</span>
                            <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred</span> <span class="ow">in</span> <span class="n">cur_labels_hierarchy</span><span class="p">[</span><span class="n">true</span><span class="p">])</span>
                        <span class="k">elif</span> <span class="n">true</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">class_topred</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;true label </span><span class="si">{</span><span class="n">true</span><span class="si">}</span><span class="s2"> not in available classes&quot;</span>
                            <span class="p">)</span>
                        <span class="k">elif</span> <span class="n">true</span> <span class="o">!=</span> <span class="s2">&quot;unknown&quot;</span><span class="p">:</span>
                            <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">true</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">class_topred</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;true label </span><span class="si">{</span><span class="n">true</span><span class="si">}</span><span class="s2"> not in available classes&quot;</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">true</span> <span class="o">!=</span> <span class="s2">&quot;unknown&quot;</span><span class="p">:</span>
                        <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">pass</span>
                <span class="n">accuracy</span><span class="p">[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">clss</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">adata</span><span class="o">.</span><span class="n">obs</span> <span class="o">=</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;category&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">adata</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">doplot</span> <span class="ow">and</span> <span class="n">adata</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">100</span><span class="p">:</span>
        <span class="n">sc</span><span class="o">.</span><span class="n">pp</span><span class="o">.</span><span class="n">neighbors</span><span class="p">(</span><span class="n">adata</span><span class="p">,</span> <span class="n">use_rep</span><span class="o">=</span><span class="s2">&quot;scprint_emb&quot;</span><span class="p">)</span>
        <span class="n">sc</span><span class="o">.</span><span class="n">tl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span><span class="n">adata</span><span class="p">)</span>
        <span class="n">sc</span><span class="o">.</span><span class="n">tl</span><span class="o">.</span><span class="n">leiden</span><span class="p">(</span><span class="n">adata</span><span class="p">,</span> <span class="n">key_added</span><span class="o">=</span><span class="s2">&quot;sprint_leiden&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">gtclass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">color</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">i</span>
                <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="s2">&quot;conv_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="k">if</span> <span class="s2">&quot;conv_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="o">.</span><span class="n">columns</span> <span class="k">else</span> <span class="n">i</span>
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">classes</span>
                    <span class="p">],</span>
                    <span class="p">[</span>
                        <span class="p">(</span>
                            <span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">i</span>
                            <span class="k">if</span> <span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="o">.</span><span class="n">columns</span>
                            <span class="k">else</span> <span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">i</span>
                        <span class="p">)</span>
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">classes</span>
                    <span class="p">],</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pair</span>
            <span class="p">]</span>
            <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span>
                <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">)</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">color</span><span class="p">):</span>
                    <span class="n">sc</span><span class="o">.</span><span class="n">pl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span>
                        <span class="n">adata</span><span class="p">,</span>
                        <span class="n">color</span><span class="o">=</span><span class="n">col</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">],</span>
                        <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">acc</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
                    <span class="k">if</span> <span class="s2">&quot;pred_&quot;</span> <span class="ow">in</span> <span class="n">col</span> <span class="ow">and</span> <span class="n">col</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;conv_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">in</span> <span class="n">accuracy</span><span class="p">:</span>
                        <span class="n">acc</span> <span class="o">=</span> <span class="s2">&quot; (accuracy: </span><span class="si">{:.2f}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="n">accuracy</span><span class="p">[</span><span class="n">col</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;conv_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
                        <span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">col</span> <span class="o">+</span> <span class="s2">&quot; UMAP&quot;</span> <span class="o">+</span> <span class="n">acc</span><span class="p">)</span>
                    <span class="k">if</span> <span class="s2">&quot;cell_type&quot;</span> <span class="ow">in</span> <span class="n">col</span><span class="p">:</span>
                        <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="s2">&quot;x-small&quot;</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;UMAP1&quot;</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;UMAP2&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">color</span><span class="p">):</span>
                    <span class="n">sc</span><span class="o">.</span><span class="n">pl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span>
                        <span class="n">adata</span><span class="p">,</span>
                        <span class="n">color</span><span class="o">=</span><span class="n">col</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">],</span>
                        <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">acc</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
                    <span class="k">if</span> <span class="s2">&quot;pred_&quot;</span> <span class="ow">in</span> <span class="n">col</span> <span class="ow">and</span> <span class="n">col</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;conv_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">in</span> <span class="n">accuracy</span><span class="p">:</span>
                        <span class="n">acc</span> <span class="o">=</span> <span class="s2">&quot; (accuracy: </span><span class="si">{:.2f}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="n">accuracy</span><span class="p">[</span><span class="n">col</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;conv_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
                        <span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">col</span> <span class="o">+</span> <span class="s2">&quot; UMAP&quot;</span> <span class="o">+</span> <span class="n">acc</span><span class="p">)</span>
                    <span class="k">if</span> <span class="s2">&quot;cell_type&quot;</span> <span class="ow">in</span> <span class="n">col</span><span class="p">:</span>
                        <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="s2">&quot;x-small&quot;</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;UMAP1&quot;</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;UMAP2&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">color</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">(</span>
                    <span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">i</span>
                    <span class="k">if</span> <span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="o">.</span><span class="n">columns</span>
                    <span class="k">else</span> <span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">i</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">classes</span>
            <span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">)</span> <span class="o">*</span> <span class="mi">8</span><span class="p">))</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">color</span><span class="p">):</span>
                    <span class="n">sc</span><span class="o">.</span><span class="n">pl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span>
                        <span class="n">adata</span><span class="p">,</span>
                        <span class="n">color</span><span class="o">=</span><span class="n">col</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                        <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">acc</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
                    <span class="k">if</span> <span class="s2">&quot;pred_&quot;</span> <span class="ow">in</span> <span class="n">col</span> <span class="ow">and</span> <span class="n">col</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;conv_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">in</span> <span class="n">accuracy</span><span class="p">:</span>
                        <span class="n">acc</span> <span class="o">=</span> <span class="s2">&quot; (accuracy: </span><span class="si">{:.2f}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="n">accuracy</span><span class="p">[</span><span class="n">col</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;conv_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
                        <span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">col</span> <span class="o">+</span> <span class="s2">&quot; UMAP&quot;</span> <span class="o">+</span> <span class="n">acc</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;UMAP1&quot;</span><span class="p">)</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;UMAP2&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">fig</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">pl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span><span class="n">adata</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_fig</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">fig</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">adata</span><span class="p">,</span> <span class="n">fig</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.utils.simple_masker" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">simple_masker</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Randomly mask a batch of data.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>shape</code></b>
                  (<code><span title="list">list</span>[<span title="int">int</span>]</code>)
              –
              <div class="doc-md-description">
                <p>The shape of the data.</p>
              </div>
            </li>
            <li>
              <b><code>mask_ratio</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.15</code>
)
              –
              <div class="doc-md-description">
                <p>The ratio of genes to mask, default to 0.15.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="torch.Tensor">Tensor</span></code>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: A tensor of masked data.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">simple_masker</span><span class="p">(</span>
    <span class="n">shape</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">mask_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly mask a batch of data.</span>

<span class="sd">    Args:</span>
<span class="sd">        shape (list[int]): The shape of the data.</span>
<span class="sd">        mask_ratio (float): The ratio of genes to mask, default to 0.15.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: A tensor of masked data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">mask_ratio</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.utils.test" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">test</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>Test the given model on the full set of benchmarks and save the results to JSON files.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>model</code></b>
                  (<code><span title="torch.nn.Module">Module</span></code>)
              –
              <div class="doc-md-description">
                <p>The model to be tested.</p>
              </div>
            </li>
            <li>
              <b><code>name</code></b>
                  (<code><span title="str">str</span></code>)
              –
              <div class="doc-md-description">
                <p>The name to be used for the output JSON files.</p>
              </div>
            </li>
            <li>
              <b><code>filedir</code></b>
                  (<code><span title="str">str</span></code>)
              –
              <div class="doc-md-description">
                <p>The directory where the data files are located.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>None</code>
              –
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">test</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">filedir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">do_class</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Test the given model on the full set of benchmarks and save the results to JSON files.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (torch.nn.Module): The model to be tested.</span>
<span class="sd">        name (str): The name to be used for the output JSON files.</span>
<span class="sd">        filedir (str): The directory where the data files are located.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">embbed_task</span><span class="o">.</span><span class="n">default_benchmark</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">default_dataset</span><span class="o">=</span><span class="s2">&quot;lung&quot;</span><span class="p">,</span> <span class="n">do_class</span><span class="o">=</span><span class="n">do_class</span><span class="p">,</span> <span class="n">coarse</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;metrics_&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.json&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s2">&quot;embed_lung&quot;</span><span class="p">:</span> <span class="n">res</span><span class="p">},</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;emb_lung/scib&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;scib&quot;</span><span class="p">][</span><span class="s2">&quot;Total&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;emb_lung/ct_class&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">res</span><span class="p">[</span><span class="s2">&quot;classif&quot;</span><span class="p">][</span><span class="s2">&quot;cell_type_ontology_term_id&quot;</span><span class="p">][</span><span class="s2">&quot;accuracy&quot;</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">do_class</span>
                <span class="k">else</span> <span class="mi">0</span>
            <span class="p">),</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">embbed_task</span><span class="o">.</span><span class="n">default_benchmark</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">default_dataset</span><span class="o">=</span><span class="s2">&quot;pancreas&quot;</span><span class="p">,</span> <span class="n">do_class</span><span class="o">=</span><span class="n">do_class</span><span class="p">,</span> <span class="n">coarse</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;metrics_&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.json&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s2">&quot;embed_panc&quot;</span><span class="p">:</span> <span class="n">res</span><span class="p">},</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;emb_panc/scib&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;scib&quot;</span><span class="p">][</span><span class="s2">&quot;Total&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;emb_panc/ct_class&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">res</span><span class="p">[</span><span class="s2">&quot;classif&quot;</span><span class="p">][</span><span class="s2">&quot;cell_type_ontology_term_id&quot;</span><span class="p">][</span><span class="s2">&quot;accuracy&quot;</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">do_class</span>
                <span class="k">else</span> <span class="mi">0</span>
            <span class="p">),</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">denoise_task</span><span class="o">.</span><span class="n">default_benchmark</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">filedir</span> <span class="o">+</span> <span class="s2">&quot;/../../data/gNNpgpo6gATjuxTE7CCp.h5ad&quot;</span>
    <span class="p">)</span>
    <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;denoise/reco2full_vs_noisy2full&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">res</span><span class="p">[</span><span class="s2">&quot;reco2full&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;noisy2full&quot;</span><span class="p">]</span>
            <span class="p">),</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;metrics_&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.json&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s2">&quot;denoise&quot;</span><span class="p">:</span> <span class="n">res</span><span class="p">},</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">grn_task</span><span class="o">.</span><span class="n">default_benchmark</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="s2">&quot;gwps&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span> <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">d_model</span> <span class="o">&lt;=</span> <span class="mi">512</span> <span class="k">else</span> <span class="mi">8</span>
    <span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;metrics_&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.json&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s2">&quot;grn_gwps&quot;</span><span class="p">:</span> <span class="n">res</span><span class="p">},</span> <span class="n">default</span><span class="o">=</span><span class="k">lambda</span> <span class="n">o</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">o</span><span class="p">),</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;grn_gwps/auprc_self&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;self&quot;</span><span class="p">][</span><span class="s2">&quot;auprc&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;grn_gwps/epr_self&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;self&quot;</span><span class="p">][</span><span class="s2">&quot;epr&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;grn_gwps/auprc_omni&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;omni&quot;</span><span class="p">][</span><span class="s2">&quot;auprc&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;grn_gwps/epr_omni&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;omni&quot;</span><span class="p">][</span><span class="s2">&quot;epr&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;grn_gwps/auprc&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">][</span><span class="s2">&quot;auprc&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;grn_gwps/epr&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">][</span><span class="s2">&quot;epr&quot;</span><span class="p">]),</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">grn_task</span><span class="o">.</span><span class="n">default_benchmark</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="s2">&quot;sroy&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span> <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">d_model</span> <span class="o">&lt;=</span> <span class="mi">512</span> <span class="k">else</span> <span class="mi">8</span>
    <span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;metrics_&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.json&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s2">&quot;grn_sroy&quot;</span><span class="p">:</span> <span class="n">res</span><span class="p">},</span> <span class="n">default</span><span class="o">=</span><span class="k">lambda</span> <span class="n">o</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">o</span><span class="p">),</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;grn_sroy/auprc_self&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">i</span><span class="p">[</span><span class="s2">&quot;auprc&quot;</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;self_&quot;</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
                            <span class="n">x</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;chip_&quot;</span><span class="p">,</span> <span class="s2">&quot;ko_&quot;</span><span class="p">,</span> <span class="s2">&quot;classifier&quot;</span><span class="p">,</span> <span class="s2">&quot;_base&quot;</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_sroy/epr_self&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">i</span><span class="p">[</span><span class="s2">&quot;epr&quot;</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;self_&quot;</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
                            <span class="n">x</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;chip_&quot;</span><span class="p">,</span> <span class="s2">&quot;ko_&quot;</span><span class="p">,</span> <span class="s2">&quot;classifier&quot;</span><span class="p">,</span> <span class="s2">&quot;_base&quot;</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_sroy/auprc_omni&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">i</span><span class="p">[</span><span class="s2">&quot;auprc&quot;</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;omni_&quot;</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
                            <span class="n">x</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;chip_&quot;</span><span class="p">,</span> <span class="s2">&quot;ko_&quot;</span><span class="p">,</span> <span class="s2">&quot;classifier&quot;</span><span class="p">,</span> <span class="s2">&quot;_base&quot;</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_sroy/epr_omni&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">i</span><span class="p">[</span><span class="s2">&quot;epr&quot;</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;omni_&quot;</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
                            <span class="n">x</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;chip_&quot;</span><span class="p">,</span> <span class="s2">&quot;ko_&quot;</span><span class="p">,</span> <span class="s2">&quot;classifier&quot;</span><span class="p">,</span> <span class="s2">&quot;_base&quot;</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_sroy/auprc&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">i</span><span class="p">[</span><span class="s2">&quot;auprc&quot;</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;mean_&quot;</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
                            <span class="n">x</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;chip_&quot;</span><span class="p">,</span> <span class="s2">&quot;ko_&quot;</span><span class="p">,</span> <span class="s2">&quot;classifier&quot;</span><span class="p">,</span> <span class="s2">&quot;_base&quot;</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_sroy/epr&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">i</span><span class="p">[</span><span class="s2">&quot;epr&quot;</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;mean_&quot;</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
                            <span class="n">x</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;chip_&quot;</span><span class="p">,</span> <span class="s2">&quot;ko_&quot;</span><span class="p">,</span> <span class="s2">&quot;classifier&quot;</span><span class="p">,</span> <span class="s2">&quot;_base&quot;</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">grn_task</span><span class="o">.</span><span class="n">default_benchmark</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">filedir</span> <span class="o">+</span> <span class="s2">&quot;/../../data/yBCKp6HmXuHa0cZptMo7.h5ad&quot;</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span> <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">d_model</span> <span class="o">&lt;=</span> <span class="mi">512</span> <span class="k">else</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">cell_types</span><span class="o">=</span><span class="p">[</span>
            <span class="s2">&quot;kidney distal convoluted tubule epithelial cell&quot;</span><span class="p">,</span>
            <span class="s2">&quot;kidney loop of Henle thick ascending limb epithelial cell&quot;</span><span class="p">,</span>
            <span class="s2">&quot;kidney collecting duct principal cell&quot;</span><span class="p">,</span>
            <span class="s2">&quot;mesangial cell&quot;</span><span class="p">,</span>
            <span class="s2">&quot;blood vessel smooth muscle cell&quot;</span><span class="p">,</span>
            <span class="s2">&quot;podocyte&quot;</span><span class="p">,</span>
            <span class="s2">&quot;macrophage&quot;</span><span class="p">,</span>
            <span class="s2">&quot;leukocyte&quot;</span><span class="p">,</span>
            <span class="s2">&quot;kidney interstitial fibroblast&quot;</span><span class="p">,</span>
            <span class="s2">&quot;endothelial cell&quot;</span><span class="p">,</span>
        <span class="p">],</span>
    <span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;metrics_&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.json&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s2">&quot;grn_omni&quot;</span><span class="p">:</span> <span class="n">res</span><span class="p">},</span> <span class="n">default</span><span class="o">=</span><span class="k">lambda</span> <span class="n">o</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">o</span><span class="p">),</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;grn_omni/auprc_class&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="s2">&quot;auprc&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_class&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">])</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_omni/epr_class&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="s2">&quot;epr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_class&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">])</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_omni/tf_enr_class&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;TF_enr&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_class&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_omni/tf_targ_enr_class&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">i</span><span class="p">[</span><span class="s2">&quot;significant_enriched_TFtargets&quot;</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="s2">&quot;_class&quot;</span> <span class="ow">in</span> <span class="n">k</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_omni/auprc&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="s2">&quot;auprc&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_mean&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">])</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_omni/epr&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="s2">&quot;epr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_mean&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">])</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_omni/tf_enr&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">i</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;TF_enr&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_mean&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">])</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_omni/tf_targ_enr&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">i</span><span class="p">[</span><span class="s2">&quot;significant_enriched_TFtargets&quot;</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="s2">&quot;_mean&quot;</span> <span class="ow">in</span> <span class="n">k</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="c1"># &#39;grn_omni/ct&#39;: res[&#39;classif&#39;][&#39;cell_type_ontology_term_id&#39;][&#39;accuracy&#39;],</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">metrics</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.utils.zinb_sample" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">zinb_sample</span></code>

</h3>


    <div class="doc doc-contents ">

        <p>zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>mu</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The mean of the Negative Binomial (NB) distribution.</p>
              </div>
            </li>
            <li>
              <b><code>theta</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The dispersion parameter of the NB distribution.</p>
              </div>
            </li>
            <li>
              <b><code>zi_probs</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The zero-inflation probabilities.</p>
              </div>
            </li>
            <li>
              <b><code>sample_shape</code></b>
                  (<code><span title="torch.Size">Size</span></code>, default:
                      <code><span title="torch.Size">Size</span>([])</code>
)
              –
              <div class="doc-md-description">
                <p>The output shape. Defaults to torch.Size([]).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: A sample from the ZINB distribution.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">zinb_sample</span><span class="p">(</span>
    <span class="n">mu</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zi_probs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        mu (torch.Tensor): The mean of the Negative Binomial (NB) distribution.</span>
<span class="sd">        theta (torch.Tensor): The dispersion parameter of the NB distribution.</span>
<span class="sd">        zi_probs (torch.Tensor): The zero-inflation probabilities.</span>
<span class="sd">        sample_shape (torch.Size, optional): The output shape. Defaults to torch.Size([]).</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: A sample from the ZINB distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">concentration</span> <span class="o">=</span> <span class="n">theta</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">/</span> <span class="n">mu</span>
    <span class="c1"># Important remark: Gamma is parametrized by the rate = 1/scale!</span>
    <span class="n">gamma_d</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="n">concentration</span><span class="o">=</span><span class="n">concentration</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">rate</span><span class="p">)</span>
    <span class="n">p_means</span> <span class="o">=</span> <span class="n">gamma_d</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span>

    <span class="c1"># Clamping as distributions objects can have buggy behaviors when</span>
    <span class="c1"># their parameters are too high</span>
    <span class="n">l_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">p_means</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1e8</span><span class="p">)</span>
    <span class="n">samp</span> <span class="o">=</span> <span class="n">Poisson</span><span class="p">(</span><span class="n">l_train</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Shape : (n_samples, n_cells_batch, n_vars)</span>
    <span class="n">is_zero</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">samp</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">zi_probs</span>
    <span class="n">samp_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">is_zero</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">samp</span><span class="p">),</span> <span class="n">samp</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">samp_</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h2 id="encoder-and-decoder-modules">encoder and decoder modules</h2>


<div class="doc doc-object doc-module">



<h2 id="scprint.model.encoders" class="doc doc-heading">
            <code>scprint.model.encoders</code>


</h2>

    <div class="doc doc-contents first">








<p><span class="doc-section-title">Classes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="CategoryValueEncoder (scprint.model.encoders.CategoryValueEncoder)" href="#scprint.model.encoders.CategoryValueEncoder">CategoryValueEncoder</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="ContinuousValueEncoder (scprint.model.encoders.ContinuousValueEncoder)" href="#scprint.model.encoders.ContinuousValueEncoder">ContinuousValueEncoder</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="DPositionalEncoding (scprint.model.encoders.DPositionalEncoding)" href="#scprint.model.encoders.DPositionalEncoding">DPositionalEncoding</a></code></td>
            <td>
              <div class="doc-md-description">
                <p>The PositionalEncoding module applies a positional encoding to a sequence of vectors.</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="GeneEncoder (scprint.model.encoders.GeneEncoder)" href="#scprint.model.encoders.GeneEncoder">GeneEncoder</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="PositionalEncoding (scprint.model.encoders.PositionalEncoding)" href="#scprint.model.encoders.PositionalEncoding">PositionalEncoding</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
      </tbody>
    </table>







  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="scprint.model.encoders.CategoryValueEncoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">CategoryValueEncoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Encodes categorical values into a vector using an embedding layer and layer normalization.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>num_embeddings</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The number of possible values.</p>
              </div>
            </li>
            <li>
              <b><code>embedding_dim</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the output vectors.</p>
              </div>
            </li>
            <li>
              <b><code>padding_idx</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The index of the padding token. Defaults to None.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: A tensor representing the encoded categorical values.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>        <p>Note: not used in the current version of scprint.</p>











                  <details class="quote">
                    <summary>Source code in <code>scprint/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encodes categorical values into a vector using an embedding layer and layer normalization.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_embeddings (int): The number of possible values.</span>
<span class="sd">        embedding_dim (int): The dimension of the output vectors.</span>
<span class="sd">        padding_idx (int, optional): The index of the padding token. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: A tensor representing the encoded categorical values.</span>

<span class="sd">    Note: not used in the current version of scprint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CategoryValueEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
        <span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.encoders.ContinuousValueEncoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">ContinuousValueEncoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Encode real number values to a vector using neural nets projection.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the input vectors.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the output of the positional encoding.</p>
              </div>
            </li>
            <li>
              <b><code>max_value</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>100000</code>
)
              –
              <div class="doc-md-description">
                <p>The maximum value of the input. Defaults to 100_000.</p>
              </div>
            </li>
            <li>
              <b><code>layers</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>The number of layers in the encoder. Defaults to 1.</p>
              </div>
            </li>
            <li>
              <b><code>size</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>The size of the input. Defaults to 1.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: A tensor representing the encoded continuous values.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>









<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint.model.encoders.ContinuousValueEncoder.forward)" href="#scprint.model.encoders.ContinuousValueEncoder.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Args:</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">max_value</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100_000</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encode real number values to a vector using neural nets projection.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the input vectors.</span>
<span class="sd">        dropout (float, optional): The dropout rate to apply to the output of the positional encoding.</span>
<span class="sd">        max_value (int, optional): The maximum value of the input. Defaults to 100_000.</span>
<span class="sd">        layers (int, optional): The number of layers in the encoder. Defaults to 1.</span>
<span class="sd">        size (int, optional): The size of the input. Defaults to 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: A tensor representing the encoded continuous values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ContinuousValueEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_value</span> <span class="o">=</span> <span class="n">max_value</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="c1"># self.mask_value = nn.Embedding(1, d_model)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.encoders.ContinuousValueEncoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape [batch_size, seq_len]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/encoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [batch_size, seq_len]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># expand last dimension</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># use the mask embedding when x=-1</span>
    <span class="c1"># mask = (x == -1).float()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_value</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">val</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># x = x.masked_fill_(mask.unsqueeze(-1), self.mask_value(0))</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.encoders.DPositionalEncoding" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">DPositionalEncoding</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>The PositionalEncoding module applies a positional encoding to a sequence of vectors.
This is necessary for the Transformer model, which does not have any inherent notion of
position in a sequence. The positional encoding is added to the input embeddings and
allows the model to attend to positions in the sequence.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the input vectors.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code><span title="float">float</span></code>)
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the output of the positional encoding.</p>
              </div>
            </li>
            <li>
              <b><code>max_len</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The maximum length of a sequence that this module can handle.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>        <p>Note: not used in the current version of scprint.</p>










<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint.model.encoders.DPositionalEncoding.forward)" href="#scprint.model.encoders.DPositionalEncoding.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Args:</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">max_len_x</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">max_len_y</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">maxvalue_x</span><span class="o">=</span><span class="mf">10000.0</span><span class="p">,</span>
    <span class="n">maxvalue_y</span><span class="o">=</span><span class="mf">10000.0</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">DPositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="n">position2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_len_y</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">position1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_len_x</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">half_n</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="mi">2</span>

    <span class="n">div_term2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">half_n</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">maxvalue_y</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">div_term1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">half_n</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">maxvalue_x</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pe1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">pe2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len_y</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">pe1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="n">half_n</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position1</span> <span class="o">*</span> <span class="n">div_term1</span><span class="p">)</span>
    <span class="n">pe1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="n">half_n</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position1</span> <span class="o">*</span> <span class="n">div_term1</span><span class="p">)</span>
    <span class="n">pe2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">half_n</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position2</span> <span class="o">*</span> <span class="n">div_term2</span><span class="p">)</span>
    <span class="n">pe2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">half_n</span> <span class="p">::</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position2</span> <span class="o">*</span> <span class="n">div_term2</span><span class="p">)</span>
    <span class="c1"># https://github.com/tatp22/multidim-positional-encoding/blob/master/positional_encodings/torch_encodings.py</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;pe1&quot;</span><span class="p">,</span> <span class="n">pe1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;pe2&quot;</span><span class="p">,</span> <span class="n">pe2</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.encoders.DPositionalEncoding.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape [seq_len, batch_size, embedding_dim]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/encoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pos_x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pos_y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [seq_len, batch_size, embedding_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe1</span><span class="p">[</span><span class="n">pos_x</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe2</span><span class="p">[</span><span class="n">pos_y</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.encoders.GeneEncoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">GeneEncoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Encodes gene sequences into a continuous vector space using an embedding layer.</p>
<p>The output is then normalized using a LayerNorm.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>num_embeddings</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The number of possible values.</p>
              </div>
            </li>
            <li>
              <b><code>embedding_dim</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the output vectors.</p>
              </div>
            </li>
            <li>
              <b><code>padding_idx</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The index of the padding token. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>weights</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The initial weights for the embedding layer. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code><span title="float">float</span></code>)
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the output of the positional encoding. Defaults to 0.1.</p>
              </div>
            </li>
            <li>
              <b><code>freeze</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to freeze the weights of the embedding layer. Defaults to False.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>        <p>Note: not used in the current version of scprint.</p>











                  <details class="quote">
                    <summary>Source code in <code>scprint/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">freeze</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encodes gene sequences into a continuous vector space using an embedding layer.</span>

<span class="sd">    The output is then normalized using a LayerNorm.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_embeddings (int): The number of possible values.</span>
<span class="sd">        embedding_dim (int): The dimension of the output vectors.</span>
<span class="sd">        padding_idx (int, optional): The index of the padding token. Defaults to None.</span>
<span class="sd">        weights (Tensor, optional): The initial weights for the embedding layer. Defaults to None.</span>
<span class="sd">        dropout (float, optional): The dropout rate to apply to the output of the positional encoding. Defaults to 0.1.</span>
<span class="sd">        freeze (bool, optional): Whether to freeze the weights of the embedding layer. Defaults to False.</span>

<span class="sd">    Note: not used in the current version of scprint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">GeneEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
        <span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span><span class="p">,</span> <span class="n">_freeze</span><span class="o">=</span><span class="n">freeze</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># concat a zero vector to the weight</span>
        <span class="c1"># this is to make the embedding of the padding token to be zero</span>
        <span class="c1"># weights = torch.cat(</span>
        <span class="c1">#    [torch.Tensor(weights), torch.zeros(1, embedding_dim)], dim=0</span>
        <span class="c1"># )</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.encoders.PositionalEncoding" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">PositionalEncoding</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>The PositionalEncoding module applies a positional encoding to a sequence of vectors.
This is necessary for the Transformer model, which does not have any inherent notion of
position in a sequence. The positional encoding is added to the input embeddings and
allows the model to attend to positions in the sequence.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the input vectors.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code><span title="float">float</span></code>)
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the output of the positional encoding.</p>
              </div>
            </li>
            <li>
              <b><code>max_len</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The maximum length of a sequence that this module can handle.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>        <p>Note: not used in the current version of scprint.</p>










<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint.model.encoders.PositionalEncoding.forward)" href="#scprint.model.encoders.PositionalEncoding.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Args:</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">max_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">token_to_pos</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>  <span class="c1"># [token, pos]</span>
    <span class="n">maxval</span><span class="o">=</span><span class="mf">10000.0</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The PositionalEncoding module applies a positional encoding to a sequence of vectors.</span>
<span class="sd">    This is necessary for the Transformer model, which does not have any inherent notion of</span>
<span class="sd">    position in a sequence. The positional encoding is added to the input embeddings and</span>
<span class="sd">    allows the model to attend to positions in the sequence.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the input vectors.</span>
<span class="sd">        dropout (float, optional): The dropout rate to apply to the output of the positional encoding.</span>
<span class="sd">        max_len (int, optional): The maximum length of a sequence that this module can handle.</span>

<span class="sd">    Note: not used in the current version of scprint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Create a dictionary to convert token to position</span>

    <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">maxval</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="c1"># we reorder them and map them to gene_id (position)</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">token_to_pos</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">arr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pe</span><span class="p">[</span><span class="n">v</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">arr</span><span class="p">))</span>
    <span class="c1"># Remove the unnecessary middle dimension since pe should be [m, d]</span>
    <span class="c1"># pe = pe.squeeze(1)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;pe&quot;</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.encoders.PositionalEncoding.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
              –
              <div class="doc-md-description">
                <p>Tensor, shape [seq_len, batch_size, embedding_dim]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/encoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span>
<span class="normal">97</span>
<span class="normal">98</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gene_pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [seq_len, batch_size, embedding_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">gene_pos</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">gene_pos</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="scprint.model.decoders" class="doc doc-heading">
            <code>scprint.model.decoders</code>


</h2>

    <div class="doc doc-contents first">








<p><span class="doc-section-title">Classes:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="ClsDecoder (scprint.model.decoders.ClsDecoder)" href="#scprint.model.decoders.ClsDecoder">ClsDecoder</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="ExprDecoder (scprint.model.decoders.ExprDecoder)" href="#scprint.model.decoders.ExprDecoder">ExprDecoder</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="GraphSDEExprDecoder (scprint.model.decoders.GraphSDEExprDecoder)" href="#scprint.model.decoders.GraphSDEExprDecoder">GraphSDEExprDecoder</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td><code><a class="autorefs autorefs-internal" title="MVCDecoder (scprint.model.decoders.MVCDecoder)" href="#scprint.model.decoders.MVCDecoder">MVCDecoder</a></code></td>
            <td>
              <div class="doc-md-description">
                
              </div>
            </td>
          </tr>
      </tbody>
    </table>







  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="scprint.model.decoders.ClsDecoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">ClsDecoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>ClsDecoder Decoder for classification task.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>int, dimension of the input.</p>
              </div>
            </li>
            <li>
              <b><code>n_cls</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>int, number of classes.</p>
              </div>
            </li>
            <li>
              <b><code>layers</code></b>
                  (<code><span title="list">list</span>[<span title="int">int</span>]</code>, default:
                      <code>[256, 128]</code>
)
              –
              <div class="doc-md-description">
                <p>list[int], list of hidden layers.</p>
              </div>
            </li>
            <li>
              <b><code>activation</code></b>
                  (<code><span title="typing.Callable">Callable</span></code>, default:
                      <code><span title="torch.nn.ReLU">ReLU</span></code>
)
              –
              <div class="doc-md-description">
                <p>nn.Module, activation function.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>float, dropout rate.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              –
              <div class="doc-md-description">
                <p>Tensor, shape [batch_size, n_cls]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>









<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint.model.decoders.ClsDecoder.forward)" href="#scprint.model.decoders.ClsDecoder.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Args:</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint/model/decoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_cls</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ClsDecoder Decoder for classification task.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model: int, dimension of the input.</span>
<span class="sd">        n_cls: int, number of classes.</span>
<span class="sd">        layers: list[int], list of hidden layers.</span>
<span class="sd">        activation: nn.Module, activation function.</span>
<span class="sd">        dropout: float, dropout rate.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, shape [batch_size, n_cls]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ClsDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="c1"># module list</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">d_model</span><span class="p">]</span> <span class="o">+</span> <span class="n">layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">l</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">l</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_cls</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.decoders.ClsDecoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape [batch_size, embsize]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/decoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [batch_size, embsize]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.decoders.ExprDecoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">ExprDecoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>ExprDecoder Decoder for the gene expression prediction.</p>
<p>Will output the mean, variance and zero logits, parameters of a zero inflated negative binomial distribution.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the model. This is the size of the input feature vector.</p>
              </div>
            </li>
            <li>
              <b><code>nfirst_tokens_to_skip</code></b>
                  (<code><span title="int">int</span></code>, default:
                      <code>0</code>
)
              –
              <div class="doc-md-description">
                <p>The number of initial labels to skip in the sequence. Defaults to 0.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code><span title="float">float</span></code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>The dropout rate applied during training to prevent overfitting. Defaults to 0.1.</p>
              </div>
            </li>
            <li>
              <b><code>zinb</code></b>
                  (<code><span title="bool">bool</span></code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use a zero inflated negative binomial distribution. Defaults to True.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>









<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint.model.decoders.ExprDecoder.forward)" href="#scprint.model.decoders.ExprDecoder.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>x is the output of the transformer, (batch, seq_len, d_model)</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint/model/decoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">nfirst_tokens_to_skip</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">zinb</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">use_depth</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ExprDecoder Decoder for the gene expression prediction.</span>

<span class="sd">    Will output the mean, variance and zero logits, parameters of a zero inflated negative binomial distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the model. This is the size of the input feature vector.</span>
<span class="sd">        nfirst_tokens_to_skip (int, optional): The number of initial labels to skip in the sequence. Defaults to 0.</span>
<span class="sd">        dropout (float, optional): The dropout rate applied during training to prevent overfitting. Defaults to 0.1.</span>
<span class="sd">        zinb (bool, optional): Whether to use a zero inflated negative binomial distribution. Defaults to True.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ExprDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nfirst_tokens_to_skip</span> <span class="o">=</span> <span class="n">nfirst_tokens_to_skip</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">use_depth</span> <span class="k">else</span> <span class="n">d_model</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">3</span> <span class="k">if</span> <span class="n">zinb</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span> <span class="o">=</span> <span class="n">zinb</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.decoders.ExprDecoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">

        <p>x is the output of the transformer, (batch, seq_len, d_model)</p>


            <details class="quote">
              <summary>Source code in <code>scprint/model/decoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">req_depth</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;x is the output of the transformer, (batch, seq_len, d_model)&quot;&quot;&quot;</span>
    <span class="c1"># we don&#39;t do it on the labels</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nfirst_tokens_to_skip</span> <span class="p">:,</span> <span class="p">:]</span>
    <span class="k">if</span> <span class="n">req_depth</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">req_depth</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)],</span>
            <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span><span class="p">:</span>
        <span class="n">pred_value</span><span class="p">,</span> <span class="n">var_value</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># (batch, seq_len)</span>
        <span class="c1"># The sigmoid function is used to map the zero_logits to a probability between 0 and 1.</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">mean</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred_value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">disp</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">var_value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="nb">max</span><span class="o">=</span><span class="mi">15</span><span class="p">)),</span>
            <span class="n">zero_logits</span><span class="o">=</span><span class="n">zero_logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pred_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred_value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.decoders.GraphSDEExprDecoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">GraphSDEExprDecoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>Initialize the ExprNeuralSDEDecoder module.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code><span title="int">int</span></code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the model.</p>
              </div>
            </li>
            <li>
              <b><code>drift</code></b>
                  (<code><span title="torch.nn.Module">Module</span></code>)
              –
              <div class="doc-md-description">
                <p>The drift component of the SDE.</p>
              </div>
            </li>
            <li>
              <b><code>diffusion</code></b>
                  (<code><span title="torch.nn.Module">Module</span></code>)
              –
              <div class="doc-md-description">
                <p>The diffusion component of the SDE.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>










                  <details class="quote">
                    <summary>Source code in <code>scprint/model/decoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">drift</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">diffusion</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initialize the ExprNeuralSDEDecoder module.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the model.</span>
<span class="sd">        drift (nn.Module): The drift component of the SDE.</span>
<span class="sd">        diffusion (nn.Module): The diffusion component of the SDE.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drift</span> <span class="o">=</span> <span class="n">drift</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">diffusion</span> <span class="o">=</span> <span class="n">diffusion</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.decoders.MVCDecoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">MVCDecoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


        <p>MVCDecoder Decoder for the masked value prediction for cell embeddings.</p>
<p>Will use the gene embeddings with the cell embeddings to predict the mean, variance and zero logits</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
              –
              <div class="doc-md-description">
                <p>obj:<code>int</code>): dimension of the gene embedding.</p>
              </div>
            </li>
            <li>
              <b><code>arch_style</code></b>
              –
              <div class="doc-md-description">
                <p>obj:<code>str</code>): architecture style of the decoder, choice from
1. "inner product" or 2. "cell product" 3. "concat query" or 4. "sum query".</p>
              </div>
            </li>
            <li>
              <b><code>query_activation</code></b>
              –
              <div class="doc-md-description">
                <p>obj:<code>nn.Module</code>): activation function for the query
vectors. Defaults to nn.Sigmoid.</p>
              </div>
            </li>
            <li>
              <b><code>hidden_activation</code></b>
              –
              <div class="doc-md-description">
                <p>obj:<code>nn.Module</code>): activation function for the hidden
layers. Defaults to nn.PReLU.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>









<p><span class="doc-section-title">Methods:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
            <tr class="doc-section-item">
              <td><code><a class="autorefs autorefs-internal" title="forward (scprint.model.decoders.MVCDecoder.forward)" href="#scprint.model.decoders.MVCDecoder.forward">forward</a></code></td>
              <td>
                <div class="doc-md-description">
                  <p>Args:</p>
                </div>
              </td>
            </tr>
      </tbody>
    </table>



                  <details class="quote">
                    <summary>Source code in <code>scprint/model/decoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">arch_style</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;inner product&quot;</span><span class="p">,</span>
    <span class="n">tot_labels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">query_activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">,</span>
    <span class="n">hidden_activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">,</span>
    <span class="n">zinb</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MVCDecoder Decoder for the masked value prediction for cell embeddings.</span>

<span class="sd">    Will use the gene embeddings with the cell embeddings to predict the mean, variance and zero logits</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (:obj:`int`): dimension of the gene embedding.</span>
<span class="sd">        arch_style (:obj:`str`): architecture style of the decoder, choice from</span>
<span class="sd">            1. &quot;inner product&quot; or 2. &quot;cell product&quot; 3. &quot;concat query&quot; or 4. &quot;sum query&quot;.</span>
<span class="sd">        query_activation (:obj:`nn.Module`): activation function for the query</span>
<span class="sd">            vectors. Defaults to nn.Sigmoid.</span>
<span class="sd">        hidden_activation (:obj:`nn.Module`): activation function for the hidden</span>
<span class="sd">            layers. Defaults to nn.PReLU.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MVCDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;inner product&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span> <span class="o">=</span> <span class="n">query_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">*</span> <span class="p">(</span><span class="mi">3</span> <span class="k">if</span> <span class="n">zinb</span> <span class="k">else</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;concat query&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span> <span class="o">=</span> <span class="n">query_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">tot_labels</span><span class="p">),</span> <span class="n">d_model</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span> <span class="o">=</span> <span class="n">hidden_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span> <span class="k">if</span> <span class="n">zinb</span> <span class="k">else</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;sum query&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span> <span class="o">=</span> <span class="n">query_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span> <span class="o">=</span> <span class="n">hidden_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span> <span class="k">if</span> <span class="n">zinb</span> <span class="k">else</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown arch_style: </span><span class="si">{</span><span class="n">arch_style</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">arch_style</span> <span class="o">=</span> <span class="n">arch_style</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_detach</span> <span class="o">=</span> <span class="n">arch_style</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;detach&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span> <span class="o">=</span> <span class="n">zinb</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.decoders.MVCDecoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>cell_emb</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape (batch, embsize=d_model)</p>
              </div>
            </li>
            <li>
              <b><code>gene_embs</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape (batch, seq_len, embsize=d_model)</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

            <details class="quote">
              <summary>Source code in <code>scprint/model/decoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">cell_emb</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">gene_embs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        cell_emb: Tensor, shape (batch, embsize=d_model)</span>
<span class="sd">        gene_embs: Tensor, shape (batch, seq_len, embsize=d_model)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;inner product&quot;</span><span class="p">:</span>
        <span class="n">query_vecs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span><span class="p">(</span><span class="n">gene_embs</span><span class="p">)))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span><span class="p">:</span>
            <span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span><span class="p">(</span><span class="n">query_vecs</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span><span class="p">(</span><span class="n">query_vecs</span><span class="p">)</span>
        <span class="n">cell_emb</span> <span class="o">=</span> <span class="n">cell_emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span><span class="p">:</span>
            <span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">cell_emb</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">cell_emb</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">zero_logits</span><span class="p">,</span> <span class="n">cell_emb</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">cell_emb</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="c1"># zero logits need to based on the cell_emb, because of input exprs</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;concat query&quot;</span><span class="p">:</span>
        <span class="n">query_vecs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span><span class="p">(</span><span class="n">gene_embs</span><span class="p">))</span>
        <span class="c1"># expand cell_emb to (batch, seq_len, embsize)</span>
        <span class="n">cell_emb</span> <span class="o">=</span> <span class="n">cell_emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">gene_embs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cell_emb</span><span class="p">,</span> <span class="n">query_vecs</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span><span class="p">:</span>
            <span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;sum query&quot;</span><span class="p">:</span>
        <span class="n">query_vecs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span><span class="p">(</span><span class="n">gene_embs</span><span class="p">))</span>
        <span class="n">cell_emb</span> <span class="o">=</span> <span class="n">cell_emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">cell_emb</span> <span class="o">+</span> <span class="n">query_vecs</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span><span class="p">:</span>
            <span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">mvc_mean</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">mvc_disp</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">15</span><span class="p">)),</span>
            <span class="n">mvc_zero_logits</span><span class="o">=</span><span class="n">zero_logits</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mvc_mean</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../notebooks/cancer_usecase_part2/" class="btn btn-neutral float-left" title="scPRINT use case on BPH (part 2, GN analysis)"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../tasks/" class="btn btn-neutral float-right" title="tasks">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../notebooks/cancer_usecase_part2/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../tasks/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
