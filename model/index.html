<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://www.jkobject.com/scPRINT/model/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>model - scprint</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "model";
        var mkdocs_page_input_path = "model.md";
        var mkdocs_page_url = "/scPRINT/model/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> scprint
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../structure/">structure</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../notebooks/pretrain/">training</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../notebooks/grn/">grn inference</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../notebooks/embeddings/">cell embedding and classification</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../notebooks/generate_gene_embeddings/">gene embeddings</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">documentation</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../utils/">utils</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">model</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.utils">utils</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.downsample_profile">downsample_profile()</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.make_adata">make_adata()</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.masker">masker()</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.translate">translate()</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.zinb_sample">zinb_sample()</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.encoders">encoders</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.encoders.CategoryValueEncoder">CategoryValueEncoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.encoders.ContinuousValueEncoder">ContinuousValueEncoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.encoders.ContinuousValueEncoder.forward">forward()</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.encoders.DPositionalEncoding">DPositionalEncoding</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.encoders.DPositionalEncoding.forward">forward()</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.encoders.GeneEncoder">GeneEncoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.encoders.PositionalEncoding">PositionalEncoding</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.encoders.PositionalEncoding.forward">forward()</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.decoders">decoders</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.decoders.ClsDecoder">ClsDecoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.decoders.ClsDecoder.forward">forward()</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.decoders.ExprDecoder">ExprDecoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.decoders.ExprDecoder.forward">forward()</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.decoders.GraphSDEExprDecoder">GraphSDEExprDecoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.decoders.MVCDecoder">MVCDecoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.decoders.MVCDecoder.forward">forward()</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.flash_attn.flashformer">flashformer</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.flash_attn.flashformer.FlashTransformerEncoder">FlashTransformerEncoder</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.model">model</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.model.scPrint">scPrint</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.configure_optimizers">configure_optimizers()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.forward">forward()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.get_cell_embs">get_cell_embs()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.log_adata">log_adata()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.on_fit_start">on_fit_start()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.on_predict_epoch_end">on_predict_epoch_end()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.on_predict_epoch_start">on_predict_epoch_start()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.on_validation_epoch_end">on_validation_epoch_end()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.optimizer_step">optimizer_step()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.predict_step">predict_step()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.test_step">test_step()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.training_step">training_step()</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.validation_step">validation_step()</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.loss">loss</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.classification">classification()</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.classifier_loss">classifier_loss()</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.criterion_neg_log_bernoulli">criterion_neg_log_bernoulli()</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.ecs">ecs()</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.graph_similarity_loss">graph_similarity_loss()</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.graph_sparsity_loss">graph_sparsity_loss()</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.masked_mae_loss">masked_mae_loss()</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.masked_mse_loss">masked_mse_loss()</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.masked_nb_loss">masked_nb_loss()</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.masked_relative_error">masked_relative_error()</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.nb">nb()</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.loss.nb--returns">Returns</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.similarity">similarity()</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.zinb">zinb()</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.loss.zinb--returns">Returns</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="" href="../embedders.md">loaders</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tasks/">tasks</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">scprint</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">documentation</li>
      <li class="breadcrumb-item active">model</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="documentation-for-the-model">Documentation for the <code>model</code></h1>


<div class="doc doc-object doc-module">




<h2 id="scprint.model.utils" class="doc doc-heading">
          <code>scprint.model.utils</code>


</h2>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="scprint.model.utils.downsample_profile" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">downsample_profile</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>This function downsamples the expression profile of a given single cell RNA matrix.</p>
<p>The noise is applied based on the renoise parameter,
the total counts of the matrix, and the number of genes. The function first calculates the noise
threshold (tnoise) based on the renoise parameter. It then generates an initial matrix count by
applying a Poisson distribution to a random tensor scaled by the total counts and the number of genes.
The function then models the sampling zeros by applying a Poisson distribution to a random tensor
scaled by the noise threshold, the total counts, and the number of genes. The function also models
the technical zeros by generating a random tensor and comparing it to the noise threshold. The final
matrix count is calculated by subtracting the sampling zeros from the initial matrix count and
multiplying by the technical zeros. The function ensures that the final matrix count is not less
than zero by taking the maximum of the final matrix count and a tensor of zeros. The function
returns the final matrix count.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>mat</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The input matrix.</p>
              </div>
            </li>
            <li>
              <b><code>renoise</code></b>
                  (<code>float</code>)
              –
              <div class="doc-md-description">
                <p>The renoise parameter.</p>
              </div>
            </li>
            <li>
              <b><code>totcounts</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The total counts of the matrix.</p>
              </div>
            </li>
            <li>
              <b><code>ngenes</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The number of genes.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
            –
            <div class="doc-md-description">
              <p>torch.Tensor: The matrix count after applying noise.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>scprint/model/utils.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">downsample_profile</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">renoise</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function downsamples the expression profile of a given single cell RNA matrix.</span>

<span class="sd">    The noise is applied based on the renoise parameter,</span>
<span class="sd">    the total counts of the matrix, and the number of genes. The function first calculates the noise</span>
<span class="sd">    threshold (tnoise) based on the renoise parameter. It then generates an initial matrix count by</span>
<span class="sd">    applying a Poisson distribution to a random tensor scaled by the total counts and the number of genes.</span>
<span class="sd">    The function then models the sampling zeros by applying a Poisson distribution to a random tensor</span>
<span class="sd">    scaled by the noise threshold, the total counts, and the number of genes. The function also models</span>
<span class="sd">    the technical zeros by generating a random tensor and comparing it to the noise threshold. The final</span>
<span class="sd">    matrix count is calculated by subtracting the sampling zeros from the initial matrix count and</span>
<span class="sd">    multiplying by the technical zeros. The function ensures that the final matrix count is not less</span>
<span class="sd">    than zero by taking the maximum of the final matrix count and a tensor of zeros. The function</span>
<span class="sd">    returns the final matrix count.</span>

<span class="sd">    Args:</span>
<span class="sd">        mat (torch.Tensor): The input matrix.</span>
<span class="sd">        renoise (float): The renoise parameter.</span>
<span class="sd">        totcounts (torch.Tensor): The total counts of the matrix.</span>
<span class="sd">        ngenes (int): The number of genes.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The matrix count after applying noise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Randomly drop on average N counts to each element of expression using a heavy tail Gaussian distribution</span>
    <span class="c1"># here we try to get the scale of the distribution so as to remove the right number of counts from each gene</span>
    <span class="c1"># https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02601-5#:~:text=Zero%20measurements%20in%20scRNA%2Dseq,generation%20of%20scRNA%2Dseq%20data.</span>
    <span class="n">totcounts</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">ngenes</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">tnoise</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">renoise</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="c1"># we model the sampling zeros (dropping 30% of the reads)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">ngenes</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="o">*</span> <span class="p">((</span><span class="n">tnoise</span> <span class="o">*</span> <span class="n">totcounts</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">ngenes</span><span class="p">))</span>
    <span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
    <span class="c1"># we model the technical zeros (dropping 50% of the genes)</span>
    <span class="n">drop</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">ngenes</span><span class="p">))</span> <span class="o">&gt;</span> <span class="n">tnoise</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">mat</span> <span class="o">=</span> <span class="p">(</span><span class="n">mat</span> <span class="o">-</span> <span class="n">res</span><span class="p">)</span> <span class="o">*</span> <span class="n">drop</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">]])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">))</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="scprint.model.utils.make_adata" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">make_adata</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>This function creates an AnnData object from the given input parameters.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>pred</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Predicted labels. The shape of the tensor is (n_cells, n_classes)</p>
              </div>
            </li>
            <li>
              <b><code>embs</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Embeddings of the cells. The shape of the tensor is (n_cells, n_features)</p>
              </div>
            </li>
            <li>
              <b><code>labels</code></b>
                  (<code>list</code>)
              –
              <div class="doc-md-description">
                <p>List of labels for the predicted classes.</p>
              </div>
            </li>
            <li>
              <b><code>step</code></b>
                  (<code>int</code>, default:
                      <code>0</code>
)
              –
              <div class="doc-md-description">
                <p>Step number. Default is 0. (for storing the anndata without overwriting others)</p>
              </div>
            </li>
            <li>
              <b><code>label_decoders</code></b>
                  (<code>dict</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Dictionary to map class codes to class names. Default is None.</p>
              </div>
            </li>
            <li>
              <b><code>gtclass</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Ground truth class. Default is None.</p>
              </div>
            </li>
            <li>
              <b><code>name</code></b>
                  (<code>str</code>, default:
                      <code>&#39;&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Name of the AnnData object. Default is an empty string.</p>
              </div>
            </li>
            <li>
              <b><code>mdir</code></b>
                  (<code>str</code>, default:
                      <code>&#39;/tmp&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Directory to save the AnnData object. Default is "/tmp".</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
<b><code>adata</code></b>(                <code><span title="anndata.AnnData">AnnData</span></code>
)            –
            <div class="doc-md-description">
              <p>The created AnnData object.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>scprint/model/utils.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">make_adata</span><span class="p">(</span>
    <span class="n">pred</span><span class="p">,</span> <span class="n">embs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">label_decoders</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">gtclass</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">mdir</span><span class="o">=</span><span class="s2">&quot;/tmp&quot;</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function creates an AnnData object from the given input parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        pred (torch.Tensor): Predicted labels. The shape of the tensor is (n_cells, n_classes)</span>
<span class="sd">        embs (torch.Tensor): Embeddings of the cells. The shape of the tensor is (n_cells, n_features)</span>
<span class="sd">        labels (list): List of labels for the predicted classes.</span>
<span class="sd">        step (int, optional): Step number. Default is 0. (for storing the anndata without overwriting others)</span>
<span class="sd">        label_decoders (dict, optional): Dictionary to map class codes to class names. Default is None.</span>
<span class="sd">        gtclass (torch.Tensor, optional): Ground truth class. Default is None.</span>
<span class="sd">        name (str, optional): Name of the AnnData object. Default is an empty string.</span>
<span class="sd">        mdir (str, optional): Directory to save the AnnData object. Default is &quot;/tmp&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        adata (anndata.AnnData): The created AnnData object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">colname</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
    <span class="c1"># label decoders is not cls_decoders. one is a dict to map class codes (ints)</span>
    <span class="c1"># to class names the other is the module the predict the class</span>
    <span class="k">if</span> <span class="n">label_decoders</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="p">[</span><span class="n">label_decoders</span><span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]][</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">name</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">obs</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span><span class="o">.</span><span class="n">T</span>

    <span class="k">if</span> <span class="n">gtclass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">colname</span> <span class="o">+=</span> <span class="n">labels</span>
        <span class="n">nobs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">gtclass</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">label_decoders</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">nobs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="p">[</span><span class="n">label_decoders</span><span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]][</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">name</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">nobs</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
                <span class="p">]</span>
            <span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">obs</span><span class="p">,</span> <span class="n">nobs</span><span class="p">])</span>

    <span class="n">adata</span> <span class="o">=</span> <span class="n">AnnData</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
        <span class="n">obs</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
            <span class="n">obs</span><span class="p">,</span>
            <span class="n">columns</span><span class="o">=</span><span class="n">colname</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>

    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">gtclass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tr</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="n">n</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">n</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">tr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s2">&quot;conv_&quot;</span> <span class="o">+</span> <span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="n">n</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">tr</span><span class="p">)</span>
        <span class="n">tr</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">n</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">n</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">n</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">tr</span><span class="p">)</span>
    <span class="n">sc</span><span class="o">.</span><span class="n">pp</span><span class="o">.</span><span class="n">neighbors</span><span class="p">(</span><span class="n">adata</span><span class="p">)</span>
    <span class="n">sc</span><span class="o">.</span><span class="n">tl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span><span class="n">adata</span><span class="p">)</span>
    <span class="n">sc</span><span class="o">.</span><span class="n">tl</span><span class="o">.</span><span class="n">leiden</span><span class="p">(</span><span class="n">adata</span><span class="p">)</span>
    <span class="n">adata</span><span class="o">.</span><span class="n">obs</span> <span class="o">=</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;category&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">adata</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">gtclass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">color</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">i</span>
            <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="s2">&quot;conv_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="k">if</span> <span class="s2">&quot;conv_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="o">.</span><span class="n">columns</span> <span class="k">else</span> <span class="n">i</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">labels</span>
                <span class="p">],</span>
                <span class="p">[</span>
                    <span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">i</span>
                    <span class="k">if</span> <span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="o">.</span><span class="n">columns</span>
                    <span class="k">else</span> <span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">i</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">labels</span>
                <span class="p">],</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pair</span>
        <span class="p">]</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">)</span> <span class="o">*</span> <span class="mi">4</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">color</span><span class="p">):</span>
            <span class="n">sc</span><span class="o">.</span><span class="n">pl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span>
                <span class="n">adata</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="n">col</span><span class="p">,</span>
                <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">],</span>
                <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">color</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="k">if</span> <span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="o">.</span><span class="n">columns</span> <span class="k">else</span> <span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">i</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">labels</span>
        <span class="p">]</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">)</span> <span class="o">*</span> <span class="mi">8</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">color</span><span class="p">):</span>
            <span class="n">sc</span><span class="o">.</span><span class="n">pl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span>
                <span class="n">adata</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="n">col</span><span class="p">,</span>
                <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="n">adata</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">mdir</span> <span class="o">+</span> <span class="s2">&quot;/step_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">step</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.h5ad&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">adata</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="scprint.model.utils.masker" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">masker</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Randomly mask a batch of data.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>values</code></b>
                  (<code>array - like</code>)
              –
              <div class="doc-md-description">
                <p>A batch of tokenized data, with shape (batch_size, n_features).</p>
              </div>
            </li>
            <li>
              <b><code>mask_ratio</code></b>
                  (<code>float</code>, default:
                      <code>0.15</code>
)
              –
              <div class="doc-md-description">
                <p>The ratio of genes to mask, default to 0.15.</p>
              </div>
            </li>
            <li>
              <b><code>mask_value</code></b>
                  (<code>int</code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>The value to mask with, default to -1.</p>
              </div>
            </li>
            <li>
              <b><code>pad_value</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The value of padding in the values, will be kept unchanged.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
                <code><span title="torch.Tensor">Tensor</span></code>
            –
            <div class="doc-md-description">
              <p>torch.Tensor: A tensor of masked data.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>scprint/model/utils.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">masker</span><span class="p">(</span>
    <span class="n">length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">mask_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">,</span>
    <span class="n">mask_prob</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># n_features</span>
    <span class="n">mask_value</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly mask a batch of data.</span>

<span class="sd">    Args:</span>
<span class="sd">        values (array-like):</span>
<span class="sd">            A batch of tokenized data, with shape (batch_size, n_features).</span>
<span class="sd">        mask_ratio (float): The ratio of genes to mask, default to 0.15.</span>
<span class="sd">        mask_value (int): The value to mask with, default to -1.</span>
<span class="sd">        pad_value (int): The value of padding in the values, will be kept unchanged.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: A tensor of masked data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
        <span class="n">loc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
            <span class="n">a</span><span class="o">=</span><span class="n">length</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">length</span> <span class="o">*</span> <span class="n">mask_ratio</span><span class="p">),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">mask_prob</span>
        <span class="p">)</span>
        <span class="n">m</span><span class="p">[</span><span class="n">loc</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask_value</span>
        <span class="n">mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="scprint.model.utils.translate" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">translate</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>translate This function translates the given value based on the specified type.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>val</code></b>
                  (<code>str / list / set / dict / <span title="collections.Counter">Counter</span></code>)
              –
              <div class="doc-md-description">
                <p>The value to be translated.</p>
              </div>
            </li>
            <li>
              <b><code>t</code></b>
                  (<code>str</code>, default:
                      <code>&#39;cell_type_ontology_term_id&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>The type of translation to be performed. Defaults to "cell_type_ontology_term_id".</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
<b><code>dict</code></b>            –
            <div class="doc-md-description">
              <p>A dictionary with the translated values.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>scprint/model/utils.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">translate</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="s2">&quot;cell_type_ontology_term_id&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    translate This function translates the given value based on the specified type.</span>

<span class="sd">    Args:</span>
<span class="sd">        val (str/list/set/dict/Counter): The value to be translated.</span>
<span class="sd">        t (str, optional): The type of translation to be performed. Defaults to &quot;cell_type_ontology_term_id&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: A dictionary with the translated values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">==</span> <span class="s2">&quot;cell_type_ontology_term_id&quot;</span><span class="p">:</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="n">bt</span><span class="o">.</span><span class="n">CellType</span><span class="o">.</span><span class="n">df</span><span class="p">()</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;ontology_id&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">t</span> <span class="o">==</span> <span class="s2">&quot;assay_ontology_term_id&quot;</span><span class="p">:</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="n">bt</span><span class="o">.</span><span class="n">ExperimentalFactor</span><span class="o">.</span><span class="n">df</span><span class="p">()</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;ontology_id&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">t</span> <span class="o">==</span> <span class="s2">&quot;tissue_ontology_term_id&quot;</span><span class="p">:</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="n">bt</span><span class="o">.</span><span class="n">Tissue</span><span class="o">.</span><span class="n">df</span><span class="p">()</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;ontology_id&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">t</span> <span class="o">==</span> <span class="s2">&quot;disease_ontology_term_id&quot;</span><span class="p">:</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="n">bt</span><span class="o">.</span><span class="n">Disease</span><span class="o">.</span><span class="n">df</span><span class="p">()</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;ontology_id&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">t</span> <span class="o">==</span> <span class="s2">&quot;self_reported_ethnicity_ontology_term_id&quot;</span><span class="p">:</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="n">bt</span><span class="o">.</span><span class="n">Ethnicity</span><span class="o">.</span><span class="n">df</span><span class="p">()</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;ontology_id&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">val</span><span class="p">:</span> <span class="n">obj</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">val</span><span class="p">][</span><span class="s2">&quot;name&quot;</span><span class="p">]}</span>
    <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">list</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">set</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">obj</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;name&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">val</span><span class="p">)}</span>
    <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">dict</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="n">Counter</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">obj</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="s2">&quot;name&quot;</span><span class="p">]:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">val</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="scprint.model.utils.zinb_sample" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">zinb_sample</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>mu</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The mean of the Negative Binomial (NB) distribution.</p>
              </div>
            </li>
            <li>
              <b><code>theta</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The dispersion parameter of the NB distribution.</p>
              </div>
            </li>
            <li>
              <b><code>zi_probs</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The zero-inflation probabilities.</p>
              </div>
            </li>
            <li>
              <b><code>sample_shape</code></b>
                  (<code><span title="torch.Size">Size</span></code>, default:
                      <code><span title="torch.Size">Size</span>([])</code>
)
              –
              <div class="doc-md-description">
                <p>The output shape. Defaults to torch.Size([]).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
            –
            <div class="doc-md-description">
              <p>torch.Tensor: A sample from the ZINB distribution.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>scprint/model/utils.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">zinb_sample</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">zi_probs</span><span class="p">,</span> <span class="n">sample_shape</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([])):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        mu (torch.Tensor): The mean of the Negative Binomial (NB) distribution.</span>
<span class="sd">        theta (torch.Tensor): The dispersion parameter of the NB distribution.</span>
<span class="sd">        zi_probs (torch.Tensor): The zero-inflation probabilities.</span>
<span class="sd">        sample_shape (torch.Size, optional): The output shape. Defaults to torch.Size([]).</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: A sample from the ZINB distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">concentration</span> <span class="o">=</span> <span class="n">theta</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">/</span> <span class="n">mu</span>
    <span class="c1"># Important remark: Gamma is parametrized by the rate = 1/scale!</span>
    <span class="n">gamma_d</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="n">concentration</span><span class="o">=</span><span class="n">concentration</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">rate</span><span class="p">)</span>
    <span class="n">p_means</span> <span class="o">=</span> <span class="n">gamma_d</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span>

    <span class="c1"># Clamping as distributions objects can have buggy behaviors when</span>
    <span class="c1"># their parameters are too high</span>
    <span class="n">l_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">p_means</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1e8</span><span class="p">)</span>
    <span class="n">samp</span> <span class="o">=</span> <span class="n">Poisson</span><span class="p">(</span><span class="n">l_train</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Shape : (n_samples, n_cells_batch, n_vars)</span>
    <span class="n">is_zero</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">samp</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">zi_probs</span>
    <span class="n">samp_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">is_zero</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">samp</span><span class="p">),</span> <span class="n">samp</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">samp_</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-module">




<h2 id="scprint.model.encoders" class="doc doc-heading">
          <code>scprint.model.encoders</code>


</h2>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">




<h3 id="scprint.model.encoders.CategoryValueEncoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">CategoryValueEncoder</span></code>

</h3>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>Encodes categorical values into a vector using an embedding layer and layer normalization.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>num_embeddings</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The number of possible values.</p>
              </div>
            </li>
            <li>
              <b><code>embedding_dim</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the output vectors.</p>
              </div>
            </li>
            <li>
              <b><code>padding_idx</code></b>
                  (<code>int</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The index of the padding token. Defaults to None.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
            –
            <div class="doc-md-description">
              <p>torch.Tensor: A tensor representing the encoded categorical values.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>      <p>Note: not used in the current version of scprint.</p>

                <details class="quote">
                  <summary>Source code in <code>scprint/model/encoders.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encodes categorical values into a vector using an embedding layer and layer normalization.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_embeddings (int): The number of possible values.</span>
<span class="sd">        embedding_dim (int): The dimension of the output vectors.</span>
<span class="sd">        padding_idx (int, optional): The index of the padding token. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: A tensor representing the encoded categorical values.</span>

<span class="sd">    Note: not used in the current version of scprint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CategoryValueEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
        <span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">enc_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h3 id="scprint.model.encoders.ContinuousValueEncoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">ContinuousValueEncoder</span></code>

</h3>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>Encode real number values to a vector using neural nets projection.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the input vectors.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the output of the positional encoding.</p>
              </div>
            </li>
            <li>
              <b><code>max_value</code></b>
                  (<code>int</code>, default:
                      <code>100000</code>
)
              –
              <div class="doc-md-description">
                <p>The maximum value of the input. Defaults to 100_000.</p>
              </div>
            </li>
            <li>
              <b><code>layers</code></b>
                  (<code>int</code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>The number of layers in the encoder. Defaults to 1.</p>
              </div>
            </li>
            <li>
              <b><code>size</code></b>
                  (<code>int</code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>The size of the input. Defaults to 1.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
            –
            <div class="doc-md-description">
              <p>torch.Tensor: A tensor representing the encoded continuous values.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
                <details class="quote">
                  <summary>Source code in <code>scprint/model/encoders.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">max_value</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100_000</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encode real number values to a vector using neural nets projection.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the input vectors.</span>
<span class="sd">        dropout (float, optional): The dropout rate to apply to the output of the positional encoding.</span>
<span class="sd">        max_value (int, optional): The maximum value of the input. Defaults to 100_000.</span>
<span class="sd">        layers (int, optional): The number of layers in the encoder. Defaults to 1.</span>
<span class="sd">        size (int, optional): The size of the input. Defaults to 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: A tensor representing the encoded continuous values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ContinuousValueEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_value</span> <span class="o">=</span> <span class="n">max_value</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">size</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h4 id="scprint.model.encoders.ContinuousValueEncoder.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


  <div class="doc doc-contents ">
  



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape [batch_size, seq_len]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>scprint/model/encoders.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [batch_size, seq_len]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: test using actual embedding layer if input is categorical</span>
    <span class="c1"># expand last dimension</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># use the mask embedding when x=-1</span>
    <span class="c1"># mask = (x == -1).float()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_value</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">val</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h3 id="scprint.model.encoders.DPositionalEncoding" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">DPositionalEncoding</span></code>

</h3>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>The PositionalEncoding module applies a positional encoding to a sequence of vectors.
This is necessary for the Transformer model, which does not have any inherent notion of
position in a sequence. The positional encoding is added to the input embeddings and
allows the model to attend to positions in the sequence.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the input vectors.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the output of the positional encoding.</p>
              </div>
            </li>
            <li>
              <b><code>max_len</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The maximum length of a sequence that this module can handle.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>      <p>Note: not used in the current version of scprint.</p>

                <details class="quote">
                  <summary>Source code in <code>scprint/model/encoders.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">max_len_x</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">max_len_y</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">maxvalue_x</span><span class="o">=</span><span class="mf">10000.0</span><span class="p">,</span>
    <span class="n">maxvalue_y</span><span class="o">=</span><span class="mf">10000.0</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">DPositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>

    <span class="n">position2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_len_y</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">position1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_len_x</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">half_n</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="mi">2</span>

    <span class="n">div_term2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">half_n</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">maxvalue_y</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">div_term1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">half_n</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">maxvalue_x</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pe1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">pe2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len_y</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">pe1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="n">half_n</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position1</span> <span class="o">*</span> <span class="n">div_term1</span><span class="p">)</span>
    <span class="n">pe1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="n">half_n</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position1</span> <span class="o">*</span> <span class="n">div_term1</span><span class="p">)</span>
    <span class="n">pe2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">half_n</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position2</span> <span class="o">*</span> <span class="n">div_term2</span><span class="p">)</span>
    <span class="n">pe2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">half_n</span> <span class="p">::</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position2</span> <span class="o">*</span> <span class="n">div_term2</span><span class="p">)</span>
    <span class="c1"># https://github.com/tatp22/multidim-positional-encoding/blob/master/positional_encodings/torch_encodings.py</span>
    <span class="c1"># TODO: seems to do it differently. I hope it still works ok!!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;pe1&quot;</span><span class="p">,</span> <span class="n">pe1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;pe2&quot;</span><span class="p">,</span> <span class="n">pe2</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h4 id="scprint.model.encoders.DPositionalEncoding.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


  <div class="doc doc-contents ">
  



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape [seq_len, batch_size, embedding_dim]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>scprint/model/encoders.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pos_x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pos_y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [seq_len, batch_size, embedding_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: try with a continuous value encoder of size 2 (start, end where they are normalized to 0-1)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe1</span><span class="p">[</span><span class="n">pos_x</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe2</span><span class="p">[</span><span class="n">pos_y</span><span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h3 id="scprint.model.encoders.GeneEncoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">GeneEncoder</span></code>

</h3>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>Encodes gene sequences into a continuous vector space using an embedding layer.</p>
<p>The output is then normalized using a LayerNorm.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>num_embeddings</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The number of possible values.</p>
              </div>
            </li>
            <li>
              <b><code>embedding_dim</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the output vectors.</p>
              </div>
            </li>
            <li>
              <b><code>padding_idx</code></b>
                  (<code>int</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The index of the padding token. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>weights</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The initial weights for the embedding layer. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the output of the positional encoding. Defaults to 0.1.</p>
              </div>
            </li>
            <li>
              <b><code>freeze</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to freeze the weights of the embedding layer. Defaults to False.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>      <p>Note: not used in the current version of scprint.</p>

                <details class="quote">
                  <summary>Source code in <code>scprint/model/encoders.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">freeze</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encodes gene sequences into a continuous vector space using an embedding layer.</span>

<span class="sd">    The output is then normalized using a LayerNorm.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_embeddings (int): The number of possible values.</span>
<span class="sd">        embedding_dim (int): The dimension of the output vectors.</span>
<span class="sd">        padding_idx (int, optional): The index of the padding token. Defaults to None.</span>
<span class="sd">        weights (Tensor, optional): The initial weights for the embedding layer. Defaults to None.</span>
<span class="sd">        dropout (float, optional): The dropout rate to apply to the output of the positional encoding. Defaults to 0.1.</span>
<span class="sd">        freeze (bool, optional): Whether to freeze the weights of the embedding layer. Defaults to False.</span>

<span class="sd">    Note: not used in the current version of scprint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">GeneEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
        <span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span><span class="p">,</span> <span class="n">_freeze</span><span class="o">=</span><span class="n">freeze</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># concat a zero vector to the weight</span>
        <span class="c1"># this is to make the embedding of the padding token to be zero</span>
        <span class="c1"># weights = torch.cat(</span>
        <span class="c1">#    [torch.Tensor(weights), torch.zeros(1, embedding_dim)], dim=0</span>
        <span class="c1"># )</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">enc_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h3 id="scprint.model.encoders.PositionalEncoding" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">PositionalEncoding</span></code>

</h3>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>The PositionalEncoding module applies a positional encoding to a sequence of vectors.
This is necessary for the Transformer model, which does not have any inherent notion of
position in a sequence. The positional encoding is added to the input embeddings and
allows the model to attend to positions in the sequence.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the input vectors.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the output of the positional encoding.</p>
              </div>
            </li>
            <li>
              <b><code>max_len</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The maximum length of a sequence that this module can handle.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>      <p>Note: not used in the current version of scprint.</p>

                <details class="quote">
                  <summary>Source code in <code>scprint/model/encoders.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">max_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">token_to_pos</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>  <span class="c1"># [token, pos]</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">maxval</span><span class="o">=</span><span class="mf">10000.0</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The PositionalEncoding module applies a positional encoding to a sequence of vectors.</span>
<span class="sd">    This is necessary for the Transformer model, which does not have any inherent notion of</span>
<span class="sd">    position in a sequence. The positional encoding is added to the input embeddings and</span>
<span class="sd">    allows the model to attend to positions in the sequence.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the input vectors.</span>
<span class="sd">        dropout (float, optional): The dropout rate to apply to the output of the positional encoding.</span>
<span class="sd">        max_len (int, optional): The maximum length of a sequence that this module can handle.</span>

<span class="sd">    Note: not used in the current version of scprint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Create a dictionary to convert token to position</span>

    <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">maxval</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="c1"># we reorder them and map them to gene_id (position)</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">token_to_pos</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">arr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pe</span><span class="p">[</span><span class="n">v</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">arr</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;pe&quot;</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h4 id="scprint.model.encoders.PositionalEncoding.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


  <div class="doc doc-contents ">
  



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
              –
              <div class="doc-md-description">
                <p>Tensor, shape [seq_len, batch_size, embedding_dim]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>scprint/model/encoders.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gene_pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [seq_len, batch_size, embedding_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">gene_pos</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="n">gene_pos</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>
        <span class="p">)</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">




<h2 id="scprint.model.decoders" class="doc doc-heading">
          <code>scprint.model.decoders</code>


</h2>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">




<h3 id="scprint.model.decoders.ClsDecoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">ClsDecoder</span></code>

</h3>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>ClsDecoder Decoder for classification task.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>int, dimension of the input.</p>
              </div>
            </li>
            <li>
              <b><code>n_cls</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>int, number of classes.</p>
              </div>
            </li>
            <li>
              <b><code>layers</code></b>
                  (<code>list[int]</code>, default:
                      <code>[256, 128]</code>
)
              –
              <div class="doc-md-description">
                <p>list[int], list of hidden layers.</p>
              </div>
            </li>
            <li>
              <b><code>activation</code></b>
                  (<code><span title="typing.Callable">Callable</span></code>, default:
                      <code><span title="torch.nn.ReLU">ReLU</span></code>
)
              –
              <div class="doc-md-description">
                <p>nn.Module, activation function.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>float, dropout rate.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
            –
            <div class="doc-md-description">
              <p>Tensor, shape [batch_size, n_cls]</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
                <details class="quote">
                  <summary>Source code in <code>scprint/model/decoders.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_cls</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ClsDecoder Decoder for classification task.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model: int, dimension of the input.</span>
<span class="sd">        n_cls: int, number of classes.</span>
<span class="sd">        layers: list[int], list of hidden layers.</span>
<span class="sd">        activation: nn.Module, activation function.</span>
<span class="sd">        dropout: float, dropout rate.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, shape [batch_size, n_cls]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ClsDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="c1"># module list</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">d_model</span><span class="p">]</span> <span class="o">+</span> <span class="n">layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">l</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">l</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_cls</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h4 id="scprint.model.decoders.ClsDecoder.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


  <div class="doc doc-contents ">
  



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape [batch_size, embsize]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>scprint/model/decoders.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [batch_size, embsize]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h3 id="scprint.model.decoders.ExprDecoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">ExprDecoder</span></code>

</h3>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>ExprDecoder Decoder for the gene expression prediction.</p>
<p>Will output the mean, variance and zero logits, parameters of a zero inflated negative binomial distribution.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the model. This is the size of the input feature vector.</p>
              </div>
            </li>
            <li>
              <b><code>nfirst_labels_to_skip</code></b>
                  (<code>int</code>, default:
                      <code>0</code>
)
              –
              <div class="doc-md-description">
                <p>The number of initial labels to skip in the sequence. Defaults to 0.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>The dropout rate applied during training to prevent overfitting. Defaults to 0.1.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                <details class="quote">
                  <summary>Source code in <code>scprint/model/decoders.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">nfirst_labels_to_skip</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ExprDecoder Decoder for the gene expression prediction.</span>

<span class="sd">    Will output the mean, variance and zero logits, parameters of a zero inflated negative binomial distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the model. This is the size of the input feature vector.</span>
<span class="sd">        nfirst_labels_to_skip (int, optional): The number of initial labels to skip in the sequence. Defaults to 0.</span>
<span class="sd">        dropout (float, optional): The dropout rate applied during training to prevent overfitting. Defaults to 0.1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ExprDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nfirst_labels_to_skip</span> <span class="o">=</span> <span class="n">nfirst_labels_to_skip</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h4 id="scprint.model.decoders.ExprDecoder.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>x is the output of the transformer, (batch, seq_len, d_model)</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/decoders.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;x is the output of the transformer, (batch, seq_len, d_model)&quot;&quot;&quot;</span>
    <span class="c1"># we don&#39;t do it on the labels</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nfirst_labels_to_skip</span> <span class="p">:,</span> <span class="p">:])</span>
    <span class="n">pred_value</span><span class="p">,</span> <span class="n">var_value</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
        <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
    <span class="p">)</span>  <span class="c1"># (batch, seq_len)</span>
    <span class="c1"># The sigmoid function is used to map the zero_logits to a probability between 0 and 1.</span>
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">mean</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred_value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">disp</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">var_value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="nb">max</span><span class="o">=</span><span class="mi">15</span><span class="p">)),</span>
        <span class="n">zero_logits</span><span class="o">=</span><span class="n">zero_logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h3 id="scprint.model.decoders.GraphSDEExprDecoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">GraphSDEExprDecoder</span></code>

</h3>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>Initialize the ExprNeuralSDEDecoder module.</p>
<p>Parameters:
d_model (int): The dimension of the model.
drift (nn.Module): The drift component of the SDE.
diffusion (nn.Module): The diffusion component of the SDE.</p>

                <details class="quote">
                  <summary>Source code in <code>scprint/model/decoders.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">drift</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">diffusion</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initialize the ExprNeuralSDEDecoder module.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    d_model (int): The dimension of the model.</span>
<span class="sd">    drift (nn.Module): The drift component of the SDE.</span>
<span class="sd">    diffusion (nn.Module): The diffusion component of the SDE.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drift</span> <span class="o">=</span> <span class="n">drift</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">diffusion</span> <span class="o">=</span> <span class="n">diffusion</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>

<div class="doc doc-object doc-class">




<h3 id="scprint.model.decoders.MVCDecoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">MVCDecoder</span></code>

</h3>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>MVCDecoder Decoder for the masked value prediction for cell embeddings.</p>
<p>Will use the gene embeddings with the cell embeddings to predict the mean, variance and zero logits</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
              –
              <div class="doc-md-description">
                <p>obj:<code>int</code>): dimension of the gene embedding.</p>
              </div>
            </li>
            <li>
              <b><code>arch_style</code></b>
              –
              <div class="doc-md-description">
                <p>obj:<code>str</code>): architecture style of the decoder, choice from
1. "inner product" or 2. "cell product" 3. "concat query" or 4. "sum query".</p>
              </div>
            </li>
            <li>
              <b><code>query_activation</code></b>
              –
              <div class="doc-md-description">
                <p>obj:<code>nn.Module</code>): activation function for the query
vectors. Defaults to nn.Sigmoid.</p>
              </div>
            </li>
            <li>
              <b><code>hidden_activation</code></b>
              –
              <div class="doc-md-description">
                <p>obj:<code>nn.Module</code>): activation function for the hidden
layers. Defaults to nn.PReLU.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                <details class="quote">
                  <summary>Source code in <code>scprint/model/decoders.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">arch_style</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;inner product&quot;</span><span class="p">,</span>
    <span class="n">query_activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">,</span>
    <span class="n">hidden_activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MVCDecoder Decoder for the masked value prediction for cell embeddings.</span>

<span class="sd">    Will use the gene embeddings with the cell embeddings to predict the mean, variance and zero logits</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (:obj:`int`): dimension of the gene embedding.</span>
<span class="sd">        arch_style (:obj:`str`): architecture style of the decoder, choice from</span>
<span class="sd">            1. &quot;inner product&quot; or 2. &quot;cell product&quot; 3. &quot;concat query&quot; or 4. &quot;sum query&quot;.</span>
<span class="sd">        query_activation (:obj:`nn.Module`): activation function for the query</span>
<span class="sd">            vectors. Defaults to nn.Sigmoid.</span>
<span class="sd">        hidden_activation (:obj:`nn.Module`): activation function for the hidden</span>
<span class="sd">            layers. Defaults to nn.PReLU.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MVCDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;inner product&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span> <span class="o">=</span> <span class="n">query_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;concat query&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span> <span class="o">=</span> <span class="n">query_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span> <span class="o">+</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span> <span class="o">=</span> <span class="n">hidden_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;sum query&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span> <span class="o">=</span> <span class="n">query_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span> <span class="o">=</span> <span class="n">hidden_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown arch_style: </span><span class="si">{</span><span class="n">arch_style</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">arch_style</span> <span class="o">=</span> <span class="n">arch_style</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_detach</span> <span class="o">=</span> <span class="n">arch_style</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;detach&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h4 id="scprint.model.decoders.MVCDecoder.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


  <div class="doc doc-contents ">
  



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>cell_emb</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape (batch, embsize=d_model)</p>
              </div>
            </li>
            <li>
              <b><code>gene_embs</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape (batch, seq_len, embsize=d_model)</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>scprint/model/decoders.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">cell_emb</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">gene_embs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        cell_emb: Tensor, shape (batch, embsize=d_model)</span>
<span class="sd">        gene_embs: Tensor, shape (batch, seq_len, embsize=d_model)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;inner product&quot;</span><span class="p">:</span>
        <span class="n">query_vecs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span><span class="p">(</span><span class="n">gene_embs</span><span class="p">))</span>
        <span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span><span class="p">(</span><span class="n">query_vecs</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">cell_emb</span> <span class="o">=</span> <span class="n">cell_emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">cell_emb</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">cell_emb</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">zero_logits</span><span class="p">,</span> <span class="n">cell_emb</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># zero logits need to based on the cell_emb, because of input exprs</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;concat query&quot;</span><span class="p">:</span>
        <span class="n">query_vecs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span><span class="p">(</span><span class="n">gene_embs</span><span class="p">))</span>
        <span class="c1"># expand cell_emb to (batch, seq_len, embsize)</span>
        <span class="n">cell_emb</span> <span class="o">=</span> <span class="n">cell_emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">gene_embs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cell_emb</span><span class="p">,</span> <span class="n">query_vecs</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;sum query&quot;</span><span class="p">:</span>
        <span class="n">query_vecs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span><span class="p">(</span><span class="n">gene_embs</span><span class="p">))</span>
        <span class="n">cell_emb</span> <span class="o">=</span> <span class="n">cell_emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">cell_emb</span> <span class="o">+</span> <span class="n">query_vecs</span><span class="p">))</span>
        <span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">mvc_mean</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">mvc_disp</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">15</span><span class="p">)),</span>
        <span class="n">mvc_zero_logits</span><span class="o">=</span><span class="n">zero_logits</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">




<h2 id="scprint.model.flash_attn.flashformer" class="doc doc-heading">
          <code>scprint.model.flash_attn.flashformer</code>


</h2>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">




<h3 id="scprint.model.flash_attn.flashformer.FlashTransformerEncoder" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">FlashTransformerEncoder</span></code>

</h3>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="torch.nn.Module">Module</span></code></p>

  
      <p>FlashTransformerEncoder a transformer encoder with flash attention.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the input vectors.</p>
              </div>
            </li>
            <li>
              <b><code>nhead</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The number of attention heads.</p>
              </div>
            </li>
            <li>
              <b><code>nlayers</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The number of layers in the transformer.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the output of the positional encoding. Defaults to 0.1.</p>
              </div>
            </li>
            <li>
              <b><code>residual_in_fp32</code></b>
                  (<code>bool</code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to force the residual to be in fp32 format. Defaults to True.</p>
              </div>
            </li>
            <li>
              <b><code>num_heads_kv</code></b>
                  (<code>_type_</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The number of heads for key/value. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>checkpointing</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use gradient checkpointing. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>fused_dropout_add_ln</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to fuse dropout, addition and layer normalization operations. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>return_residual</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to return the residual. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>prenorm</code></b>
                  (<code>bool</code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use pre-normalization. Defaults to True.</p>
              </div>
            </li>
            <li>
              <b><code>mlp_ratio</code></b>
                  (<code>float</code>, default:
                      <code>4.0</code>
)
              –
              <div class="doc-md-description">
                <p>The ratio for MLP. Defaults to 4.0.</p>
              </div>
            </li>
            <li>
              <b><code>fused_mlp</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use fused MLP. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>fused_bias_fc</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to fuse bias and fully connected layers. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>sequence_parallel</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use sequence parallelism. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>drop_path_rate</code></b>
                  (<code>float</code>, default:
                      <code>0.0</code>
)
              –
              <div class="doc-md-description">
                <p>The drop path rate. Defaults to 0.0.</p>
              </div>
            </li>
            <li>
              <b><code>weight_init</code></b>
                  (<code>str</code>, default:
                      <code>&#39;&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>The weight initialization method. Defaults to "".</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>ImportError</code>
              –
              <div class="doc-md-description">
                <p>Raised when Triton is not installed but fused_dropout_add_ln is set to True.</p>
              </div>
            </li>
            <li>
                  <code>NotImplementedError</code>
              –
              <div class="doc-md-description">
                <p>Raised when an unsupported operation is attempted.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                <details class="quote">
                  <summary>Source code in <code>scprint/model/flash_attn/flashformer.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">nhead</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">nlayers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">residual_in_fp32</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">num_heads_kv</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">checkpointing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fused_dropout_add_ln</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">return_residual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">prenorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">4.0</span><span class="p">,</span>
    <span class="n">fused_mlp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fused_bias_fc</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sequence_parallel</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">drop_path_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">use_flash_attn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_init</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    FlashTransformerEncoder a transformer encoder with flash attention.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the input vectors.</span>
<span class="sd">        nhead (int): The number of attention heads.</span>
<span class="sd">        nlayers (int): The number of layers in the transformer.</span>
<span class="sd">        dropout (float, optional): The dropout rate to apply to the output of the positional encoding. Defaults to 0.1.</span>
<span class="sd">        residual_in_fp32 (bool, optional): Whether to force the residual to be in fp32 format. Defaults to True.</span>
<span class="sd">        num_heads_kv (_type_, optional): The number of heads for key/value. Defaults to None.</span>
<span class="sd">        checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.</span>
<span class="sd">        fused_dropout_add_ln (bool, optional): Whether to fuse dropout, addition and layer normalization operations. Defaults to False.</span>
<span class="sd">        return_residual (bool, optional): Whether to return the residual. Defaults to False.</span>
<span class="sd">        prenorm (bool, optional): Whether to use pre-normalization. Defaults to True.</span>
<span class="sd">        mlp_ratio (float, optional): The ratio for MLP. Defaults to 4.0.</span>
<span class="sd">        fused_mlp (bool, optional): Whether to use fused MLP. Defaults to False.</span>
<span class="sd">        fused_bias_fc (bool, optional): Whether to fuse bias and fully connected layers. Defaults to False.</span>
<span class="sd">        sequence_parallel (bool, optional): Whether to use sequence parallelism. Defaults to False.</span>
<span class="sd">        drop_path_rate (float, optional): The drop path rate. Defaults to 0.0.</span>
<span class="sd">        weight_init (str, optional): The weight initialization method. Defaults to &quot;&quot;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ImportError: Raised when Triton is not installed but fused_dropout_add_ln is set to True.</span>
<span class="sd">        NotImplementedError: Raised when an unsupported operation is attempted.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">FlashTransformerEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="n">dpr</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="p">,</span> <span class="n">nlayers</span><span class="p">)</span>
    <span class="p">]</span>  <span class="c1"># stochastic depth decay rule</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nlayers</span><span class="p">):</span>
        <span class="n">mlp</span> <span class="o">=</span> <span class="n">create_mlp_cls</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span> <span class="n">fused_mlp</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
            <span class="n">MHA</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">nhead</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">use_flash_attn</span><span class="o">=</span><span class="n">use_flash_attn</span><span class="p">,</span>
            <span class="n">num_heads_kv</span><span class="o">=</span><span class="n">num_heads_kv</span><span class="p">,</span>
            <span class="n">checkpointing</span><span class="o">=</span><span class="n">checkpointing</span><span class="p">,</span>
            <span class="n">fused_bias_fc</span><span class="o">=</span><span class="n">fused_bias_fc</span><span class="p">,</span>
            <span class="n">layer_idx</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># or use parallelBlock where attn &amp; MLP are done in parallel</span>
        <span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="n">attention</span><span class="p">,</span>
            <span class="n">mlp</span><span class="p">,</span>
            <span class="n">prenorm</span><span class="o">=</span><span class="n">prenorm</span><span class="p">,</span>
            <span class="c1"># need to set it here for now although it hinders some performances as it returns the residual and I need to see what to do with it</span>
            <span class="c1"># TD [2022-07-30]: Force residual in fp32, seems to make fp16 training more stable</span>
            <span class="n">residual_in_fp32</span><span class="o">=</span><span class="n">residual_in_fp32</span><span class="p">,</span>
            <span class="n">sequence_parallel</span><span class="o">=</span><span class="n">sequence_parallel</span><span class="p">,</span>  <span class="c1"># for more parallelism</span>
            <span class="n">resid_dropout1</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">resid_dropout2</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">drop_path1</span><span class="o">=</span><span class="n">dpr</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
            <span class="n">drop_path2</span><span class="o">=</span><span class="n">dpr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">fused_dropout_add_ln</span><span class="o">=</span><span class="n">fused_dropout_add_ln</span><span class="p">,</span>
            <span class="n">return_residual</span><span class="o">=</span><span class="n">return_residual</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">encoder_layers</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span> <span class="o">=</span> <span class="n">StochasticDepth</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dpr</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;row&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">fused_dropout_add_ln</span> <span class="o">=</span> <span class="n">fused_dropout_add_ln</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fused_dropout_add_ln</span> <span class="ow">and</span> <span class="n">layer_norm_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Triton is not installed&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sequence_parallel</span><span class="p">:</span>
        <span class="c1"># This seems to only be important when doing tensor parallelism across GPUs, to increase even more the context length I guess?</span>
        <span class="c1"># not really necessary here I think</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;sequence_parallel not implemented yet&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">(</span><span class="n">weight_init</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">











  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">




<h2 id="scprint.model.model" class="doc doc-heading">
          <code>scprint.model.model</code>


</h2>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">




<h3 id="scprint.model.model.scPrint" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">scPrint</span></code>

</h3>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="lightning.LightningModule">LightningModule</span></code></p>

  
      <p>scPrint transformer for single cell biology and the inference of Gene Regulatory networks</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>genes</code></b>
                  (<code>list</code>)
              –
              <div class="doc-md-description">
                <p>the genenames with which the model will work</p>
              </div>
            </li>
            <li>
              <b><code>precpt_gene_emb</code></b>
                  (<code><span title="np.array">array</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The gene embeddings. should be of size len(genes), d_model.
it should be in the same order as the genes. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>gene_pos_enc</code></b>
                  (<code>list</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The gene position encoding. Should be of the same size as genes.
for each gene in genes, gives it a location value. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>d_model</code></b>
                  (<code>int</code>, default:
                      <code>512</code>
)
              –
              <div class="doc-md-description">
                <p>The dimension of the model. Defaults to 512.</p>
              </div>
            </li>
            <li>
              <b><code>nhead</code></b>
                  (<code>int</code>, default:
                      <code>8</code>
)
              –
              <div class="doc-md-description">
                <p>The number of heads in the multiheadattention models. Defaults to 8.</p>
              </div>
            </li>
            <li>
              <b><code>d_hid</code></b>
                  (<code>int</code>, default:
                      <code>512</code>
)
              –
              <div class="doc-md-description">
                <p>The dimension of the feedforward network model. Defaults to 512.</p>
              </div>
            </li>
            <li>
              <b><code>nlayers</code></b>
                  (<code>int</code>, default:
                      <code>6</code>
)
              –
              <div class="doc-md-description">
                <p>The number of layers in the transformer model. Defaults to 6.</p>
              </div>
            </li>
            <li>
              <b><code>nlayers_cls</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The number of layers in the classifier. Defaults to 3.</p>
              </div>
            </li>
            <li>
              <b><code>labels</code></b>
                  (<code>dict</code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>The classes to predict with number of labels for each. Defaults to {}.</p>
              </div>
            </li>
            <li>
              <b><code>cls_hierarchy</code></b>
                  (<code>dict</code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>The class hierarchy for classes that have hierarchical labels. Defaults to {}.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>, default:
                      <code>0.2</code>
)
              –
              <div class="doc-md-description">
                <p>The dropout value. Defaults to 0.5.</p>
              </div>
            </li>
            <li>
              <b><code>transformer</code></b>
                  (<code>str</code>, default:
                      <code>&#39;fast&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>(flag, optional) the transformer type to use. one of "linear", "flash", "flashsparse", "scprint". Defaults to "flash".</p>
              </div>
            </li>
            <li>
              <b><code>domain_spec_batchnorm</code></b>
                  (<code>str</code>, default:
                      <code>&#39;None&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to apply domain specific batch normalization. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>expr_emb_style</code></b>
                  (<code>str</code>, default:
                      <code>&#39;continuous&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>The style of input embedding (one of "continuous_concat", "binned_pos", "full_pos"). Defaults to "continuous_concat".</p>
              </div>
            </li>
            <li>
              <b><code>mvc_decoder</code></b>
                  (<code>str</code>, default:
                      <code>&#39;None&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>The style of MVC decoder one of "None", "inner product", "concat query", "sum query". Defaults to "inner product".</p>
              </div>
            </li>
            <li>
              <b><code>pred_embedding</code></b>
                  (<code>list</code>, default:
                      <code>[]</code>
)
              –
              <div class="doc-md-description">
                <p>The list of labels to use for plotting embeddings. Defaults to [].</p>
              </div>
            </li>
            <li>
              <b><code>cell_emb_style</code></b>
                  (<code>str</code>, default:
                      <code>&#39;cls&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>The style of cell embedding. one of "cls", "avg-pool", "w-pool". Defaults to "cls".</p>
              </div>
            </li>
            <li>
              <b><code>lr</code></b>
                  (<code>float</code>, default:
                      <code>0.001</code>
)
              –
              <div class="doc-md-description">
                <p>The learning rate. Defaults to 0.001.</p>
              </div>
            </li>
            <li>
              <b><code>label_decoders</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Dict">Dict</span>[int, str]]]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>(dict, optional) the label decoders to use for plotting the umap during validations. Defaults to None.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>ValueError</code>
              –
              <div class="doc-md-description">
                <p>If the expr_emb_style is not one of "category", "continuous", "none".</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                <details class="quote">
                  <summary>Source code in <code>scprint/model/model.py</code></summary>
                  <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">genes</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
    <span class="n">precpt_gene_emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">gene_pos_enc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">nhead</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">d_hid</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">edge_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>
    <span class="n">nlayers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">layers_cls</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">cls_hierarchy</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">transformer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;fast&quot;</span><span class="p">,</span>
    <span class="n">expr_emb_style</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;continuous&quot;</span><span class="p">,</span>  <span class="c1"># &quot;binned_pos&quot;, &quot;cont_pos&quot;</span>
    <span class="n">domain_spec_batchnorm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
    <span class="n">n_input_bins</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">mvc_decoder</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
    <span class="n">pred_embedding</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">cell_emb_style</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cls&quot;</span><span class="p">,</span>
    <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
    <span class="n">label_decoders</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">flash_attention_kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    scPrint transformer for single cell biology and the inference of Gene Regulatory networks</span>

<span class="sd">    Args:</span>
<span class="sd">        genes (list): the genenames with which the model will work</span>
<span class="sd">        precpt_gene_emb (np.array, optional): The gene embeddings. should be of size len(genes), d_model.</span>
<span class="sd">            it should be in the same order as the genes. Defaults to None.</span>
<span class="sd">        gene_pos_enc (list, optional): The gene position encoding. Should be of the same size as genes.</span>
<span class="sd">            for each gene in genes, gives it a location value. Defaults to None.</span>
<span class="sd">        d_model (int, optional): The dimension of the model. Defaults to 512.</span>
<span class="sd">        nhead (int, optional): The number of heads in the multiheadattention models. Defaults to 8.</span>
<span class="sd">        d_hid (int, optional): The dimension of the feedforward network model. Defaults to 512.</span>
<span class="sd">        nlayers (int, optional): The number of layers in the transformer model. Defaults to 6.</span>
<span class="sd">        nlayers_cls (int, optional): The number of layers in the classifier. Defaults to 3.</span>
<span class="sd">        labels (dict, optional): The classes to predict with number of labels for each. Defaults to {}.</span>
<span class="sd">        cls_hierarchy (dict, optional): The class hierarchy for classes that have hierarchical labels. Defaults to {}.</span>
<span class="sd">        dropout (float, optional): The dropout value. Defaults to 0.5.</span>
<span class="sd">        transformer: (flag, optional) the transformer type to use. one of &quot;linear&quot;, &quot;flash&quot;, &quot;flashsparse&quot;, &quot;scprint&quot;. Defaults to &quot;flash&quot;.</span>
<span class="sd">        domain_spec_batchnorm (str, optional): Whether to apply domain specific batch normalization. Defaults to False.</span>
<span class="sd">        expr_emb_style (str, optional): The style of input embedding (one of &quot;continuous_concat&quot;, &quot;binned_pos&quot;, &quot;full_pos&quot;). Defaults to &quot;continuous_concat&quot;.</span>
<span class="sd">        mvc_decoder (str, optional): The style of MVC decoder one of &quot;None&quot;, &quot;inner product&quot;, &quot;concat query&quot;, &quot;sum query&quot;. Defaults to &quot;inner product&quot;.</span>
<span class="sd">        pred_embedding (list, optional): The list of labels to use for plotting embeddings. Defaults to [].</span>
<span class="sd">        cell_emb_style (str, optional): The style of cell embedding. one of &quot;cls&quot;, &quot;avg-pool&quot;, &quot;w-pool&quot;. Defaults to &quot;cls&quot;.</span>
<span class="sd">        lr (float, optional): The learning rate. Defaults to 0.001.</span>
<span class="sd">        label_decoders: (dict, optional) the label decoders to use for plotting the umap during validations. Defaults to None.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the expr_emb_style is not one of &quot;category&quot;, &quot;continuous&quot;, &quot;none&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="c1"># default</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_denoise</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_cce</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cce_sim</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_ecs</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ecs_threshold</span> <span class="o">=</span> <span class="mf">0.3</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ecs_scale</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_mvc</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_adv_cls</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_next_tp</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_generate</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">class_scale</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mask_ratio</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.15</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">warmup_duration</span> <span class="o">=</span> <span class="mi">500</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fused_adam</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr_patience</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">get_attention_layer</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embs</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># should be stored somehow</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">edge_dim</span> <span class="o">=</span> <span class="n">edge_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nlayers</span> <span class="o">=</span> <span class="n">nlayers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gene_pos_enc</span> <span class="o">=</span> <span class="n">gene_pos_enc</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mvc_decoder</span> <span class="o">=</span> <span class="n">mvc_decoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">domain_spec_batchnorm</span> <span class="o">=</span> <span class="n">domain_spec_batchnorm</span>
    <span class="c1"># need to store</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_input_bins</span> <span class="o">=</span> <span class="n">n_input_bins</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">transformer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">labels_counts</span> <span class="o">=</span> <span class="n">labels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cell_emb_style</span> <span class="o">=</span> <span class="n">cell_emb_style</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">label_decoders</span> <span class="o">=</span> <span class="n">label_decoders</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred_embedding</span> <span class="o">=</span> <span class="n">pred_embedding</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="c1"># compute tensor for mat_cls_hierarchy</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mat_cls_hierarchy</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cls_hierarchy</span> <span class="o">=</span> <span class="n">cls_hierarchy</span>

    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">cls_hierarchy</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">tens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">labels</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
        <span class="k">for</span> <span class="n">k2</span><span class="p">,</span> <span class="n">v2</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">tens</span><span class="p">[</span><span class="n">k2</span> <span class="o">-</span> <span class="n">labels</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">v2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mat_cls_hierarchy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">tens</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">expr_emb_style</span> <span class="o">=</span> <span class="n">expr_emb_style</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">expr_emb_style</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;category&quot;</span><span class="p">,</span> <span class="s2">&quot;continuous&quot;</span><span class="p">,</span> <span class="s2">&quot;none&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;expr_emb_style should be one of category, continuous, scaling, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;got </span><span class="si">{</span><span class="n">expr_emb_style</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">cell_emb_style</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;cls&quot;</span><span class="p">,</span> <span class="s2">&quot;avg-pool&quot;</span><span class="p">,</span> <span class="s2">&quot;w-pool&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown cell_emb_style: </span><span class="si">{</span><span class="n">cell_emb_style</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">genes</span> <span class="o">=</span> <span class="n">genes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">n</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">genes</span><span class="p">)}</span>

    <span class="c1"># encoder</span>
    <span class="c1"># gene encoder</span>
    <span class="k">if</span> <span class="n">precpt_gene_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">precpt_gene_emb</span><span class="p">)</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;the gene embeddings file </span><span class="si">{</span><span class="n">precpt_gene_emb</span><span class="si">}</span><span class="s2"> does not contain any of the genes given to the model&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="s2">&quot;Warning: only a subset of the genes available in the embeddings file.&quot;</span>
            <span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;number of genes: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">))</span>
        <span class="n">sembeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="n">d_model</span><span class="p">)(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gene_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">GeneEncoder</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">sembeddings</span><span class="p">,</span> <span class="n">freeze</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">GeneEncoder</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="c1"># Value Encoder, NOTE: the scaling style is also handled in _encode method</span>
    <span class="k">if</span> <span class="n">expr_emb_style</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;continuous&quot;</span><span class="p">,</span> <span class="s2">&quot;full_pos&quot;</span><span class="p">]:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">ContinuousValueEncoder</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">expr_emb_style</span> <span class="o">==</span> <span class="s2">&quot;binned_pos&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">n_input_bins</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">CategoryValueEncoder</span><span class="p">(</span><span class="n">n_input_bins</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>

    <span class="c1"># Positional Encoding</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gene_pos_enc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">gene_pos_enc</span><span class="p">)</span>
        <span class="n">token_to_pos</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">pos</span> <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene_pos_enc</span><span class="p">)}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">PositionalEncoding</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="n">max_len</span><span class="p">,</span> <span class="n">token_to_pos</span><span class="o">=</span><span class="n">token_to_pos</span>
        <span class="p">)</span>

    <span class="c1"># Batch Encoder</span>
    <span class="c1"># always have [base_cell_emb, time_embedding, depth_embedding] + any other class info</span>
    <span class="c1"># base cell embedding will store other cell specific information</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">label_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">CategoryValueEncoder</span><span class="p">(</span>
        <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">d_model</span>
    <span class="p">)</span>
    <span class="c1"># self.time_encoder = encoders.ContinuousValueEncoder(d_model, dropout)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">depth_decoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">ContinuousValueEncoder</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>

    <span class="c1"># Model</span>
    <span class="c1"># Batch Norm</span>
    <span class="k">if</span> <span class="n">domain_spec_batchnorm</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">or</span> <span class="n">domain_spec_batchnorm</span> <span class="o">==</span> <span class="s2">&quot;dsbn&quot;</span><span class="p">:</span>
        <span class="n">use_affine</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">domain_spec_batchnorm</span> <span class="o">==</span> <span class="s2">&quot;do_affine&quot;</span> <span class="k">else</span> <span class="kc">False</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Use domain specific batchnorm with affine=</span><span class="si">{</span><span class="n">use_affine</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dsbn</span> <span class="o">=</span> <span class="n">DomainSpecificBatchNorm1d</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">6.1e-5</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="n">use_affine</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">domain_spec_batchnorm</span> <span class="o">==</span> <span class="s2">&quot;batchnorm&quot;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Using simple batchnorm instead of domain specific batchnorm&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">6.1e-5</span><span class="p">)</span>

    <span class="c1"># Transformer</span>
    <span class="c1"># Linear</span>
    <span class="k">if</span> <span class="n">transformer</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
        <span class="c1"># linear transformer using the fast transformer package</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">FastTransformerEncoder</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="p">,</span> <span class="n">d_hid</span><span class="p">,</span> <span class="n">nlayers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="s2">&quot;linear&quot;</span>
        <span class="p">)</span>
    <span class="c1"># flashsparse</span>
    <span class="k">elif</span> <span class="n">transformer</span> <span class="o">==</span> <span class="s2">&quot;flashsparse&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">Hashformer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Hashformer transformer requires cuda kernels&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">Hashformer</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="n">nlayers</span><span class="p">,</span>
            <span class="mi">2</span><span class="p">,</span>
            <span class="n">nhead</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="c1"># flash EGT</span>
    <span class="c1"># We found that the results can be further improved by freezing the</span>
    <span class="c1"># node channel layers and training the edge channel layers for a</span>
    <span class="c1"># few additional epochs.</span>
    <span class="c1"># However, its effect on transfer learning has not yet been studied.</span>
    <span class="c1"># That is why we include checkpoints for both tuned and untuned models.</span>
    <span class="c1"># https://github.com/shamim-hussain/egt/blob/master/README.md</span>
    <span class="c1"># https://github.com/shamim-hussain/egt_pytorch</span>
    <span class="k">elif</span> <span class="n">transformer</span> <span class="o">==</span> <span class="s2">&quot;scprint&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">EGT</span><span class="p">(</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="n">nlayers</span><span class="p">,</span>
            <span class="n">feat_size</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span>
            <span class="n">edge_feat_size</span><span class="o">=</span><span class="n">edge_dim</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">nhead</span><span class="p">,</span>
            <span class="n">num_virtual_nodes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="c1"># regular or flash</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">transformer</span> <span class="o">==</span> <span class="s2">&quot;flash&quot;</span> <span class="ow">and</span> <span class="n">FlashTransformerEncoder</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;flash transformer requires flash package&quot;</span><span class="p">)</span>
            <span class="c1"># NOT flash transformer using the special tritton kernel</span>
            <span class="c1"># or parallelMHA (add the process group thing and faster)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">FlashTransformerEncoder</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="n">nhead</span><span class="p">,</span>
            <span class="n">nlayers</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">use_flash_attn</span><span class="o">=</span><span class="p">(</span><span class="n">transformer</span> <span class="o">==</span> <span class="s2">&quot;flash&quot;</span><span class="p">),</span>
            <span class="o">**</span><span class="n">flash_attention_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># decoders</span>
    <span class="c1"># expression</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">expr_decoder</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">ExprDecoder</span><span class="p">(</span>
        <span class="n">d_model</span><span class="p">,</span>
        <span class="n">nfirst_labels_to_skip</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># cls decoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cls_decoders</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
    <span class="c1"># should be a very simple classifier for most things</span>
    <span class="c1"># (maybe scale with the number of classes) should be 1 layer...</span>
    <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">n_cls</span> <span class="ow">in</span> <span class="n">labels</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cls_decoders</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">ClsDecoder</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="n">n_cls</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">layers_cls</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span>
        <span class="p">)</span>

    <span class="c1"># expression decoder from batch embbedding</span>
    <span class="k">if</span> <span class="n">mvc_decoder</span> <span class="o">!=</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mvc_decoder</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">MVCDecoder</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="n">arch_style</span><span class="o">=</span><span class="n">mvc_decoder</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mvc_decoder</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span>
            <span class="n">utils</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">,</span>
            <span class="n">n_layer</span><span class="o">=</span><span class="n">nlayers</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h4 id="scprint.model.model.scPrint.configure_optimizers" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">configure_optimizers</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>@see pl.LightningModule</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="c1"># https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam</span>
    <span class="c1"># optimizer = optim.Adam(</span>
    <span class="c1">#    self.parameters(),</span>
    <span class="c1">#    lr=self.hparams.lr,</span>
    <span class="c1">#    betas=(0.9, 0.999),</span>
    <span class="c1">#    eps=1e-08,</span>
    <span class="c1">#    weight_decay=0,</span>
    <span class="c1">#    amsgrad=False,</span>
    <span class="c1">#    fused=False,</span>
    <span class="c1"># )</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span>
        <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
        <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
        <span class="n">amsgrad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">fused</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fused_adam</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_patience</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.5</span>
    <span class="p">)</span>
    <span class="n">lr_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
        <span class="c1"># The unit of the scheduler&#39;s step size, could also be &#39;step&#39;.</span>
        <span class="c1"># &#39;epoch&#39; updates the scheduler on epoch end whereas &#39;step&#39;</span>
        <span class="c1"># updates it after a optimizer update.</span>
        <span class="s2">&quot;interval&quot;</span><span class="p">:</span> <span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
        <span class="c1"># How many epochs/steps should pass between calls to</span>
        <span class="c1"># `scheduler.step()`. 1 corresponds to updating the learning</span>
        <span class="c1"># rate after every epoch/step.</span>
        <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="c1"># Metric to to monitor for schedulers like `ReduceLROnPlateau`</span>
        <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="s2">&quot;val_loss&quot;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">val_dataloaders</span> <span class="k">else</span> <span class="s2">&quot;train_loss&quot;</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">callbacks</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="n">_LRCallback</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">=</span> <span class="n">val</span><span class="o">.</span><span class="n">num_training</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="n">LearningRateFinder</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">=</span> <span class="n">val</span><span class="o">.</span><span class="n">_num_training_steps</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">optimizer</span><span class="p">],</span> <span class="p">[</span><span class="n">lr_dict</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h4 id="scprint.model.model.scPrint.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>forward also called on self(), a full forward pass on the model</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>gene_pos</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>A tensor of shape (minibatch, seq_len)
representing the genes used for each cell in the minibatch.</p>
              </div>
            </li>
            <li>
              <b><code>expression</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>A tensor of shape (minibatch, seq_len)
representing the expression levels of genes in the minibatch. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>mask</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>A tensor of shape (minibatch, seq_len)
used to mask certain elements in the sequence during the forward pass. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>full_depth</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>A tensor of shape (minibatch,)
representing the full depth of each sequence in the minibatch. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>timepoint</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>A tensor of shape (minibatch,)
representing the timepoint associated with each sequence in the minibatch. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>get_gene_emb</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>A flag indicating whether to return the gene embeddings.
If True, the gene embeddings are included in the output. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>do_sample</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>A flag indicating whether to sample the expression levels.
If True, the expression levels are sampled during the forward pass. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>get_attention_layer</code></b>
                  (<code>list</code>, default:
                      <code>[]</code>
)
              –
              <div class="doc-md-description">
                <p>A list indicating which attention layers to return.
If not empty, the specified attention layers are included in the output. Defaults to [].</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
            –
            <div class="doc-md-description">
              <p>dict of output Tensors: A dictionary containing the output tensors from the forward pass.
The keys of the dictionary depend on the input flags (get_gene_emb, do_sample, get_attention_layer).
at minima, the dictionary contains the following:
- "mean": the mean expression levels
- "zero_logits": the logits for zero-inflated expression levels
- "disp": the dispersion parameter
- "cell_embs": the cell embeddings per class
- "cell_emb": the main cell embedding
- "cls_output": the output of the classifier</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>scprint/model/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">gene_pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">depth_mult</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">expression</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="c1"># (minibatch,) unormalized total counts</span>
    <span class="n">full_depth</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">timepoint</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># (new_minibatch_of_nxt_cells,)</span>
    <span class="n">get_gene_emb</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">get_attention_layer</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[],</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    forward also called on self(), a full forward pass on the model</span>

<span class="sd">    Args:</span>
<span class="sd">        gene_pos (Tensor): A tensor of shape (minibatch, seq_len)</span>
<span class="sd">            representing the genes used for each cell in the minibatch.</span>
<span class="sd">        expression (Tensor, optional): A tensor of shape (minibatch, seq_len)</span>
<span class="sd">            representing the expression levels of genes in the minibatch. Defaults to None.</span>
<span class="sd">        mask (Tensor, optional): A tensor of shape (minibatch, seq_len)</span>
<span class="sd">            used to mask certain elements in the sequence during the forward pass. Defaults to None.</span>
<span class="sd">        full_depth (Tensor, optional): A tensor of shape (minibatch,)</span>
<span class="sd">            representing the full depth of each sequence in the minibatch. Defaults to None.</span>
<span class="sd">        timepoint (Tensor, optional): A tensor of shape (minibatch,)</span>
<span class="sd">            representing the timepoint associated with each sequence in the minibatch. Defaults to None.</span>
<span class="sd">        get_gene_emb (bool, optional): A flag indicating whether to return the gene embeddings.</span>
<span class="sd">            If True, the gene embeddings are included in the output. Defaults to False.</span>
<span class="sd">        do_sample (bool, optional): A flag indicating whether to sample the expression levels.</span>
<span class="sd">            If True, the expression levels are sampled during the forward pass. Defaults to False.</span>
<span class="sd">        get_attention_layer (list, optional): A list indicating which attention layers to return.</span>
<span class="sd">            If not empty, the specified attention layers are included in the output. Defaults to [].</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict of output Tensors: A dictionary containing the output tensors from the forward pass.</span>
<span class="sd">            The keys of the dictionary depend on the input flags (get_gene_emb, do_sample, get_attention_layer).</span>
<span class="sd">            at minima, the dictionary contains the following:</span>
<span class="sd">            - &quot;mean&quot;: the mean expression levels</span>
<span class="sd">            - &quot;zero_logits&quot;: the logits for zero-inflated expression levels</span>
<span class="sd">            - &quot;disp&quot;: the dispersion parameter</span>
<span class="sd">            - &quot;cell_embs&quot;: the cell embeddings per class</span>
<span class="sd">            - &quot;cell_emb&quot;: the main cell embedding</span>
<span class="sd">            - &quot;cls_output&quot;: the output of the classifier</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span><span class="p">(</span><span class="n">gene_pos</span><span class="p">,</span> <span class="n">expression</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">full_depth</span><span class="p">,</span> <span class="n">timepoint</span><span class="p">)</span>
    <span class="n">transformer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span><span class="n">encoding</span><span class="p">,</span> <span class="n">return_qkv</span><span class="o">=</span><span class="n">get_attention_layer</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">get_attention_layer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">transformer_output</span><span class="p">,</span> <span class="n">qkvs</span> <span class="o">=</span> <span class="n">transformer_output</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_decoder</span><span class="p">(</span><span class="n">transformer_output</span><span class="p">,</span> <span class="n">depth_mult</span><span class="p">,</span> <span class="n">get_gene_emb</span><span class="p">,</span> <span class="n">do_sample</span><span class="p">),</span>
            <span class="n">qkvs</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decoder</span><span class="p">(</span>
            <span class="n">transformer_output</span><span class="p">,</span> <span class="n">depth_mult</span><span class="p">,</span> <span class="n">get_gene_emb</span><span class="p">,</span> <span class="n">do_sample</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h4 id="scprint.model.model.scPrint.get_cell_embs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">get_cell_embs</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>get_cell_embs</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>layer_output</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The output tensor from a layer in the model.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>ValueError</code>
              –
              <div class="doc-md-description">
                <p>Raised when an unknown cell embedding style is encountered.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
<b><code>Tensor</code></b>            –
            <div class="doc-md-description">
              <p>The cell embeddings tensor.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>scprint/model/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span>
<span class="normal">1085</span>
<span class="normal">1086</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_cell_embs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_output</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    get_cell_embs</span>

<span class="sd">    Args:</span>
<span class="sd">        layer_output (Tensor): The output tensor from a layer in the model.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: Raised when an unknown cell embedding style is encountered.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: The cell embeddings tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_emb_style</span> <span class="o">==</span> <span class="s2">&quot;cls&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># (minibatch, embsize)</span>
        <span class="n">cell_emb</span> <span class="o">=</span> <span class="n">layer_output</span><span class="p">[:,</span> <span class="p">:</span> <span class="mi">2</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)]</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_emb_style</span> <span class="o">==</span> <span class="s2">&quot;avg-pool&quot;</span><span class="p">:</span>
        <span class="n">cell_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown cell_emb_style: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cell_emb_style</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cell_emb</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h4 id="scprint.model.model.scPrint.log_adata" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">log_adata</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>log_adata will log an adata from predictions.
It will log to tensorboard and wandb if available</p>
<p>see @utils.log_adata</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span>
<span class="normal">1131</span>
<span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span>
<span class="normal">1151</span>
<span class="normal">1152</span>
<span class="normal">1153</span>
<span class="normal">1154</span>
<span class="normal">1155</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_adata</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gtclass</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    log_adata will log an adata from predictions.</span>
<span class="sd">    It will log to tensorboard and wandb if available</span>

<span class="sd">    see @utils.log_adata</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">mdir</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">save_dir</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">save_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;/tmp&quot;</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">mdir</span> <span class="o">=</span> <span class="s2">&quot;/tmp&quot;</span>
    <span class="n">adata</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">make_adata</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embs</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_decoders</span><span class="p">,</span>
        <span class="n">gtclass</span><span class="p">,</span>
        <span class="n">name</span><span class="p">,</span>
        <span class="n">mdir</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_figure</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;couldn&#39;t log to tensorboard&quot;</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">log_image</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s2">&quot;umaps&quot;</span><span class="p">,</span> <span class="n">images</span><span class="o">=</span><span class="p">[</span><span class="n">fig</span><span class="p">])</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;couldn&#39;t log to wandb&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">adata</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h4 id="scprint.model.model.scPrint.on_fit_start" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">on_fit_start</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>@see pl.LightningModule</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">on_fit_start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">)</span> <span class="ow">is</span> <span class="n">FlashTransformerEncoder</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">encoder_layers</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
            <span class="n">encoder_layers</span><span class="o">.</span><span class="n">set_seq_parallel</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mat_cls_hierarchy</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mat_cls_hierarchy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h4 id="scprint.model.model.scPrint.on_predict_epoch_end" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">on_predict_epoch_end</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>@see pl.LightningModule will</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">on_predict_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule will&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">is_global_zero</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;you are not on the main node. cancelling logging step&quot;</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">expr_pred</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">i</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">expr_pred</span>
    <span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mean_attn</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_pred_batch</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_attn</span><span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_adata</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h4 id="scprint.model.model.scPrint.on_predict_epoch_start" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">on_predict_epoch_start</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>@see pl.LightningModule</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">945</span>
<span class="normal">946</span>
<span class="normal">947</span>
<span class="normal">948</span>
<span class="normal">949</span>
<span class="normal">950</span>
<span class="normal">951</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">on_predict_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embs</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_pred_batch</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">)</span> <span class="ow">is</span> <span class="n">FlashTransformerEncoder</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">encoder_layers</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
            <span class="n">encoder_layers</span><span class="o">.</span><span class="n">set_seq_parallel</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h4 id="scprint.model.model.scPrint.on_validation_epoch_end" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">on_validation_epoch_end</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>@see pl.LightningModule</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">912</span>
<span class="normal">913</span>
<span class="normal">914</span>
<span class="normal">915</span>
<span class="normal">916</span>
<span class="normal">917</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">on_validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">is_global_zero</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;you are not on the main node. cancelling logging step&quot;</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_adata</span><span class="p">(</span><span class="n">gtclass</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">info</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h4 id="scprint.model.model.scPrint.optimizer_step" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">optimizer_step</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>@see pl.LightningModule</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">846</span>
<span class="normal">847</span>
<span class="normal">848</span>
<span class="normal">849</span>
<span class="normal">850</span>
<span class="normal">851</span>
<span class="normal">852</span>
<span class="normal">853</span>
<span class="normal">854</span>
<span class="normal">855</span>
<span class="normal">856</span>
<span class="normal">857</span>
<span class="normal">858</span>
<span class="normal">859</span>
<span class="normal">860</span>
<span class="normal">861</span>
<span class="normal">862</span>
<span class="normal">863</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_closure</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="c1"># update params</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

    <span class="c1"># manually warm up lr without a scheduler</span>
    <span class="c1"># making sure that we don&#39;t do this during lrfinder</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_duration</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span>
    <span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span><span class="p">:</span>
        <span class="n">lr_scale</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span>
            <span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_duration</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">pg</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="n">lr_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h4 id="scprint.model.model.scPrint.predict_step" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">predict_step</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>embed given gene expression, encode the gene embedding and cell embedding.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
<b><code>Tensor</code></b>            –
            <div class="doc-md-description">
              <p><em>description</em></p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>scprint/model/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">953</span>
<span class="normal">954</span>
<span class="normal">955</span>
<span class="normal">956</span>
<span class="normal">957</span>
<span class="normal">958</span>
<span class="normal">959</span>
<span class="normal">960</span>
<span class="normal">961</span>
<span class="normal">962</span>
<span class="normal">963</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    embed given gene expression, encode the gene embedding and cell embedding.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch @see training_step</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: _description_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;genes&quot;</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;depth&quot;</span><span class="p">])</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h4 id="scprint.model.model.scPrint.test_step" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">test_step</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>@see pl.LightningModule</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">921</span>
<span class="normal">922</span>
<span class="normal">923</span>
<span class="normal">924</span>
<span class="normal">925</span>
<span class="normal">926</span>
<span class="normal">927</span>
<span class="normal">928</span>
<span class="normal">929</span>
<span class="normal">930</span>
<span class="normal">931</span>
<span class="normal">932</span>
<span class="normal">933</span>
<span class="normal">934</span>
<span class="normal">935</span>
<span class="normal">936</span>
<span class="normal">937</span>
<span class="normal">938</span>
<span class="normal">939</span>
<span class="normal">940</span>
<span class="normal">941</span>
<span class="normal">942</span>
<span class="normal">943</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    @see pl.LightningModule</span>

<span class="sd">    Args:</span>
<span class="sd">        batch @see training_step</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">total_loss</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_training</span><span class="p">(</span>
        <span class="n">batch</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_denoise</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_next_tp</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_cce</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cce_sim</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_ecs</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_mvc</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_adv_cls</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_generate</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask_ratio</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;test_loss: &quot;</span><span class="p">,</span> <span class="n">total_loss</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total_loss</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h4 id="scprint.model.model.scPrint.training_step" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">training_step</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>training_step defines the train loop. It is independent of forward</p>
<p>@see pl.LightningModule</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
<b><code>_type_</code></b>            –
            <div class="doc-md-description">
              <p><em>description</em></p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>scprint/model/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">batch_idx</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    training_step defines the train loop. It is independent of forward</span>

<span class="sd">    @see pl.LightningModule</span>

<span class="sd">    Returns:</span>
<span class="sd">        _type_: _description_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TASK 1 &amp; 2 &amp; 3 (first pass, expression reconstruction, label prediction)</span>
    <span class="n">total_loss</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_training</span><span class="p">(</span>
        <span class="n">batch</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_denoise</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_next_tp</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_cce</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cce_sim</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_ecs</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_mvc</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_adv_cls</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_generate</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask_ratio</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;train_loss&quot;</span><span class="p">,</span> <span class="n">total_loss</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total_loss</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h4 id="scprint.model.model.scPrint.validation_step" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">validation_step</span></code>

</h4>


  <div class="doc doc-contents ">
  
      <p>validation_step defines the validation loop. It is independent of forward
@see pl.LightningModule</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>batch</code></b>
                  (<code>list[<span title="torch.Tensor">Tensor</span>]</code>)
              –
              <div class="doc-md-description">
                <p>@see training_step</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>scprint/model/model.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">872</span>
<span class="normal">873</span>
<span class="normal">874</span>
<span class="normal">875</span>
<span class="normal">876</span>
<span class="normal">877</span>
<span class="normal">878</span>
<span class="normal">879</span>
<span class="normal">880</span>
<span class="normal">881</span>
<span class="normal">882</span>
<span class="normal">883</span>
<span class="normal">884</span>
<span class="normal">885</span>
<span class="normal">886</span>
<span class="normal">887</span>
<span class="normal">888</span>
<span class="normal">889</span>
<span class="normal">890</span>
<span class="normal">891</span>
<span class="normal">892</span>
<span class="normal">893</span>
<span class="normal">894</span>
<span class="normal">895</span>
<span class="normal">896</span>
<span class="normal">897</span>
<span class="normal">898</span>
<span class="normal">899</span>
<span class="normal">900</span>
<span class="normal">901</span>
<span class="normal">902</span>
<span class="normal">903</span>
<span class="normal">904</span>
<span class="normal">905</span>
<span class="normal">906</span>
<span class="normal">907</span>
<span class="normal">908</span>
<span class="normal">909</span>
<span class="normal">910</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    validation_step defines the validation loop. It is independent of forward</span>
<span class="sd">    @see pl.LightningModule</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (list[Tensor]): @see training_step</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">val_loss</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_training</span><span class="p">(</span>
        <span class="n">batch</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_denoise</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_next_tp</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_cce</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cce_sim</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_ecs</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_mvc</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_adv_cls</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">do_generate</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask_ratio</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">expression</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">]</span>
    <span class="n">gene_pos</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;genes&quot;</span><span class="p">]</span>
    <span class="n">depth</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;depth&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">embs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">embs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">10000</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">gene_pos</span><span class="p">,</span> <span class="n">expression</span><span class="p">,</span> <span class="n">depth</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">info</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">info</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span><span class="n">gene_pos</span><span class="p">,</span> <span class="n">expression</span><span class="p">,</span> <span class="n">depth</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">info</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">val_loss</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>




  </div>

  </div>

</div>

<div class="doc doc-object doc-module">




<h2 id="scprint.model.loss" class="doc doc-heading">
          <code>scprint.model.loss</code>


</h2>

  <div class="doc doc-contents first">

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">




<h3 id="scprint.model.loss.classification" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">classification</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Computes the classification loss for a given batch of predictions and ground truth labels.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>labelname</code></b>
                  (<code>str</code>)
              –
              <div class="doc-md-description">
                <p>The name of the label.</p>
              </div>
            </li>
            <li>
              <b><code>pred</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The predicted logits for the batch.</p>
              </div>
            </li>
            <li>
              <b><code>cl</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The ground truth labels for the batch.</p>
              </div>
            </li>
            <li>
              <b><code>maxsize</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The number of possible labels.</p>
              </div>
            </li>
            <li>
              <b><code>cls_hierarchy</code></b>
                  (<code>dict</code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>The hierarchical structure of the labels. Defaults to {}.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>ValueError</code>
              –
              <div class="doc-md-description">
                <p>If the labelname is not found in the cls_hierarchy dictionary.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
            –
            <div class="doc-md-description">
              <p>torch.Tensor: The computed binary cross entropy loss for the given batch.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>scprint/model/loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">classification</span><span class="p">(</span><span class="n">labelname</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">cl</span><span class="p">,</span> <span class="n">maxsize</span><span class="p">,</span> <span class="n">cls_hierarchy</span><span class="o">=</span><span class="p">{}):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the classification loss for a given batch of predictions and ground truth labels.</span>

<span class="sd">    Args:</span>
<span class="sd">        labelname (str): The name of the label.</span>
<span class="sd">        pred (torch.Tensor): The predicted logits for the batch.</span>
<span class="sd">        cl (torch.Tensor): The ground truth labels for the batch.</span>
<span class="sd">        maxsize (int): The number of possible labels.</span>
<span class="sd">        cls_hierarchy (dict, optional): The hierarchical structure of the labels. Defaults to {}.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the labelname is not found in the cls_hierarchy dictionary.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The computed binary cross entropy loss for the given batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">newcl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="p">(</span><span class="n">cl</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">maxsize</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">cl</span><span class="o">.</span><span class="n">device</span>
    <span class="p">)</span>  <span class="c1"># batchsize * n_labels</span>
    <span class="c1"># if we don&#39;t know the label we set the weight to 0 else to 1</span>
    <span class="n">valid_indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">cl</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">cl</span> <span class="o">&lt;</span> <span class="n">maxsize</span><span class="p">)</span>
    <span class="n">valid_cl</span> <span class="o">=</span> <span class="n">cl</span><span class="p">[</span><span class="n">valid_indices</span><span class="p">]</span>
    <span class="n">newcl</span><span class="p">[</span><span class="n">valid_indices</span><span class="p">,</span> <span class="n">valid_cl</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">newcl</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cl</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">weight</span><span class="p">[</span><span class="n">cl</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">inv</span> <span class="o">=</span> <span class="n">cl</span> <span class="o">&gt;=</span> <span class="n">maxsize</span>
    <span class="c1"># if we have non leaf values, we don&#39;t know so we don&#39;t compute grad and set weight to 0</span>
    <span class="c1"># and add labels that won&#39;t be counted but so that we can still use them</span>
    <span class="k">if</span> <span class="n">inv</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">labelname</span> <span class="ow">in</span> <span class="n">cls_hierarchy</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">clhier</span> <span class="o">=</span> <span class="n">cls_hierarchy</span><span class="p">[</span><span class="n">labelname</span><span class="p">]</span>

            <span class="n">invw</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="n">inv</span><span class="p">]</span>
            <span class="n">invw</span><span class="p">[</span><span class="n">clhier</span><span class="p">[</span><span class="n">cl</span><span class="p">[</span><span class="n">inv</span><span class="p">]</span> <span class="o">-</span> <span class="n">maxsize</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">weight</span><span class="p">[</span><span class="n">inv</span><span class="p">]</span> <span class="o">=</span> <span class="n">invw</span>

            <span class="n">addnewcl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
                <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">pred</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>  <span class="c1"># no need to set the other to 0</span>
            <span class="n">addweight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">pred</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">addweight</span><span class="p">[</span><span class="n">inv</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="c1"># computing hierarchical labels and adding them to cl</span>
            <span class="n">cpred</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="n">cpred</span><span class="p">[</span><span class="o">~</span><span class="n">inv</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span>
            <span class="n">cpred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">cpred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">newcl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">newcl</span><span class="p">,</span> <span class="n">addnewcl</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pred</span><span class="p">,</span> <span class="n">cpred</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">weight</span><span class="p">,</span> <span class="n">addweight</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;need to use cls_hierarchy for this usecase&quot;</span><span class="p">)</span>

    <span class="n">myloss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">newcl</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">myloss</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="scprint.model.loss.classifier_loss" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">classifier_loss</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Compute the cross entropy loss between prediction and target.</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">classifier_loss</span><span class="p">(</span><span class="n">pred</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the cross entropy loss between prediction and target.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="scprint.model.loss.criterion_neg_log_bernoulli" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">criterion_neg_log_bernoulli</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Compute the negative log-likelihood of Bernoulli distribution</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">criterion_neg_log_bernoulli</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the negative log-likelihood of Bernoulli distribution</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">bernoulli</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">masked_log_probs</span> <span class="o">=</span> <span class="n">bernoulli</span><span class="o">.</span><span class="n">log_prob</span><span class="p">((</span><span class="n">target</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span> <span class="o">*</span> <span class="n">mask</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">masked_log_probs</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="scprint.model.loss.ecs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">ecs</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>ecs Computes the similarity of cell embeddings based on a threshold.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>cell_emb</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>A tensor representing cell embeddings.</p>
              </div>
            </li>
            <li>
              <b><code>ecs_threshold</code></b>
                  (<code>float</code>, default:
                      <code>0.5</code>
)
              –
              <div class="doc-md-description">
                <p>A threshold for determining similarity. Defaults to 0.5.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
            –
            <div class="doc-md-description">
              <p>torch.Tensor: A tensor representing the mean of 1 minus the square of the difference between the cosine similarity and the threshold.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>scprint/model/loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">ecs</span><span class="p">(</span><span class="n">cell_emb</span><span class="p">,</span> <span class="n">ecs_threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ecs Computes the similarity of cell embeddings based on a threshold.</span>

<span class="sd">    Args:</span>
<span class="sd">        cell_emb (torch.Tensor): A tensor representing cell embeddings.</span>
<span class="sd">        ecs_threshold (float, optional): A threshold for determining similarity. Defaults to 0.5.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: A tensor representing the mean of 1 minus the square of the difference between the cosine similarity and the threshold.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Here using customized cosine similarity instead of F.cosine_similarity</span>
    <span class="c1"># to avoid the pytorch issue of similarity larger than 1.0, pytorch # 78064</span>
    <span class="c1"># normalize the embedding</span>
    <span class="n">cell_emb_normed</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">cell_emb</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ecs_sim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">cell_emb_normed</span><span class="p">,</span> <span class="n">cell_emb_normed</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>

    <span class="c1"># mask out diagnal elements</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">ecs_sim</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ecs_sim</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">cell_emb_normed</span><span class="p">,</span> <span class="n">cell_emb_normed</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
    <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">cos_sim</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="c1"># only optimize positive similarities</span>
    <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">cos_sim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">cos_sim</span> <span class="o">-</span> <span class="n">ecs_threshold</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="scprint.model.loss.graph_similarity_loss" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">graph_similarity_loss</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Compute the similarity of 2 generated graphs.</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">graph_similarity_loss</span><span class="p">(</span>
    <span class="n">input1</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">input2</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the similarity of 2 generated graphs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">input1</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">input2</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="scprint.model.loss.graph_sparsity_loss" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">graph_sparsity_loss</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Compute the sparsity of generated graphs.</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">graph_sparsity_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the sparsity of generated graphs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="nb">input</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="scprint.model.loss.masked_mae_loss" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">masked_mae_loss</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Compute the masked MAE loss between input and target.
MAE = mean absolute error</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">masked_mae_loss</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the masked MAE loss between input and target.</span>
<span class="sd">    MAE = mean absolute error</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="nb">input</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">target</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="scprint.model.loss.masked_mse_loss" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">masked_mse_loss</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Compute the masked MSE loss between input and target.</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">masked_mse_loss</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the masked MSE loss between input and target.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="nb">input</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">target</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="scprint.model.loss.masked_nb_loss" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">masked_nb_loss</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Compute the masked negative binomial loss between input and target.</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">masked_nb_loss</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the masked negative binomial loss between input and target.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">nb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">NegativeBinomial</span><span class="p">(</span><span class="n">total_count</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">probs</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">masked_log_probs</span> <span class="o">=</span> <span class="n">nb</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">masked_log_probs</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="scprint.model.loss.masked_relative_error" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">masked_relative_error</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Compute the masked relative error between input and target.</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">masked_relative_error</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the masked relative error between input and target.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">mask</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">-</span> <span class="n">target</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">target</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="scprint.model.loss.nb" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">nb</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>This negative binomial function was taken from:
Title: scvi-tools
Authors: Romain Lopez <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#114;&#111;&#109;&#97;&#105;&#110;&#95;&#108;&#111;&#112;&#101;&#122;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;">&#114;&#111;&#109;&#97;&#105;&#110;&#95;&#108;&#111;&#112;&#101;&#122;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;</a>,
         Adam Gayoso <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#97;&#100;&#97;&#109;&#103;&#97;&#121;&#111;&#115;&#111;&#64;&#98;&#101;&#114;&#107;&#101;&#108;&#101;&#121;&#46;&#101;&#100;&#117;">&#97;&#100;&#97;&#109;&#103;&#97;&#121;&#111;&#115;&#111;&#64;&#98;&#101;&#114;&#107;&#101;&#108;&#101;&#121;&#46;&#101;&#100;&#117;</a>,
         Galen Xing <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#103;&#120;&#50;&#49;&#49;&#51;&#64;&#99;&#111;&#108;&#117;&#109;&#98;&#105;&#97;&#46;&#101;&#100;&#117;">&#103;&#120;&#50;&#49;&#49;&#51;&#64;&#99;&#111;&#108;&#117;&#109;&#98;&#105;&#97;&#46;&#101;&#100;&#117;</a>
Date: 16th November 2020
Code version: 0.8.1
Availability: https://github.com/YosefLab/scvi-tools/blob/8f5a9cc362325abbb7be1e07f9523cfcf7e55ec0/scvi/core/distributions/_negative_binomial.py</p>
<p>Computes negative binomial loss.
Parameters</p>
<hr />
<p>x: torch.Tensor
     Torch Tensor of ground truth data.
mu: torch.Tensor
     Torch Tensor of means of the negative binomial (has to be positive support).
theta: torch.Tensor
     Torch Tensor of inverse dispersion parameter (has to be positive support).
eps: Float
     numerical stability constant.</p>
<h5 id="scprint.model.loss.nb--returns">Returns</h5>
<p>If 'mean' is 'True' NB loss value gets returned, otherwise Torch tensor of losses gets returned.</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">nb</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">mu</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This negative binomial function was taken from:</span>
<span class="sd">    Title: scvi-tools</span>
<span class="sd">    Authors: Romain Lopez &lt;romain_lopez@gmail.com&gt;,</span>
<span class="sd">             Adam Gayoso &lt;adamgayoso@berkeley.edu&gt;,</span>
<span class="sd">             Galen Xing &lt;gx2113@columbia.edu&gt;</span>
<span class="sd">    Date: 16th November 2020</span>
<span class="sd">    Code version: 0.8.1</span>
<span class="sd">    Availability: https://github.com/YosefLab/scvi-tools/blob/8f5a9cc362325abbb7be1e07f9523cfcf7e55ec0/scvi/core/distributions/_negative_binomial.py</span>

<span class="sd">    Computes negative binomial loss.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x: torch.Tensor</span>
<span class="sd">         Torch Tensor of ground truth data.</span>
<span class="sd">    mu: torch.Tensor</span>
<span class="sd">         Torch Tensor of means of the negative binomial (has to be positive support).</span>
<span class="sd">    theta: torch.Tensor</span>
<span class="sd">         Torch Tensor of inverse dispersion parameter (has to be positive support).</span>
<span class="sd">    eps: Float</span>
<span class="sd">         numerical stability constant.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    If &#39;mean&#39; is &#39;True&#39; NB loss value gets returned, otherwise Torch tensor of losses gets returned.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">theta</span><span class="o">.</span><span class="n">ndimension</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

    <span class="n">log_theta_mu_eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">theta</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_theta_mu_eps</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_theta_mu_eps</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">theta</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="scprint.model.loss.similarity" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">similarity</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Dot product or cosine similarity</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">similarity</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">temp</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dot product or cosine similarity</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">temp</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">res</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">




<h3 id="scprint.model.loss.zinb" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">zinb</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>This zero-inflated negative binomial function was taken from:
Title: scvi-tools
Authors: Romain Lopez <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#114;&#111;&#109;&#97;&#105;&#110;&#95;&#108;&#111;&#112;&#101;&#122;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;">&#114;&#111;&#109;&#97;&#105;&#110;&#95;&#108;&#111;&#112;&#101;&#122;&#64;&#103;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;</a>,
         Adam Gayoso <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#97;&#100;&#97;&#109;&#103;&#97;&#121;&#111;&#115;&#111;&#64;&#98;&#101;&#114;&#107;&#101;&#108;&#101;&#121;&#46;&#101;&#100;&#117;">&#97;&#100;&#97;&#109;&#103;&#97;&#121;&#111;&#115;&#111;&#64;&#98;&#101;&#114;&#107;&#101;&#108;&#101;&#121;&#46;&#101;&#100;&#117;</a>,
         Galen Xing <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#103;&#120;&#50;&#49;&#49;&#51;&#64;&#99;&#111;&#108;&#117;&#109;&#98;&#105;&#97;&#46;&#101;&#100;&#117;">&#103;&#120;&#50;&#49;&#49;&#51;&#64;&#99;&#111;&#108;&#117;&#109;&#98;&#105;&#97;&#46;&#101;&#100;&#117;</a>
Date: 16th November 2020
Code version: 0.8.1
Availability: https://github.com/YosefLab/scvi-tools/blob/8f5a9cc362325abbb7be1e07f9523cfcf7e55ec0/scvi/core/distributions/_negative_binomial.py</p>
<p>Computes zero inflated negative binomial loss.
Parameters</p>
<hr />
<p>x: torch.Tensor
     Torch Tensor of ground truth data.
mu: torch.Tensor
     Torch Tensor of means of the negative binomial (has to be positive support).
theta: torch.Tensor
     Torch Tensor of inverses dispersion parameter (has to be positive support).
pi: torch.Tensor
     Torch Tensor of logits of the dropout parameter (real support)
eps: Float
     numerical stability constant.</p>
<h5 id="scprint.model.loss.zinb--returns">Returns</h5>
<p>If 'mean' is 'True' ZINB loss value gets returned, otherwise Torch tensor of losses gets returned.</p>

          <details class="quote">
            <summary>Source code in <code>scprint/model/loss.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">zinb</span><span class="p">(</span>
    <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">mu</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">pi</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span>
    <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This zero-inflated negative binomial function was taken from:</span>
<span class="sd">    Title: scvi-tools</span>
<span class="sd">    Authors: Romain Lopez &lt;romain_lopez@gmail.com&gt;,</span>
<span class="sd">             Adam Gayoso &lt;adamgayoso@berkeley.edu&gt;,</span>
<span class="sd">             Galen Xing &lt;gx2113@columbia.edu&gt;</span>
<span class="sd">    Date: 16th November 2020</span>
<span class="sd">    Code version: 0.8.1</span>
<span class="sd">    Availability: https://github.com/YosefLab/scvi-tools/blob/8f5a9cc362325abbb7be1e07f9523cfcf7e55ec0/scvi/core/distributions/_negative_binomial.py</span>

<span class="sd">    Computes zero inflated negative binomial loss.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x: torch.Tensor</span>
<span class="sd">         Torch Tensor of ground truth data.</span>
<span class="sd">    mu: torch.Tensor</span>
<span class="sd">         Torch Tensor of means of the negative binomial (has to be positive support).</span>
<span class="sd">    theta: torch.Tensor</span>
<span class="sd">         Torch Tensor of inverses dispersion parameter (has to be positive support).</span>
<span class="sd">    pi: torch.Tensor</span>
<span class="sd">         Torch Tensor of logits of the dropout parameter (real support)</span>
<span class="sd">    eps: Float</span>
<span class="sd">         numerical stability constant.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    If &#39;mean&#39; is &#39;True&#39; ZINB loss value gets returned, otherwise Torch tensor of losses gets returned.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">softplus_pi</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="o">-</span><span class="n">pi</span><span class="p">)</span>  <span class="c1">#  uses log(sigmoid(x)) = -softplus(-x)</span>
    <span class="n">log_theta_eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">log_theta_mu_eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">pi_theta_log</span> <span class="o">=</span> <span class="o">-</span><span class="n">pi</span> <span class="o">+</span> <span class="n">theta</span> <span class="o">*</span> <span class="p">(</span><span class="n">log_theta_eps</span> <span class="o">-</span> <span class="n">log_theta_mu_eps</span><span class="p">)</span>

    <span class="n">case_zero</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">pi_theta_log</span><span class="p">)</span> <span class="o">-</span> <span class="n">softplus_pi</span>
    <span class="n">mul_case_zero</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">((</span><span class="n">target</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">case_zero</span><span class="p">)</span>

    <span class="n">case_non_zero</span> <span class="o">=</span> <span class="p">(</span>
        <span class="o">-</span><span class="n">softplus_pi</span>
        <span class="o">+</span> <span class="n">pi_theta_log</span>
        <span class="o">+</span> <span class="n">target</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_theta_mu_eps</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="n">theta</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">mul_case_non_zero</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">((</span><span class="n">target</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">case_non_zero</span><span class="p">)</span>

    <span class="n">res</span> <span class="o">=</span> <span class="n">mul_case_zero</span> <span class="o">+</span> <span class="n">mul_case_non_zero</span>
    <span class="c1"># we want to minize the loss but maximize the log likelyhood</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">res</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../utils/" class="btn btn-neutral float-left" title="utils"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../tasks/" class="btn btn-neutral float-right" title="tasks">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../utils/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../tasks/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
