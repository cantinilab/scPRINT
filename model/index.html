<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://www.jkobject.com/scPRINT/model/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>model - scprint</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "model";
        var mkdocs_page_input_path = "model.md";
        var mkdocs_page_url = "/scPRINT/model/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> scprint
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../structure/">structure</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../pretrain/">pre-training</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../usage/">usage example</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">example notebooks</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../notebooks/cancer_usecase/">scPRINT use case on BPH</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../notebooks/cancer_usecase_part2/">scPRINT use case on BPH (part 2, GN analysis)</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">documentation</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">model</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#model-description">model description</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.model">model</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.model.scPrint">scPrint</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.configure_optimizers">configure_optimizers</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.forward">forward</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.get_cell_embs">get_cell_embs</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.log_adata">log_adata</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.on_fit_start">on_fit_start</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.on_predict_epoch_end">on_predict_epoch_end</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.on_predict_epoch_start">on_predict_epoch_start</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.on_validation_epoch_end">on_validation_epoch_end</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.optimizer_step">optimizer_step</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.predict_step">predict_step</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.training_step">training_step</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.model.scPrint.validation_step">validation_step</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#losses">losses</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.loss">loss</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.AdversarialDiscriminatorLoss">AdversarialDiscriminatorLoss</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.loss.AdversarialDiscriminatorLoss.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.classification">classification</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.criterion_neg_log_bernoulli">criterion_neg_log_bernoulli</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.ecs">ecs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.grad_reverse">grad_reverse</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.masked_mae">masked_mae</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.masked_mse">masked_mse</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.masked_nb">masked_nb</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.masked_relative_error">masked_relative_error</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.mse">mse</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.nb">nb</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.nb_dist">nb_dist</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.similarity">similarity</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.loss.zinb">zinb</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#utils">utils</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.utils">utils</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.Attention">Attention</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.utils.Attention.add">add</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.utils.Attention.agg">agg</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.utils.Attention.get">get</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.downsample_profile">downsample_profile</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.make_adata">make_adata</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.simple_masker">simple_masker</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.test">test</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.translate">translate</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.weighted_masker">weighted_masker</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.utils.zinb_sample">zinb_sample</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#encoder-and-decoder-modules">encoder and decoder modules</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.encoders">encoders</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.encoders.CategoryValueEncoder">CategoryValueEncoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.encoders.ContinuousValueEncoder">ContinuousValueEncoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.encoders.ContinuousValueEncoder.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.encoders.DPositionalEncoding">DPositionalEncoding</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.encoders.DPositionalEncoding.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.encoders.GeneEncoder">GeneEncoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.encoders.PositionalEncoding">PositionalEncoding</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.encoders.PositionalEncoding.forward">forward</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.decoders">decoders</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.decoders.ClsDecoder">ClsDecoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.decoders.ClsDecoder.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.decoders.ExprDecoder">ExprDecoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.decoders.ExprDecoder.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.decoders.GraphSDEExprDecoder">GraphSDEExprDecoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.decoders.MVCDecoder">MVCDecoder</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.decoders.MVCDecoder.forward">forward</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#flashattention">flashattention</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.flash_attn.flashformer">flashformer</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.flash_attn.flashformer.FlashTransformerEncoder">FlashTransformerEncoder</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.flash_attn.mha">mha</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.flash_attn.mha.CrossAttention">CrossAttention</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.flash_attn.mha.CrossAttention.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.flash_attn.mha.FlashCrossAttention">FlashCrossAttention</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.flash_attn.mha.FlashCrossAttention.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.flash_attn.mha.FlashSelfAttention">FlashSelfAttention</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.flash_attn.mha.FlashSelfAttention.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.flash_attn.mha.LinearResidual">LinearResidual</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.flash_attn.mha.MHA">MHA</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.flash_attn.mha.MHA.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.flash_attn.mha.SelfAttention">SelfAttention</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.flash_attn.mha.SelfAttention.forward">forward</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.flash_attn.mlp">mlp</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.flash_attn.mlp.Mlp">Mlp</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.flash_attn.mlp.Mlp.forward">forward</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.flash_attn.block">block</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.flash_attn.block.Block">Block</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.flash_attn.block.Block.forward">forward</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.flash_attn.flashattention">flashattention</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.flash_attn.flashattention.FlashAttnFunc">FlashAttnFunc</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.flash_attn.flashattention.FlashAttnFunc.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.flash_attn.flashattention.FlashAttnKVPackedFunc">FlashAttnKVPackedFunc</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.flash_attn.flashattention.FlashAttnKVPackedFunc.forward">forward</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.flash_attn.flashattention.FlashAttnQKVPackedFunc">FlashAttnQKVPackedFunc</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#scprint.model.flash_attn.flashattention.FlashAttnQKVPackedFunc.forward">forward</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.flash_attn.activations">activations</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.flash_attn.activations.bias_gelu_back">bias_gelu_back</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#scprint.model.flash_attn.layer_norm">layer_norm</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.flash_attn.layer_norm.layer_norm_ref">layer_norm_ref</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#scprint.model.flash_attn.layer_norm.rms_norm_ref">rms_norm_ref</a>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tasks/">tasks</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../cli/">cli</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../embedder/">embedders</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../utils/">utils</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">scprint</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">documentation</li>
      <li class="breadcrumb-item active">model</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="documentation-for-the-model">Documentation for the <code>model</code></h1>
<h2 id="model-description">model description</h2>


<div class="doc doc-object doc-module">



<h2 id="scprint.model.model" class="doc doc-heading">
            <code>scprint.model.model</code>


</h2>

    <div class="doc doc-contents first">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="scprint.model.model.scPrint" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">scPrint</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="lightning.LightningModule">LightningModule</span></code>, <code><span title="huggingface_hub.PyTorchModelHubMixin">PyTorchModelHubMixin</span></code></p>


      <p>scPRINT transformer for single cell biology and the inference of Gene Regulatory networks</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>genes</code></b>
                  (<code>list</code>)
              –
              <div class="doc-md-description">
                <p>List of gene names the model will work with.</p>
              </div>
            </li>
            <li>
              <b><code>precpt_gene_emb</code></b>
                  (<code><span title="np.array">array</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Gene embeddings of size (len(genes), d_model). Should be in the same order as the genes. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>gene_pos_enc</code></b>
                  (<code>list</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Gene position encoding of the same size as genes. Provides a location value for each gene in genes. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>d_model</code></b>
                  (<code>int</code>, default:
                      <code>512</code>
)
              –
              <div class="doc-md-description">
                <p>Dimension of the model. Defaults to 512.</p>
              </div>
            </li>
            <li>
              <b><code>nhead</code></b>
                  (<code>int</code>, default:
                      <code>8</code>
)
              –
              <div class="doc-md-description">
                <p>Number of heads in the multihead attention models. Defaults to 8.</p>
              </div>
            </li>
            <li>
              <b><code>d_hid</code></b>
                  (<code>int</code>, default:
                      <code>512</code>
)
              –
              <div class="doc-md-description">
                <p>Dimension of the feedforward network model. Defaults to 512.</p>
              </div>
            </li>
            <li>
              <b><code>nlayers</code></b>
                  (<code>int</code>, default:
                      <code>6</code>
)
              –
              <div class="doc-md-description">
                <p>Number of layers in the transformer model. Defaults to 6.</p>
              </div>
            </li>
            <li>
              <b><code>expr_encoder_layers</code></b>
                  (<code>int</code>, default:
                      <code>2</code>
)
              –
              <div class="doc-md-description">
                <p>Number of layers in the expression encoder. Defaults to 2.</p>
              </div>
            </li>
            <li>
              <b><code>layers_cls</code></b>
                  (<code>list[int]</code>, default:
                      <code>[]</code>
)
              –
              <div class="doc-md-description">
                <p>List specifying the number of layers in the classifier. Defaults to [].</p>
              </div>
            </li>
            <li>
              <b><code>classes</code></b>
                  (<code><span title="typing.Dict">Dict</span>[str, int]</code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>Classes to predict with the number of classes for each. Defaults to {}.</p>
              </div>
            </li>
            <li>
              <b><code>labels_hierarchy</code></b>
                  (<code><span title="typing.Dict">Dict</span>[str, <span title="typing.Dict">Dict</span>[int, list[int]]]</code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>Class hierarchy for classes with hierarchical classes. Defaults to {}.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>, default:
                      <code>0.2</code>
)
              –
              <div class="doc-md-description">
                <p>Dropout value. Defaults to 0.2.</p>
              </div>
            </li>
            <li>
              <b><code>transformer</code></b>
                  (<code>str</code>, default:
                      <code>&#39;fast&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Transformer type to use. One of "linear", "flash", "flashsparse", "scprint". Defaults to "fast".</p>
              </div>
            </li>
            <li>
              <b><code>domain_spec_batchnorm</code></b>
                  (<code>str</code>, default:
                      <code>&#39;None&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to apply domain-specific batch normalization. Defaults to "None".</p>
              </div>
            </li>
            <li>
              <b><code>expr_emb_style</code></b>
                  (<code>str</code>, default:
                      <code>&#39;continuous&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Style of input embedding. One of "continuous", "binned_pos", "cont_pos". Defaults to "continuous".</p>
              </div>
            </li>
            <li>
              <b><code>mvc_decoder</code></b>
                  (<code>str</code>, default:
                      <code>&#39;None&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Style of MVC decoder. One of "None", "inner product", "concat query", "sum query". Defaults to "None".</p>
              </div>
            </li>
            <li>
              <b><code>pred_embedding</code></b>
                  (<code>list[str]</code>, default:
                      <code>[]</code>
)
              –
              <div class="doc-md-description">
                <p>List of classes to use for plotting embeddings. Defaults to [].</p>
              </div>
            </li>
            <li>
              <b><code>cell_emb_style</code></b>
                  (<code>str</code>, default:
                      <code>&#39;cls&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Style of cell embedding. One of "cls", "avg-pool", "w-pool". Defaults to "cls".</p>
              </div>
            </li>
            <li>
              <b><code>freeze_embeddings</code></b>
                  (<code>bool</code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to freeze the embeddings during training. Defaults to True.</p>
              </div>
            </li>
            <li>
              <b><code>label_decoders</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Dict">Dict</span>[int, str]]]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Label decoders to use for plotting the UMAP during validations. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>zinb</code></b>
                  (<code>bool</code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use Zero-Inflated Negative Binomial distribution. Defaults to True.</p>
              </div>
            </li>
            <li>
              <b><code>lr</code></b>
                  (<code>float</code>, default:
                      <code>0.0001</code>
)
              –
              <div class="doc-md-description">
                <p>Learning rate. Defaults to 0.0001.</p>
              </div>
            </li>
            <li>
              <b><code>optim</code></b>
                  (<code>str</code>, default:
                      <code>&#39;adamW&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Optimizer type. Defaults to "adamW".</p>
              </div>
            </li>
            <li>
              <b><code>weight_decay</code></b>
                  (<code>float</code>, default:
                      <code>0.01</code>
)
              –
              <div class="doc-md-description">
                <p>Weight decay for the optimizer. Defaults to 0.01.</p>
              </div>
            </li>
            <li>
              <b><code>**flash_attention_kwargs</code></b>
                  (<code>dict</code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>Additional keyword arguments for the model. see @flashformer.py</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<details class="notes" open>
  <summary>Notes</summary>
  <p>for other parameters of the model that are not part of its class definition, see @trainer.trainer.py</p>
</details>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>ValueError</code>
              –
              <div class="doc-md-description">
                <p>If the expr_emb_style is not one of "continuous", "binned_pos", "cont_pos".</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                  <details class="quote">
                    <summary>Source code in <code>scprint/model/model.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">genes</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
    <span class="n">organisms</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;NCBITaxon:9606&quot;</span><span class="p">],</span>
    <span class="n">precpt_gene_emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">gene_pos_enc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">normalization</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">nhead</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">attn_bias</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span>
    <span class="n">d_hid</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">edge_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>
    <span class="n">nlayers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">expr_encoder_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
    <span class="n">layers_cls</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">classes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">labels_hierarchy</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">transformer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;fast&quot;</span><span class="p">,</span>
    <span class="n">expr_emb_style</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;continuous&quot;</span><span class="p">,</span>  <span class="c1"># &quot;binned_pos&quot;, &quot;cont_pos&quot;</span>
    <span class="n">domain_spec_batchnorm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
    <span class="n">n_input_bins</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">num_batch_labels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">mvc_decoder</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;None&quot;</span><span class="p">,</span>
    <span class="n">pred_embedding</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
    <span class="n">cell_emb_style</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cls&quot;</span><span class="p">,</span>
    <span class="n">freeze_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">label_decoders</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">zinb</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0001</span><span class="p">,</span>
    <span class="n">optim</span><span class="o">=</span><span class="s2">&quot;adamW&quot;</span><span class="p">,</span>  <span class="c1"># TODEL</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>  <span class="c1"># TODEL</span>
    <span class="o">**</span><span class="n">flash_attention_kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    scPRINT transformer for single cell biology and the inference of Gene Regulatory networks</span>

<span class="sd">    Args:</span>
<span class="sd">        genes (list): List of gene names the model will work with.</span>
<span class="sd">        precpt_gene_emb (np.array, optional): Gene embeddings of size (len(genes), d_model). Should be in the same order as the genes. Defaults to None.</span>
<span class="sd">        gene_pos_enc (list, optional): Gene position encoding of the same size as genes. Provides a location value for each gene in genes. Defaults to None.</span>
<span class="sd">        d_model (int, optional): Dimension of the model. Defaults to 512.</span>
<span class="sd">        nhead (int, optional): Number of heads in the multihead attention models. Defaults to 8.</span>
<span class="sd">        d_hid (int, optional): Dimension of the feedforward network model. Defaults to 512.</span>
<span class="sd">        nlayers (int, optional): Number of layers in the transformer model. Defaults to 6.</span>
<span class="sd">        expr_encoder_layers (int, optional): Number of layers in the expression encoder. Defaults to 2.</span>
<span class="sd">        layers_cls (list[int], optional): List specifying the number of layers in the classifier. Defaults to [].</span>
<span class="sd">        classes (Dict[str, int], optional): Classes to predict with the number of classes for each. Defaults to {}.</span>
<span class="sd">        labels_hierarchy (Dict[str, Dict[int, list[int]]], optional): Class hierarchy for classes with hierarchical classes. Defaults to {}.</span>
<span class="sd">        dropout (float, optional): Dropout value. Defaults to 0.2.</span>
<span class="sd">        transformer (str, optional): Transformer type to use. One of &quot;linear&quot;, &quot;flash&quot;, &quot;flashsparse&quot;, &quot;scprint&quot;. Defaults to &quot;fast&quot;.</span>
<span class="sd">        domain_spec_batchnorm (str, optional): Whether to apply domain-specific batch normalization. Defaults to &quot;None&quot;.</span>
<span class="sd">        expr_emb_style (str, optional): Style of input embedding. One of &quot;continuous&quot;, &quot;binned_pos&quot;, &quot;cont_pos&quot;. Defaults to &quot;continuous&quot;.</span>
<span class="sd">        mvc_decoder (str, optional): Style of MVC decoder. One of &quot;None&quot;, &quot;inner product&quot;, &quot;concat query&quot;, &quot;sum query&quot;. Defaults to &quot;None&quot;.</span>
<span class="sd">        pred_embedding (list[str], optional): List of classes to use for plotting embeddings. Defaults to [].</span>
<span class="sd">        cell_emb_style (str, optional): Style of cell embedding. One of &quot;cls&quot;, &quot;avg-pool&quot;, &quot;w-pool&quot;. Defaults to &quot;cls&quot;.</span>
<span class="sd">        freeze_embeddings (bool, optional): Whether to freeze the embeddings during training. Defaults to True.</span>
<span class="sd">        label_decoders (Optional[Dict[str, Dict[int, str]]], optional): Label decoders to use for plotting the UMAP during validations. Defaults to None.</span>
<span class="sd">        zinb (bool, optional): Whether to use Zero-Inflated Negative Binomial distribution. Defaults to True.</span>
<span class="sd">        lr (float, optional): Learning rate. Defaults to 0.0001.</span>
<span class="sd">        optim (str, optional): Optimizer type. Defaults to &quot;adamW&quot;.</span>
<span class="sd">        weight_decay (float, optional): Weight decay for the optimizer. Defaults to 0.01.</span>
<span class="sd">        **flash_attention_kwargs (dict): Additional keyword arguments for the model. see @flashformer.py</span>

<span class="sd">    Notes:</span>
<span class="sd">        for other parameters of the model that are not part of its class definition, see @trainer.trainer.py</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the expr_emb_style is not one of &quot;continuous&quot;, &quot;binned_pos&quot;, &quot;cont_pos&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="c1"># training flags</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_denoise</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.6</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_cce</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cce_sim</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cce_scale</span> <span class="o">=</span> <span class="mf">0.002</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_ecs</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ecs_threshold</span> <span class="o">=</span> <span class="mf">0.3</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ecs_scale</span> <span class="o">=</span> <span class="mf">0.05</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_mvc</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mvc_scale</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">class_embd_diss_scale</span> <span class="o">=</span> <span class="mf">0.2</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_adv_cls</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">adv_class_scale</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_cls</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mean_attn_tot</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mean_attn_tot_c</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_adv_batch</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">run_full_forward</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">class_scale</span> <span class="o">=</span> <span class="mf">0.4</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_next_tp</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_generate</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mask_ratio</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">warmup_duration</span> <span class="o">=</span> <span class="mi">500</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">0.01</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optim</span> <span class="o">=</span> <span class="s2">&quot;adamW&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fused_adam</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_patience</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_factor</span> <span class="o">=</span> <span class="mf">0.6</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_monitor</span> <span class="o">=</span> <span class="s2">&quot;val_loss&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">doplot</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">get_attention_layer</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embs</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred_log_adata</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">Attention</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">genes</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">predict_depth_mult</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">predict_mode</span> <span class="o">=</span> <span class="s2">&quot;none&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">keep_all_cls_pred</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># should be stored somehow</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">normalization</span> <span class="o">=</span> <span class="n">normalization</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">organisms</span> <span class="o">=</span> <span class="n">organisms</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">edge_dim</span> <span class="o">=</span> <span class="n">edge_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn_bias</span> <span class="o">=</span> <span class="n">attn_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nlayers</span> <span class="o">=</span> <span class="n">nlayers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gene_pos_enc</span> <span class="o">=</span> <span class="n">gene_pos_enc</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mvc_decoder</span> <span class="o">=</span> <span class="n">mvc_decoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">domain_spec_batchnorm</span> <span class="o">=</span> <span class="n">domain_spec_batchnorm</span>
    <span class="c1"># need to store</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_input_bins</span> <span class="o">=</span> <span class="n">n_input_bins</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">transformer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">label_counts</span> <span class="o">=</span> <span class="n">classes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">classes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">classes</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cell_emb_style</span> <span class="o">=</span> <span class="n">cell_emb_style</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">label_decoders</span> <span class="o">=</span> <span class="n">label_decoders</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred_embedding</span> <span class="o">=</span> <span class="n">pred_embedding</span>
    <span class="c1"># compute tensor for mat_labels_hierarchy</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mat_labels_hierarchy</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">labels_hierarchy</span> <span class="o">=</span> <span class="n">labels_hierarchy</span>
    <span class="k">if</span> <span class="s2">&quot;strict_loading&quot;</span> <span class="ow">in</span> <span class="n">flash_attention_kwargs</span><span class="p">:</span>
        <span class="n">flash_attention_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;strict_loading&quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">labels_hierarchy</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">tens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">v</span><span class="p">),</span> <span class="n">classes</span><span class="p">[</span><span class="n">k</span><span class="p">]))</span>
        <span class="k">for</span> <span class="n">k2</span><span class="p">,</span> <span class="n">v2</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">tens</span><span class="p">[</span><span class="n">k2</span> <span class="o">-</span> <span class="n">classes</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">v2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mat_labels_hierarchy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">tens</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">expr_emb_style</span> <span class="o">=</span> <span class="n">expr_emb_style</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">expr_emb_style</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;category&quot;</span><span class="p">,</span> <span class="s2">&quot;continuous&quot;</span><span class="p">,</span> <span class="s2">&quot;none&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;expr_emb_style should be one of category, continuous, scaling, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;got </span><span class="si">{</span><span class="n">expr_emb_style</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">cell_emb_style</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;cls&quot;</span><span class="p">,</span> <span class="s2">&quot;avg-pool&quot;</span><span class="p">,</span> <span class="s2">&quot;w-pool&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown cell_emb_style: </span><span class="si">{</span><span class="n">cell_emb_style</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">genes</span> <span class="o">=</span> <span class="n">genes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">n</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">genes</span><span class="p">)}</span>

    <span class="c1"># encoder</span>
    <span class="c1"># gene encoder</span>
    <span class="k">if</span> <span class="n">precpt_gene_emb</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="n">precpt_gene_emb</span><span class="p">)</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;the gene embeddings file </span><span class="si">{</span><span class="n">precpt_gene_emb</span><span class="si">}</span><span class="s2"> does not contain any of the genes given to the model&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">genes</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="s2">&quot;Warning: only a subset of the genes available in the embeddings file.&quot;</span>
            <span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;number of genes: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">))</span>
        <span class="n">sembeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool1d</span><span class="p">(</span><span class="n">d_model</span><span class="p">)(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gene_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">GeneEncoder</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">sembeddings</span><span class="p">,</span> <span class="n">freeze</span><span class="o">=</span><span class="n">freeze_embeddings</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">GeneEncoder</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="c1"># Value Encoder, NOTE: the scaling style is also handled in _encode method</span>
    <span class="k">if</span> <span class="n">expr_emb_style</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;continuous&quot;</span><span class="p">,</span> <span class="s2">&quot;full_pos&quot;</span><span class="p">]:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">ContinuousValueEncoder</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">expr_encoder_layers</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="n">expr_emb_style</span> <span class="o">==</span> <span class="s2">&quot;binned_pos&quot;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">n_input_bins</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">CategoryValueEncoder</span><span class="p">(</span><span class="n">n_input_bins</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">expr_encoder</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>

    <span class="c1"># Positional Encoding</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gene_pos_enc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">gene_pos_enc</span><span class="p">)</span>
        <span class="n">token_to_pos</span> <span class="o">=</span> <span class="p">{</span><span class="n">token</span><span class="p">:</span> <span class="n">pos</span> <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene_pos_enc</span><span class="p">)}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">PositionalEncoding</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="n">max_len</span><span class="p">,</span> <span class="n">token_to_pos</span><span class="o">=</span><span class="n">token_to_pos</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">cell_embs_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
    <span class="c1"># Class Encoder</span>
    <span class="c1"># always have [base_cell_emb, time_embedding, depth_embedding] + any other class info</span>
    <span class="c1"># base cell embedding will store other cell specific information</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">class_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">CategoryValueEncoder</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cell_embs_count</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span>
    <span class="p">)</span>
    <span class="c1"># self.time_encoder = encoders.ContinuousValueEncoder(d_model, dropout)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">depth_encoder</span> <span class="o">=</span> <span class="n">encoders</span><span class="o">.</span><span class="n">ContinuousValueEncoder</span><span class="p">(</span>
        <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">expr_encoder_layers</span>
    <span class="p">)</span>

    <span class="c1"># Transformer</span>
    <span class="c1"># Linear</span>
    <span class="k">if</span> <span class="n">transformer</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
        <span class="c1"># linear transformer using the fast transformer package</span>
        <span class="c1"># self.transformer = FastTransformerEncoder(</span>
        <span class="c1">#    d_model, nhead, d_hid, nlayers, dropout, &quot;linear&quot;</span>
        <span class="c1"># )</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Linear transformer is not implemented&quot;</span><span class="p">)</span>
    <span class="c1"># regular or flash</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">FlashTransformerEncoder</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="n">nhead</span><span class="p">,</span>
            <span class="n">nlayers</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">use_flash_attn</span><span class="o">=</span><span class="p">(</span><span class="n">transformer</span> <span class="o">==</span> <span class="s2">&quot;flash&quot;</span><span class="p">),</span>
            <span class="o">**</span><span class="n">flash_attention_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># decoders</span>
    <span class="c1"># expression</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">expr_decoder</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">ExprDecoder</span><span class="p">(</span>
        <span class="n">d_model</span><span class="p">,</span>
        <span class="n">nfirst_tokens_to_skip</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cell_embs_count</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">zinb</span><span class="o">=</span><span class="n">zinb</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># cls decoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cls_decoders</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">()</span>
    <span class="c1"># should be a very simple classifier for most things</span>
    <span class="c1"># (maybe scale with the number of classes) should be 1 layer...</span>
    <span class="k">for</span> <span class="n">clss</span><span class="p">,</span> <span class="n">n_cls</span> <span class="ow">in</span> <span class="n">classes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cls_decoders</span><span class="p">[</span><span class="n">clss</span><span class="p">]</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">ClsDecoder</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span> <span class="n">n_cls</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="n">layers_cls</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span>
        <span class="p">)</span>

    <span class="c1"># Batch effect correction via adversarial training on batch classes</span>
    <span class="k">if</span> <span class="n">num_batch_labels</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_reverse_discriminator_loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">AdversarialDiscriminatorLoss</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="n">n_cls</span><span class="o">=</span><span class="n">num_batch_labels</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grad_reverse_discriminator_loss</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># expression decoder from batch embbedding</span>
    <span class="k">if</span> <span class="n">mvc_decoder</span> <span class="o">!=</span> <span class="s2">&quot;None&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mvc_decoder</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">MVCDecoder</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="n">arch_style</span><span class="o">=</span><span class="n">mvc_decoder</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mvc_decoder</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span>
            <span class="n">utils</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">,</span>
            <span class="n">n_layer</span><span class="o">=</span><span class="n">nlayers</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dec</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_decoders</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">dec</span><span class="o">.</span><span class="n">out_layer</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.13</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.configure_optimizers" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">configure_optimizers</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>@see pl.LightningModule</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="c1"># https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam</span>
    <span class="c1"># not working because of poor weight decay implem</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim</span> <span class="o">==</span> <span class="s2">&quot;adam&quot;</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
            <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span>
            <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
            <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
            <span class="n">amsgrad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">fused</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fused_adam</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim</span> <span class="o">==</span> <span class="s2">&quot;adamW&quot;</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
            <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span><span class="p">,</span>
            <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span>
            <span class="n">eps</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">,</span>
            <span class="n">amsgrad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">fused</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">fused_adam</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">optim</span> <span class="o">==</span> <span class="s2">&quot;galore&quot;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Galore optimizer not implemented&quot;</span><span class="p">)</span>
        <span class="c1"># param_groups = [</span>
        <span class="c1">#    {</span>
        <span class="c1">#        &quot;params&quot;: [</span>
        <span class="c1">#            v for k, v in self.named_parameters() if &quot;transformer&quot; not in k</span>
        <span class="c1">#        ]</span>
        <span class="c1">#    },</span>
        <span class="c1">#    {</span>
        <span class="c1">#        &quot;params&quot;: [</span>
        <span class="c1">#            v for k, v in self.named_parameters() if &quot;transformer&quot; in k</span>
        <span class="c1">#        ],</span>
        <span class="c1">#        &quot;rank&quot;: 128,</span>
        <span class="c1">#        &quot;update_proj_gap&quot;: 200,</span>
        <span class="c1">#        &quot;scale&quot;: 0.25,</span>
        <span class="c1">#        &quot;proj_type&quot;: &quot;std&quot;,</span>
        <span class="c1">#    },</span>
        <span class="c1"># ]</span>
        <span class="c1"># optimizer = GaLoreAdamW(param_groups, lr=self.hparams.lr)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown optimizer: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">ReduceLROnPlateau</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
        <span class="n">patience</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_patience</span><span class="p">,</span>
        <span class="n">factor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_factor</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">lr_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
        <span class="c1"># The unit of the scheduler&#39;s step size, could also be &#39;step&#39;.</span>
        <span class="c1"># &#39;epoch&#39; updates the scheduler on epoch end whereas &#39;step&#39;</span>
        <span class="c1"># updates it after a optimizer update.</span>
        <span class="s2">&quot;interval&quot;</span><span class="p">:</span> <span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
        <span class="c1"># How many epochs/steps should pass between calls to</span>
        <span class="c1"># `scheduler.step()`. 1 corresponds to updating the learning</span>
        <span class="c1"># rate after every epoch/step.</span>
        <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="c1"># Metric to to monitor for schedulers like `ReduceLROnPlateau`</span>
        <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_reduce_monitor</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">callbacks</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="n">_LRCallback</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">=</span> <span class="n">val</span><span class="o">.</span><span class="n">num_training</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="n">LearningRateFinder</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">=</span> <span class="n">val</span><span class="o">.</span><span class="n">_num_training_steps</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">optimizer</span><span class="p">],</span> <span class="p">[</span><span class="n">lr_dict</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>forward also called on self(), a full forward pass on the model</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>gene_pos</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>A tensor of shape (minibatch, seq_len)
representing the genes used for each cell in the minibatch.</p>
              </div>
            </li>
            <li>
              <b><code>expression</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>A tensor of shape (minibatch, seq_len)
representing the expression levels of genes in the minibatch. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>mask</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>A tensor of shape (minibatch, seq_len)
used to mask certain elements in the sequence during the forward pass. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>full_depth</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>A tensor of shape (minibatch,)
representing the full depth of each sequence in the minibatch. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>timepoint</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>A tensor of shape (minibatch,)
representing the timepoint associated with each sequence in the minibatch. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>get_gene_emb</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>A flag indicating whether to return the gene embeddings.
If True, the gene embeddings are included in the output. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>do_sample</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>A flag indicating whether to sample the expression levels.
If True, the expression levels are sampled during the forward pass. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>get_attention_layer</code></b>
                  (<code>list</code>, default:
                      <code>[]</code>
)
              –
              <div class="doc-md-description">
                <p>A list indicating which attention layers to return.
If not empty, the specified attention layers are included in the output. Defaults to [].</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              –
              <div class="doc-md-description">
                <p>dict of output Tensors: A dictionary containing the output tensors from the forward pass.
The keys of the dictionary depend on the input flags (get_gene_emb, do_sample, get_attention_layer).
at minima, the dictionary codntains the following:
- "mean": the mean expression levels
- "zero_logits": the logits for zero-inflated expression levels
- "disp": the dispersion parameter
- "cell_embs": the cell embeddings per class
- "cell_emb": the main cell embedding
- "cls_output": the output of the classifier</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">gene_pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">expression</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">full_depth</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">timepoint</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># (new_minibatch_of_nxt_cells,)</span>
    <span class="n">get_gene_emb</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">depth_mult</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">do_sample</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">do_mvc</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">do_class</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">get_attention_layer</span><span class="p">:</span> <span class="nb">list</span> <span class="o">=</span> <span class="p">[],</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    forward also called on self(), a full forward pass on the model</span>

<span class="sd">    Args:</span>
<span class="sd">        gene_pos (Tensor): A tensor of shape (minibatch, seq_len)</span>
<span class="sd">            representing the genes used for each cell in the minibatch.</span>
<span class="sd">        expression (Tensor, optional): A tensor of shape (minibatch, seq_len)</span>
<span class="sd">            representing the expression levels of genes in the minibatch. Defaults to None.</span>
<span class="sd">        mask (Tensor, optional): A tensor of shape (minibatch, seq_len)</span>
<span class="sd">            used to mask certain elements in the sequence during the forward pass. Defaults to None.</span>
<span class="sd">        full_depth (Tensor, optional): A tensor of shape (minibatch,)</span>
<span class="sd">            representing the full depth of each sequence in the minibatch. Defaults to None.</span>
<span class="sd">        timepoint (Tensor, optional): A tensor of shape (minibatch,)</span>
<span class="sd">            representing the timepoint associated with each sequence in the minibatch. Defaults to None.</span>
<span class="sd">        get_gene_emb (bool, optional): A flag indicating whether to return the gene embeddings.</span>
<span class="sd">            If True, the gene embeddings are included in the output. Defaults to False.</span>
<span class="sd">        do_sample (bool, optional): A flag indicating whether to sample the expression levels.</span>
<span class="sd">            If True, the expression levels are sampled during the forward pass. Defaults to False.</span>
<span class="sd">        get_attention_layer (list, optional): A list indicating which attention layers to return.</span>
<span class="sd">            If not empty, the specified attention layers are included in the output. Defaults to [].</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict of output Tensors: A dictionary containing the output tensors from the forward pass.</span>
<span class="sd">            The keys of the dictionary depend on the input flags (get_gene_emb, do_sample, get_attention_layer).</span>
<span class="sd">            at minima, the dictionary codntains the following:</span>
<span class="sd">            - &quot;mean&quot;: the mean expression levels</span>
<span class="sd">            - &quot;zero_logits&quot;: the logits for zero-inflated expression levels</span>
<span class="sd">            - &quot;disp&quot;: the dispersion parameter</span>
<span class="sd">            - &quot;cell_embs&quot;: the cell embeddings per class</span>
<span class="sd">            - &quot;cell_emb&quot;: the main cell embedding</span>
<span class="sd">            - &quot;cls_output&quot;: the output of the classifier</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">encoding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encoder</span><span class="p">(</span><span class="n">gene_pos</span><span class="p">,</span> <span class="n">expression</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">full_depth</span><span class="p">,</span> <span class="n">timepoint</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_bias</span> <span class="o">!=</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;nbias&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">nbias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
                <span class="n">load_npz</span><span class="p">(</span><span class="n">FILEDIR</span> <span class="o">+</span> <span class="s2">&quot;/../../data/bias_sparse.npz&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">todense</span><span class="p">()</span>
            <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">gene_pos</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
        <span class="n">num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">gene_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">gene_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">num</span><span class="p">,</span>
                <span class="n">gene_pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">num</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">device</span><span class="o">=</span><span class="n">gene_pos</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">bias</span><span class="p">[:,</span> <span class="n">num</span><span class="p">:,</span> <span class="p">:</span><span class="n">num</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10_000</span>  <span class="c1"># do not pay attention to the cls embeddings</span>
        <span class="n">bias</span><span class="p">[:,</span> <span class="n">num</span><span class="p">:,</span> <span class="n">num</span><span class="p">:]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">nbias</span><span class="p">[</span><span class="n">gene_pos</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">gene_pos</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]]</span>
    <span class="n">transformer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span>
        <span class="n">encoding</span><span class="p">,</span>
        <span class="n">return_qkv</span><span class="o">=</span><span class="n">get_attention_layer</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">bias</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn_bias</span> <span class="o">!=</span> <span class="s2">&quot;none&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bias_layer</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nlayers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)),</span>
    <span class="p">)</span>

    <span class="n">depth_mult</span> <span class="o">=</span> <span class="n">expression</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">depth_mult</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">depth_mult</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">get_attention_layer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">transformer_output</span><span class="p">,</span> <span class="n">qkvs</span> <span class="o">=</span> <span class="n">transformer_output</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_decoder</span><span class="p">(</span>
                <span class="n">transformer_output</span><span class="p">,</span>
                <span class="n">depth_mult</span><span class="p">,</span>
                <span class="n">get_gene_emb</span><span class="p">,</span>
                <span class="n">do_sample</span><span class="p">,</span>
                <span class="n">do_mvc</span><span class="p">,</span>
                <span class="n">do_class</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">qkvs</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decoder</span><span class="p">(</span>
            <span class="n">transformer_output</span><span class="p">,</span>
            <span class="n">depth_mult</span><span class="p">,</span>
            <span class="n">get_gene_emb</span><span class="p">,</span>
            <span class="n">do_sample</span><span class="p">,</span>
            <span class="n">do_mvc</span><span class="p">,</span>
            <span class="n">do_class</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.get_cell_embs" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get_cell_embs</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>get_cell_embs</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>layer_output</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The output tensor from a layer in the model.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>ValueError</code>
              –
              <div class="doc-md-description">
                <p>Raised when an unknown cell embedding style is encountered.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>              –
              <div class="doc-md-description">
                <p>The cell embeddings tensor.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span>
<span class="normal">1341</span>
<span class="normal">1342</span>
<span class="normal">1343</span>
<span class="normal">1344</span>
<span class="normal">1345</span>
<span class="normal">1346</span>
<span class="normal">1347</span>
<span class="normal">1348</span>
<span class="normal">1349</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get_cell_embs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_output</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    get_cell_embs</span>

<span class="sd">    Args:</span>
<span class="sd">        layer_output (Tensor): The output tensor from a layer in the model.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: Raised when an unknown cell embedding style is encountered.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: The cell embeddings tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_emb_style</span> <span class="o">==</span> <span class="s2">&quot;cls&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># (minibatch, embsize)</span>
        <span class="n">cell_emb</span> <span class="o">=</span> <span class="n">layer_output</span><span class="p">[:,</span> <span class="p">:</span> <span class="mi">2</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">)]</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">cell_emb_style</span> <span class="o">==</span> <span class="s2">&quot;avg-pool&quot;</span><span class="p">:</span>
        <span class="n">cell_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">layer_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown cell_emb_style: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">cell_emb_style</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cell_emb</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.log_adata" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">log_adata</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>log_adata will log an adata from predictions.
It will log to tensorboard and wandb if available</p>
<p>see @utils.log_adata</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1391</span>
<span class="normal">1392</span>
<span class="normal">1393</span>
<span class="normal">1394</span>
<span class="normal">1395</span>
<span class="normal">1396</span>
<span class="normal">1397</span>
<span class="normal">1398</span>
<span class="normal">1399</span>
<span class="normal">1400</span>
<span class="normal">1401</span>
<span class="normal">1402</span>
<span class="normal">1403</span>
<span class="normal">1404</span>
<span class="normal">1405</span>
<span class="normal">1406</span>
<span class="normal">1407</span>
<span class="normal">1408</span>
<span class="normal">1409</span>
<span class="normal">1410</span>
<span class="normal">1411</span>
<span class="normal">1412</span>
<span class="normal">1413</span>
<span class="normal">1414</span>
<span class="normal">1415</span>
<span class="normal">1416</span>
<span class="normal">1417</span>
<span class="normal">1418</span>
<span class="normal">1419</span>
<span class="normal">1420</span>
<span class="normal">1421</span>
<span class="normal">1422</span>
<span class="normal">1423</span>
<span class="normal">1424</span>
<span class="normal">1425</span>
<span class="normal">1426</span>
<span class="normal">1427</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">log_adata</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gtclass</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    log_adata will log an adata from predictions.</span>
<span class="sd">    It will log to tensorboard and wandb if available</span>

<span class="sd">    see @utils.log_adata</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">mdir</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">save_dir</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">save_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;/tmp&quot;</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">mdir</span> <span class="o">=</span> <span class="s2">&quot;data/&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">mdir</span><span class="p">):</span>
        <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">mdir</span><span class="p">)</span>
    <span class="n">adata</span><span class="p">,</span> <span class="n">fig</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">make_adata</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embs</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">keep_all_cls_pred</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_step</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_decoders</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels_hierarchy</span><span class="p">,</span>
        <span class="n">gtclass</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span><span class="p">),</span>
        <span class="n">mdir</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">doplot</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">doplot</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_figure</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;couldn&#39;t log to tensorboard&quot;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">log_image</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="s2">&quot;umaps&quot;</span><span class="p">,</span> <span class="n">images</span><span class="o">=</span><span class="p">[</span><span class="n">fig</span><span class="p">])</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;couldn&#39;t log to wandb&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">adata</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.on_fit_start" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">on_fit_start</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>@see pl.LightningModule</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">on_fit_start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">)</span> <span class="ow">is</span> <span class="n">FlashTransformerEncoder</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">encoder_layers</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
            <span class="n">encoder_layers</span><span class="o">.</span><span class="n">set_seq_parallel</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mat_labels_hierarchy</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mat_labels_hierarchy</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.on_predict_epoch_end" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">on_predict_epoch_end</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>@see pl.LightningModule will</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">on_predict_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule will&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">100</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_log_adata</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;adding on disk&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_adata</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;predict_part_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">counter</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.on_predict_epoch_start" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">on_predict_epoch_start</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>@see pl.LightningModule</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1122</span>
<span class="normal">1123</span>
<span class="normal">1124</span>
<span class="normal">1125</span>
<span class="normal">1126</span>
<span class="normal">1127</span>
<span class="normal">1128</span>
<span class="normal">1129</span>
<span class="normal">1130</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">on_predict_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embs</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">)</span> <span class="ow">is</span> <span class="n">FlashTransformerEncoder</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">encoder_layers</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="o">.</span><span class="n">blocks</span><span class="p">:</span>
            <span class="n">encoder_layers</span><span class="o">.</span><span class="n">set_seq_parallel</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.on_validation_epoch_end" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">on_validation_epoch_end</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>@see pl.LightningModule</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1086</span>
<span class="normal">1087</span>
<span class="normal">1088</span>
<span class="normal">1089</span>
<span class="normal">1090</span>
<span class="normal">1091</span>
<span class="normal">1092</span>
<span class="normal">1093</span>
<span class="normal">1094</span>
<span class="normal">1095</span>
<span class="normal">1096</span>
<span class="normal">1097</span>
<span class="normal">1098</span>
<span class="normal">1099</span>
<span class="normal">1100</span>
<span class="normal">1101</span>
<span class="normal">1102</span>
<span class="normal">1103</span>
<span class="normal">1104</span>
<span class="normal">1105</span>
<span class="normal">1106</span>
<span class="normal">1107</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">on_validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embs</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">info</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pred</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="kc">None</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pos</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">all_gather</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pos</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">is_global_zero</span><span class="p">:</span>
        <span class="c1"># print(&quot;you are not on the main node. cancelling logging step&quot;)</span>
        <span class="k">return</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">stage</span> <span class="o">!=</span> <span class="s2">&quot;sanity_check&quot;</span><span class="p">:</span>
        <span class="n">sch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_schedulers</span><span class="p">()</span>
        <span class="n">sch</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">callback_metrics</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">])</span>
        <span class="c1"># run the test function on specific dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_adata</span><span class="p">(</span>
            <span class="n">gtclass</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">info</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;validation_part_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">counter</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">30</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">on_test_epoch_end</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.optimizer_step" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">optimizer_step</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>@see pl.LightningModule</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1005</span>
<span class="normal">1006</span>
<span class="normal">1007</span>
<span class="normal">1008</span>
<span class="normal">1009</span>
<span class="normal">1010</span>
<span class="normal">1011</span>
<span class="normal">1012</span>
<span class="normal">1013</span>
<span class="normal">1014</span>
<span class="normal">1015</span>
<span class="normal">1016</span>
<span class="normal">1017</span>
<span class="normal">1018</span>
<span class="normal">1019</span>
<span class="normal">1020</span>
<span class="normal">1021</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_closure</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;@see pl.LightningModule&quot;&quot;&quot;</span>
    <span class="c1"># update params</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

    <span class="c1"># manually warm up lr without a scheduler</span>
    <span class="c1"># making sure that we don&#39;t do this during lrfinder</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">global_step</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_duration</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span>
        <span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">lrfinder_steps</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_step</span><span class="p">:</span>
            <span class="n">lr_scale</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">global_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_duration</span><span class="p">)</span>
            <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr_scale</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="o">.</span><span class="n">lr</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
        <span class="c1"># if pg[&quot;lr&quot;] &lt; 2e-5:</span>
        <span class="c1">#    pg[&quot;lr&quot;] = 2e-5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;lr_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">pg</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">])</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.predict_step" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">predict_step</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>embed given gene expression, encode the gene embedding and cell embedding.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>              –
              <div class="doc-md-description">
                <p><em>description</em></p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1132</span>
<span class="normal">1133</span>
<span class="normal">1134</span>
<span class="normal">1135</span>
<span class="normal">1136</span>
<span class="normal">1137</span>
<span class="normal">1138</span>
<span class="normal">1139</span>
<span class="normal">1140</span>
<span class="normal">1141</span>
<span class="normal">1142</span>
<span class="normal">1143</span>
<span class="normal">1144</span>
<span class="normal">1145</span>
<span class="normal">1146</span>
<span class="normal">1147</span>
<span class="normal">1148</span>
<span class="normal">1149</span>
<span class="normal">1150</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    embed given gene expression, encode the gene embedding and cell embedding.</span>

<span class="sd">    Args:</span>
<span class="sd">        batch @see training_step</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: _description_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span>
        <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;genes&quot;</span><span class="p">],</span>
        <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span>
        <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;depth&quot;</span><span class="p">],</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">predict_mode</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred_embedding</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_attention_layer</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">predict_depth_mult</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.training_step" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">training_step</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>training_step defines the train loop. It is independent of forward</p>
<p>@see pl.LightningModule</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>_type_</code></b>              –
              <div class="doc-md-description">
                <p><em>description</em></p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span>
    <span class="n">batch_idx</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    training_step defines the train loop. It is independent of forward</span>

<span class="sd">    @see pl.LightningModule</span>

<span class="sd">    Returns:</span>
<span class="sd">        _type_: _description_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TASK 1 &amp; 2 &amp; 3 (first pass, expression reconstruction, label prediction)</span>
    <span class="n">total_loss</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_training</span><span class="p">(</span>
        <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
        <span class="n">do_denoise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_denoise</span><span class="p">,</span>
        <span class="n">noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">,</span>
        <span class="n">do_next_tp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_next_tp</span><span class="p">,</span>
        <span class="n">do_cce</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_cce</span><span class="p">,</span>
        <span class="n">cce_sim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cce_sim</span><span class="p">,</span>
        <span class="n">do_ecs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_ecs</span><span class="p">,</span>
        <span class="n">do_mvc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_mvc</span><span class="p">,</span>
        <span class="n">do_adv_cls</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_adv_cls</span><span class="p">,</span>
        <span class="n">do_adv_batch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_adv_batch</span><span class="p">,</span>
        <span class="n">do_cls</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_cls</span><span class="p">,</span>
        <span class="n">do_generate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_generate</span><span class="p">,</span>
        <span class="n">run_full_forward</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">run_full_forward</span><span class="p">,</span>
        <span class="n">mask_ratio</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mask_ratio</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;train_loss&quot;</span><span class="p">,</span> <span class="n">total_loss</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total_loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.model.scPrint.validation_step" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">validation_step</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>validation_step defines the validation loop. It is independent of forward
@see pl.LightningModule</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>batch</code></b>
                  (<code>list[<span title="torch.Tensor">Tensor</span>]</code>)
              –
              <div class="doc-md-description">
                <p>@see training_step</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/model.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1031</span>
<span class="normal">1032</span>
<span class="normal">1033</span>
<span class="normal">1034</span>
<span class="normal">1035</span>
<span class="normal">1036</span>
<span class="normal">1037</span>
<span class="normal">1038</span>
<span class="normal">1039</span>
<span class="normal">1040</span>
<span class="normal">1041</span>
<span class="normal">1042</span>
<span class="normal">1043</span>
<span class="normal">1044</span>
<span class="normal">1045</span>
<span class="normal">1046</span>
<span class="normal">1047</span>
<span class="normal">1048</span>
<span class="normal">1049</span>
<span class="normal">1050</span>
<span class="normal">1051</span>
<span class="normal">1052</span>
<span class="normal">1053</span>
<span class="normal">1054</span>
<span class="normal">1055</span>
<span class="normal">1056</span>
<span class="normal">1057</span>
<span class="normal">1058</span>
<span class="normal">1059</span>
<span class="normal">1060</span>
<span class="normal">1061</span>
<span class="normal">1062</span>
<span class="normal">1063</span>
<span class="normal">1064</span>
<span class="normal">1065</span>
<span class="normal">1066</span>
<span class="normal">1067</span>
<span class="normal">1068</span>
<span class="normal">1069</span>
<span class="normal">1070</span>
<span class="normal">1071</span>
<span class="normal">1072</span>
<span class="normal">1073</span>
<span class="normal">1074</span>
<span class="normal">1075</span>
<span class="normal">1076</span>
<span class="normal">1077</span>
<span class="normal">1078</span>
<span class="normal">1079</span>
<span class="normal">1080</span>
<span class="normal">1081</span>
<span class="normal">1082</span>
<span class="normal">1083</span>
<span class="normal">1084</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">batch</span><span class="p">,</span>
    <span class="n">batch_idx</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    validation_step defines the validation loop. It is independent of forward</span>
<span class="sd">    @see pl.LightningModule</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (list[Tensor]): @see training_step</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">val_loss</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_full_training</span><span class="p">(</span>
        <span class="n">batch</span><span class="o">=</span><span class="n">batch</span><span class="p">,</span>
        <span class="n">do_denoise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_denoise</span><span class="p">,</span>
        <span class="n">noise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">,</span>
        <span class="n">do_next_tp</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_next_tp</span><span class="p">,</span>
        <span class="n">do_cce</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_cce</span><span class="p">,</span>
        <span class="n">cce_sim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cce_sim</span><span class="p">,</span>
        <span class="n">do_ecs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_ecs</span><span class="p">,</span>
        <span class="n">do_mvc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_mvc</span><span class="p">,</span>
        <span class="n">do_adv_cls</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_adv_cls</span><span class="p">,</span>
        <span class="n">do_adv_batch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_adv_batch</span><span class="p">,</span>
        <span class="n">do_cls</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_cls</span><span class="p">,</span>
        <span class="n">do_generate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">do_generate</span><span class="p">,</span>
        <span class="n">run_full_forward</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">run_full_forward</span><span class="p">,</span>
        <span class="n">mask_ratio</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mask_ratio</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">expression</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">]</span>
    <span class="n">gene_pos</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;genes&quot;</span><span class="p">]</span>
    <span class="n">depth</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;depth&quot;</span><span class="p">]</span>
    <span class="c1"># TODO: make this faster by only calling val loss</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">embs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">embs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">100_000</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">info</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">info</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span>
                <span class="n">gene_pos</span><span class="p">,</span>
                <span class="n">expression</span><span class="p">,</span>
                <span class="n">depth</span><span class="p">,</span>
                <span class="n">pred_embedding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pred_embedding</span><span class="p">,</span>
                <span class="n">max_size_in_mem</span><span class="o">=</span><span class="mi">100_000</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">info</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_predict</span><span class="p">(</span>
            <span class="n">gene_pos</span><span class="p">,</span>
            <span class="n">expression</span><span class="p">,</span>
            <span class="n">depth</span><span class="p">,</span>
            <span class="n">pred_embedding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pred_embedding</span><span class="p">,</span>
            <span class="n">max_size_in_mem</span><span class="o">=</span><span class="mi">100_000</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">sync_dist</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">val_loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div><h2 id="losses">losses</h2>


<div class="doc doc-object doc-module">



<h2 id="scprint.model.loss" class="doc doc-heading">
            <code>scprint.model.loss</code>


</h2>

    <div class="doc doc-contents first">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="scprint.model.loss.AdversarialDiscriminatorLoss" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">AdversarialDiscriminatorLoss</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>Discriminator for the adversarial training for batch correction.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The size of the input tensor.</p>
              </div>
            </li>
            <li>
              <b><code>n_cls</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The number of classes.</p>
              </div>
            </li>
            <li>
              <b><code>nlayers</code></b>
                  (<code>int</code>, default:
                      <code>3</code>
)
              –
              <div class="doc-md-description">
                <p>The number of layers in the discriminator. Defaults to 3.</p>
              </div>
            </li>
            <li>
              <b><code>activation</code></b>
                  (<code>callable</code>, default:
                      <code><span title="torch.nn.LeakyReLU">LeakyReLU</span></code>
)
              –
              <div class="doc-md-description">
                <p>The activation function. Defaults to nn.LeakyReLU.</p>
              </div>
            </li>
            <li>
              <b><code>reverse_grad</code></b>
                  (<code>bool</code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to reverse the gradient. Defaults</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                  <details class="quote">
                    <summary>Source code in <code>scprint/model/loss.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_cls</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">nlayers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">activation</span><span class="p">:</span> <span class="nb">callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">,</span>
    <span class="n">reverse_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Discriminator for the adversarial training for batch correction.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The size of the input tensor.</span>
<span class="sd">        n_cls (int): The number of classes.</span>
<span class="sd">        nlayers (int, optional): The number of layers in the discriminator. Defaults to 3.</span>
<span class="sd">        activation (callable, optional): The activation function. Defaults to nn.LeakyReLU.</span>
<span class="sd">        reverse_grad (bool, optional): Whether to reverse the gradient. Defaults</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="c1"># module list</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nlayers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_cls</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reverse_grad</span> <span class="o">=</span> <span class="n">reverse_grad</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.loss.AdversarialDiscriminatorLoss.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape [batch_size, embsize]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [batch_size, embsize]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reverse_grad</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">grad_reverse</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.classification" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">classification</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Computes the classification loss for a given batch of predictions and ground truth labels.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>clsname</code></b>
                  (<code>str</code>)
              –
              <div class="doc-md-description">
                <p>The name of the label.</p>
              </div>
            </li>
            <li>
              <b><code>pred</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The predicted logits for the batch.</p>
              </div>
            </li>
            <li>
              <b><code>cl</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The ground truth labels for the batch.</p>
              </div>
            </li>
            <li>
              <b><code>maxsize</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The number of possible labels.</p>
              </div>
            </li>
            <li>
              <b><code>labels_hierarchy</code></b>
                  (<code>dict</code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>The hierarchical structure of the labels. Defaults to {}.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>ValueError</code>
              –
              <div class="doc-md-description">
                <p>If the clsname is not found in the labels_hierarchy dictionary.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>The computed binary cross entropy loss for the given batch.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">classification</span><span class="p">(</span>
    <span class="n">clsname</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">pred</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">cl</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">maxsize</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">labels_hierarchy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]]</span> <span class="o">=</span> <span class="p">{},</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the classification loss for a given batch of predictions and ground truth labels.</span>

<span class="sd">    Args:</span>
<span class="sd">        clsname (str): The name of the label.</span>
<span class="sd">        pred (Tensor): The predicted logits for the batch.</span>
<span class="sd">        cl (Tensor): The ground truth labels for the batch.</span>
<span class="sd">        maxsize (int): The number of possible labels.</span>
<span class="sd">        labels_hierarchy (dict, optional): The hierarchical structure of the labels. Defaults to {}.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If the clsname is not found in the labels_hierarchy dictionary.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: The computed binary cross entropy loss for the given batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">newcl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
        <span class="p">(</span><span class="n">cl</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">maxsize</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">cl</span><span class="o">.</span><span class="n">device</span>
    <span class="p">)</span>  <span class="c1"># batchsize * n_labels</span>
    <span class="c1"># if we don&#39;t know the label we set the weight to 0 else to 1</span>
    <span class="n">valid_indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">cl</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">cl</span> <span class="o">&lt;</span> <span class="n">maxsize</span><span class="p">)</span>
    <span class="n">valid_cl</span> <span class="o">=</span> <span class="n">cl</span><span class="p">[</span><span class="n">valid_indices</span><span class="p">]</span>
    <span class="n">newcl</span><span class="p">[</span><span class="n">valid_indices</span><span class="p">,</span> <span class="n">valid_cl</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">newcl</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">cl</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">weight</span><span class="p">[</span><span class="n">cl</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">inv</span> <span class="o">=</span> <span class="n">cl</span> <span class="o">&gt;=</span> <span class="n">maxsize</span>
    <span class="c1"># if we have non leaf values, we don&#39;t know so we don&#39;t compute grad and set weight to 0</span>
    <span class="c1"># and add labels that won&#39;t be counted but so that we can still use them</span>
    <span class="k">if</span> <span class="n">inv</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">clsname</span> <span class="ow">in</span> <span class="n">labels_hierarchy</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">clhier</span> <span class="o">=</span> <span class="n">labels_hierarchy</span><span class="p">[</span><span class="n">clsname</span><span class="p">]</span>

            <span class="n">inv_weight</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="n">inv</span><span class="p">]</span>
            <span class="c1"># we set the weight of the elements that are not leaf to 0</span>
            <span class="c1"># i.e. the elements where we will compute the max</span>
            <span class="n">inv_weight</span><span class="p">[</span><span class="n">clhier</span><span class="p">[</span><span class="n">cl</span><span class="p">[</span><span class="n">inv</span><span class="p">]</span> <span class="o">-</span> <span class="n">maxsize</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">weight</span><span class="p">[</span><span class="n">inv</span><span class="p">]</span> <span class="o">=</span> <span class="n">inv_weight</span>

            <span class="n">addnewcl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
                <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">pred</span><span class="o">.</span><span class="n">device</span>
            <span class="p">)</span>  <span class="c1"># no need to set the other to 0 as the weight of the loss is set to 0</span>
            <span class="n">addweight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">pred</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">addweight</span><span class="p">[</span><span class="n">inv</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="c1"># computing hierarchical labels and adding them to cl</span>
            <span class="n">addpred</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="c1"># we only keep the elements where we need to compute the max,</span>
            <span class="c1"># for the rest we set them to -inf, so that they won&#39;t have any impact on the max()</span>
            <span class="n">inv_addpred</span> <span class="o">=</span> <span class="n">addpred</span><span class="p">[</span><span class="n">inv</span><span class="p">]</span>
            <span class="n">inv_addpred</span><span class="p">[</span><span class="n">inv_weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="nb">bool</span><span class="p">)]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">min</span>
            <span class="n">addpred</span><span class="p">[</span><span class="n">inv</span><span class="p">]</span> <span class="o">=</span> <span class="n">inv_addpred</span>

            <span class="c1"># differentiable max</span>
            <span class="n">addpred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">addpred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># we add the new labels to the cl</span>
            <span class="n">newcl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">newcl</span><span class="p">,</span> <span class="n">addnewcl</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">pred</span><span class="p">,</span> <span class="n">addpred</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">weight</span><span class="p">,</span> <span class="n">addweight</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;need to use labels_hierarchy for this usecase&quot;</span><span class="p">)</span>

    <span class="n">myloss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span>
        <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">newcl</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">myloss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.criterion_neg_log_bernoulli" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">criterion_neg_log_bernoulli</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Compute the negative log-likelihood of Bernoulli distribution</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">criterion_neg_log_bernoulli</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the negative log-likelihood of Bernoulli distribution</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">bernoulli</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">masked_log_probs</span> <span class="o">=</span> <span class="n">bernoulli</span><span class="o">.</span><span class="n">log_prob</span><span class="p">((</span><span class="n">target</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span> <span class="o">*</span> <span class="n">mask</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">masked_log_probs</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.ecs" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">ecs</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>ecs Computes the similarity of cell embeddings based on a threshold.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>cell_emb</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>A tensor representing cell embeddings.</p>
              </div>
            </li>
            <li>
              <b><code>ecs_threshold</code></b>
                  (<code>float</code>, default:
                      <code>0.5</code>
)
              –
              <div class="doc-md-description">
                <p>A threshold for determining similarity. Defaults to 0.5.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>A tensor representing the mean of 1 minus the square of the difference between the cosine similarity and the threshold.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">ecs</span><span class="p">(</span><span class="n">cell_emb</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ecs_threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ecs Computes the similarity of cell embeddings based on a threshold.</span>

<span class="sd">    Args:</span>
<span class="sd">        cell_emb (Tensor): A tensor representing cell embeddings.</span>
<span class="sd">        ecs_threshold (float, optional): A threshold for determining similarity. Defaults to 0.5.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: A tensor representing the mean of 1 minus the square of the difference between the cosine similarity and the threshold.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Here using customized cosine similarity instead of F.cosine_similarity</span>
    <span class="c1"># to avoid the pytorch issue of similarity larger than 1.0, pytorch # 78064</span>
    <span class="c1"># normalize the embedding</span>
    <span class="n">cell_emb_normed</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">cell_emb</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">cell_emb_normed</span><span class="p">,</span> <span class="n">cell_emb_normed</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>

    <span class="c1"># mask out diagnal elements</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">cos_sim</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">bool</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cos_sim</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">cos_sim</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
    <span class="c1"># only optimize positive similarities</span>
    <span class="n">cos_sim</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">cos_sim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">cos_sim</span> <span class="o">-</span> <span class="n">ecs_threshold</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.grad_reverse" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">grad_reverse</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>grad_reverse Reverses the gradient of the input tensor.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The input tensor whose gradient is to be reversed.</p>
              </div>
            </li>
            <li>
              <b><code>lambd</code></b>
                  (<code>float</code>, default:
                      <code>1.0</code>
)
              –
              <div class="doc-md-description">
                <p>The scaling factor for the reversed gradient. Defaults to 1.0.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>(                  <code><span title="torch.Tensor">Tensor</span></code>
)              –
              <div class="doc-md-description">
                <p>The input tensor with its gradient reversed during the backward pass.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">grad_reverse</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">lambd</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    grad_reverse Reverses the gradient of the input tensor.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): The input tensor whose gradient is to be reversed.</span>
<span class="sd">        lambd (float, optional): The scaling factor for the reversed gradient. Defaults to 1.0.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: The input tensor with its gradient reversed during the backward pass.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">GradReverse</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lambd</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.masked_mae" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">masked_mae</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Compute the masked MAE loss between input and target.
MAE = mean absolute error</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">masked_mae</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the masked MAE loss between input and target.</span>
<span class="sd">    MAE = mean absolute error</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="nb">input</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">target</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.masked_mse" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">masked_mse</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Compute the masked MSE loss between input and target.</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">masked_mse</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the masked MSE loss between input and target.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="nb">input</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">target</span> <span class="o">*</span> <span class="n">mask</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.masked_nb" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">masked_nb</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Compute the masked negative binomial loss between input and target.</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">masked_nb</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the masked negative binomial loss between input and target.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">nb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">NegativeBinomial</span><span class="p">(</span><span class="n">total_count</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">probs</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">masked_log_probs</span> <span class="o">=</span> <span class="n">nb</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">*</span> <span class="n">mask</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">masked_log_probs</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.masked_relative_error" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">masked_relative_error</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Compute the masked relative error between input and target.</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">masked_relative_error</span><span class="p">(</span>
    <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the masked relative error between input and target.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">mask</span><span class="o">.</span><span class="n">any</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">-</span> <span class="n">target</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="n">target</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.mse" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">mse</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Compute the MSE loss between input and target.</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">mse</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the MSE loss between input and target.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="nb">input</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span> <span class="o">*</span> <span class="mi">10000</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10000</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.nb" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nb</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Computes the negative binomial (NB) loss.</p>
<p>This function was adapted from scvi-tools.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>target</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Ground truth data.</p>
              </div>
            </li>
            <li>
              <b><code>mu</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Means of the negative binomial distribution (must have positive support).</p>
              </div>
            </li>
            <li>
              <b><code>theta</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Inverse dispersion parameter (must have positive support).</p>
              </div>
            </li>
            <li>
              <b><code>eps</code></b>
                  (<code>float</code>, default:
                      <code>1e-08</code>
)
              –
              <div class="doc-md-description">
                <p>Numerical stability constant. Defaults to 1e-8.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>              –
              <div class="doc-md-description">
                <p>NB loss value.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">nb</span><span class="p">(</span><span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mu</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the negative binomial (NB) loss.</span>

<span class="sd">    This function was adapted from scvi-tools.</span>

<span class="sd">    Args:</span>
<span class="sd">        target (Tensor): Ground truth data.</span>
<span class="sd">        mu (Tensor): Means of the negative binomial distribution (must have positive support).</span>
<span class="sd">        theta (Tensor): Inverse dispersion parameter (must have positive support).</span>
<span class="sd">        eps (float, optional): Numerical stability constant. Defaults to 1e-8.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: NB loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">theta</span><span class="o">.</span><span class="n">ndimension</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

    <span class="n">log_theta_mu_eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">theta</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_theta_mu_eps</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">target</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_theta_mu_eps</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="n">theta</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="o">-</span><span class="n">res</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.nb_dist" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">nb_dist</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>nb_dist Computes the negative binomial distribution.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of observed data.</p>
              </div>
            </li>
            <li>
              <b><code>mu</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of means of the negative binomial distribution (must have positive support).</p>
              </div>
            </li>
            <li>
              <b><code>theta</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of inverse dispersion parameter (must have positive support).</p>
              </div>
            </li>
            <li>
              <b><code>eps</code></b>
                  (<code>float</code>, default:
                      <code>1e-08</code>
)
              –
              <div class="doc-md-description">
                <p>Numerical stability constant. Defaults to 1e-8.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>              –
              <div class="doc-md-description">
                <p>Negative binomial loss value.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">nb_dist</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mu</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    nb_dist Computes the negative binomial distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (Tensor): Torch Tensor of observed data.</span>
<span class="sd">        mu (Tensor): Torch Tensor of means of the negative binomial distribution (must have positive support).</span>
<span class="sd">        theta (Tensor): Torch Tensor of inverse dispersion parameter (must have positive support).</span>
<span class="sd">        eps (float, optional): Numerical stability constant. Defaults to 1e-8.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: Negative binomial loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">NegativeBinomial</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="n">theta</span><span class="p">)</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.similarity" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">similarity</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Dot product or cosine similarity</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">similarity</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">temp</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dot product or cosine similarity</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="n">temp</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">res</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.loss.zinb" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">zinb</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Computes zero-inflated negative binomial (ZINB) loss.</p>
<p>This function was modified from scvi-tools.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>target</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of ground truth data.</p>
              </div>
            </li>
            <li>
              <b><code>mu</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of means of the negative binomial (must have positive support).</p>
              </div>
            </li>
            <li>
              <b><code>theta</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of inverse dispersion parameter (must have positive support).</p>
              </div>
            </li>
            <li>
              <b><code>pi</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Torch Tensor of logits of the dropout parameter (real support).</p>
              </div>
            </li>
            <li>
              <b><code>eps</code></b>
                  (<code>float</code>, default:
                      <code>1e-08</code>
)
              –
              <div class="doc-md-description">
                <p>Numerical stability constant. Defaults to 1e-8.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>Tensor</code></b>              –
              <div class="doc-md-description">
                <p>ZINB loss value.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/loss.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">zinb</span><span class="p">(</span>
    <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">mu</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">pi</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes zero-inflated negative binomial (ZINB) loss.</span>

<span class="sd">    This function was modified from scvi-tools.</span>

<span class="sd">    Args:</span>
<span class="sd">        target (Tensor): Torch Tensor of ground truth data.</span>
<span class="sd">        mu (Tensor): Torch Tensor of means of the negative binomial (must have positive support).</span>
<span class="sd">        theta (Tensor): Torch Tensor of inverse dispersion parameter (must have positive support).</span>
<span class="sd">        pi (Tensor): Torch Tensor of logits of the dropout parameter (real support).</span>
<span class="sd">        eps (float, optional): Numerical stability constant. Defaults to 1e-8.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor: ZINB loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">#  uses log(sigmoid(x)) = -softplus(-x)</span>
    <span class="n">softplus_pi</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="o">-</span><span class="n">pi</span><span class="p">)</span>
    <span class="c1"># eps to make it positive support and taking the log</span>
    <span class="n">log_theta_mu_eps</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">pi_theta_log</span> <span class="o">=</span> <span class="o">-</span><span class="n">pi</span> <span class="o">+</span> <span class="n">theta</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">theta</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_theta_mu_eps</span><span class="p">)</span>

    <span class="n">case_zero</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">pi_theta_log</span><span class="p">)</span> <span class="o">-</span> <span class="n">softplus_pi</span>
    <span class="n">mul_case_zero</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">((</span><span class="n">target</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">case_zero</span><span class="p">)</span>

    <span class="n">case_non_zero</span> <span class="o">=</span> <span class="p">(</span>
        <span class="o">-</span><span class="n">softplus_pi</span>
        <span class="o">+</span> <span class="n">pi_theta_log</span>
        <span class="o">+</span> <span class="n">target</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">-</span> <span class="n">log_theta_mu_eps</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="n">theta</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">lgamma</span><span class="p">(</span><span class="n">target</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">mul_case_non_zero</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span><span class="p">((</span><span class="n">target</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">case_non_zero</span><span class="p">)</span>

    <span class="n">res</span> <span class="o">=</span> <span class="n">mul_case_zero</span> <span class="o">+</span> <span class="n">mul_case_non_zero</span>
    <span class="c1"># we want to minize the loss but maximize the log likelyhood</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">res</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h2 id="utils">utils</h2>


<div class="doc doc-object doc-module">



<h2 id="scprint.model.utils" class="doc doc-heading">
            <code>scprint.model.utils</code>


</h2>

    <div class="doc doc-contents first">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="scprint.model.utils.Attention" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">Attention</span></code>

</h3>


    <div class="doc doc-contents ">


      <p>Initialize the Attention class.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>gene_dim</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the gene.</p>
              </div>
            </li>
            <li>
              <b><code>comp_attn</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to compute attention. Defaults to False.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                  <details class="quote">
                    <summary>Source code in <code>scprint/model/utils.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gene_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">comp_attn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initialize the Attention class.</span>

<span class="sd">    Args:</span>
<span class="sd">        gene_dim (int): The dimension of the gene.</span>
<span class="sd">        comp_attn (bool, optional): Whether to compute attention. Defaults to False.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">gene_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">comp_attn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">comp_attn</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.utils.Attention.add" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">add</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>Add data to the internal storage.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="typing.List">List</span>[<span title="torch.Tensor">Tensor</span>]</code>)
              –
              <div class="doc-md-description">
                <p>List of tensors to add.</p>
              </div>
            </li>
            <li>
              <b><code>pos</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Position tensor.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Add data to the internal storage.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (List[Tensor]): List of tensors to add.</span>
<span class="sd">        pos (Tensor): Position tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">pos</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="c1"># loc = torch.cat([torch.Tensor([r for r in range(8)]), pos[i] + 8]).int()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))])</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.utils.Attention.agg" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">agg</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>Aggregate the attention or data based on the comp_attn flag.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="typing.List">List</span>[<span title="torch.Tensor">Tensor</span>]</code>)
              –
              <div class="doc-md-description">
                <p>List of tensors to aggregate.</p>
              </div>
            </li>
            <li>
              <b><code>pos</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Position tensor.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">agg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Aggregate the attention or data based on the comp_attn flag.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (List[Tensor]): List of tensors to aggregate.</span>
<span class="sd">        pos (Tensor): Position tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comp_attn</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>  <span class="c1"># •cells, •context, •QK, •heads, •dim</span>
            <span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">),</span> <span class="n">pos</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="mi">8</span><span class="p">])</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">[</span><span class="n">loc</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">loc</span><span class="p">]</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
                        <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="p">:]</span> <span class="o">@</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
                        <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">),</span>
                        <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="p">[</span><span class="n">loc</span><span class="p">]</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">pos</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">div</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene_dim</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">loc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">),</span> <span class="n">pos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">8</span><span class="p">])</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="p">[</span><span class="n">loc</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h4 id="scprint.model.utils.Attention.get" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">get</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>Get the aggregated attention or data.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="typing.Optional">Optional</span>[<span title="numpy.ndarray">ndarray</span>]</code>
              –
              <div class="doc-md-description">
                <p>Optional[np.ndarray]: The aggregated attention or data.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">get</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the aggregated attention or data.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Optional[np.ndarray]: The aggregated attention or data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">comp_attn</span><span class="p">:</span>
        <span class="n">loc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">[</span><span class="n">loc</span><span class="p">][:,</span> <span class="n">loc</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="n">loc</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.0001</span><span class="p">))</span>
            <span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>
        <span class="c1"># shape is (layers, genes, qkv, heads, emb)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">div</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h3 id="scprint.model.utils.downsample_profile" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">downsample_profile</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>This function downsamples the expression profile of a given single cell RNA matrix.</p>
<p>The noise is applied based on the renoise parameter,
the total counts of the matrix, and the number of genes. The function first calculates the noise
threshold (scaler) based on the renoise parameter. It then generates an initial matrix count by
applying a Poisson distribution to a random tensor scaled by the total counts and the number of genes.
The function then models the sampling zeros by applying a Poisson distribution to a random tensor
scaled by the noise threshold, the total counts, and the number of genes. The function also models
the technical zeros by generating a random tensor and comparing it to the noise threshold. The final
matrix count is calculated by subtracting the sampling zeros from the initial matrix count and
multiplying by the technical zeros. The function ensures that the final matrix count is not less
than zero by taking the maximum of the final matrix count and a tensor of zeros. The function
returns the final matrix count.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>mat</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The input matrix.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>)
              –
              <div class="doc-md-description">
                <p>The renoise parameter.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: The matrix count after applying noise.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">downsample_profile</span><span class="p">(</span><span class="n">mat</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;new&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function downsamples the expression profile of a given single cell RNA matrix.</span>

<span class="sd">    The noise is applied based on the renoise parameter,</span>
<span class="sd">    the total counts of the matrix, and the number of genes. The function first calculates the noise</span>
<span class="sd">    threshold (scaler) based on the renoise parameter. It then generates an initial matrix count by</span>
<span class="sd">    applying a Poisson distribution to a random tensor scaled by the total counts and the number of genes.</span>
<span class="sd">    The function then models the sampling zeros by applying a Poisson distribution to a random tensor</span>
<span class="sd">    scaled by the noise threshold, the total counts, and the number of genes. The function also models</span>
<span class="sd">    the technical zeros by generating a random tensor and comparing it to the noise threshold. The final</span>
<span class="sd">    matrix count is calculated by subtracting the sampling zeros from the initial matrix count and</span>
<span class="sd">    multiplying by the technical zeros. The function ensures that the final matrix count is not less</span>
<span class="sd">    than zero by taking the maximum of the final matrix count and a tensor of zeros. The function</span>
<span class="sd">    returns the final matrix count.</span>

<span class="sd">    Args:</span>
<span class="sd">        mat (torch.Tensor): The input matrix.</span>
<span class="sd">        dropout (float): The renoise parameter.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The matrix count after applying noise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Randomly drop on average N counts to each element of expression using a heavy tail Gaussian distribution</span>
    <span class="c1"># here we try to get the scale of the distribution so as to remove the right number of counts from each gene</span>
    <span class="c1"># https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02601-5#:~:text=Zero%20measurements%20in%20scRNA%2Dseq,generation%20of%20scRNA%2Dseq%20data.</span>
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;old&quot;</span><span class="p">:</span>
        <span class="n">totcounts</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">ngenes</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">tnoise</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># we model the sampling zeros (dropping 30% of the reads)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">ngenes</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="o">*</span> <span class="p">((</span><span class="n">tnoise</span> <span class="o">*</span> <span class="n">totcounts</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">ngenes</span><span class="p">))</span>
        <span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="c1"># we model the technical zeros (dropping 50% of the genes)</span>
        <span class="n">drop</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">ngenes</span><span class="p">))</span> <span class="o">&gt;</span> <span class="n">tnoise</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">mat</span> <span class="o">=</span> <span class="p">(</span><span class="n">mat</span> <span class="o">-</span> <span class="n">res</span><span class="p">)</span> <span class="o">*</span> <span class="n">drop</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">]])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">))</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;jules&quot;</span><span class="p">:</span>
        <span class="n">scaler</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">dropout</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">notdrop</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span>
                <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="o">&lt;</span> <span class="n">scaler</span>
        <span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="n">notdrop</span><span class="p">[</span><span class="n">mat</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># apply the dropout after the poisson, right?</span>
        <span class="k">return</span> <span class="n">notdrop</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">poisson</span><span class="p">(</span><span class="n">mat</span> <span class="o">*</span> <span class="n">scaler</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">method</span> <span class="o">==</span> <span class="s2">&quot;new&quot;</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">ngenes</span> <span class="o">=</span> <span class="n">mat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span> <span class="o">*</span> <span class="mf">1.1</span>
        <span class="c1"># we model the sampling zeros (dropping 30% of the reads)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">poisson</span><span class="p">((</span><span class="n">mat</span> <span class="o">*</span> <span class="p">(</span><span class="n">dropout</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)))</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="c1"># we model the technical zeros (dropping 50% of the genes)</span>
        <span class="n">notdrop</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">ngenes</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">dropout</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
        <span class="n">mat</span> <span class="o">=</span> <span class="p">(</span><span class="n">mat</span> <span class="o">-</span> <span class="n">res</span><span class="p">)</span> <span class="o">*</span> <span class="n">notdrop</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
            <span class="n">mat</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">mat</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;method </span><span class="si">{</span><span class="n">method</span><span class="si">}</span><span class="s2"> not recognized&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.utils.make_adata" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">make_adata</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>This function creates an AnnData object from the given input parameters.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>embs</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Embeddings of the cells. The shape of the tensor is (n_cells, n_features).</p>
              </div>
            </li>
            <li>
              <b><code>labels</code></b>
                  (<code>list</code>)
              –
              <div class="doc-md-description">
                <p>List of labels for the predicted classes.</p>
              </div>
            </li>
            <li>
              <b><code>pred</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Predicted labels. The shape of the tensor is (n_cells, n_classes). Default is None.</p>
              </div>
            </li>
            <li>
              <b><code>attention</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Attention weights. Default is None.</p>
              </div>
            </li>
            <li>
              <b><code>step</code></b>
                  (<code>int</code>, default:
                      <code>0</code>
)
              –
              <div class="doc-md-description">
                <p>Step number for storing the AnnData without overwriting others. Default is 0.</p>
              </div>
            </li>
            <li>
              <b><code>label_decoders</code></b>
                  (<code>dict</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Dictionary to map class codes to class names. Default is None.</p>
              </div>
            </li>
            <li>
              <b><code>labels_hierarchy</code></b>
                  (<code>dict</code>, default:
                      <code>{}</code>
)
              –
              <div class="doc-md-description">
                <p>Dictionary representing the hierarchy of labels. Default is {}.</p>
              </div>
            </li>
            <li>
              <b><code>gtclass</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Ground truth class. Default is None.</p>
              </div>
            </li>
            <li>
              <b><code>name</code></b>
                  (<code>str</code>, default:
                      <code>&#39;&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Name of the AnnData object. Default is an empty string.</p>
              </div>
            </li>
            <li>
              <b><code>mdir</code></b>
                  (<code>str</code>, default:
                      <code>&#39;/tmp&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>Directory to save the AnnData object. Default is "/tmp".</p>
              </div>
            </li>
            <li>
              <b><code>doplot</code></b>
                  (<code>bool</code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to generate plots. Default is True.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              –
              <div class="doc-md-description">
                <p>anndata.AnnData: The created AnnData object.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">make_adata</span><span class="p">(</span>
    <span class="n">embs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">labels</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">pred</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">step</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">label_decoders</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">labels_hierarchy</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="p">{},</span>
    <span class="n">gtclass</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="n">mdir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;/tmp&quot;</span><span class="p">,</span>
    <span class="n">doplot</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function creates an AnnData object from the given input parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        embs (torch.Tensor): Embeddings of the cells. The shape of the tensor is (n_cells, n_features).</span>
<span class="sd">        labels (list): List of labels for the predicted classes.</span>
<span class="sd">        pred (torch.Tensor, optional): Predicted labels. The shape of the tensor is (n_cells, n_classes). Default is None.</span>
<span class="sd">        attention (torch.Tensor, optional): Attention weights. Default is None.</span>
<span class="sd">        step (int, optional): Step number for storing the AnnData without overwriting others. Default is 0.</span>
<span class="sd">        label_decoders (dict, optional): Dictionary to map class codes to class names. Default is None.</span>
<span class="sd">        labels_hierarchy (dict, optional): Dictionary representing the hierarchy of labels. Default is {}.</span>
<span class="sd">        gtclass (torch.Tensor, optional): Ground truth class. Default is None.</span>
<span class="sd">        name (str, optional): Name of the AnnData object. Default is an empty string.</span>
<span class="sd">        mdir (str, optional): Directory to save the AnnData object. Default is &quot;/tmp&quot;.</span>
<span class="sd">        doplot (bool, optional): Whether to generate plots. Default is True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        anndata.AnnData: The created AnnData object.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">colname</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">pred</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
        <span class="c1"># label decoders is not cls_decoders. one is a dict to map class codes (ints)</span>
        <span class="c1"># to class names the other is the module the predict the class</span>
        <span class="k">if</span> <span class="n">label_decoders</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="p">[</span><span class="n">label_decoders</span><span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]][</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">name</span><span class="p">]</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">obs</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
                <span class="p">]</span>
            <span class="p">)</span><span class="o">.</span><span class="n">T</span>

        <span class="k">if</span> <span class="n">gtclass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">colname</span> <span class="o">+=</span> <span class="n">labels</span>
            <span class="n">nobs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">gtclass</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">label_decoders</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">nobs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="p">[</span><span class="n">label_decoders</span><span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]][</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">name</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">nobs</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
                    <span class="p">]</span>
                <span class="p">)</span><span class="o">.</span><span class="n">T</span>
            <span class="n">obs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">obs</span><span class="p">,</span> <span class="n">nobs</span><span class="p">])</span>

        <span class="n">adata</span> <span class="o">=</span> <span class="n">AnnData</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
            <span class="n">obs</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                <span class="n">obs</span><span class="p">,</span>
                <span class="n">columns</span><span class="o">=</span><span class="n">colname</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">gtclass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">tr</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="n">label</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">label</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">tr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s2">&quot;conv_&quot;</span> <span class="o">+</span> <span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="n">label</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">tr</span><span class="p">)</span>
            <span class="n">tr</span> <span class="o">=</span> <span class="n">translate</span><span class="p">(</span><span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">label</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">label</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">tr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">label</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">tr</span><span class="p">)</span>
            <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="n">label_decoders</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">gtclass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">class_topred</span> <span class="o">=</span> <span class="n">label_decoders</span><span class="p">[</span><span class="n">label</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">labels_hierarchy</span><span class="p">:</span>
                    <span class="n">cur_labels_hierarchy</span> <span class="o">=</span> <span class="p">{</span>
                        <span class="n">label_decoders</span><span class="p">[</span><span class="n">label</span><span class="p">][</span><span class="n">k</span><span class="p">]:</span> <span class="p">[</span><span class="n">label_decoders</span><span class="p">[</span><span class="n">label</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">v</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">labels_hierarchy</span><span class="p">[</span><span class="n">label</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                    <span class="p">}</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">cur_labels_hierarchy</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">for</span> <span class="n">pred</span><span class="p">,</span> <span class="n">true</span> <span class="ow">in</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="p">[[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">label</span><span class="p">,</span> <span class="n">label</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">pred</span> <span class="o">==</span> <span class="n">true</span><span class="p">:</span>
                        <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
                        <span class="k">continue</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels_hierarchy</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">true</span> <span class="ow">in</span> <span class="n">cur_labels_hierarchy</span><span class="p">:</span>
                            <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred</span> <span class="ow">in</span> <span class="n">cur_labels_hierarchy</span><span class="p">[</span><span class="n">true</span><span class="p">])</span>
                        <span class="k">elif</span> <span class="n">true</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">class_topred</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;true label </span><span class="si">{</span><span class="n">true</span><span class="si">}</span><span class="s2"> not in available classes&quot;</span>
                            <span class="p">)</span>
                        <span class="k">elif</span> <span class="n">true</span> <span class="o">!=</span> <span class="s2">&quot;unknown&quot;</span><span class="p">:</span>
                            <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">true</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">class_topred</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;true label </span><span class="si">{</span><span class="n">true</span><span class="si">}</span><span class="s2"> not in available classes&quot;</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">true</span> <span class="o">!=</span> <span class="s2">&quot;unknown&quot;</span><span class="p">:</span>
                        <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">pass</span>
                <span class="n">accuracy</span><span class="p">[</span><span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">adata</span><span class="o">.</span><span class="n">obs</span> <span class="o">=</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;category&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">adata</span> <span class="o">=</span> <span class="n">AnnData</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="kc">False</span><span class="p">:</span>
        <span class="n">adata</span><span class="o">.</span><span class="n">varm</span><span class="p">[</span><span class="s2">&quot;Qs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">attention</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
            <span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">attention</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">attention</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">attention</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">attention</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="n">adata</span><span class="o">.</span><span class="n">varm</span><span class="p">[</span><span class="s2">&quot;Ks&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">attention</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
            <span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">attention</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">attention</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">attention</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">attention</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">adata</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">doplot</span> <span class="ow">and</span> <span class="n">adata</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">100</span> <span class="ow">and</span> <span class="n">pred</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">sc</span><span class="o">.</span><span class="n">pp</span><span class="o">.</span><span class="n">neighbors</span><span class="p">(</span><span class="n">adata</span><span class="p">,</span> <span class="n">use_rep</span><span class="o">=</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
        <span class="n">sc</span><span class="o">.</span><span class="n">tl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span><span class="n">adata</span><span class="p">)</span>
        <span class="n">sc</span><span class="o">.</span><span class="n">tl</span><span class="o">.</span><span class="n">leiden</span><span class="p">(</span><span class="n">adata</span><span class="p">,</span> <span class="n">key_added</span><span class="o">=</span><span class="s2">&quot;sprint_leiden&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">gtclass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">color</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">i</span>
                <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="s2">&quot;conv_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="k">if</span> <span class="s2">&quot;conv_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="o">.</span><span class="n">columns</span> <span class="k">else</span> <span class="n">i</span>
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">labels</span>
                    <span class="p">],</span>
                    <span class="p">[</span>
                        <span class="p">(</span>
                            <span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">i</span>
                            <span class="k">if</span> <span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="o">.</span><span class="n">columns</span>
                            <span class="k">else</span> <span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">i</span>
                        <span class="p">)</span>
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">labels</span>
                    <span class="p">],</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pair</span>
            <span class="p">]</span>
            <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span>
                <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">)</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">color</span><span class="p">):</span>
                <span class="n">sc</span><span class="o">.</span><span class="n">pl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span>
                    <span class="n">adata</span><span class="p">,</span>
                    <span class="n">color</span><span class="o">=</span><span class="n">col</span><span class="p">,</span>
                    <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">],</span>
                    <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">acc</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
                <span class="k">if</span> <span class="s2">&quot;_pred_&quot;</span> <span class="ow">in</span> <span class="n">col</span> <span class="ow">and</span> <span class="n">col</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;conv_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">in</span> <span class="n">accuracy</span><span class="p">:</span>
                    <span class="n">acc</span> <span class="o">=</span> <span class="s2">&quot; (accuracy: </span><span class="si">{:.2f}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">[</span><span class="n">col</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;conv_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
                <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">col</span> <span class="o">+</span> <span class="s2">&quot; UMAP&quot;</span> <span class="o">+</span> <span class="n">acc</span><span class="p">)</span>
                <span class="k">if</span> <span class="s2">&quot;cell_type&quot;</span> <span class="ow">in</span> <span class="n">col</span><span class="p">:</span>
                    <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="s2">&quot;x-small&quot;</span><span class="p">)</span>
                <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;UMAP1&quot;</span><span class="p">)</span>
                <span class="n">axs</span><span class="p">[</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;UMAP2&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">color</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">(</span>
                    <span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">i</span>
                    <span class="k">if</span> <span class="s2">&quot;conv_pred_&quot;</span> <span class="o">+</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">adata</span><span class="o">.</span><span class="n">obs</span><span class="o">.</span><span class="n">columns</span>
                    <span class="k">else</span> <span class="s2">&quot;pred_&quot;</span> <span class="o">+</span> <span class="n">i</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">labels</span>
            <span class="p">]</span>
            <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">color</span><span class="p">)</span> <span class="o">*</span> <span class="mi">8</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">color</span><span class="p">):</span>
                <span class="n">sc</span><span class="o">.</span><span class="n">pl</span><span class="o">.</span><span class="n">umap</span><span class="p">(</span>
                    <span class="n">adata</span><span class="p">,</span>
                    <span class="n">color</span><span class="o">=</span><span class="n">col</span><span class="p">,</span>
                    <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                    <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">acc</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
                <span class="k">if</span> <span class="s2">&quot;_pred_&quot;</span> <span class="ow">in</span> <span class="n">col</span> <span class="ow">and</span> <span class="n">col</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;conv_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">in</span> <span class="n">accuracy</span><span class="p">:</span>
                    <span class="n">acc</span> <span class="o">=</span> <span class="s2">&quot; (accuracy: </span><span class="si">{:.2f}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">[</span><span class="n">col</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;conv_&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
                <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">col</span> <span class="o">+</span> <span class="s2">&quot; UMAP&quot;</span> <span class="o">+</span> <span class="n">acc</span><span class="p">)</span>
                <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;UMAP1&quot;</span><span class="p">)</span>
                <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;UMAP2&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">fig</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">adata</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">mdir</span> <span class="o">+</span> <span class="s2">&quot;/step_&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">step</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.h5ad&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">adata</span><span class="p">,</span> <span class="n">fig</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.utils.simple_masker" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">simple_masker</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Randomly mask a batch of data.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>shape</code></b>
                  (<code>list[int]</code>)
              –
              <div class="doc-md-description">
                <p>The shape of the data.</p>
              </div>
            </li>
            <li>
              <b><code>mask_ratio</code></b>
                  (<code>float</code>, default:
                      <code>0.15</code>
)
              –
              <div class="doc-md-description">
                <p>The ratio of genes to mask, default to 0.15.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="torch.Tensor">Tensor</span></code>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: A tensor of masked data.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">simple_masker</span><span class="p">(</span>
    <span class="n">shape</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">mask_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly mask a batch of data.</span>

<span class="sd">    Args:</span>
<span class="sd">        shape (list[int]): The shape of the data.</span>
<span class="sd">        mask_ratio (float): The ratio of genes to mask, default to 0.15.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: A tensor of masked data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">mask_ratio</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.utils.test" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">test</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Test the given model on the full set of benchmarks and save the results to JSON files.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>model</code></b>
                  (<code><span title="torch.nn.Module">Module</span></code>)
              –
              <div class="doc-md-description">
                <p>The model to be tested.</p>
              </div>
            </li>
            <li>
              <b><code>name</code></b>
                  (<code>str</code>)
              –
              <div class="doc-md-description">
                <p>The name to be used for the output JSON files.</p>
              </div>
            </li>
            <li>
              <b><code>filedir</code></b>
                  (<code>str</code>)
              –
              <div class="doc-md-description">
                <p>The directory where the data files are located.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>None</code>
              –
              <div class="doc-md-description">
                <p>None</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">filedir</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Test the given model on the full set of benchmarks and save the results to JSON files.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (torch.nn.Module): The model to be tested.</span>
<span class="sd">        name (str): The name to be used for the output JSON files.</span>
<span class="sd">        filedir (str): The directory where the data files are located.</span>

<span class="sd">    Returns:</span>
<span class="sd">        None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">embbed_task</span><span class="o">.</span><span class="n">default_benchmark</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">default_dataset</span><span class="o">=</span><span class="s2">&quot;lung&quot;</span><span class="p">,</span> <span class="n">do_class</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">coarse</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;metrics_&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.json&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s2">&quot;embed_lung&quot;</span><span class="p">:</span> <span class="n">res</span><span class="p">},</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;emb_lung/scib&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;scib&quot;</span><span class="p">][</span><span class="s2">&quot;Total&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;emb_lung/ct_class&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">res</span><span class="p">[</span><span class="s2">&quot;classif&quot;</span><span class="p">][</span><span class="s2">&quot;cell_type_ontology_term_id&quot;</span><span class="p">][</span><span class="s2">&quot;accuracy&quot;</span><span class="p">]</span>
            <span class="p">),</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">embbed_task</span><span class="o">.</span><span class="n">default_benchmark</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">default_dataset</span><span class="o">=</span><span class="s2">&quot;pancreas&quot;</span><span class="p">,</span> <span class="n">do_class</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">coarse</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;metrics_&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.json&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s2">&quot;embed_panc&quot;</span><span class="p">:</span> <span class="n">res</span><span class="p">},</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;emb_panc/scib&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;scib&quot;</span><span class="p">][</span><span class="s2">&quot;Total&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;emb_panc/ct_class&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">res</span><span class="p">[</span><span class="s2">&quot;classif&quot;</span><span class="p">][</span><span class="s2">&quot;cell_type_ontology_term_id&quot;</span><span class="p">][</span><span class="s2">&quot;accuracy&quot;</span><span class="p">]</span>
            <span class="p">),</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">denoise_task</span><span class="o">.</span><span class="n">default_benchmark</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">filedir</span> <span class="o">+</span> <span class="s2">&quot;/../../data/gNNpgpo6gATjuxTE7CCp.h5ad&quot;</span>
    <span class="p">)</span>
    <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;denoise/reco2full_vs_noisy2full&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">res</span><span class="p">[</span><span class="s2">&quot;reco2full&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;noisy2full&quot;</span><span class="p">]</span>
            <span class="p">),</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;metrics_&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.json&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s2">&quot;denoise&quot;</span><span class="p">:</span> <span class="n">res</span><span class="p">},</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">grn_task</span><span class="o">.</span><span class="n">default_benchmark</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="s2">&quot;gwps&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span> <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">d_model</span> <span class="o">&lt;=</span> <span class="mi">512</span> <span class="k">else</span> <span class="mi">8</span>
    <span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;metrics_&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.json&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s2">&quot;grn_gwps&quot;</span><span class="p">:</span> <span class="n">res</span><span class="p">},</span> <span class="n">default</span><span class="o">=</span><span class="k">lambda</span> <span class="n">o</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">o</span><span class="p">),</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;grn_gwps/auprc_self&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;self&quot;</span><span class="p">][</span><span class="s2">&quot;auprc&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;grn_gwps/epr_self&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;self&quot;</span><span class="p">][</span><span class="s2">&quot;epr&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;grn_gwps/auprc_omni&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;omni&quot;</span><span class="p">][</span><span class="s2">&quot;auprc&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;grn_gwps/epr_omni&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;omni&quot;</span><span class="p">][</span><span class="s2">&quot;epr&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;grn_gwps/auprc&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">][</span><span class="s2">&quot;auprc&quot;</span><span class="p">]),</span>
            <span class="s2">&quot;grn_gwps/epr&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">][</span><span class="s2">&quot;epr&quot;</span><span class="p">]),</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">grn_task</span><span class="o">.</span><span class="n">default_benchmark</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span> <span class="s2">&quot;sroy&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span> <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">d_model</span> <span class="o">&lt;=</span> <span class="mi">512</span> <span class="k">else</span> <span class="mi">8</span>
    <span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;metrics_&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.json&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s2">&quot;grn_sroy&quot;</span><span class="p">:</span> <span class="n">res</span><span class="p">},</span> <span class="n">default</span><span class="o">=</span><span class="k">lambda</span> <span class="n">o</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">o</span><span class="p">),</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;grn_sroy/auprc_self&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">i</span><span class="p">[</span><span class="s2">&quot;auprc&quot;</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;self_&quot;</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
                            <span class="n">x</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;chip_&quot;</span><span class="p">,</span> <span class="s2">&quot;ko_&quot;</span><span class="p">,</span> <span class="s2">&quot;classifier&quot;</span><span class="p">,</span> <span class="s2">&quot;_base&quot;</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_sroy/epr_self&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">i</span><span class="p">[</span><span class="s2">&quot;epr&quot;</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;self_&quot;</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
                            <span class="n">x</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;chip_&quot;</span><span class="p">,</span> <span class="s2">&quot;ko_&quot;</span><span class="p">,</span> <span class="s2">&quot;classifier&quot;</span><span class="p">,</span> <span class="s2">&quot;_base&quot;</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_sroy/auprc_omni&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">i</span><span class="p">[</span><span class="s2">&quot;auprc&quot;</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;omni_&quot;</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
                            <span class="n">x</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;chip_&quot;</span><span class="p">,</span> <span class="s2">&quot;ko_&quot;</span><span class="p">,</span> <span class="s2">&quot;classifier&quot;</span><span class="p">,</span> <span class="s2">&quot;_base&quot;</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_sroy/epr_omni&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">i</span><span class="p">[</span><span class="s2">&quot;epr&quot;</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;omni_&quot;</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
                            <span class="n">x</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;chip_&quot;</span><span class="p">,</span> <span class="s2">&quot;ko_&quot;</span><span class="p">,</span> <span class="s2">&quot;classifier&quot;</span><span class="p">,</span> <span class="s2">&quot;_base&quot;</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_sroy/auprc&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">i</span><span class="p">[</span><span class="s2">&quot;auprc&quot;</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;mean_&quot;</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
                            <span class="n">x</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;chip_&quot;</span><span class="p">,</span> <span class="s2">&quot;ko_&quot;</span><span class="p">,</span> <span class="s2">&quot;classifier&quot;</span><span class="p">,</span> <span class="s2">&quot;_base&quot;</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_sroy/epr&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">i</span><span class="p">[</span><span class="s2">&quot;epr&quot;</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">k</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;mean_&quot;</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span>
                            <span class="n">x</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;chip_&quot;</span><span class="p">,</span> <span class="s2">&quot;ko_&quot;</span><span class="p">,</span> <span class="s2">&quot;classifier&quot;</span><span class="p">,</span> <span class="s2">&quot;_base&quot;</span><span class="p">]</span>
                        <span class="p">)</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
    <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">grn_task</span><span class="o">.</span><span class="n">default_benchmark</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">filedir</span> <span class="o">+</span> <span class="s2">&quot;/../../data/yBCKp6HmXuHa0cZptMo7.h5ad&quot;</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span> <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">d_model</span> <span class="o">&lt;=</span> <span class="mi">512</span> <span class="k">else</span> <span class="mi">8</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;metrics_&quot;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.json&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">)</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">({</span><span class="s2">&quot;grn_omni&quot;</span><span class="p">:</span> <span class="n">res</span><span class="p">},</span> <span class="n">default</span><span class="o">=</span><span class="k">lambda</span> <span class="n">o</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">o</span><span class="p">),</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">metrics</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;grn_omni/auprc_class&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="s2">&quot;auprc&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_class&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">])</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_omni/epr_class&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="s2">&quot;epr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_class&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">])</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_omni/tf_enr_class&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;TF_enr&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_class&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_omni/tf_targ_enr_class&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">i</span><span class="p">[</span><span class="s2">&quot;significant_enriched_TFtargets&quot;</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="s2">&quot;_class&quot;</span> <span class="ow">in</span> <span class="n">k</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_omni/auprc&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="s2">&quot;auprc&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_mean&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">])</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_omni/epr&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">i</span><span class="p">[</span><span class="s2">&quot;epr&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_mean&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">])</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_omni/tf_enr&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">i</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;TF_enr&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="s2">&quot;_mean&quot;</span> <span class="ow">in</span> <span class="n">k</span><span class="p">])</span>
            <span class="p">),</span>
            <span class="s2">&quot;grn_omni/tf_targ_enr&quot;</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">i</span><span class="p">[</span><span class="s2">&quot;significant_enriched_TFtargets&quot;</span><span class="p">]</span>
                        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                        <span class="k">if</span> <span class="s2">&quot;_mean&quot;</span> <span class="ow">in</span> <span class="n">k</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="c1"># &#39;grn_omni/ct&#39;: res[&#39;classif&#39;][&#39;cell_type_ontology_term_id&#39;][&#39;accuracy&#39;],</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">metrics</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.utils.translate" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">translate</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>translate This function translates the given value based on the specified type.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>val</code></b>
                  (<code>str / list / set / dict / <span title="collections.Counter">Counter</span></code>)
              –
              <div class="doc-md-description">
                <p>The value to be translated.</p>
              </div>
            </li>
            <li>
              <b><code>t</code></b>
                  (<code>str</code>, default:
                      <code>&#39;cell_type_ontology_term_id&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>The type of translation to be performed. Defaults to "cell_type_ontology_term_id".</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>dict</code></b>              –
              <div class="doc-md-description">
                <p>A dictionary with the translated values.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">translate</span><span class="p">(</span>
    <span class="n">val</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">set</span><span class="p">,</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">Counter</span><span class="p">],</span> <span class="n">t</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cell_type_ontology_term_id&quot;</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    translate This function translates the given value based on the specified type.</span>

<span class="sd">    Args:</span>
<span class="sd">        val (str/list/set/dict/Counter): The value to be translated.</span>
<span class="sd">        t (str, optional): The type of translation to be performed. Defaults to &quot;cell_type_ontology_term_id&quot;.</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: A dictionary with the translated values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">t</span> <span class="o">==</span> <span class="s2">&quot;cell_type_ontology_term_id&quot;</span><span class="p">:</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="n">bt</span><span class="o">.</span><span class="n">CellType</span><span class="o">.</span><span class="n">df</span><span class="p">()</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;ontology_id&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">t</span> <span class="o">==</span> <span class="s2">&quot;assay_ontology_term_id&quot;</span><span class="p">:</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="n">bt</span><span class="o">.</span><span class="n">ExperimentalFactor</span><span class="o">.</span><span class="n">df</span><span class="p">()</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;ontology_id&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">t</span> <span class="o">==</span> <span class="s2">&quot;tissue_ontology_term_id&quot;</span><span class="p">:</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="n">bt</span><span class="o">.</span><span class="n">Tissue</span><span class="o">.</span><span class="n">df</span><span class="p">()</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;ontology_id&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">t</span> <span class="o">==</span> <span class="s2">&quot;disease_ontology_term_id&quot;</span><span class="p">:</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="n">bt</span><span class="o">.</span><span class="n">Disease</span><span class="o">.</span><span class="n">df</span><span class="p">()</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;ontology_id&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">t</span> <span class="o">==</span> <span class="s2">&quot;self_reported_ethnicity_ontology_term_id&quot;</span><span class="p">:</span>
        <span class="n">obj</span> <span class="o">=</span> <span class="n">bt</span><span class="o">.</span><span class="n">Ethnicity</span><span class="o">.</span><span class="n">df</span><span class="p">()</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;ontology_id&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">val</span> <span class="o">==</span> <span class="s2">&quot;unknown&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{</span><span class="n">val</span><span class="p">:</span> <span class="n">val</span><span class="p">}</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">val</span><span class="p">:</span> <span class="n">obj</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">val</span><span class="p">][</span><span class="s2">&quot;name&quot;</span><span class="p">]}</span>
    <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">list</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">set</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">obj</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;name&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="s2">&quot;unknown&quot;</span> <span class="k">else</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">val</span><span class="p">)}</span>
    <span class="k">elif</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="nb">dict</span> <span class="ow">or</span> <span class="nb">type</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="ow">is</span> <span class="n">Counter</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">obj</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="s2">&quot;name&quot;</span><span class="p">]</span> <span class="k">if</span> <span class="n">k</span> <span class="o">!=</span> <span class="s2">&quot;unknown&quot;</span> <span class="k">else</span> <span class="n">k</span><span class="p">:</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">val</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.utils.weighted_masker" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">weighted_masker</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Randomly mask a batch of data.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>shape</code></b>
                  (<code>list[int]</code>)
              –
              <div class="doc-md-description">
                <p>The shape of the data.</p>
              </div>
            </li>
            <li>
              <b><code>mask_ratio</code></b>
                  (<code>float</code>, default:
                      <code>0.15</code>
)
              –
              <div class="doc-md-description">
                <p>The ratio of genes to mask, default to 0.15.</p>
              </div>
            </li>
            <li>
              <b><code>mask_value</code></b>
                  (<code>int</code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>The value to mask with, default to -1.</p>
              </div>
            </li>
            <li>
              <b><code>pad_value</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The value of padding in the values, will be kept unchanged.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="torch.Tensor">Tensor</span></code>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: A tensor of masked data.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">weighted_masker</span><span class="p">(</span>
    <span class="n">shape</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">mask_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">,</span>
    <span class="n">mask_prob</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># n_features</span>
    <span class="n">mask_value</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly mask a batch of data.</span>

<span class="sd">    Args:</span>
<span class="sd">        shape (list[int]): The shape of the data.</span>
<span class="sd">        mask_ratio (float): The ratio of genes to mask, default to 0.15.</span>
<span class="sd">        mask_value (int): The value to mask with, default to -1.</span>
<span class="sd">        pad_value (int): The value of padding in the values, will be kept unchanged.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: A tensor of masked data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">loc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span>
            <span class="n">a</span><span class="o">=</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask_ratio</span><span class="p">),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">mask_prob</span>
        <span class="p">)</span>
        <span class="n">m</span><span class="p">[</span><span class="n">loc</span><span class="p">]</span> <span class="o">=</span> <span class="n">mask_value</span>
        <span class="n">mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.utils.zinb_sample" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">zinb_sample</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>mu</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The mean of the Negative Binomial (NB) distribution.</p>
              </div>
            </li>
            <li>
              <b><code>theta</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The dispersion parameter of the NB distribution.</p>
              </div>
            </li>
            <li>
              <b><code>zi_probs</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The zero-inflation probabilities.</p>
              </div>
            </li>
            <li>
              <b><code>sample_shape</code></b>
                  (<code><span title="torch.Size">Size</span></code>, default:
                      <code><span title="torch.Size">Size</span>([])</code>
)
              –
              <div class="doc-md-description">
                <p>The output shape. Defaults to torch.Size([]).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: A sample from the ZINB distribution.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/utils.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">zinb_sample</span><span class="p">(</span>
    <span class="n">mu</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">theta</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">zi_probs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">sample_shape</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]),</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    zinb_sample This function generates a sample from a Zero-Inflated Negative Binomial (ZINB) distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        mu (torch.Tensor): The mean of the Negative Binomial (NB) distribution.</span>
<span class="sd">        theta (torch.Tensor): The dispersion parameter of the NB distribution.</span>
<span class="sd">        zi_probs (torch.Tensor): The zero-inflation probabilities.</span>
<span class="sd">        sample_shape (torch.Size, optional): The output shape. Defaults to torch.Size([]).</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: A sample from the ZINB distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">concentration</span> <span class="o">=</span> <span class="n">theta</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">/</span> <span class="n">mu</span>
    <span class="c1"># Important remark: Gamma is parametrized by the rate = 1/scale!</span>
    <span class="n">gamma_d</span> <span class="o">=</span> <span class="n">Gamma</span><span class="p">(</span><span class="n">concentration</span><span class="o">=</span><span class="n">concentration</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">rate</span><span class="p">)</span>
    <span class="n">p_means</span> <span class="o">=</span> <span class="n">gamma_d</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">sample_shape</span><span class="p">)</span>

    <span class="c1"># Clamping as distributions objects can have buggy behaviors when</span>
    <span class="c1"># their parameters are too high</span>
    <span class="n">l_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">p_means</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1e8</span><span class="p">)</span>
    <span class="n">samp</span> <span class="o">=</span> <span class="n">Poisson</span><span class="p">(</span><span class="n">l_train</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>  <span class="c1"># Shape : (n_samples, n_cells_batch, n_vars)</span>
    <span class="n">is_zero</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">samp</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">zi_probs</span>
    <span class="n">samp_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">is_zero</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">samp</span><span class="p">),</span> <span class="n">samp</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">samp_</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div><h2 id="encoder-and-decoder-modules">encoder and decoder modules</h2>


<div class="doc doc-object doc-module">



<h2 id="scprint.model.encoders" class="doc doc-heading">
            <code>scprint.model.encoders</code>


</h2>

    <div class="doc doc-contents first">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="scprint.model.encoders.CategoryValueEncoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">CategoryValueEncoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>Encodes categorical values into a vector using an embedding layer and layer normalization.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>num_embeddings</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The number of possible values.</p>
              </div>
            </li>
            <li>
              <b><code>embedding_dim</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the output vectors.</p>
              </div>
            </li>
            <li>
              <b><code>padding_idx</code></b>
                  (<code>int</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The index of the padding token. Defaults to None.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: A tensor representing the encoded categorical values.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>      <p>Note: not used in the current version of scprint.</p>

                  <details class="quote">
                    <summary>Source code in <code>scprint/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encodes categorical values into a vector using an embedding layer and layer normalization.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_embeddings (int): The number of possible values.</span>
<span class="sd">        embedding_dim (int): The dimension of the output vectors.</span>
<span class="sd">        padding_idx (int, optional): The index of the padding token. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: A tensor representing the encoded categorical values.</span>

<span class="sd">    Note: not used in the current version of scprint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CategoryValueEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
        <span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.encoders.ContinuousValueEncoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">ContinuousValueEncoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>Encode real number values to a vector using neural nets projection.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the input vectors.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the output of the positional encoding.</p>
              </div>
            </li>
            <li>
              <b><code>max_value</code></b>
                  (<code>int</code>, default:
                      <code>100000</code>
)
              –
              <div class="doc-md-description">
                <p>The maximum value of the input. Defaults to 100_000.</p>
              </div>
            </li>
            <li>
              <b><code>layers</code></b>
                  (<code>int</code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>The number of layers in the encoder. Defaults to 1.</p>
              </div>
            </li>
            <li>
              <b><code>size</code></b>
                  (<code>int</code>, default:
                      <code>1</code>
)
              –
              <div class="doc-md-description">
                <p>The size of the input. Defaults to 1.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: A tensor representing the encoded continuous values.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                  <details class="quote">
                    <summary>Source code in <code>scprint/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">max_value</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100_000</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encode real number values to a vector using neural nets projection.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the input vectors.</span>
<span class="sd">        dropout (float, optional): The dropout rate to apply to the output of the positional encoding.</span>
<span class="sd">        max_value (int, optional): The maximum value of the input. Defaults to 100_000.</span>
<span class="sd">        layers (int, optional): The number of layers in the encoder. Defaults to 1.</span>
<span class="sd">        size (int, optional): The size of the input. Defaults to 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: A tensor representing the encoded continuous values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ContinuousValueEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_value</span> <span class="o">=</span> <span class="n">max_value</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.encoders.ContinuousValueEncoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape [batch_size, seq_len]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/encoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">mask</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [batch_size, seq_len]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># expand last dimension</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># use the mask embedding when x=-1</span>
    <span class="c1"># mask = (x == -1).float()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_value</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">val</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.encoders.DPositionalEncoding" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">DPositionalEncoding</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>The PositionalEncoding module applies a positional encoding to a sequence of vectors.
This is necessary for the Transformer model, which does not have any inherent notion of
position in a sequence. The positional encoding is added to the input embeddings and
allows the model to attend to positions in the sequence.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the input vectors.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>)
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the output of the positional encoding.</p>
              </div>
            </li>
            <li>
              <b><code>max_len</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The maximum length of a sequence that this module can handle.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>      <p>Note: not used in the current version of scprint.</p>

                  <details class="quote">
                    <summary>Source code in <code>scprint/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">max_len_x</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">max_len_y</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">maxvalue_x</span><span class="o">=</span><span class="mf">10000.0</span><span class="p">,</span>
    <span class="n">maxvalue_y</span><span class="o">=</span><span class="mf">10000.0</span><span class="p">,</span>
<span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">DPositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="n">position2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_len_y</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">position1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_len_x</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">half_n</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="mi">2</span>

    <span class="n">div_term2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">half_n</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">maxvalue_y</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">div_term1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">half_n</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">maxvalue_x</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pe1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len_x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">pe2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len_y</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">pe1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="n">half_n</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position1</span> <span class="o">*</span> <span class="n">div_term1</span><span class="p">)</span>
    <span class="n">pe1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="n">half_n</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position1</span> <span class="o">*</span> <span class="n">div_term1</span><span class="p">)</span>
    <span class="n">pe2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">half_n</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position2</span> <span class="o">*</span> <span class="n">div_term2</span><span class="p">)</span>
    <span class="n">pe2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">half_n</span> <span class="p">::</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position2</span> <span class="o">*</span> <span class="n">div_term2</span><span class="p">)</span>
    <span class="c1"># https://github.com/tatp22/multidim-positional-encoding/blob/master/positional_encodings/torch_encodings.py</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;pe1&quot;</span><span class="p">,</span> <span class="n">pe1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;pe2&quot;</span><span class="p">,</span> <span class="n">pe2</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.encoders.DPositionalEncoding.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape [seq_len, batch_size, embedding_dim]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/encoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pos_x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">pos_y</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [seq_len, batch_size, embedding_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe1</span><span class="p">[</span><span class="n">pos_x</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe2</span><span class="p">[</span><span class="n">pos_y</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.encoders.GeneEncoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">GeneEncoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>Encodes gene sequences into a continuous vector space using an embedding layer.</p>
<p>The output is then normalized using a LayerNorm.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>num_embeddings</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The number of possible values.</p>
              </div>
            </li>
            <li>
              <b><code>embedding_dim</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the output vectors.</p>
              </div>
            </li>
            <li>
              <b><code>padding_idx</code></b>
                  (<code>int</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The index of the padding token. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>weights</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The initial weights for the embedding layer. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>)
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the output of the positional encoding. Defaults to 0.1.</p>
              </div>
            </li>
            <li>
              <b><code>freeze</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to freeze the weights of the embedding layer. Defaults to False.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>      <p>Note: not used in the current version of scprint.</p>

                  <details class="quote">
                    <summary>Source code in <code>scprint/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">padding_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">freeze</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Encodes gene sequences into a continuous vector space using an embedding layer.</span>

<span class="sd">    The output is then normalized using a LayerNorm.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_embeddings (int): The number of possible values.</span>
<span class="sd">        embedding_dim (int): The dimension of the output vectors.</span>
<span class="sd">        padding_idx (int, optional): The index of the padding token. Defaults to None.</span>
<span class="sd">        weights (Tensor, optional): The initial weights for the embedding layer. Defaults to None.</span>
<span class="sd">        dropout (float, optional): The dropout rate to apply to the output of the positional encoding. Defaults to 0.1.</span>
<span class="sd">        freeze (bool, optional): Whether to freeze the weights of the embedding layer. Defaults to False.</span>

<span class="sd">    Note: not used in the current version of scprint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">GeneEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
        <span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span><span class="p">,</span> <span class="n">_freeze</span><span class="o">=</span><span class="n">freeze</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># concat a zero vector to the weight</span>
        <span class="c1"># this is to make the embedding of the padding token to be zero</span>
        <span class="c1"># weights = torch.cat(</span>
        <span class="c1">#    [torch.Tensor(weights), torch.zeros(1, embedding_dim)], dim=0</span>
        <span class="c1"># )</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.encoders.PositionalEncoding" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">PositionalEncoding</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>The PositionalEncoding module applies a positional encoding to a sequence of vectors.
This is necessary for the Transformer model, which does not have any inherent notion of
position in a sequence. The positional encoding is added to the input embeddings and
allows the model to attend to positions in the sequence.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the input vectors.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>)
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the output of the positional encoding.</p>
              </div>
            </li>
            <li>
              <b><code>max_len</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The maximum length of a sequence that this module can handle.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>      <p>Note: not used in the current version of scprint.</p>

                  <details class="quote">
                    <summary>Source code in <code>scprint/model/encoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">max_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">token_to_pos</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>  <span class="c1"># [token, pos]</span>
    <span class="n">maxval</span><span class="o">=</span><span class="mf">10000.0</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The PositionalEncoding module applies a positional encoding to a sequence of vectors.</span>
<span class="sd">    This is necessary for the Transformer model, which does not have any inherent notion of</span>
<span class="sd">    position in a sequence. The positional encoding is added to the input embeddings and</span>
<span class="sd">    allows the model to attend to positions in the sequence.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the input vectors.</span>
<span class="sd">        dropout (float, optional): The dropout rate to apply to the output of the positional encoding.</span>
<span class="sd">        max_len (int, optional): The maximum length of a sequence that this module can handle.</span>

<span class="sd">    Note: not used in the current version of scprint.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_len</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Create a dictionary to convert token to position</span>

    <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">maxval</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="c1"># we reorder them and map them to gene_id (position)</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">token_to_pos</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">arr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pe</span><span class="p">[</span><span class="n">v</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">arr</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;pe&quot;</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.encoders.PositionalEncoding.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
              –
              <div class="doc-md-description">
                <p>Tensor, shape [seq_len, batch_size, embedding_dim]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/encoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gene_pos</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [seq_len, batch_size, embedding_dim]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">gene_pos</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
        <span class="n">gene_pos</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="scprint.model.decoders" class="doc doc-heading">
            <code>scprint.model.decoders</code>


</h2>

    <div class="doc doc-contents first">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="scprint.model.decoders.ClsDecoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">ClsDecoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>ClsDecoder Decoder for classification task.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>int, dimension of the input.</p>
              </div>
            </li>
            <li>
              <b><code>n_cls</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>int, number of classes.</p>
              </div>
            </li>
            <li>
              <b><code>layers</code></b>
                  (<code>list[int]</code>, default:
                      <code>[256, 128]</code>
)
              –
              <div class="doc-md-description">
                <p>list[int], list of hidden layers.</p>
              </div>
            </li>
            <li>
              <b><code>activation</code></b>
                  (<code><span title="typing.Callable">Callable</span></code>, default:
                      <code><span title="torch.nn.ReLU">ReLU</span></code>
)
              –
              <div class="doc-md-description">
                <p>nn.Module, activation function.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>float, dropout rate.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              –
              <div class="doc-md-description">
                <p>Tensor, shape [batch_size, n_cls]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                  <details class="quote">
                    <summary>Source code in <code>scprint/model/decoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_cls</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">],</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ClsDecoder Decoder for classification task.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model: int, dimension of the input.</span>
<span class="sd">        n_cls: int, number of classes.</span>
<span class="sd">        layers: list[int], list of hidden layers.</span>
<span class="sd">        activation: nn.Module, activation function.</span>
<span class="sd">        dropout: float, dropout rate.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor, shape [batch_size, n_cls]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ClsDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="c1"># module list</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">d_model</span><span class="p">]</span> <span class="o">+</span> <span class="n">layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">l</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">l</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_cls</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.decoders.ClsDecoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape [batch_size, embsize]</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/decoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: Tensor, shape [batch_size, embsize]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.decoders.ExprDecoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">ExprDecoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>ExprDecoder Decoder for the gene expression prediction.</p>
<p>Will output the mean, variance and zero logits, parameters of a zero inflated negative binomial distribution.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the model. This is the size of the input feature vector.</p>
              </div>
            </li>
            <li>
              <b><code>nfirst_tokens_to_skip</code></b>
                  (<code>int</code>, default:
                      <code>0</code>
)
              –
              <div class="doc-md-description">
                <p>The number of initial labels to skip in the sequence. Defaults to 0.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>The dropout rate applied during training to prevent overfitting. Defaults to 0.1.</p>
              </div>
            </li>
            <li>
              <b><code>zinb</code></b>
                  (<code>bool</code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use a zero inflated negative binomial distribution. Defaults to True.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                  <details class="quote">
                    <summary>Source code in <code>scprint/model/decoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">nfirst_tokens_to_skip</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">zinb</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ExprDecoder Decoder for the gene expression prediction.</span>

<span class="sd">    Will output the mean, variance and zero logits, parameters of a zero inflated negative binomial distribution.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the model. This is the size of the input feature vector.</span>
<span class="sd">        nfirst_tokens_to_skip (int, optional): The number of initial labels to skip in the sequence. Defaults to 0.</span>
<span class="sd">        dropout (float, optional): The dropout rate applied during training to prevent overfitting. Defaults to 0.1.</span>
<span class="sd">        zinb (bool, optional): Whether to use a zero inflated negative binomial distribution. Defaults to True.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ExprDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nfirst_tokens_to_skip</span> <span class="o">=</span> <span class="n">nfirst_tokens_to_skip</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">3</span> <span class="k">if</span> <span class="n">zinb</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span> <span class="o">=</span> <span class="n">zinb</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.decoders.ExprDecoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>x is the output of the transformer, (batch, seq_len, d_model)</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/decoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;x is the output of the transformer, (batch, seq_len, d_model)&quot;&quot;&quot;</span>
    <span class="c1"># we don&#39;t do it on the labels</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nfirst_tokens_to_skip</span> <span class="p">:,</span> <span class="p">:])</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">zinb</span><span class="p">:</span>
        <span class="n">pred_value</span><span class="p">,</span> <span class="n">var_value</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># (batch, seq_len)</span>
        <span class="c1"># The sigmoid function is used to map the zero_logits to a probability between 0 and 1.</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">mean</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred_value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">disp</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">var_value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="nb">max</span><span class="o">=</span><span class="mi">15</span><span class="p">)),</span>
            <span class="n">zero_logits</span><span class="o">=</span><span class="n">zero_logits</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pred_value</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred_value</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.decoders.GraphSDEExprDecoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">GraphSDEExprDecoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>Initialize the ExprNeuralSDEDecoder module.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the model.</p>
              </div>
            </li>
            <li>
              <b><code>drift</code></b>
                  (<code><span title="torch.nn.Module">Module</span></code>)
              –
              <div class="doc-md-description">
                <p>The drift component of the SDE.</p>
              </div>
            </li>
            <li>
              <b><code>diffusion</code></b>
                  (<code><span title="torch.nn.Module">Module</span></code>)
              –
              <div class="doc-md-description">
                <p>The diffusion component of the SDE.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                  <details class="quote">
                    <summary>Source code in <code>scprint/model/decoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">drift</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">diffusion</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initialize the ExprNeuralSDEDecoder module.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the model.</span>
<span class="sd">        drift (nn.Module): The drift component of the SDE.</span>
<span class="sd">        diffusion (nn.Module): The diffusion component of the SDE.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drift</span> <span class="o">=</span> <span class="n">drift</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">diffusion</span> <span class="o">=</span> <span class="n">diffusion</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.decoders.MVCDecoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">MVCDecoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>MVCDecoder Decoder for the masked value prediction for cell embeddings.</p>
<p>Will use the gene embeddings with the cell embeddings to predict the mean, variance and zero logits</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
              –
              <div class="doc-md-description">
                <p>obj:<code>int</code>): dimension of the gene embedding.</p>
              </div>
            </li>
            <li>
              <b><code>arch_style</code></b>
              –
              <div class="doc-md-description">
                <p>obj:<code>str</code>): architecture style of the decoder, choice from
1. "inner product" or 2. "cell product" 3. "concat query" or 4. "sum query".</p>
              </div>
            </li>
            <li>
              <b><code>query_activation</code></b>
              –
              <div class="doc-md-description">
                <p>obj:<code>nn.Module</code>): activation function for the query
vectors. Defaults to nn.Sigmoid.</p>
              </div>
            </li>
            <li>
              <b><code>hidden_activation</code></b>
              –
              <div class="doc-md-description">
                <p>obj:<code>nn.Module</code>): activation function for the hidden
layers. Defaults to nn.PReLU.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                  <details class="quote">
                    <summary>Source code in <code>scprint/model/decoders.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">arch_style</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;inner product&quot;</span><span class="p">,</span>
    <span class="n">tot_labels</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">query_activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">,</span>
    <span class="n">hidden_activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">PReLU</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MVCDecoder Decoder for the masked value prediction for cell embeddings.</span>

<span class="sd">    Will use the gene embeddings with the cell embeddings to predict the mean, variance and zero logits</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (:obj:`int`): dimension of the gene embedding.</span>
<span class="sd">        arch_style (:obj:`str`): architecture style of the decoder, choice from</span>
<span class="sd">            1. &quot;inner product&quot; or 2. &quot;cell product&quot; 3. &quot;concat query&quot; or 4. &quot;sum query&quot;.</span>
<span class="sd">        query_activation (:obj:`nn.Module`): activation function for the query</span>
<span class="sd">            vectors. Defaults to nn.Sigmoid.</span>
<span class="sd">        hidden_activation (:obj:`nn.Module`): activation function for the hidden</span>
<span class="sd">            layers. Defaults to nn.PReLU.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MVCDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;inner product&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span> <span class="o">=</span> <span class="n">query_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;concat query&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span> <span class="o">=</span> <span class="n">query_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">tot_labels</span><span class="p">),</span> <span class="n">d_model</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span> <span class="o">=</span> <span class="n">hidden_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;sum query&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span> <span class="o">=</span> <span class="n">query_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span> <span class="o">=</span> <span class="n">hidden_activation</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown arch_style: </span><span class="si">{</span><span class="n">arch_style</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">arch_style</span> <span class="o">=</span> <span class="n">arch_style</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">do_detach</span> <span class="o">=</span> <span class="n">arch_style</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;detach&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.decoders.MVCDecoder.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>cell_emb</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape (batch, embsize=d_model)</p>
              </div>
            </li>
            <li>
              <b><code>gene_embs</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Tensor, shape (batch, seq_len, embsize=d_model)</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/decoders.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">cell_emb</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">gene_embs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        cell_emb: Tensor, shape (batch, embsize=d_model)</span>
<span class="sd">        gene_embs: Tensor, shape (batch, seq_len, embsize=d_model)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;inner product&quot;</span><span class="p">:</span>
        <span class="n">query_vecs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span><span class="p">(</span><span class="n">gene_embs</span><span class="p">)))</span>
        <span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pred_var_zero</span><span class="p">(</span><span class="n">query_vecs</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">cell_emb</span> <span class="o">=</span> <span class="n">cell_emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">cell_emb</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="n">cell_emb</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">zero_logits</span><span class="p">,</span> <span class="n">cell_emb</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># zero logits need to based on the cell_emb, because of input exprs</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;concat query&quot;</span><span class="p">:</span>
        <span class="n">query_vecs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span><span class="p">(</span><span class="n">gene_embs</span><span class="p">))</span>
        <span class="c1"># expand cell_emb to (batch, seq_len, embsize)</span>
        <span class="n">cell_emb</span> <span class="o">=</span> <span class="n">cell_emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">gene_embs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cell_emb</span><span class="p">,</span> <span class="n">query_vecs</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">arch_style</span> <span class="o">==</span> <span class="s2">&quot;sum query&quot;</span><span class="p">:</span>
        <span class="n">query_vecs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">query_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gene2query</span><span class="p">(</span><span class="n">gene_embs</span><span class="p">))</span>
        <span class="n">cell_emb</span> <span class="o">=</span> <span class="n">cell_emb</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">cell_emb</span> <span class="o">+</span> <span class="n">query_vecs</span><span class="p">))</span>
        <span class="n">pred</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">zero_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">mvc_mean</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">mvc_disp</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">var</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">15</span><span class="p">)),</span>
        <span class="n">mvc_zero_logits</span><span class="o">=</span><span class="n">zero_logits</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div><h2 id="flashattention">flashattention</h2>


<div class="doc doc-object doc-module">



<h2 id="scprint.model.flash_attn.flashformer" class="doc doc-heading">
            <code>scprint.model.flash_attn.flashformer</code>


</h2>

    <div class="doc doc-contents first">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="scprint.model.flash_attn.flashformer.FlashTransformerEncoder" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">FlashTransformerEncoder</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>FlashTransformerEncoder a transformer encoder with flash attention.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>d_model</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The dimension of the input vectors.</p>
              </div>
            </li>
            <li>
              <b><code>nhead</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The number of attention heads.</p>
              </div>
            </li>
            <li>
              <b><code>nlayers</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>The number of layers in the transformer.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>, default:
                      <code>0.1</code>
)
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the output of the positional encoding. Defaults to 0.1.</p>
              </div>
            </li>
            <li>
              <b><code>residual_in_fp32</code></b>
                  (<code>bool</code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to force the residual to be in fp32 format. Defaults to True.</p>
              </div>
            </li>
            <li>
              <b><code>num_heads_kv</code></b>
                  (<code>_type_</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The number of heads for key/value. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>checkpointing</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use gradient checkpointing. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>fused_dropout_add_ln</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to fuse dropout, addition and layer normalization operations. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>return_residual</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to return the residual. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>prenorm</code></b>
                  (<code>bool</code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use pre-normalization. Defaults to True.</p>
              </div>
            </li>
            <li>
              <b><code>mlp_ratio</code></b>
                  (<code>float</code>, default:
                      <code>4.0</code>
)
              –
              <div class="doc-md-description">
                <p>The ratio for MLP. Defaults to 4.0.</p>
              </div>
            </li>
            <li>
              <b><code>fused_mlp</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use fused MLP. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>fused_bias_fc</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to fuse bias and fully connected layers. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>sequence_parallel</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use sequence parallelism. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>drop_path_rate</code></b>
                  (<code>float</code>, default:
                      <code>0.0</code>
)
              –
              <div class="doc-md-description">
                <p>The drop path rate. Defaults to 0.0.</p>
              </div>
            </li>
            <li>
              <b><code>weight_init</code></b>
                  (<code>str</code>, default:
                      <code>&#39;&#39;</code>
)
              –
              <div class="doc-md-description">
                <p>The weight initialization method. Defaults to "".</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Raises:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code>ImportError</code>
              –
              <div class="doc-md-description">
                <p>Raised when Triton is not installed but fused_dropout_add_ln is set to True.</p>
              </div>
            </li>
            <li>
                  <code>NotImplementedError</code>
              –
              <div class="doc-md-description">
                <p>Raised when an unsupported operation is attempted.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                  <details class="quote">
                    <summary>Source code in <code>scprint/model/flash_attn/flashformer.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">nhead</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">nlayers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">residual_in_fp32</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">num_heads_kv</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">checkpointing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fused_dropout_add_ln</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">return_residual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">prenorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">4.0</span><span class="p">,</span>
    <span class="n">fused_mlp</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fused_bias_fc</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sequence_parallel</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">drop_path_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">use_flash_attn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">weight_init</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    FlashTransformerEncoder a transformer encoder with flash attention.</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): The dimension of the input vectors.</span>
<span class="sd">        nhead (int): The number of attention heads.</span>
<span class="sd">        nlayers (int): The number of layers in the transformer.</span>
<span class="sd">        dropout (float, optional): The dropout rate to apply to the output of the positional encoding. Defaults to 0.1.</span>
<span class="sd">        residual_in_fp32 (bool, optional): Whether to force the residual to be in fp32 format. Defaults to True.</span>
<span class="sd">        num_heads_kv (_type_, optional): The number of heads for key/value. Defaults to None.</span>
<span class="sd">        checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.</span>
<span class="sd">        fused_dropout_add_ln (bool, optional): Whether to fuse dropout, addition and layer normalization operations. Defaults to False.</span>
<span class="sd">        return_residual (bool, optional): Whether to return the residual. Defaults to False.</span>
<span class="sd">        prenorm (bool, optional): Whether to use pre-normalization. Defaults to True.</span>
<span class="sd">        mlp_ratio (float, optional): The ratio for MLP. Defaults to 4.0.</span>
<span class="sd">        fused_mlp (bool, optional): Whether to use fused MLP. Defaults to False.</span>
<span class="sd">        fused_bias_fc (bool, optional): Whether to fuse bias and fully connected layers. Defaults to False.</span>
<span class="sd">        sequence_parallel (bool, optional): Whether to use sequence parallelism. Defaults to False.</span>
<span class="sd">        drop_path_rate (float, optional): The drop path rate. Defaults to 0.0.</span>
<span class="sd">        weight_init (str, optional): The weight initialization method. Defaults to &quot;&quot;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ImportError: Raised when Triton is not installed but fused_dropout_add_ln is set to True.</span>
<span class="sd">        NotImplementedError: Raised when an unsupported operation is attempted.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">FlashTransformerEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
    <span class="n">dpr</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">x</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="p">,</span> <span class="n">nlayers</span><span class="p">)</span>
    <span class="p">]</span>  <span class="c1"># stochastic depth decay rule</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nlayers</span><span class="p">):</span>
        <span class="n">mlp</span> <span class="o">=</span> <span class="n">create_mlp_cls</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">,</span> <span class="n">fused_mlp</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
            <span class="n">MHA</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">nhead</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">use_flash_attn</span><span class="o">=</span><span class="n">use_flash_attn</span><span class="p">,</span>
            <span class="n">num_heads_kv</span><span class="o">=</span><span class="n">num_heads_kv</span><span class="p">,</span>
            <span class="n">checkpointing</span><span class="o">=</span><span class="n">checkpointing</span><span class="p">,</span>
            <span class="n">fused_bias_fc</span><span class="o">=</span><span class="n">fused_bias_fc</span><span class="p">,</span>
            <span class="n">layer_idx</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># or use parallelBlock where attn &amp; MLP are done in parallel</span>
        <span class="n">encoder_layers</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="n">attention</span><span class="p">,</span>
            <span class="n">mlp</span><span class="p">,</span>
            <span class="n">prenorm</span><span class="o">=</span><span class="n">prenorm</span><span class="p">,</span>
            <span class="c1"># need to set it here for now although it hinders some performances as it returns the residual and I need to see what to do with it</span>
            <span class="c1"># TD [2022-07-30]: Force residual in fp32, seems to make fp16 training more stable</span>
            <span class="n">residual_in_fp32</span><span class="o">=</span><span class="n">residual_in_fp32</span><span class="p">,</span>
            <span class="n">sequence_parallel</span><span class="o">=</span><span class="n">sequence_parallel</span><span class="p">,</span>  <span class="c1"># for more parallelism</span>
            <span class="n">resid_dropout1</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">resid_dropout2</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">drop_path1</span><span class="o">=</span><span class="n">dpr</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
            <span class="n">drop_path2</span><span class="o">=</span><span class="n">dpr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
            <span class="n">fused_dropout_add_ln</span><span class="o">=</span><span class="n">fused_dropout_add_ln</span><span class="p">,</span>
            <span class="n">return_residual</span><span class="o">=</span><span class="n">return_residual</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">encoder_layers</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">prenorm</span> <span class="o">=</span> <span class="n">prenorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop_path</span> <span class="o">=</span> <span class="n">StochasticDepth</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dpr</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;row&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">fused_dropout_add_ln</span> <span class="o">=</span> <span class="n">fused_dropout_add_ln</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fused_dropout_add_ln</span> <span class="ow">and</span> <span class="n">layer_norm_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Triton is not installed&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">sequence_parallel</span><span class="p">:</span>
        <span class="c1"># This seems to only be important when doing tensor parallelism across GPUs, to increase even more the context length I guess?</span>
        <span class="c1"># not really necessary here I think</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;sequence_parallel not implemented yet&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">init_weights</span><span class="p">(</span><span class="n">weight_init</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="scprint.model.flash_attn.mha" class="doc doc-heading">
            <code>scprint.model.flash_attn.mha</code>


</h2>

    <div class="doc doc-contents first">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="scprint.model.flash_attn.mha.CrossAttention" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">CrossAttention</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>Implement the scaled dot product attention with softmax.</p>
<p>Args
    softmax_scale: The temperature to use for the softmax attention. Default to 1/sqrt(d_keys) where d_keys is computed at runtime
    attention_dropout: The dropout rate to apply to the attention. default to 0.0.</p>

                  <details class="quote">
                    <summary>Source code in <code>scprint/model/flash_attn/mha.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">softmax_scale</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">=</span> <span class="n">softmax_scale</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">attention_dropout</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.flash_attn.mha.CrossAttention.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>Implements the multihead softmax attention.</p>
<p>Args
    q: The tensor containing the query. (B, Sq, H, D)
    kv: The tensor containing the key and value. (B, Sk, 2, H_k, D)
    causal: if passed, will override self.causal
    key_padding_mask: boolean mask to apply to the attention weights. True means to keep,
        False means to mask out. (B, Sk)</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/flash_attn/mha.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">kv</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Implements the multihead softmax attention.</span>

<span class="sd">    Args</span>
<span class="sd">        q: The tensor containing the query. (B, Sq, H, D)</span>
<span class="sd">        kv: The tensor containing the key and value. (B, Sk, 2, H_k, D)</span>
<span class="sd">        causal: if passed, will override self.causal</span>
<span class="sd">        key_padding_mask: boolean mask to apply to the attention weights. True means to keep,</span>
<span class="sd">            False means to mask out. (B, Sk)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seqlen_q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">causal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">causal</span> <span class="k">if</span> <span class="n">causal</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">causal</span>
    <span class="n">seqlen_k</span> <span class="o">=</span> <span class="n">kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">batch_size</span> <span class="ow">and</span> <span class="n">kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">==</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">!=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>  <span class="c1"># MQA/GQA</span>
        <span class="n">kv</span> <span class="o">=</span> <span class="n">repeat</span><span class="p">(</span><span class="n">kv</span><span class="p">,</span> <span class="s2">&quot;... hkv d -&gt; ... (hkv g) d&quot;</span><span class="p">,</span> <span class="n">g</span><span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">//</span> <span class="n">kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
    <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">kv</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">softmax_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="ow">or</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bthd,bshd-&gt;bhts&quot;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="o">*</span> <span class="n">softmax_scale</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
            <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seqlen_k</span><span class="p">),</span>
            <span class="o">-</span><span class="mf">10000.0</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">scores</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">padding_mask</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">key_padding_mask</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
        <span class="c1"># TD [2022-09-30]: Adding is faster than masked_fill_ (idk why, just better kernel I guess)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="s2">&quot;b s -&gt; b 1 1 s&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">causal</span><span class="p">:</span>
        <span class="c1"># causal mask needs to take into account the difference between seqlen_q and seqlen_k</span>
        <span class="n">row_idx</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seqlen_q</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">),</span> <span class="s2">&quot;s -&gt; s 1&quot;</span>
        <span class="p">)</span>
        <span class="n">col_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seqlen_k</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">kv</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
        <span class="n">sk</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">seqlen_k</span>
            <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">key_padding_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="s2">&quot;b -&gt; b 1 1 1&quot;</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">col_idx</span> <span class="o">&gt;</span> <span class="n">row_idx</span> <span class="o">+</span> <span class="n">sk</span> <span class="o">-</span> <span class="n">seqlen_q</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">causal_mask</span><span class="p">,</span> <span class="o">-</span><span class="mf">10000.0</span><span class="p">)</span>
    <span class="n">attention</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">attention_drop</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">attention</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bhts,bshd-&gt;bthd&quot;</span><span class="p">,</span> <span class="n">attention_drop</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.flash_attn.mha.FlashCrossAttention" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">FlashCrossAttention</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>Implement the scaled dot product attention with softmax.</p>
<p>Args
    softmax_scale: The temperature to use for the softmax attention.
        (default: 1/sqrt(d_keys) where d_keys is computed at
        runtime)
    attention_dropout: The dropout rate to apply to the attention
        (default: 0.0)</p>

                  <details class="quote">
                    <summary>Source code in <code>scprint/model/flash_attn/mha.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">softmax_scale</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">alibi_slopes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the scaled dot product attention with softmax.</span>

<span class="sd">    Args</span>
<span class="sd">        softmax_scale: The temperature to use for the softmax attention.</span>
<span class="sd">            (default: 1/sqrt(d_keys) where d_keys is computed at</span>
<span class="sd">            runtime)</span>
<span class="sd">        attention_dropout: The dropout rate to apply to the attention</span>
<span class="sd">            (default: 0.0)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">flash_attn_kvpacked_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;FlashAttention is not installed&quot;</span>
    <span class="k">assert</span> <span class="n">flash_attn_kvpacked_func</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;FlashAttention is not installed&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">=</span> <span class="n">softmax_scale</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">attention_dropout</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;alibi_slopes&quot;</span><span class="p">,</span> <span class="n">alibi_slopes</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="n">deterministic</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.flash_attn.mha.FlashCrossAttention.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>Implements the multihead softmax attention.</p>
<p>Args
    q: The tensor containing the query. (B, Sq, H, D)
    kv: The tensor containing the key and value. (B, Sk, 2, H_k, D)
    causal: if passed, will override self.causal
    cu_seqlens: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths
        of the sequences in the batch, used to index into q.
    max_seqlen: int. Maximum sequence length in the batch of q.
    cu_seqlens_k: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths
        of the sequences in the batch, used to index into kv.
    max_seqlen_k: int. Maximum sequence length in the batch of k and v.</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/flash_attn/mha.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">q</span><span class="p">,</span>
    <span class="n">kv</span><span class="p">,</span>
    <span class="n">causal</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cu_seqlens</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">max_seqlen</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">cu_seqlens_k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">max_seqlen_k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements the multihead softmax attention.</span>

<span class="sd">    Args</span>
<span class="sd">        q: The tensor containing the query. (B, Sq, H, D)</span>
<span class="sd">        kv: The tensor containing the key and value. (B, Sk, 2, H_k, D)</span>
<span class="sd">        causal: if passed, will override self.causal</span>
<span class="sd">        cu_seqlens: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths</span>
<span class="sd">            of the sequences in the batch, used to index into q.</span>
<span class="sd">        max_seqlen: int. Maximum sequence length in the batch of q.</span>
<span class="sd">        cu_seqlens_k: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths</span>
<span class="sd">            of the sequences in the batch, used to index into kv.</span>
<span class="sd">        max_seqlen_k: int. Maximum sequence length in the batch of k and v.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">q</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">q</span><span class="o">.</span><span class="n">is_cuda</span> <span class="ow">and</span> <span class="n">kv</span><span class="o">.</span><span class="n">is_cuda</span>
    <span class="n">causal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">causal</span> <span class="k">if</span> <span class="n">causal</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">causal</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">batch_size</span> <span class="ow">and</span> <span class="n">kv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">==</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">flash_attn_kvpacked_func</span><span class="p">(</span>
        <span class="n">q</span><span class="p">,</span>
        <span class="n">kv</span><span class="p">,</span>
        <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># self.drop.p if self.training else 0.0,</span>
        <span class="n">causal</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span><span class="p">,</span>
        <span class="c1"># alibi_slopes=self.alibi_slopes,</span>
        <span class="c1"># deterministic=self.deterministic,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.flash_attn.mha.FlashSelfAttention" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">FlashSelfAttention</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>Implement the scaled dot product attention with softmax.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>softmax_scale</code></b>
                  (<code>float</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The temperature to use for the softmax attention.
(default: 1/sqrt(d_keys) where d_keys is computed at
runtime)</p>
              </div>
            </li>
            <li>
              <b><code>attention_dropout</code></b>
                  (<code>float</code>, default:
                      <code>0.0</code>
)
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the attention
(default: 0.0)</p>
              </div>
            </li>
            <li>
              <b><code>causal</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to use causal attention. Defaults to False.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                  <details class="quote">
                    <summary>Source code in <code>scprint/model/flash_attn/mha.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">softmax_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">alibi_slopes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_triton</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Implement the scaled dot product attention with softmax.</span>

<span class="sd">    Args:</span>
<span class="sd">        softmax_scale (float, optional): The temperature to use for the softmax attention.</span>
<span class="sd">            (default: 1/sqrt(d_keys) where d_keys is computed at</span>
<span class="sd">            runtime)</span>
<span class="sd">        attention_dropout (float, optional): The dropout rate to apply to the attention</span>
<span class="sd">            (default: 0.0)</span>
<span class="sd">        causal (bool, optional): Whether to use causal attention. Defaults to False.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">flash_attn_qkvpacked_func</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FlashAttention is not installed, using triton instead&quot;</span><span class="p">)</span>
        <span class="n">use_triton</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_triton</span> <span class="o">=</span> <span class="n">use_triton</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">=</span> <span class="n">softmax_scale</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.flash_attn.mha.FlashSelfAttention.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>Implements the multihead softmax attention.</p>
<p>Args
    qkv (Tensor): The tensor containing the query, key, and value.
        If cu_seqlens is None and max_seqlen is None, then qkv has shape (B, S, 3, H, D).
        If cu_seqlens is not None and max_seqlen is not None, then qkv has shape
        (total, 3, H, D), where total is the sum of the sequence lengths in the batch.
    causal (bool): if passed, will override self.causal
    cu_seqlens (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths
        of the sequences in the batch, used to index into qkv.
    max_seqlen (int). Maximum sequence length in the batch.
Returns:
    out: (total, H, D) if cu_seqlens is not None and max_seqlen is not None,
        else (B, S, H, D).</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/flash_attn/mha.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">qkv</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">causal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cu_seqlens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_seqlen</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cu_seqlens_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_seqlen_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Implements the multihead softmax attention.</span>

<span class="sd">    Args</span>
<span class="sd">        qkv (Tensor): The tensor containing the query, key, and value.</span>
<span class="sd">            If cu_seqlens is None and max_seqlen is None, then qkv has shape (B, S, 3, H, D).</span>
<span class="sd">            If cu_seqlens is not None and max_seqlen is not None, then qkv has shape</span>
<span class="sd">            (total, 3, H, D), where total is the sum of the sequence lengths in the batch.</span>
<span class="sd">        causal (bool): if passed, will override self.causal</span>
<span class="sd">        cu_seqlens (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths</span>
<span class="sd">            of the sequences in the batch, used to index into qkv.</span>
<span class="sd">        max_seqlen (int). Maximum sequence length in the batch.</span>
<span class="sd">    Returns:</span>
<span class="sd">        out: (total, H, D) if cu_seqlens is not None and max_seqlen is not None,</span>
<span class="sd">            else (B, S, H, D).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">qkv</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">qkv</span><span class="o">.</span><span class="n">is_cuda</span>
    <span class="n">causal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">causal</span> <span class="k">if</span> <span class="n">causal</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">causal</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_triton</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;OpenAI&#39;s flashattention is not implemented&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">qkv</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">qkv</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="c1"># return triton_attention(</span>
        <span class="c1">#    qkv[:, :, 0],</span>
        <span class="c1">#    qkv[:, :, 1],</span>
        <span class="c1">#    qkv[:, :, 2],</span>
        <span class="c1">#    bias,</span>
        <span class="c1">#    causal,</span>
        <span class="c1">#    self.softmax_scale,</span>
        <span class="c1"># )</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">flash_attn_qkvpacked_func</span><span class="p">(</span>
            <span class="n">qkv</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="c1"># self.drop.p if self.training else 0.0,</span>
            <span class="n">causal</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.flash_attn.mha.LinearResidual" class="doc doc-heading">
            <code>LinearResidual</code>


</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Linear">Linear</span></code></p>


      <p>Wrap nn.Linear to return the residual as well. For compatibility with FusedDense.</p>




  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.flash_attn.mha.MHA" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">MHA</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>MHA Multi-head self-attention and cross-attention</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>num_heads_kv</code></b>
                  (<code>int</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>can be used to toggle MQA / GQA. If None, use num_heads.</p>
              </div>
            </li>
            <li>
              <b><code>return_residual</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>whether to return the input x along with the output. This is for
performance reason: for post-norm architecture, returning the input allows us
to fuse the backward of nn.Linear with the residual connection.
Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>checkpointing</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>whether to use checkpointing to save memory.
Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>num_heads_kv</code></b>
                  (<code>int</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>can be used to toggle MQA / GQA. If None, use num_heads.</p>
              </div>
            </li>
            <li>
              <b><code>cross_attn</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>whether to use cross-attention. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>qkv_proj_bias</code></b>
                  (<code>bool</code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>whether to use bias in the query, key, value projection. Defaults to True.</p>
              </div>
            </li>
            <li>
              <b><code>out_proj_bias</code></b>
                  (<code>bool</code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>whether to use bias in the output projection. Defaults to True.</p>
              </div>
            </li>
            <li>
              <b><code>dropout</code></b>
                  (<code>float</code>, default:
                      <code>0.0</code>
)
              –
              <div class="doc-md-description">
                <p>dropout rate. Defaults to 0.0.</p>
              </div>
            </li>
            <li>
              <b><code>softmax_scale</code></b>
                  (<code>float</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The temperature to use for the softmax attention.</p>
              </div>
            </li>
            <li>
              <b><code>causal</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>whether to use causal attention. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>layer_idx</code></b>
                  (<code>int</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>layer index for inference cache. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>dwconv</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>whether to use depthwise convolution. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>fused_bias_fc</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>whether to use fused_bias_fc. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>use_flash_attn</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>whether to use FlashAttention. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>device</code></b>
                  (<code><span title="torch.device">device</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>device. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>dtype</code></b>
                  (<code><span title="torch.dtype">dtype</span></code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>dtype. Defaults to None.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                  <details class="quote">
                    <summary>Source code in <code>scprint/model/flash_attn/mha.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_heads_kv</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cross_attn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">qkv_proj_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">out_proj_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">softmax_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">layer_idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dwconv</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">rotary_emb_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">rotary_emb_base</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">10000.0</span><span class="p">,</span>
    <span class="n">rotary_emb_scale_base</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">rotary_emb_interleaved</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_alibi</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">fused_bias_fc</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">use_flash_attn</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">return_residual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">checkpointing</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    MHA Multi-head self-attention and cross-attention</span>

<span class="sd">    Args:</span>
<span class="sd">        embed_dim</span>
<span class="sd">        num_heads_kv (int): can be used to toggle MQA / GQA. If None, use num_heads.</span>
<span class="sd">        return_residual (bool, optional): whether to return the input x along with the output. This is for</span>
<span class="sd">            performance reason: for post-norm architecture, returning the input allows us</span>
<span class="sd">            to fuse the backward of nn.Linear with the residual connection.</span>
<span class="sd">            Defaults to False.</span>
<span class="sd">        checkpointing (bool, optional): whether to use checkpointing to save memory.</span>
<span class="sd">            Defaults to False.</span>
<span class="sd">        num_heads_kv (int, optional): can be used to toggle MQA / GQA. If None, use num_heads.</span>
<span class="sd">        cross_attn (bool, optional): whether to use cross-attention. Defaults to False.</span>
<span class="sd">        qkv_proj_bias (bool, optional): whether to use bias in the query, key, value projection. Defaults to True.</span>
<span class="sd">        out_proj_bias (bool, optional): whether to use bias in the output projection. Defaults to True.</span>
<span class="sd">        dropout (float, optional): dropout rate. Defaults to 0.0.</span>
<span class="sd">        softmax_scale (float, optional): The temperature to use for the softmax attention.</span>
<span class="sd">        causal (bool, optional): whether to use causal attention. Defaults to False.</span>
<span class="sd">        layer_idx (int, optional): layer index for inference cache. Defaults to None.</span>
<span class="sd">        dwconv (bool, optional): whether to use depthwise convolution. Defaults to False.</span>
<span class="sd">        fused_bias_fc (bool, optional): whether to use fused_bias_fc. Defaults to False.</span>
<span class="sd">        use_flash_attn (bool, optional): whether to use FlashAttention. Defaults to False.</span>
<span class="sd">        device (torch.device, optional): device. Defaults to None.</span>
<span class="sd">        dtype (torch.dtype, optional): dtype. Defaults to None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span> <span class="o">=</span> <span class="n">cross_attn</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">layer_idx</span> <span class="o">=</span> <span class="n">layer_idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dwconv</span> <span class="o">=</span> <span class="n">dwconv</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb_dim</span> <span class="o">=</span> <span class="n">rotary_emb_dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_flash_attn</span> <span class="o">=</span> <span class="n">use_flash_attn</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_flash_attn</span> <span class="ow">and</span> <span class="p">(</span><span class="n">flash_attn_kvpacked_func</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="s2">&quot;you requested flash transformer but it requires the flash package which is not installed&quot;</span>
        <span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;falling back to regular transformer...&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_flash_attn</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="c1"># NOT flash transformer using the special tritton kernel</span>
        <span class="c1"># or parallelMHA (add the process group thing and faster)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">return_residual</span> <span class="o">=</span> <span class="n">return_residual</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">checkpointing</span> <span class="o">=</span> <span class="n">checkpointing</span>
    <span class="n">alibi_slopes</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads_kv</span> <span class="o">=</span> <span class="n">num_heads_kv</span> <span class="k">if</span> <span class="n">num_heads_kv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_heads</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads_kv</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="p">),</span> <span class="s2">&quot;num_heads must be divisible by num_heads_kv&quot;</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="p">),</span> <span class="s2">&quot;embed_dim must be divisible by num_heads&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
    <span class="n">qkv_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads_kv</span><span class="p">)</span>
    <span class="n">kv_dim</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads_kv</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb_dim</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="n">cross_attn</span>
        <span class="p">),</span> <span class="s2">&quot;MHA with rotary embedding does not support cross-attention yet&quot;</span>
        <span class="k">assert</span> <span class="n">RotaryEmbedding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;rotary_emb is not installed&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span> <span class="o">=</span> <span class="n">RotaryEmbedding</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb_dim</span><span class="p">,</span>
            <span class="n">base</span><span class="o">=</span><span class="n">rotary_emb_base</span><span class="p">,</span>
            <span class="n">scale_base</span><span class="o">=</span><span class="n">rotary_emb_scale_base</span><span class="p">,</span>
            <span class="n">interleaved</span><span class="o">=</span><span class="n">rotary_emb_interleaved</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">fused_bias_fc</span> <span class="ow">and</span> <span class="n">FusedDense</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;fused_dense is not installed&quot;</span><span class="p">)</span>
    <span class="n">linear_cls</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">fused_bias_fc</span> <span class="k">else</span> <span class="n">FusedDense</span>
    <span class="n">linear_resid_cls</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">LinearResidual</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">fused_bias_fc</span>
        <span class="k">else</span> <span class="n">partial</span><span class="p">(</span><span class="n">FusedDense</span><span class="p">,</span> <span class="n">return_residual</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">wqkv_cls</span> <span class="o">=</span> <span class="n">linear_cls</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_residual</span> <span class="k">else</span> <span class="n">linear_resid_cls</span>
    <span class="n">inner_attn_cls</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span><span class="n">FlashSelfAttention</span><span class="p">,</span> <span class="n">alibi_slopes</span><span class="o">=</span><span class="n">alibi_slopes</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_flash_attn</span>
        <span class="k">else</span> <span class="n">SelfAttention</span>
    <span class="p">)</span>
    <span class="n">inner_cross_attn_cls</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">partial</span><span class="p">(</span><span class="n">FlashCrossAttention</span><span class="p">,</span> <span class="n">alibi_slopes</span><span class="o">=</span><span class="n">alibi_slopes</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_flash_attn</span>
        <span class="k">else</span> <span class="n">CrossAttention</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Wqkv</span> <span class="o">=</span> <span class="n">wqkv_cls</span><span class="p">(</span>
            <span class="n">embed_dim</span><span class="p">,</span> <span class="n">qkv_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_proj_bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Wq</span> <span class="o">=</span> <span class="n">linear_cls</span><span class="p">(</span>
            <span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_proj_bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Wkv</span> <span class="o">=</span> <span class="n">wqkv_cls</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">kv_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_proj_bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dwconv</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads_kv</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dwconv_qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
                <span class="n">qkv_dim</span><span class="p">,</span> <span class="n">qkv_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">qkv_dim</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dwconv_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
                <span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">embed_dim</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dwconv_kv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
                <span class="n">kv_dim</span><span class="p">,</span> <span class="n">kv_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">kv_dim</span>
            <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inner_attn</span> <span class="o">=</span> <span class="n">inner_attn_cls</span><span class="p">(</span>
        <span class="n">causal</span><span class="o">=</span><span class="n">causal</span><span class="p">,</span>
        <span class="n">softmax_scale</span><span class="o">=</span><span class="n">softmax_scale</span><span class="p">,</span>
        <span class="n">attention_dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inner_cross_attn</span> <span class="o">=</span> <span class="n">inner_cross_attn_cls</span><span class="p">(</span>
        <span class="n">causal</span><span class="o">=</span><span class="n">causal</span><span class="p">,</span> <span class="n">softmax_scale</span><span class="o">=</span><span class="n">softmax_scale</span><span class="p">,</span> <span class="n">attention_dropout</span><span class="o">=</span><span class="n">dropout</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">linear_cls</span><span class="p">(</span>
        <span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">out_proj_bias</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.flash_attn.mha.MHA.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>(batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if
cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total
is the is the sum of the sequence lengths in the batch.</p>
              </div>
            </li>
            <li>
              <b><code>x_kv</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>(batch, seqlen, hidden_dim), only applicable for cross-attention. If None, use x.</p>
              </div>
            </li>
            <li>
              <b><code>cu_seqlens</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>(batch_size + 1,), dtype torch.int32. The cumulative sequence lengths
of the sequences in the batch, used to index into x. Only applicable when using
FlashAttention.</p>
              </div>
            </li>
            <li>
              <b><code>max_seqlen</code></b>
                  (<code><span title="typing.Optional">Optional</span>[int]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>int. Maximum sequence length in the batch.</p>
              </div>
            </li>
            <li>
              <b><code>key_padding_mask</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>boolean mask, True means to keep, False means to mask out.
(batch, seqlen). Only applicable when not using FlashAttention.</p>
              </div>
            </li>
            <li>
              <b><code>mixer_subset</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>for cross-attention only. If not None, will take a subset of x
before applying the query projection. Useful for e.g., ViT where we only care
about the CLS token in the last layer.</p>
              </div>
            </li>
            <li>
              <b><code>inference_params</code></b>
                  (<code><span title="typing.Optional">Optional</span>[dict]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>for generation. Adapted from Megatron-LM (and Apex)</p>
              </div>
            </li>
            <li>
              <b><code>https</code></b>
              –
              <div class="doc-md-description">
                <p>//github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470</p>
              </div>
            </li>
            <li>
              <b><code>return_qkv</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>whether to return the qkv tensor. Defaults to False.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
<b><code>out</code></b>              –
              <div class="doc-md-description">
                <p>(batch, seqlen, hidden_dim) if cu_seqlens is None and max_seqlen is None,
else (total, hidden_dim) where total is the sum of the sequence lengths in the batch.</p>
              </div>
            </li>
            <li>
<b><code>qkv</code></b>              –
              <div class="doc-md-description">
                <p>(batch, seqlen, 3, hidden_dim) if cu_seqlens is None and max_seqlen is None,
else (total, 3, hidden_dim) where total is the sum of the sequence lengths in the batch.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/flash_attn/mha.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">x_kv</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">key_padding_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cu_seqlens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">max_seqlen</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mixer_subset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inference_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_qkv</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if</span>
<span class="sd">            cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total</span>
<span class="sd">            is the is the sum of the sequence lengths in the batch.</span>
<span class="sd">        x_kv: (batch, seqlen, hidden_dim), only applicable for cross-attention. If None, use x.</span>
<span class="sd">        cu_seqlens: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths</span>
<span class="sd">            of the sequences in the batch, used to index into x. Only applicable when using</span>
<span class="sd">            FlashAttention.</span>
<span class="sd">        max_seqlen: int. Maximum sequence length in the batch.</span>
<span class="sd">        key_padding_mask: boolean mask, True means to keep, False means to mask out.</span>
<span class="sd">            (batch, seqlen). Only applicable when not using FlashAttention.</span>
<span class="sd">        mixer_subset: for cross-attention only. If not None, will take a subset of x</span>
<span class="sd">            before applying the query projection. Useful for e.g., ViT where we only care</span>
<span class="sd">            about the CLS token in the last layer.</span>
<span class="sd">        inference_params: for generation. Adapted from Megatron-LM (and Apex)</span>
<span class="sd">        https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470</span>
<span class="sd">        return_qkv: whether to return the qkv tensor. Defaults to False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        out: (batch, seqlen, hidden_dim) if cu_seqlens is None and max_seqlen is None,</span>
<span class="sd">            else (total, hidden_dim) where total is the sum of the sequence lengths in the batch.</span>
<span class="sd">        qkv: (batch, seqlen, 3, hidden_dim) if cu_seqlens is None and max_seqlen is None,</span>
<span class="sd">            else (total, 3, hidden_dim) where total is the sum of the sequence lengths in the batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">cu_seqlens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">max_seqlen</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">assert</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_flash_attn</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">dwconv</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb_dim</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">cu_seqlens</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">assert</span> <span class="n">max_seqlen</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_flash_attn</span>
    <span class="k">if</span> <span class="n">inference_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">assert</span> <span class="n">cu_seqlens</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">max_seqlen</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">dwconv</span>

    <span class="n">kwargs</span> <span class="o">=</span> <span class="p">(</span>
        <span class="p">{}</span>  <span class="c1"># &quot;cu_seqlens&quot;: cu_seqlens, &quot;max_seqlen&quot;: max_seqlen, **kwargs}</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_flash_attn</span>
        <span class="k">else</span> <span class="p">{</span><span class="s2">&quot;key_padding_mask&quot;</span><span class="p">:</span> <span class="n">key_padding_mask</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">}</span>
    <span class="p">)</span>
    <span class="n">seqlen_offset</span> <span class="o">=</span> <span class="p">(</span>
        <span class="mi">0</span>
        <span class="k">if</span> <span class="n">inference_params</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="p">(</span>
            <span class="n">inference_params</span><span class="o">.</span><span class="n">lengths_per_sample</span>
            <span class="k">if</span> <span class="n">inference_params</span><span class="o">.</span><span class="n">lengths_per_sample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">inference_params</span><span class="o">.</span><span class="n">seqlen_offset</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">rotary_max_seqlen</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">inference_params</span><span class="o">.</span><span class="n">max_seqlen</span> <span class="k">if</span> <span class="n">inference_params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="p">)</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">seqlen</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads_kv</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">x_kv</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">mixer_subset</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_residual</span><span class="p">:</span>
            <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Wqkv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># .to(torch.float16, device=&quot;cuda&quot;)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">qkv</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Wqkv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dwconv</span><span class="p">:</span>
            <span class="n">qkv</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dwconv_qkv</span><span class="p">(</span><span class="n">rearrange</span><span class="p">(</span><span class="n">qkv</span><span class="p">,</span> <span class="s2">&quot;b s d -&gt; b d s&quot;</span><span class="p">))[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                <span class="s2">&quot;b d s -&gt; b s d&quot;</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
            <span class="n">qkv</span><span class="p">,</span> <span class="s2">&quot;... (three h d) -&gt; ... three h d&quot;</span><span class="p">,</span> <span class="n">three</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">inference_params</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">or</span> <span class="n">inference_params</span><span class="o">.</span><span class="n">seqlen_offset</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb_dim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb_dim</span> <span class="o">%</span> <span class="mi">16</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
            <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_flash_attn</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb_dim</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span><span class="p">(</span>
                    <span class="n">qkv</span><span class="p">,</span> <span class="n">seqlen_offset</span><span class="o">=</span><span class="n">seqlen_offset</span><span class="p">,</span> <span class="n">max_seqlen</span><span class="o">=</span><span class="n">rotary_max_seqlen</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">inference_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpointing</span><span class="p">:</span>
                    <span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inner_attn</span><span class="p">(</span><span class="n">qkv</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">inner_attn</span><span class="p">,</span> <span class="n">qkv</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_kvcache_attention</span><span class="p">(</span>
                    <span class="n">qkv</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">qkv</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">inference_params</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_rotary_update_kvcache_attention</span><span class="p">(</span>
                <span class="n">qkv</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">qkv</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">inference_params</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attn</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_residual</span><span class="p">:</span>
                <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Wq</span><span class="p">(</span><span class="n">x</span> <span class="k">if</span> <span class="n">mixer_subset</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">mixer_subset</span><span class="p">])</span>
                <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Wkv</span><span class="p">(</span><span class="n">x_kv</span> <span class="k">if</span> <span class="n">x_kv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">x</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">x_kv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">kv</span><span class="p">,</span> <span class="n">x_kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Wkv</span><span class="p">(</span><span class="n">x_kv</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">kv</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Wkv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Wq</span><span class="p">(</span><span class="n">x</span> <span class="k">if</span> <span class="n">mixer_subset</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">mixer_subset</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads_kv</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_residual</span><span class="p">:</span>
                <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Wqkv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">qkv</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Wqkv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">]</span>
            <span class="n">kv</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="p">:]</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="s2">&quot;... (h d) -&gt; ... h d&quot;</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">kv</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
            <span class="n">kv</span><span class="p">,</span> <span class="s2">&quot;... (two hkv d) -&gt; ... two hkv d&quot;</span><span class="p">,</span> <span class="n">two</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dwconv</span><span class="p">:</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dwconv_q</span><span class="p">(</span><span class="n">rearrange</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="s2">&quot;b s d -&gt; b d s&quot;</span><span class="p">))[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                <span class="s2">&quot;b d s -&gt; b s d&quot;</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="n">kv</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">dwconv_kv</span><span class="p">(</span><span class="n">rearrange</span><span class="p">(</span><span class="n">kv</span><span class="p">,</span> <span class="s2">&quot;b s d -&gt; b d s&quot;</span><span class="p">))[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span>
                <span class="s2">&quot;b d s -&gt; b s d&quot;</span><span class="p">,</span>
            <span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">inference_params</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">or</span> <span class="n">inference_params</span><span class="o">.</span><span class="n">seqlen_offset</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb_dim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb_dim</span> <span class="o">%</span> <span class="mi">16</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
            <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_flash_attn</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb_dim</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">q</span><span class="p">,</span> <span class="n">kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span><span class="p">(</span>
                    <span class="n">q</span><span class="p">,</span> <span class="n">kv</span><span class="p">,</span> <span class="n">seqlen_offset</span><span class="o">=</span><span class="n">seqlen_offset</span><span class="p">,</span> <span class="n">max_seqlen</span><span class="o">=</span><span class="n">rotary_max_seqlen</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">inference_params</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">checkpointing</span><span class="p">:</span>
                    <span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inner_cross_attn</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kv</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">inner_cross_attn</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">kv</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_update_kvcache_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kv</span><span class="p">,</span> <span class="n">inference_params</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_apply_rotary_update_kvcache_attention</span><span class="p">(</span>
                <span class="n">q</span><span class="p">,</span> <span class="n">kv</span><span class="p">,</span> <span class="n">inference_params</span>
            <span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">rearrange</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="s2">&quot;... h d -&gt; ... (h d)&quot;</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">return_qkv</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_residual</span> <span class="k">else</span> <span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">qkv</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_residual</span> <span class="k">else</span> <span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.flash_attn.mha.SelfAttention" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">SelfAttention</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>Implement the scaled dot product attention with softmax.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>softmax_scale</code></b>
              –
              <div class="doc-md-description">
                <p>The temperature to use for the softmax attention.
(default: 1/sqrt(d_keys) where d_keys is computed at
runtime)</p>
              </div>
            </li>
            <li>
              <b><code>attention_dropout</code></b>
              –
              <div class="doc-md-description">
                <p>The dropout rate to apply to the attention
(default: 0.0)</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                  <details class="quote">
                    <summary>Source code in <code>scprint/model/flash_attn/mha.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">softmax_scale</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">=</span> <span class="n">softmax_scale</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">attention_dropout</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.flash_attn.mha.SelfAttention.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>Implements the multihead softmax attention.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>qkv</code></b>
              –
              <div class="doc-md-description">
                <p>The tensor containing the query, key, and value. (B, S, 3, H, D)</p>
              </div>
            </li>
            <li>
              <b><code>causal</code></b>
              –
              <div class="doc-md-description">
                <p>if passed, will override self.causal</p>
              </div>
            </li>
            <li>
              <b><code>key_padding_mask</code></b>
              –
              <div class="doc-md-description">
                <p>boolean mask to apply to the attention weights. True means to keep,
False means to mask out. (B, S)</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/flash_attn/mha.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">qkv</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements the multihead softmax attention.</span>

<span class="sd">    Args:</span>
<span class="sd">        qkv: The tensor containing the query, key, and value. (B, S, 3, H, D)</span>
<span class="sd">        causal: if passed, will override self.causal</span>
<span class="sd">        key_padding_mask: boolean mask to apply to the attention weights. True means to keep,</span>
<span class="sd">            False means to mask out. (B, S)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seqlen</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">qkv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">causal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">causal</span> <span class="k">if</span> <span class="n">causal</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">causal</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">softmax_scale</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="ow">or</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bthd,bshd-&gt;bhts&quot;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="o">*</span> <span class="n">softmax_scale</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">(</span>
            <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">),</span> <span class="o">-</span><span class="mf">10000.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">scores</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>
        <span class="n">padding_mask</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">key_padding_mask</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
        <span class="c1"># TD [2022-09-30]: Adding is faster than masked_fill_ (idk why, just better kernel I guess)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="s2">&quot;b s -&gt; b 1 1 s&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">causal</span><span class="p">:</span>
        <span class="c1"># &quot;triu_tril_cuda_template&quot; not implemented for &#39;BFloat16&#39;</span>
        <span class="c1"># So we have to construct the mask in float</span>
        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">seqlen</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">),</span> <span class="o">-</span><span class="mf">10000.0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">scores</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="c1"># TD [2022-09-30]: Adding is faster than masked_fill_ (idk why, just better kernel I guess)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">causal_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">scores</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">attention</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">attention_drop</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">attention</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bhts,bshd-&gt;bthd&quot;</span><span class="p">,</span> <span class="n">attention_drop</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="scprint.model.flash_attn.mlp" class="doc doc-heading">
            <code>scprint.model.flash_attn.mlp</code>


</h2>

    <div class="doc doc-contents first">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="scprint.model.flash_attn.mlp.Mlp" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">Mlp</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>Multi-layer perceptron (MLP) module.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>in_features</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>Size of each input sample.</p>
              </div>
            </li>
            <li>
              <b><code>hidden_features</code></b>
                  (<code><span title="typing.Optional">Optional</span>[int]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Size of the hidden layer. Defaults to 4 * in_features.</p>
              </div>
            </li>
            <li>
              <b><code>out_features</code></b>
                  (<code><span title="typing.Optional">Optional</span>[int]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Size of each output sample. Defaults to in_features.</p>
              </div>
            </li>
            <li>
              <b><code>activation</code></b>
                  (<code><span title="typing.Callable">Callable</span>[[<span title="torch.Tensor">Tensor</span>], <span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code><span title="torch.nn.functional.gelu">gelu</span></code>
)
              –
              <div class="doc-md-description">
                <p>Activation function. Defaults to F.gelu.</p>
              </div>
            </li>
            <li>
              <b><code>bias1</code></b>
                  (<code>bool</code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>If set to False, the first linear layer will not learn an additive bias. Defaults to True.</p>
              </div>
            </li>
            <li>
              <b><code>bias2</code></b>
                  (<code>bool</code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>If set to False, the second linear layer will not learn an additive bias. Defaults to True.</p>
              </div>
            </li>
            <li>
              <b><code>return_residual</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>If set to True, the forward method will return a tuple (output, input). Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>device</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.device">device</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The desired device of the parameters. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>dtype</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.dtype">dtype</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>The desired data type of the parameters. Defaults to None.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                  <details class="quote">
                    <summary>Source code in <code>scprint/model/flash_attn/mlp.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">hidden_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">out_features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">gelu</span><span class="p">,</span>
    <span class="n">bias1</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">bias2</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">return_residual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Multi-layer perceptron (MLP) module.</span>

<span class="sd">    Args:</span>
<span class="sd">        in_features (int): Size of each input sample.</span>
<span class="sd">        hidden_features (Optional[int], optional): Size of the hidden layer. Defaults to 4 * in_features.</span>
<span class="sd">        out_features (Optional[int], optional): Size of each output sample. Defaults to in_features.</span>
<span class="sd">        activation (Callable[[torch.Tensor], torch.Tensor], optional): Activation function. Defaults to F.gelu.</span>
<span class="sd">        bias1 (bool, optional): If set to False, the first linear layer will not learn an additive bias. Defaults to True.</span>
<span class="sd">        bias2 (bool, optional): If set to False, the second linear layer will not learn an additive bias. Defaults to True.</span>
<span class="sd">        return_residual (bool, optional): If set to True, the forward method will return a tuple (output, input). Defaults to False.</span>
<span class="sd">        device (Optional[torch.device], optional): The desired device of the parameters. Defaults to None.</span>
<span class="sd">        dtype (Optional[torch.dtype], optional): The desired data type of the parameters. Defaults to None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="n">out_features</span> <span class="o">=</span> <span class="n">out_features</span> <span class="k">if</span> <span class="n">out_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">in_features</span>
    <span class="n">hidden_features</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">hidden_features</span> <span class="k">if</span> <span class="n">hidden_features</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">in_features</span> <span class="o">*</span> <span class="mi">4</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">return_residual</span> <span class="o">=</span> <span class="n">return_residual</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias1</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
        <span class="n">hidden_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias2</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.flash_attn.mlp.Mlp.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>Forward pass of the MLP.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Input tensor.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>, <span title="torch.Tensor">Tensor</span>]]</code>
              –
              <div class="doc-md-description">
                <p>Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]: Output tensor, or a tuple (output, input) if return_residual is True.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/flash_attn/mlp.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass of the MLP.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): Input tensor.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]: Output tensor, or a tuple (output, input) if return_residual is True.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span> <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_residual</span> <span class="k">else</span> <span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="scprint.model.flash_attn.block" class="doc doc-heading">
            <code>scprint.model.flash_attn.block</code>


</h2>

    <div class="doc doc-contents first">



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="scprint.model.flash_attn.block.Block" class="doc doc-heading">
              <code class="highlight language-python"><span class="n">Block</span></code>

</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>


      <p>For prenorm=True, this Block has a slightly different structure compared to a regular
prenorm Transformer block.
The standard block is: LN -&gt; MHA -&gt; Dropout -&gt; Add -&gt; LN -&gt; MLP -&gt; Dropout -&gt; Add.
[Ref: https://arxiv.org/abs/2002.04745]
Here we have: Dropout -&gt; Add -&gt; LN -&gt; MHA -&gt; Dropout -&gt; Add -&gt; LN -&gt; MLP, returning both
the hidden_states (output of the MLP) and the residual.
This is for performance reasons, as we can fuse the dropout, add and LayerNorm.
The residual needs to be provided (except for the very first block).</p>
<p>For prenorm=False, this Block has the same structure as a regular postnorm Transformer
block: MHA -&gt; Dropout -&gt; Add -&gt; LN -&gt; MLP -&gt; Dropout -&gt; Add -&gt; LN.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>dim</code></b>
                  (<code>int</code>)
              –
              <div class="doc-md-description">
                <p>the number of features in the input.</p>
              </div>
            </li>
            <li>
              <b><code>mixer_cls</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>the class to use for the mixer layer. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>mlp_cls</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="typing.Callable">Callable</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>the class to use for the mlp layer. Defaults to None.</p>
              </div>
            </li>
            <li>
              <b><code>norm_cls</code></b>
                  (<code><span title="typing.Callable">Callable</span></code>, default:
                      <code><span title="functools.partial">partial</span>(<span title="torch.nn.LayerNorm">LayerNorm</span>, eps=1e-06)</code>
)
              –
              <div class="doc-md-description">
                <p>the class to use for the layer norm. Defaults to partial(nn.LayerNorm, eps=1e-6).</p>
              </div>
            </li>
            <li>
              <b><code>dropout_cls</code></b>
                  (<code><span title="typing.Type">Type</span>[<span title="torch.nn.Dropout">Dropout</span>]</code>, default:
                      <code><span title="torch.nn.Dropout">Dropout</span></code>
)
              –
              <div class="doc-md-description">
                <p>the class to use for the dropout. Defaults to nn.Dropout.</p>
              </div>
            </li>
            <li>
              <b><code>prenorm</code></b>
                  (<code>bool</code>, default:
                      <code>True</code>
)
              –
              <div class="doc-md-description">
                <p>whether to use pre-norm or post-norm. Defaults to True.</p>
              </div>
            </li>
            <li>
              <b><code>resid_dropout1</code></b>
                  (<code>float</code>, default:
                      <code>0.0</code>
)
              –
              <div class="doc-md-description">
                <p>the dropout probability for the first dropout layer. Defaults to 0.0.</p>
              </div>
            </li>
            <li>
              <b><code>resid_dropout2</code></b>
                  (<code>float</code>, default:
                      <code>0.0</code>
)
              –
              <div class="doc-md-description">
                <p>the dropout probability for the second dropout layer. Defaults to 0.0.</p>
              </div>
            </li>
            <li>
              <b><code>drop_path1</code></b>
                  (<code>float</code>, default:
                      <code>0.0</code>
)
              –
              <div class="doc-md-description">
                <p>the drop path probability for the first drop path layer. Defaults to 0.0.</p>
              </div>
            </li>
            <li>
              <b><code>drop_path2</code></b>
                  (<code>float</code>, default:
                      <code>0.0</code>
)
              –
              <div class="doc-md-description">
                <p>the drop path probability for the second drop path layer. Defaults to 0.0.</p>
              </div>
            </li>
            <li>
              <b><code>fused_dropout_add_ln</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>whether to fuse the dropout, add and layer norm. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>return_residual</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>whether each of the sub-layers (mixer and mlp) will return the residual.
This is for performance reason: for post-norm architecture, returning the input allows us
to fuse the backward of nn.Linear with the residual connection.
Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>residual_in_fp32</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>whether to keep the residual in fp32. This is for performance reason:
for post-norm architecture, keeping the residual in fp32 allows us to fuse the backward of nn.Linear
with the residual connection. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>sequence_parallel</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>whether to use sequence parallelism. Defaults to False.</p>
              </div>
            </li>
            <li>
              <b><code>mark_shared_params</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>whether to mark the norm parameters as "shared_params".
This is useful when we want to sync the norm parameters across workers. Defaults to False.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
                  <details class="quote">
                    <summary>Source code in <code>scprint/model/flash_attn/block.py</code></summary>
                    <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">mixer_cls</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mlp_cls</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">norm_cls</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">),</span>
    <span class="n">dropout_cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">]</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">,</span>
    <span class="n">prenorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">resid_dropout1</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">resid_dropout2</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">drop_path1</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">drop_path2</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">fused_dropout_add_ln</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">return_residual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">residual_in_fp32</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sequence_parallel</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mark_shared_params</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    For prenorm=True, this Block has a slightly different structure compared to a regular</span>
<span class="sd">    prenorm Transformer block.</span>
<span class="sd">    The standard block is: LN -&gt; MHA -&gt; Dropout -&gt; Add -&gt; LN -&gt; MLP -&gt; Dropout -&gt; Add.</span>
<span class="sd">    [Ref: https://arxiv.org/abs/2002.04745]</span>
<span class="sd">    Here we have: Dropout -&gt; Add -&gt; LN -&gt; MHA -&gt; Dropout -&gt; Add -&gt; LN -&gt; MLP, returning both</span>
<span class="sd">    the hidden_states (output of the MLP) and the residual.</span>
<span class="sd">    This is for performance reasons, as we can fuse the dropout, add and LayerNorm.</span>
<span class="sd">    The residual needs to be provided (except for the very first block).</span>

<span class="sd">    For prenorm=False, this Block has the same structure as a regular postnorm Transformer</span>
<span class="sd">    block: MHA -&gt; Dropout -&gt; Add -&gt; LN -&gt; MLP -&gt; Dropout -&gt; Add -&gt; LN.</span>

<span class="sd">    Args:</span>
<span class="sd">        dim (int): the number of features in the input.</span>
<span class="sd">        mixer_cls (Optional[Callable], optional): the class to use for the mixer layer. Defaults to None.</span>
<span class="sd">        mlp_cls (Optional[Callable], optional): the class to use for the mlp layer. Defaults to None.</span>
<span class="sd">        norm_cls (Callable, optional): the class to use for the layer norm. Defaults to partial(nn.LayerNorm, eps=1e-6).</span>
<span class="sd">        dropout_cls (Type[nn.Dropout], optional): the class to use for the dropout. Defaults to nn.Dropout.</span>
<span class="sd">        prenorm (bool, optional): whether to use pre-norm or post-norm. Defaults to True.</span>
<span class="sd">        resid_dropout1 (float, optional): the dropout probability for the first dropout layer. Defaults to 0.0.</span>
<span class="sd">        resid_dropout2 (float, optional): the dropout probability for the second dropout layer. Defaults to 0.0.</span>
<span class="sd">        drop_path1 (float, optional): the drop path probability for the first drop path layer. Defaults to 0.0.</span>
<span class="sd">        drop_path2 (float, optional): the drop path probability for the second drop path layer. Defaults to 0.0.</span>
<span class="sd">        fused_dropout_add_ln (bool, optional): whether to fuse the dropout, add and layer norm. Defaults to False.</span>
<span class="sd">        return_residual (bool, optional): whether each of the sub-layers (mixer and mlp) will return the residual.</span>
<span class="sd">            This is for performance reason: for post-norm architecture, returning the input allows us</span>
<span class="sd">            to fuse the backward of nn.Linear with the residual connection.</span>
<span class="sd">            Defaults to False.</span>
<span class="sd">        residual_in_fp32 (bool, optional): whether to keep the residual in fp32. This is for performance reason:</span>
<span class="sd">            for post-norm architecture, keeping the residual in fp32 allows us to fuse the backward of nn.Linear</span>
<span class="sd">            with the residual connection. Defaults to False.</span>
<span class="sd">        sequence_parallel (bool, optional): whether to use sequence parallelism. Defaults to False.</span>
<span class="sd">        mark_shared_params (bool, optional): whether to mark the norm parameters as &quot;shared_params&quot;.</span>
<span class="sd">            This is useful when we want to sync the norm parameters across workers. Defaults to False.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prenorm</span> <span class="o">=</span> <span class="n">prenorm</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fused_dropout_add_ln</span> <span class="o">=</span> <span class="n">fused_dropout_add_ln</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">return_residual</span> <span class="o">=</span> <span class="n">return_residual</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">residual_in_fp32</span> <span class="o">=</span> <span class="n">residual_in_fp32</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual_in_fp32</span><span class="p">:</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">prenorm</span><span class="p">,</span> <span class="s2">&quot;residual_in_fp32 is only compatible with prenorm=True&quot;</span>
    <span class="k">if</span> <span class="n">mixer_cls</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mixer_cls</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">MHA</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">dim</span> <span class="o">//</span> <span class="mi">64</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mlp_cls</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mlp_cls</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">Mlp</span><span class="p">,</span> <span class="n">hidden_features</span><span class="o">=</span><span class="mi">4</span> <span class="o">*</span> <span class="n">dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mixer</span> <span class="o">=</span> <span class="n">mixer_cls</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">dropout_cls</span><span class="p">(</span><span class="n">resid_dropout1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop_path1</span> <span class="o">=</span> <span class="n">StochasticDepth</span><span class="p">(</span><span class="n">drop_path1</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;row&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">norm_cls</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">mlp_cls</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">dropout_cls</span><span class="p">(</span><span class="n">resid_dropout2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_path2</span> <span class="o">=</span> <span class="n">StochasticDepth</span><span class="p">(</span><span class="n">drop_path2</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;row&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">norm_cls</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fused_dropout_add_ln</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">layer_norm_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Triton is not installed&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">,</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="n">RMSNorm</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span>
        <span class="p">)</span>

    <span class="c1"># TD [2023-01-07]: TODO: During training, if sequence_parallel is False and dropout != 0.0,</span>
    <span class="c1"># then the input to each worker in the tensor parallel group will be different.</span>
    <span class="c1"># This would produce wrong outputs? Somehow we&#39;d need to sync the RNG state across workers.</span>
    <span class="c1"># For now this is not an issue because we always use sequence_parallel=True during training</span>
    <span class="c1"># and only use sequence_parallel=False during inference.</span>

    <span class="c1"># Mark the norm parameters as &quot;sequence_parallel&quot; so that we run all-reduce on their grads.</span>
    <span class="k">if</span> <span class="n">sequence_parallel</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">p</span><span class="o">.</span><span class="n">_sequence_parallel</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;norm2&quot;</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="n">p</span><span class="o">.</span><span class="n">_sequence_parallel</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="c1"># Mark the norm parameters as &quot;shared_params&quot; so that we sync their values at init.</span>
    <span class="k">if</span> <span class="n">mark_shared_params</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="n">p</span><span class="o">.</span><span class="n">_shared_params</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;norm2&quot;</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="n">p</span><span class="o">.</span><span class="n">_shared_params</span> <span class="o">=</span> <span class="kc">True</span>
</code></pre></div></td></tr></table></div>
                  </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.flash_attn.block.Block.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

</h4>


    <div class="doc doc-contents ">

      <p>Pass the input through the encoder layer.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>hidden_states</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>The sequence to be passed to the encoder layer. This is a required argument.</p>
              </div>
            </li>
            <li>
              <b><code>residual</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>This argument is used differently based on the normalization method.
If postnorm is used, residual should be None. If prenorm is used, hidden_states is updated as Attn/MLP(LN(residual)).</p>
              </div>
            </li>
            <li>
              <b><code>mixer_subset</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>This argument is used only for cross-attention.
If not None, a subset of the input sequence 'x' is taken before applying the query projection.
This is particularly useful for models like ViT where only the CLS token in the last layer is of interest.</p>
              </div>
            </li>
            <li>
              <b><code>mixer_kwargs</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[str, <span title="typing.Any">Any</span>]]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>This argument is used only for cross-attention.
It is a dictionary of additional arguments to be passed to the mixer.</p>
              </div>
            </li>
            <li>
              <b><code>return_qkv</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>If True, the function will return the query, key, and value tensors.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              –
              <div class="doc-md-description">
                <p>Tensor or Tuple[Tensor, Tensor]: The output tensor of the encoder layer.</p>
              </div>
            </li>
            <li>
              –
              <div class="doc-md-description">
                <p>If return_qkv is True, the function will return a tuple of the output tensor and the query, key, and value tensors.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/flash_attn/block.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">residual</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">src_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">is_causal</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">src_key_padding_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mixer_subset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mixer_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">return_qkv</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pass the input through the encoder layer.</span>

<span class="sd">    Args:</span>
<span class="sd">        hidden_states (Tensor): The sequence to be passed to the encoder layer. This is a required argument.</span>
<span class="sd">        residual (Optional[Tensor]): This argument is used differently based on the normalization method.</span>
<span class="sd">            If postnorm is used, residual should be None. If prenorm is used, hidden_states is updated as Attn/MLP(LN(residual)).</span>
<span class="sd">        mixer_subset: This argument is used only for cross-attention.</span>
<span class="sd">            If not None, a subset of the input sequence &#39;x&#39; is taken before applying the query projection.</span>
<span class="sd">            This is particularly useful for models like ViT where only the CLS token in the last layer is of interest.</span>
<span class="sd">        mixer_kwargs: This argument is used only for cross-attention.</span>
<span class="sd">            It is a dictionary of additional arguments to be passed to the mixer.</span>
<span class="sd">        return_qkv: If True, the function will return the query, key, and value tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tensor or Tuple[Tensor, Tensor]: The output tensor of the encoder layer.</span>
<span class="sd">        If return_qkv is True, the function will return a tuple of the output tensor and the query, key, and value tensors.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prenorm</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">fused_dropout_add_ln</span><span class="p">:</span>
            <span class="n">dropped</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="p">(</span><span class="n">dropped</span> <span class="o">+</span> <span class="n">residual</span><span class="p">)</span> <span class="k">if</span> <span class="n">residual</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dropped</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">residual</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual_in_fp32</span><span class="p">:</span>
                <span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path1</span><span class="o">.</span><span class="n">p</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">rowscale1</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">rowscale1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path1</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
                        <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                        <span class="n">device</span><span class="o">=</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">residual</span> <span class="o">=</span> <span class="n">layer_norm_fn</span><span class="p">(</span>
                <span class="n">hidden_states</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                <span class="n">residual</span><span class="o">=</span><span class="n">residual</span><span class="p">,</span>
                <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
                <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="o">.</span><span class="n">p</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
                <span class="n">rowscale</span><span class="o">=</span><span class="n">rowscale1</span><span class="p">,</span>
                <span class="n">prenorm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">residual_in_fp32</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">residual_in_fp32</span><span class="p">,</span>
                <span class="n">is_rms_norm</span><span class="o">=</span><span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">,</span> <span class="n">RMSNorm</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">mixer_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mixer_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">mixer_subset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mixer_kwargs</span><span class="p">[</span><span class="s2">&quot;mixer_subset&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mixer_subset</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixer</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="p">,</span> <span class="n">return_qkv</span><span class="o">=</span><span class="n">return_qkv</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="o">**</span><span class="n">mixer_kwargs</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">return_qkv</span><span class="p">:</span>
            <span class="n">qkv</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">mixer_subset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span><span class="p">[:,</span> <span class="n">mixer_subset</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">fused_dropout_add_ln</span><span class="p">:</span>
                <span class="n">dropped</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
                <span class="n">residual</span> <span class="o">=</span> <span class="p">(</span><span class="n">dropped</span> <span class="o">+</span> <span class="n">residual</span><span class="p">)</span> <span class="k">if</span> <span class="n">residual</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dropped</span>
                <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span>
                    <span class="n">residual</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual_in_fp32</span><span class="p">:</span>
                    <span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path2</span><span class="o">.</span><span class="n">p</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                    <span class="n">rowscale2</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">rowscale2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path2</span><span class="p">(</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
                            <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                            <span class="n">device</span><span class="o">=</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="n">hidden_states</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="n">hidden_states</span><span class="p">,</span> <span class="n">residual</span> <span class="o">=</span> <span class="n">layer_norm_fn</span><span class="p">(</span>
                    <span class="n">hidden_states</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                    <span class="n">residual</span><span class="o">=</span><span class="n">residual</span><span class="p">,</span>
                    <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
                    <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="o">.</span><span class="n">p</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
                    <span class="n">rowscale</span><span class="o">=</span><span class="n">rowscale2</span><span class="p">,</span>
                    <span class="n">prenorm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">residual_in_fp32</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">residual_in_fp32</span><span class="p">,</span>
                    <span class="n">is_rms_norm</span><span class="o">=</span><span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">,</span> <span class="n">RMSNorm</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">residual</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">return_qkv</span>
            <span class="k">else</span> <span class="p">(</span>
                <span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">residual</span><span class="p">,</span>
                <span class="n">qkv</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
    <span class="c1"># if not prenorm (disregard for scPRINT)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">residual</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">mixer_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mixer</span><span class="p">(</span>
            <span class="n">hidden_states</span><span class="p">,</span>
            <span class="n">return_qkv</span><span class="o">=</span><span class="n">return_qkv</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
            <span class="o">**</span><span class="p">(</span><span class="n">mixer_kwargs</span> <span class="k">if</span> <span class="n">mixer_kwargs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{})</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">return_qkv</span><span class="p">:</span>
            <span class="n">qkv</span> <span class="o">=</span> <span class="n">mixer_out</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">mixer_out</span> <span class="o">=</span> <span class="n">mixer_out</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_residual</span><span class="p">:</span>  <span class="c1"># mixer out is actually a pair here</span>
            <span class="n">mixer_out</span><span class="p">,</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mixer_out</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">fused_dropout_add_ln</span><span class="p">:</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">drop_path1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">mixer_out</span><span class="p">))</span> <span class="o">+</span> <span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path1</span><span class="o">.</span><span class="n">p</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                <span class="n">rowscale1</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">rowscale1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path1</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
                        <span class="n">mixer_out</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                        <span class="n">device</span><span class="o">=</span><span class="n">mixer_out</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">mixer_out</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_norm_fn</span><span class="p">(</span>
                <span class="n">mixer_out</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                <span class="n">residual</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
                <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
                <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="o">.</span><span class="n">p</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
                <span class="n">rowscale</span><span class="o">=</span><span class="n">rowscale1</span><span class="p">,</span>
                <span class="n">prenorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">is_rms_norm</span><span class="o">=</span><span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">,</span> <span class="n">RMSNorm</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">):</span>
            <span class="n">mlp_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_residual</span><span class="p">:</span>  <span class="c1"># mlp out is actually a pair here</span>
                <span class="n">mlp_out</span><span class="p">,</span> <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">mlp_out</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">fused_dropout_add_ln</span><span class="p">:</span>
                <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">drop_path2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">mlp_out</span><span class="p">))</span> <span class="o">+</span> <span class="n">hidden_states</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path2</span><span class="o">.</span><span class="n">p</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                    <span class="n">rowscale2</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">rowscale2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_path2</span><span class="p">(</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span>
                            <span class="n">mlp_out</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                            <span class="n">device</span><span class="o">=</span><span class="n">mlp_out</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="n">mlp_out</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer_norm_fn</span><span class="p">(</span>
                    <span class="n">mlp_out</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                    <span class="n">residual</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">,</span>
                    <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
                    <span class="n">dropout_p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="o">.</span><span class="n">p</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
                    <span class="n">rowscale</span><span class="o">=</span><span class="n">rowscale2</span><span class="p">,</span>
                    <span class="n">prenorm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">is_rms_norm</span><span class="o">=</span><span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">,</span> <span class="n">RMSNorm</span><span class="p">),</span>
                <span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">return_qkv</span> <span class="k">else</span> <span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">qkv</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="scprint.model.flash_attn.flashattention" class="doc doc-heading">
            <code>scprint.model.flash_attn.flashattention</code>


</h2>

    <div class="doc doc-contents first">

      <p><em>Experimental</em> implementation of FlashAttention in Triton.
Tested with triton==2.0.0.dev20221202.
Triton 2.0 has a new backend (MLIR) but seems like it doesn't yet work for head dimensions
other than 64:
https://github.com/openai/triton/blob/d376020f90002757eea3ea9475d4f7cfc2ec5ead/python/triton/ops/flash_attention.py#L207
We'll update this implementation with the new Triton backend once this is fixed.</p>
<p>We use the FlashAttention implementation from Phil Tillet a starting point.
https://github.com/openai/triton/blob/master/python/tutorials/06-fused-attention.py</p>
<p>Changes:
- Implement both causal and non-causal attention.
- Implement both self-attention and cross-attention.
- Support arbitrary seqlens (not just multiples of 128), for both forward and backward.
- Support all head dimensions up to 128 (not just 16, 32, 64, 128), for both forward and backward.
- Support attention bias.
- Speed up the forward pass a bit, and only store the LSE instead of m and l.
- Make the backward for d=128 much faster by reducing register spilling.
- Optionally parallelize the backward pass across seqlen_k, to deal with the case of
small batch size * nheads.</p>
<p>Caution:
- This is an <em>experimental</em> implementation. The forward pass should be quite robust but
I'm not 100% sure that the backward pass doesn't have race conditions (due to the Triton compiler).
- This implementation has only been tested on A100.
- If you plan to use headdim other than 64 and 128, you should test for race conditions
(due to the Triton compiler), as done in tests/test_flash_attn.py
"test_flash_attn_triton_race_condition". I've tested and fixed many race conditions
for different head dimensions (40, 48, 64, 128, 80, 88, 96), but I'm still not 100% confident
that there are none left for other head dimensions.</p>
<p>Differences between this Triton version and the CUDA version:
- Triton version doesn't support dropout.
- Triton forward is generally faster than CUDA forward, while Triton backward is
generally slower than CUDA backward. Overall Triton forward + backward is slightly slower
than CUDA forward + backward.
- Triton version doesn't support different sequence lengths in a batch (i.e., RaggedTensor/NestedTensor).
- Triton version supports attention bias, while CUDA version doesn't.</p>



  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h3 id="scprint.model.flash_attn.flashattention.FlashAttnFunc" class="doc doc-heading">
            <code>FlashAttnFunc</code>


</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.autograd.Function">Function</span></code></p>





  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.flash_attn.flashattention.FlashAttnFunc.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h4>


    <div class="doc doc-contents ">

      <p>Perform the forward pass of FlashAttention.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>q</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Query tensor of shape (batch_size, seqlen_q, nheads, headdim).</p>
              </div>
            </li>
            <li>
              <b><code>k</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Key tensor of shape (batch_size, seqlen_k, nheads, headdim).</p>
              </div>
            </li>
            <li>
              <b><code>v</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Value tensor of shape (batch_size, seqlen_k, nheads, headdim).</p>
              </div>
            </li>
            <li>
              <b><code>bias</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Bias tensor, shape broadcastible to (batch, nheads, seqlen_q, seqlen_k).
For example, ALiBi mask for causal would have shape (1, nheads, 1, seqlen_k).
ALiBi mask for non-causal would have shape (1, nheads, seqlen_q, seqlen_k).</p>
              </div>
            </li>
            <li>
              <b><code>causal</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to apply causal masking. Default is False.</p>
              </div>
            </li>
            <li>
              <b><code>softmax_scale</code></b>
                  (<code><span title="typing.Optional">Optional</span>[float]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Scaling factor for the softmax operation. Default is None.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="torch.Tensor">Tensor</span></code>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: Output tensor after applying FlashAttention.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/flash_attn/flashattention.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1307</span>
<span class="normal">1308</span>
<span class="normal">1309</span>
<span class="normal">1310</span>
<span class="normal">1311</span>
<span class="normal">1312</span>
<span class="normal">1313</span>
<span class="normal">1314</span>
<span class="normal">1315</span>
<span class="normal">1316</span>
<span class="normal">1317</span>
<span class="normal">1318</span>
<span class="normal">1319</span>
<span class="normal">1320</span>
<span class="normal">1321</span>
<span class="normal">1322</span>
<span class="normal">1323</span>
<span class="normal">1324</span>
<span class="normal">1325</span>
<span class="normal">1326</span>
<span class="normal">1327</span>
<span class="normal">1328</span>
<span class="normal">1329</span>
<span class="normal">1330</span>
<span class="normal">1331</span>
<span class="normal">1332</span>
<span class="normal">1333</span>
<span class="normal">1334</span>
<span class="normal">1335</span>
<span class="normal">1336</span>
<span class="normal">1337</span>
<span class="normal">1338</span>
<span class="normal">1339</span>
<span class="normal">1340</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="n">ctx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">,</span>
    <span class="n">q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">softmax_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform the forward pass of FlashAttention.</span>

<span class="sd">    Args:</span>
<span class="sd">        q (torch.Tensor): Query tensor of shape (batch_size, seqlen_q, nheads, headdim).</span>
<span class="sd">        k (torch.Tensor): Key tensor of shape (batch_size, seqlen_k, nheads, headdim).</span>
<span class="sd">        v (torch.Tensor): Value tensor of shape (batch_size, seqlen_k, nheads, headdim).</span>
<span class="sd">        bias (Optional[torch.Tensor]): Bias tensor, shape broadcastible to (batch, nheads, seqlen_q, seqlen_k).</span>
<span class="sd">            For example, ALiBi mask for causal would have shape (1, nheads, 1, seqlen_k).</span>
<span class="sd">            ALiBi mask for non-causal would have shape (1, nheads, seqlen_q, seqlen_k).</span>
<span class="sd">        causal (bool): Whether to apply causal masking. Default is False.</span>
<span class="sd">        softmax_scale (Optional[float]): Scaling factor for the softmax operation. Default is None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Output tensor after applying FlashAttention.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Make sure that the last dimension is contiguous</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">x</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">]]</span>
    <span class="n">o</span><span class="p">,</span> <span class="n">lse</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">=</span> <span class="n">_flash_attn_forward</span><span class="p">(</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="n">causal</span><span class="p">,</span> <span class="n">softmax_scale</span><span class="o">=</span><span class="n">softmax_scale</span>
    <span class="p">)</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">lse</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>
    <span class="k">return</span> <span class="n">o</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.flash_attn.flashattention.FlashAttnKVPackedFunc" class="doc doc-heading">
            <code>FlashAttnKVPackedFunc</code>


</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.autograd.Function">Function</span></code></p>





  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.flash_attn.flashattention.FlashAttnKVPackedFunc.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h4>


    <div class="doc doc-contents ">

      <p>Perform the forward pass of FlashAttention with packed key and value tensors.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>q</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Query tensor of shape (batch, seqlen_q, nheads, headdim).</p>
              </div>
            </li>
            <li>
              <b><code>kv</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Key and value tensor of shape (batch, seqlen_k, 2, nheads, headdim).</p>
              </div>
            </li>
            <li>
              <b><code>bias</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Bias tensor, shape broadcastable to (batch, nheads, seqlen_q, seqlen_k).
For example, ALiBi mask for causal would have shape (1, nheads, 1, seqlen_k).
ALiBi mask for non-causal would have shape (1, nheads, seqlen_q, seqlen_k).</p>
              </div>
            </li>
            <li>
              <b><code>causal</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to apply causal masking. Default is False.</p>
              </div>
            </li>
            <li>
              <b><code>softmax_scale</code></b>
                  (<code><span title="typing.Optional">Optional</span>[float]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Scaling factor for the softmax operation. Default is None.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="torch.Tensor">Tensor</span></code>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: Output tensor after applying FlashAttention.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/flash_attn/flashattention.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1236</span>
<span class="normal">1237</span>
<span class="normal">1238</span>
<span class="normal">1239</span>
<span class="normal">1240</span>
<span class="normal">1241</span>
<span class="normal">1242</span>
<span class="normal">1243</span>
<span class="normal">1244</span>
<span class="normal">1245</span>
<span class="normal">1246</span>
<span class="normal">1247</span>
<span class="normal">1248</span>
<span class="normal">1249</span>
<span class="normal">1250</span>
<span class="normal">1251</span>
<span class="normal">1252</span>
<span class="normal">1253</span>
<span class="normal">1254</span>
<span class="normal">1255</span>
<span class="normal">1256</span>
<span class="normal">1257</span>
<span class="normal">1258</span>
<span class="normal">1259</span>
<span class="normal">1260</span>
<span class="normal">1261</span>
<span class="normal">1262</span>
<span class="normal">1263</span>
<span class="normal">1264</span>
<span class="normal">1265</span>
<span class="normal">1266</span>
<span class="normal">1267</span>
<span class="normal">1268</span>
<span class="normal">1269</span>
<span class="normal">1270</span>
<span class="normal">1271</span>
<span class="normal">1272</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="n">ctx</span><span class="p">,</span>
    <span class="n">q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">kv</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">softmax_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform the forward pass of FlashAttention with packed key and value tensors.</span>

<span class="sd">    Args:</span>
<span class="sd">        q (torch.Tensor): Query tensor of shape (batch, seqlen_q, nheads, headdim).</span>
<span class="sd">        kv (torch.Tensor): Key and value tensor of shape (batch, seqlen_k, 2, nheads, headdim).</span>
<span class="sd">        bias (Optional[torch.Tensor]): Bias tensor, shape broadcastable to (batch, nheads, seqlen_q, seqlen_k).</span>
<span class="sd">            For example, ALiBi mask for causal would have shape (1, nheads, 1, seqlen_k).</span>
<span class="sd">            ALiBi mask for non-causal would have shape (1, nheads, seqlen_q, seqlen_k).</span>
<span class="sd">        causal (bool): Whether to apply causal masking. Default is False.</span>
<span class="sd">        softmax_scale (Optional[float]): Scaling factor for the softmax operation. Default is None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Output tensor after applying FlashAttention.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Make sure that the last dimension is contiguous</span>
    <span class="n">q</span><span class="p">,</span> <span class="n">kv</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">x</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="n">q</span><span class="p">,</span> <span class="n">kv</span><span class="p">]]</span>
    <span class="n">o</span><span class="p">,</span> <span class="n">lse</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">=</span> <span class="n">_flash_attn_forward</span><span class="p">(</span>
        <span class="n">q</span><span class="p">,</span>
        <span class="n">kv</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">kv</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
        <span class="n">causal</span><span class="o">=</span><span class="n">causal</span><span class="p">,</span>
        <span class="n">softmax_scale</span><span class="o">=</span><span class="n">softmax_scale</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">kv</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">lse</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>
    <span class="k">return</span> <span class="n">o</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h3 id="scprint.model.flash_attn.flashattention.FlashAttnQKVPackedFunc" class="doc doc-heading">
            <code>FlashAttnQKVPackedFunc</code>


</h3>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.autograd.Function">Function</span></code></p>





  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h4 id="scprint.model.flash_attn.flashattention.FlashAttnQKVPackedFunc.forward" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">forward</span></code>

  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-staticmethod"><code>staticmethod</code></small>
  </span>

</h4>


    <div class="doc doc-contents ">

      <p>Forward pass for FlashAttention.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>ctx</code></b>
                  (<code><span title="torch.autograd.Function">Function</span></code>)
              –
              <div class="doc-md-description">
                <p>The context object to save information for backward computation.</p>
              </div>
            </li>
            <li>
              <b><code>qkv</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Input tensor of shape (batch, seqlen, 3, nheads, headdim).</p>
              </div>
            </li>
            <li>
              <b><code>bias</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Optional bias tensor, shape broadcastible to (batch, nheads, seqlen, seqlen).
For example, ALiBi mask for causal would have shape (1, nheads, 1, seqlen).
ALiBi mask for non-causal would have shape (1, nheads, seqlen, seqlen).</p>
              </div>
            </li>
            <li>
              <b><code>causal</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to apply causal masking. Default is False.</p>
              </div>
            </li>
            <li>
              <b><code>softmax_scale</code></b>
                  (<code><span title="typing.Optional">Optional</span>[float]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Optional scaling factor for softmax. Default is None.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="torch.Tensor">Tensor</span></code>
              –
              <div class="doc-md-description">
                <p>torch.Tensor: Output tensor after applying FlashAttention.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/flash_attn/flashattention.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1167</span>
<span class="normal">1168</span>
<span class="normal">1169</span>
<span class="normal">1170</span>
<span class="normal">1171</span>
<span class="normal">1172</span>
<span class="normal">1173</span>
<span class="normal">1174</span>
<span class="normal">1175</span>
<span class="normal">1176</span>
<span class="normal">1177</span>
<span class="normal">1178</span>
<span class="normal">1179</span>
<span class="normal">1180</span>
<span class="normal">1181</span>
<span class="normal">1182</span>
<span class="normal">1183</span>
<span class="normal">1184</span>
<span class="normal">1185</span>
<span class="normal">1186</span>
<span class="normal">1187</span>
<span class="normal">1188</span>
<span class="normal">1189</span>
<span class="normal">1190</span>
<span class="normal">1191</span>
<span class="normal">1192</span>
<span class="normal">1193</span>
<span class="normal">1194</span>
<span class="normal">1195</span>
<span class="normal">1196</span>
<span class="normal">1197</span>
<span class="normal">1198</span>
<span class="normal">1199</span>
<span class="normal">1200</span>
<span class="normal">1201</span>
<span class="normal">1202</span>
<span class="normal">1203</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="n">ctx</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">,</span>
    <span class="n">qkv</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">causal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">softmax_scale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Forward pass for FlashAttention.</span>

<span class="sd">    Args:</span>
<span class="sd">        ctx (torch.autograd.Function): The context object to save information for backward computation.</span>
<span class="sd">        qkv (torch.Tensor): Input tensor of shape (batch, seqlen, 3, nheads, headdim).</span>
<span class="sd">        bias (Optional[torch.Tensor]): Optional bias tensor, shape broadcastible to (batch, nheads, seqlen, seqlen).</span>
<span class="sd">            For example, ALiBi mask for causal would have shape (1, nheads, 1, seqlen).</span>
<span class="sd">            ALiBi mask for non-causal would have shape (1, nheads, seqlen, seqlen).</span>
<span class="sd">        causal (bool): Whether to apply causal masking. Default is False.</span>
<span class="sd">        softmax_scale (Optional[float]): Optional scaling factor for softmax. Default is None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Output tensor after applying FlashAttention.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Make sure that the last dimension is contiguous</span>
    <span class="k">if</span> <span class="n">qkv</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">qkv</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="n">o</span><span class="p">,</span> <span class="n">lse</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">softmax_scale</span> <span class="o">=</span> <span class="n">_flash_attn_forward</span><span class="p">(</span>
        <span class="n">qkv</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">qkv</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">qkv</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">],</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
        <span class="n">causal</span><span class="o">=</span><span class="n">causal</span><span class="p">,</span>
        <span class="n">softmax_scale</span><span class="o">=</span><span class="n">softmax_scale</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">qkv</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="n">lse</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
    <span class="n">ctx</span><span class="o">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>
    <span class="k">return</span> <span class="n">o</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="scprint.model.flash_attn.activations" class="doc doc-heading">
            <code>scprint.model.flash_attn.activations</code>


</h2>

    <div class="doc doc-contents first">



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="scprint.model.flash_attn.activations.bias_gelu_back" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">bias_gelu_back</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Assume that y has shape (B, D) and bias has shape (D)</p>

            <details class="quote">
              <summary>Source code in <code>scprint/model/flash_attn/activations.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">script</span>
<span class="k">def</span> <span class="nf">bias_gelu_back</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Assume that y has shape (B, D) and bias has shape (D)&quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">+</span> <span class="n">y</span>
    <span class="n">tanh_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="mf">0.79788456</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mf">0.044715</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span><span class="p">))</span>
    <span class="c1"># sqrt(2/pi) * 3 * 0.044715 -&gt; 0.1070322243</span>
    <span class="n">ff</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span>
        <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">tanh_out</span> <span class="o">*</span> <span class="n">tanh_out</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.79788456</span> <span class="o">+</span> <span class="mf">0.1070322243</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
    <span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">tanh_out</span><span class="p">)</span>
    <span class="n">grad_y</span> <span class="o">=</span> <span class="n">ff</span> <span class="o">*</span> <span class="n">g</span>
    <span class="k">return</span> <span class="n">grad_y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">grad_y</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">bias</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<h2 id="scprint.model.flash_attn.layer_norm" class="doc doc-heading">
            <code>scprint.model.flash_attn.layer_norm</code>


</h2>

    <div class="doc doc-contents first">



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="scprint.model.flash_attn.layer_norm.layer_norm_ref" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">layer_norm_ref</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Reference implementation of Layer Normalization with optional dropout and residual connections.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Input tensor.</p>
              </div>
            </li>
            <li>
              <b><code>weight</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Weight tensor for normalization.</p>
              </div>
            </li>
            <li>
              <b><code>bias</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Bias tensor for normalization.</p>
              </div>
            </li>
            <li>
              <b><code>residual</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Residual tensor to be added to the input.</p>
              </div>
            </li>
            <li>
              <b><code>x1</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Additional input tensor for parallel LayerNorm.</p>
              </div>
            </li>
            <li>
              <b><code>weight1</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Additional weight tensor for parallel LayerNorm.</p>
              </div>
            </li>
            <li>
              <b><code>bias1</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Additional bias tensor for parallel LayerNorm.</p>
              </div>
            </li>
            <li>
              <b><code>eps</code></b>
                  (<code>float</code>, default:
                      <code>1e-06</code>
)
              –
              <div class="doc-md-description">
                <p>Epsilon value to avoid division by zero.</p>
              </div>
            </li>
            <li>
              <b><code>dropout_p</code></b>
                  (<code>float</code>, default:
                      <code>0.0</code>
)
              –
              <div class="doc-md-description">
                <p>Dropout probability.</p>
              </div>
            </li>
            <li>
              <b><code>rowscale</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Row scaling tensor.</p>
              </div>
            </li>
            <li>
              <b><code>prenorm</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to return the prenormalized output.</p>
              </div>
            </li>
            <li>
              <b><code>dropout_mask</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Dropout mask for the input tensor.</p>
              </div>
            </li>
            <li>
              <b><code>dropout_mask1</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Dropout mask for the additional input tensor.</p>
              </div>
            </li>
            <li>
              <b><code>upcast</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to upcast the input tensors to float.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>, ...]]</code>
              –
              <div class="doc-md-description">
                <p>Union[torch.Tensor, Tuple[torch.Tensor, ...]]: Normalized output tensor(s).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/flash_attn/layer_norm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">layer_norm_ref</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">residual</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">x1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">weight1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bias1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
    <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">rowscale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prenorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dropout_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout_mask1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">upcast</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reference implementation of Layer Normalization with optional dropout and residual connections.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): Input tensor.</span>
<span class="sd">        weight (torch.Tensor): Weight tensor for normalization.</span>
<span class="sd">        bias (Optional[torch.Tensor]): Bias tensor for normalization.</span>
<span class="sd">        residual (Optional[torch.Tensor]): Residual tensor to be added to the input.</span>
<span class="sd">        x1 (Optional[torch.Tensor]): Additional input tensor for parallel LayerNorm.</span>
<span class="sd">        weight1 (Optional[torch.Tensor]): Additional weight tensor for parallel LayerNorm.</span>
<span class="sd">        bias1 (Optional[torch.Tensor]): Additional bias tensor for parallel LayerNorm.</span>
<span class="sd">        eps (float): Epsilon value to avoid division by zero.</span>
<span class="sd">        dropout_p (float): Dropout probability.</span>
<span class="sd">        rowscale (Optional[torch.Tensor]): Row scaling tensor.</span>
<span class="sd">        prenorm (bool): Whether to return the prenormalized output.</span>
<span class="sd">        dropout_mask (Optional[torch.Tensor]): Dropout mask for the input tensor.</span>
<span class="sd">        dropout_mask1 (Optional[torch.Tensor]): Dropout mask for the additional input tensor.</span>
<span class="sd">        upcast (bool): Whether to upcast the input tensors to float.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Union[torch.Tensor, Tuple[torch.Tensor, ...]]: Normalized output tensor(s).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">upcast</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">residual</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">residual</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">x1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">weight1</span> <span class="o">=</span> <span class="n">weight1</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">weight1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">bias1</span> <span class="o">=</span> <span class="n">bias1</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">bias1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">x1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">rowscale</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;rowscale is not supported with parallel LayerNorm&quot;</span>
    <span class="k">if</span> <span class="n">rowscale</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">rowscale</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">dropout_p</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">dropout_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">dropout_mask</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">dropout_p</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">dropout_p</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">dropout_mask1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">x1</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">dropout_mask1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">dropout_p</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">dropout_p</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x1</span>
    <span class="k">if</span> <span class="n">residual</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">residual</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span>
        <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:],</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weight1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">prenorm</span> <span class="k">else</span> <span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span>
            <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">weight1</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:],</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">out1</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">prenorm</span> <span class="k">else</span> <span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">out1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="scprint.model.flash_attn.layer_norm.rms_norm_ref" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">rms_norm_ref</span></code>

</h3>


    <div class="doc doc-contents ">

      <p>Reference implementation of RMS Normalization with optional dropout and residual connections.</p>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>x</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Input tensor.</p>
              </div>
            </li>
            <li>
              <b><code>weight</code></b>
                  (<code><span title="torch.Tensor">Tensor</span></code>)
              –
              <div class="doc-md-description">
                <p>Weight tensor for normalization.</p>
              </div>
            </li>
            <li>
              <b><code>bias</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Bias tensor for normalization.</p>
              </div>
            </li>
            <li>
              <b><code>residual</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Residual tensor to be added to the input.</p>
              </div>
            </li>
            <li>
              <b><code>x1</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Additional input tensor for parallel RMSNorm.</p>
              </div>
            </li>
            <li>
              <b><code>weight1</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Additional weight tensor for parallel RMSNorm.</p>
              </div>
            </li>
            <li>
              <b><code>bias1</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Additional bias tensor for parallel RMSNorm.</p>
              </div>
            </li>
            <li>
              <b><code>eps</code></b>
                  (<code>float</code>, default:
                      <code>1e-06</code>
)
              –
              <div class="doc-md-description">
                <p>Epsilon value to avoid division by zero.</p>
              </div>
            </li>
            <li>
              <b><code>dropout_p</code></b>
                  (<code>float</code>, default:
                      <code>0.0</code>
)
              –
              <div class="doc-md-description">
                <p>Dropout probability.</p>
              </div>
            </li>
            <li>
              <b><code>rowscale</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Row scaling tensor.</p>
              </div>
            </li>
            <li>
              <b><code>prenorm</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to return the prenormalized output.</p>
              </div>
            </li>
            <li>
              <b><code>dropout_mask</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Dropout mask for the input tensor.</p>
              </div>
            </li>
            <li>
              <b><code>dropout_mask1</code></b>
                  (<code><span title="typing.Optional">Optional</span>[<span title="torch.Tensor">Tensor</span>]</code>, default:
                      <code>None</code>
)
              –
              <div class="doc-md-description">
                <p>Dropout mask for the additional input tensor.</p>
              </div>
            </li>
            <li>
              <b><code>upcast</code></b>
                  (<code>bool</code>, default:
                      <code>False</code>
)
              –
              <div class="doc-md-description">
                <p>Whether to upcast the input tensors to float.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>

<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Returns:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
                  <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, <span title="typing.Tuple">Tuple</span>[<span title="torch.Tensor">Tensor</span>, ...]]</code>
              –
              <div class="doc-md-description">
                <p>Union[torch.Tensor, Tuple[torch.Tensor, ...]]: Normalized output tensor(s).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>
            <details class="quote">
              <summary>Source code in <code>scprint/model/flash_attn/layer_norm.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">rms_norm_ref</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">residual</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">x1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">weight1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bias1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
    <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">rowscale</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prenorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dropout_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout_mask1</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">upcast</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reference implementation of RMS Normalization with optional dropout and residual connections.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): Input tensor.</span>
<span class="sd">        weight (torch.Tensor): Weight tensor for normalization.</span>
<span class="sd">        bias (Optional[torch.Tensor]): Bias tensor for normalization.</span>
<span class="sd">        residual (Optional[torch.Tensor]): Residual tensor to be added to the input.</span>
<span class="sd">        x1 (Optional[torch.Tensor]): Additional input tensor for parallel RMSNorm.</span>
<span class="sd">        weight1 (Optional[torch.Tensor]): Additional weight tensor for parallel RMSNorm.</span>
<span class="sd">        bias1 (Optional[torch.Tensor]): Additional bias tensor for parallel RMSNorm.</span>
<span class="sd">        eps (float): Epsilon value to avoid division by zero.</span>
<span class="sd">        dropout_p (float): Dropout probability.</span>
<span class="sd">        rowscale (Optional[torch.Tensor]): Row scaling tensor.</span>
<span class="sd">        prenorm (bool): Whether to return the prenormalized output.</span>
<span class="sd">        dropout_mask (Optional[torch.Tensor]): Dropout mask for the input tensor.</span>
<span class="sd">        dropout_mask1 (Optional[torch.Tensor]): Dropout mask for the additional input tensor.</span>
<span class="sd">        upcast (bool): Whether to upcast the input tensors to float.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Union[torch.Tensor, Tuple[torch.Tensor, ...]]: Normalized output tensor(s).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dtype</span>
    <span class="k">if</span> <span class="n">upcast</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">residual</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">residual</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">x1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">weight1</span> <span class="o">=</span> <span class="n">weight1</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">weight1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="n">bias1</span> <span class="o">=</span> <span class="n">bias1</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="k">if</span> <span class="n">bias1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">x1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">rowscale</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;rowscale is not supported with parallel LayerNorm&quot;</span>
    <span class="k">if</span> <span class="n">rowscale</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">rowscale</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">dropout_p</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">dropout_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">dropout_mask</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">dropout_p</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">dropout_p</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">dropout_mask1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">x1</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">dropout_mask1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">dropout_p</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">dropout_p</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x1</span>
    <span class="k">if</span> <span class="n">residual</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">residual</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">rstd</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">square</span><span class="p">())</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">((</span><span class="n">x</span> <span class="o">*</span> <span class="n">rstd</span> <span class="o">*</span> <span class="n">weight</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span> <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">rstd</span> <span class="o">*</span> <span class="n">weight</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
        <span class="n">dtype</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">weight1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">prenorm</span> <span class="k">else</span> <span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out1</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">rstd</span> <span class="o">*</span> <span class="n">weight1</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias1</span> <span class="k">if</span> <span class="n">bias1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">rstd</span> <span class="o">*</span> <span class="n">weight1</span><span class="p">)</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">out1</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">prenorm</span> <span class="k">else</span> <span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">out1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../notebooks/cancer_usecase_part2/" class="btn btn-neutral float-left" title="scPRINT use case on BPH (part 2, GN analysis)"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../tasks/" class="btn btn-neutral float-right" title="tasks">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../notebooks/cancer_usecase_part2/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../tasks/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
