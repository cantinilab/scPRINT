trainer:
  precision: 16-mixed
  # profiler: simple
  gradient_clip_val: 500
  strategy: ddp_find_unused_parameters_true
  limit_val_batches: 8000
  callbacks:
    - class_path: scprint.trainer.TrainingMode
      init_args:
        do_denoise: True
        noise: 
          - 0.3
        do_generate: True
        run_full_forward: False
        do_cls: True
        class_scale: 2
        warmup_duration: 500
        fused_adam: True
        lr_reduce_factor: .66
        lr_reduce_patience: 2
model:
  lr: 0.0001
  optim: "adamW"
  weight_decay: 0.02
  nhead: 8
  nlayers: 16
  layers_cls: [512]
  d_model: 512
data:
  max_len: 2200
  weight_scaler: 40
  train_oversampling_per_epoch: 0.4
  batch_size: 16
  num_workers: 12
