trainer:
  gradient_clip_val: 500
  log_every_n_steps: 100
  limit_train_batches: 5000
  limit_val_batches: 1000
  max_time:
    hours: 48
model:
  lr: 0.0001
  optim: "adamW"
  weight_decay: 0.02
  nhead: 4
  nlayers: 8
  freeze_embeddings: False
  layers_cls: [256]
  d_model: 256
data:
  collection_name: all no zhang13M #preprocessed dataset
  max_len: 2200
  weight_scaler: 50
  train_oversampling_per_epoch: 0.3
  validation_split: 0.02
  test_split: 0.02
  batch_size: 64
  num_workers: 16
