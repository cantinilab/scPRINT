{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn, Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zinb(\n",
    "    target: Tensor,\n",
    "    mu: Tensor,\n",
    "    theta: Tensor,\n",
    "    pi: Tensor,\n",
    "    eps=1e-8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes zero-inflated negative binomial (ZINB) loss.\n",
    "\n",
    "    This function was modified from scvi-tools.\n",
    "\n",
    "    Args:\n",
    "        target (Tensor): Torch Tensor of ground truth data.\n",
    "        mu (Tensor): Torch Tensor of means of the negative binomial (must have positive support).\n",
    "        theta (Tensor): Torch Tensor of inverse dispersion parameter (must have positive support).\n",
    "        pi (Tensor): Torch Tensor of logits of the dropout parameter (real support).\n",
    "        eps (float, optional): Numerical stability constant. Defaults to 1e-8.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: ZINB loss value.\n",
    "    \"\"\"\n",
    "    # Â uses log(sigmoid(x)) = -softplus(-x)\n",
    "    softplus_pi = F.softplus(-pi)\n",
    "    # eps to make it positive support and taking the log\n",
    "    log_theta_mu_eps = torch.log(theta + mu + eps)\n",
    "    pi_theta_log = -pi + theta * (torch.log(theta + eps) - log_theta_mu_eps)\n",
    "\n",
    "    case_zero = F.softplus(pi_theta_log) - softplus_pi\n",
    "    mul_case_zero = torch.mul((target < eps).type(torch.float32), case_zero)\n",
    "\n",
    "    case_non_zero = (\n",
    "        -softplus_pi\n",
    "        + pi_theta_log\n",
    "        + target * (torch.log(mu + eps) - log_theta_mu_eps)\n",
    "        + torch.lgamma(target + theta)\n",
    "        - torch.lgamma(theta)\n",
    "        - torch.lgamma(target + 1)\n",
    "    )\n",
    "    mul_case_non_zero = torch.mul((target > eps).type(torch.float32), case_non_zero)\n",
    "\n",
    "    res = mul_case_zero + mul_case_non_zero\n",
    "    # we want to minize the loss but maximize the log likelyhood\n",
    "    return -res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zinb_sonnet(\n",
    "    target: Tensor,\n",
    "    mu: Tensor,\n",
    "    theta: Tensor,\n",
    "    pi: Tensor,\n",
    "    eps=1e-8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes zero-inflated negative binomial (ZINB) loss updated to improve numerical stability with sonnet\n",
    "\n",
    "    This function is modified to improve numerical stability and avoid using lgamma.\n",
    "\n",
    "    Args:\n",
    "        target (Tensor): Torch Tensor of ground truth data.\n",
    "        mu (Tensor): Torch Tensor of means of the negative binomial (must have positive support).\n",
    "        theta (Tensor): Torch Tensor of inverse dispersion parameter (must have positive support).\n",
    "        pi (Tensor): Torch Tensor of logits of the dropout parameter (real support).\n",
    "        eps (float, optional): Numerical stability constant. Defaults to 1e-8.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: ZINB loss value.\n",
    "    \"\"\"\n",
    "    # Compute log(1 - sigmoid(pi)) more accurately using -softplus(pi)\n",
    "    log_neg_pi = -F.softplus(pi)\n",
    "    \n",
    "    # Compute log(theta + mu) more accurately\n",
    "    log_theta_mu = torch.log(theta + mu + eps)\n",
    "    \n",
    "    # Compute log(1 + mu/theta) more accurately\n",
    "    log_1_plus_mu_theta = F.softplus(torch.log(mu + eps) - torch.log(theta + eps))\n",
    "    \n",
    "    # Compute log likelihood for zero values\n",
    "    ll_zero = F.softplus(theta * (torch.log(theta + eps) - log_theta_mu) - pi)\n",
    "    \n",
    "    # Compute log likelihood for non-zero values\n",
    "    ll_non_zero = (\n",
    "        log_neg_pi\n",
    "        + theta * torch.log(theta + eps)\n",
    "        - (theta + target) * log_theta_mu\n",
    "        + target * torch.log(mu + eps)\n",
    "        - torch.lgamma(target + 1)\n",
    "        + torch.lgamma(theta + target)\n",
    "        - torch.lgamma(theta)\n",
    "    )\n",
    "    \n",
    "    # Combine zero and non-zero cases\n",
    "    ll = torch.where(target < eps, ll_zero, ll_non_zero)\n",
    "    \n",
    "    # Return negative mean log-likelihood\n",
    "    return -ll.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb(target: Tensor, mu: Tensor, theta: Tensor, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Computes the negative binomial (NB) loss.\n",
    "\n",
    "    This function was adapted from scvi-tools.\n",
    "\n",
    "    Args:\n",
    "        target (Tensor): Ground truth data.\n",
    "        mu (Tensor): Means of the negative binomial distribution (must have positive support).\n",
    "        theta (Tensor): Inverse dispersion parameter (must have positive support).\n",
    "        eps (float, optional): Numerical stability constant. Defaults to 1e-8.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: NB loss value.\n",
    "    \"\"\"\n",
    "    if theta.ndimension() == 1:\n",
    "        theta = theta.view(1, theta.size(0))\n",
    "\n",
    "    log_theta_mu_eps = torch.log(theta + mu + eps)\n",
    "    res = (\n",
    "        theta * (torch.log(theta + eps) - log_theta_mu_eps)\n",
    "        + target * (torch.log(mu + eps) - log_theta_mu_eps)\n",
    "        + torch.lgamma(target + theta)\n",
    "        - torch.lgamma(theta)\n",
    "        - torch.lgamma(target + 1)\n",
    "    )\n",
    "\n",
    "    return -res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ZINB Loss: 1.615840196609497\n",
      "Original ZINB Loss with error term: 1.616633653640747\n",
      "New ZINB Loss: 1.615966796875\n",
      "NB Loss: 1.179560899734497\n"
     ]
    }
   ],
   "source": [
    "# Test both functions with the same input\n",
    "THETA = 10_000 # above this, it gets worse\n",
    "\n",
    "TARGET = [100,10,10,1,1,0,0,0]\n",
    "MINPI = 0.01\n",
    "MAXPI = 100\n",
    "ERROR = [1,0.1,0.1,0,0,100,100,100]\n",
    "\n",
    "target = torch.Tensor(TARGET)\n",
    "mu = torch.Tensor(TARGET)\n",
    "theta = torch.Tensor([THETA]*len(TARGET))\n",
    "pi = torch.Tensor([MINPI,MINPI,MINPI,MINPI,MINPI,MAXPI,MAXPI,MAXPI])\n",
    "\n",
    "# Test original zinb function\n",
    "original_loss = zinb(target, mu, theta, pi)\n",
    "print(f\"Original ZINB Loss: {original_loss.item()}\")\n",
    "\n",
    "# Test original zinb function with error\n",
    "original_loss = zinb(target, mu+torch.Tensor(ERROR), theta, pi)\n",
    "print(f\"Original ZINB Loss with error term: {original_loss.item()}\")\n",
    "\n",
    "# Test updated zinb_sonnet function\n",
    "new_loss = zinb_sonnet(target, mu, theta, pi)\n",
    "print(f\"New ZINB Loss: {new_loss.item()}\")\n",
    "\n",
    "# Test nb function\n",
    "nb_loss = nb(target, mu, theta)\n",
    "print(f\"NB Loss: {nb_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scprint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
