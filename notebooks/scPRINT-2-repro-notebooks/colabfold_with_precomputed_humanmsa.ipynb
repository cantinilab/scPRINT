{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/gist/jkobject/4871462939758394793fde702666ba1c/colabfold_with_precomputed_humanmsa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4yBrceuFbf3"
   },
   "source": [
    "## Colabfold with pre-computed MSA\n",
    "\n",
    "Protein structure and complex prediction using\n",
    "[AlphaFold2](https://www.nature.com/articles/s41586-021-03819-2) and\n",
    "[Alphafold2-multimer](https://www.biorxiv.org/content/10.1101/2021.10.04.463034v1)\n",
    "with pre-constructed alignments for human proteomes. All pre-constructed\n",
    "alignments are available from\n",
    "[human PPI database](http://prodata.swmed.edu/humanPPI). This Colab notebook is\n",
    "only for view. To run the code, please use playground mode here:\n",
    "[run code](https://colab.research.google.com/drive/1suhoIB5q6xn0APFHJE8c1eMiCuv9gCk_#offline=true&sandboxMode=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOblAo-xetgx"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ef6ce49db04af1afcab0a8bfd709aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5c9838acd64834ade7a73656d6b957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Add entity', layout=Layout(width='200px'), style=ButtonStyle()),), layout=L…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09756dc21a041fea3e2129be3e57059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='Submit', layout=Layout(width='200px'), style=ButtonStyle()),), layout=Layou…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Input protein(s), then hit `Submit` -> `Submit selections`\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "import random\n",
    "import ipywidgets as widgets\n",
    "import IPython\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from sys import version_info\n",
    "python_version = f\"{version_info.major}.{version_info.minor}\"\n",
    "\n",
    "os.system(\"wget --no-check-certificate -qnc https://conglab.swmed.edu/humanPPI/uniprot_function .\")\n",
    "\n",
    "global collected_protein_data\n",
    "collected_protein_data = []\n",
    "\n",
    "# Predefined options for dropdowns\n",
    "with open('uniprot_function') as f:\n",
    "  prots=f.readlines()[1:]\n",
    "prots=[i.strip().split('\\t') for i in prots]\n",
    "options=[]\n",
    "map2uniprot={}\n",
    "for i in prots:\n",
    "  options.append(i[0])\n",
    "  map2uniprot[i[0]]=i[0]\n",
    "  for j in i[1].split():\n",
    "    options.append(j)\n",
    "    map2uniprot[j]=i[0]\n",
    "  options.append(i[2])\n",
    "  map2uniprot[i[2]]=i[0]\n",
    "\n",
    "# Create container for entries\n",
    "entry_container = widgets.VBox([])\n",
    "display(entry_container)\n",
    "\n",
    "def create_entry():\n",
    "    dropdown = widgets.Combobox(\n",
    "        options=options,\n",
    "        description='Protein:',\n",
    "        placeholder='Type or select a protein',\n",
    "        ensure_option=True,\n",
    "        disabled=False\n",
    "    )\n",
    "    textbox = widgets.Text(\n",
    "        description='Copies:',\n",
    "        disabled=False\n",
    "    )\n",
    "    hbox = widgets.HBox([dropdown, textbox],layout=widgets.Layout(display='flex',justify_content='center', width='100%'))\n",
    "    return hbox\n",
    "\n",
    "first_entry = create_entry()\n",
    "entry_container.children = [first_entry]\n",
    "\n",
    "\n",
    "def on_add_button_clicked(b):\n",
    "    new_entry = create_entry()\n",
    "    entry_container.children = tuple(list(entry_container.children) + [new_entry])\n",
    "\n",
    "\n",
    "add_button = widgets.Button(description=\"Add entity\",layout=widgets.Layout(width='200px'))\n",
    "hbox_addbutton = widgets.HBox(children=[add_button],layout=widgets.Layout(display='flex',justify_content='center', width='100%'))\n",
    "#add_button = widgets.Button(description=\"Add More Protein\", layout=button_layout)\n",
    "display(hbox_addbutton)\n",
    "\n",
    "add_button.on_click(on_add_button_clicked)\n",
    "\n",
    "def gather_and_process_inputs():\n",
    "    protein_data = []\n",
    "    for entry in entry_container.children:\n",
    "        protein_name = entry.children[0].value\n",
    "        copy_number = entry.children[1].value\n",
    "        if protein_name and copy_number:\n",
    "            protein_data.append((protein_name, copy_number))\n",
    "    return protein_data\n",
    "\n",
    "submit_button = widgets.Button(description=\"Submit\",layout=widgets.Layout(width='200px'))\n",
    "hbox_submitbutton = widgets.HBox(children=[submit_button],layout=widgets.Layout(display='flex',justify_content='center', width='100%'))\n",
    "display(hbox_submitbutton)\n",
    "\n",
    "def display_protein_data():\n",
    "    print(\"Current Protein Data:\", collected_protein_data)\n",
    "\n",
    "def on_submit_clicked(b):\n",
    "    global collected_protein_data\n",
    "    collected_protein_data = gather_and_process_inputs()\n",
    "    collected_protein_data=[[map2uniprot[i[0]],i[1]] for i in collected_protein_data]\n",
    "    if len(set([i[0] for i in collected_protein_data]))!=len(collected_protein_data):\n",
    "      print(\"Duplicated entries submitted, please check.\")\n",
    "      display_protein_data()\n",
    "      return\n",
    "    display_protein_data()\n",
    "\n",
    "submit_button.on_click(on_submit_clicked)\n",
    "\n",
    "collected_protein_data=[[map2uniprot[i[0]],i[1]] for i in collected_protein_data]P41567_P62328\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_OKCONZZM87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobname test2_cd4fc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "import random\n",
    "\n",
    "\n",
    "def add_hash(x, y):\n",
    "    return x + \"_\" + hashlib.sha1(y.encode()).hexdigest()[:5]\n",
    "\n",
    "\n",
    "jobname = \"test2\"  # @param {type:\"string\"}\n",
    "# number of models to use\n",
    "num_relax = 0  # @param [0, 1, 5] {type:\"raw\"}\n",
    "# @markdown - specify how many of the top ranked structures to relax using amber\n",
    "template_mode = \"pdb100\"  # @param [\"none\", \"pdb100\",\"custom\"]\n",
    "# @markdown - `none` = no template information is used. `pdb100` = detect templates in pdb100 (see [notes](#pdb100)). `custom` - upload and search own templates (PDB or mmCIF format, see [notes](#custom_templates))\n",
    "\n",
    "basejobname = \"\".join(jobname.split())\n",
    "basejobname = re.sub(r\"\\W+\", \"\", basejobname)\n",
    "jobname = add_hash(basejobname, \"_\".join([i[0] for i in collected_protein_data]))\n",
    "\n",
    "\n",
    "# check if directory with jobname exists\n",
    "def check(folder):\n",
    "    if os.path.exists(folder):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "if not check(jobname):\n",
    "    n = 0\n",
    "    while not check(f\"{jobname}_{n}\"):\n",
    "        n += 1\n",
    "    jobname = f\"{jobname}_{n}\"\n",
    "\n",
    "# make directory to save results\n",
    "os.makedirs(jobname, exist_ok=True)\n",
    "\n",
    "use_templates = True\n",
    "custom_template_path = None\n",
    "\n",
    "print(\"jobname\", jobname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzIKiDiCaHAn"
   },
   "outputs": [],
   "source": [
    "#@title Install dependencies\n",
    "%%time\n",
    "import os\n",
    "USE_TEMPLATES = use_templates\n",
    "PYTHON_VERSION = python_version\n",
    "\n",
    "if not os.path.isfile(\"COLABFOLD_READY\"):\n",
    "  print(\"installing colabfold...\")\n",
    "  os.system(\"uv pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\")\n",
    "  if os.environ.get('TPU_NAME', False) != False:\n",
    "    os.system(\"uv pip uninstall -y jax jaxlib\")\n",
    "    os.system(\"uv pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\")\n",
    "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\")\n",
    "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\")\n",
    "  # hack to fix TF crash\n",
    "  os.system(\"rm -f /usr/local/lib/python3.*/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so\")\n",
    "  os.system(\"touch COLABFOLD_READY\")\n",
    "\n",
    "if not os.path.isfile(\"HH_READY\"):\n",
    "  if not os.path.isfile(\"CONDA_READY\"):\n",
    "    print(\"installing conda...\")\n",
    "    os.system(\"wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\")\n",
    "    os.system(\"bash Miniforge3-Linux-x86_64.sh -bfp ../\")\n",
    "    os.system(\"mamba config --set auto_update_conda false\")\n",
    "    os.system(\"touch CONDA_READY\")\n",
    "  os.system(f\"mamba install -y -c conda-forge -c bioconda hhsuite=3.3.0\")\n",
    "  os.system(\"touch HH_READY\")\n",
    "\n",
    "if USE_TEMPLATES:\n",
    "  os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 python='{PYTHON_VERSION}'\")\n",
    "\n",
    "if not os.path.exists('bad_pairs'):\n",
    "  print(\"downloading the supplemetary file\")\n",
    "  os.system(\"wget --no-check-certificate -qnc https://conglab.swmed.edu/humanPPI/bad_pairs.gz .\")\n",
    "  os.system('gzip -d bad_pairs.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2_sh2uAonJH"
   },
   "outputs": [],
   "source": [
    "# @markdown ## Other settings\n",
    "msa_mode = \"custom\"\n",
    "# @markdown #### MSA generation settings\n",
    "MSA_pair_mode = (\n",
    "    \"paired_unpaired\"  # @param [\"paired_unpaired\",\"paired\",\"unpaired\"] {type:\"string\"}\n",
    ")\n",
    "# @markdown - \"unpaired_paired\" = pair sequences from same species + unpaired MSA, \"unpaired\" = seperate MSA for each chain, \"paired\" - only use paired sequences.\n",
    "\n",
    "pairing_strategy = \"greedy\"  # @param [\"greedy\", \"complete\"] {type:\"string\"}\n",
    "# @markdown - `greedy` = pair any taxonomically matching subsets, `complete` = all sequences have to match in one line.\n",
    "\n",
    "filtering_val = True  # @param [\"True\", \"False\"] {type:\"string\"}\n",
    "# @markdown Choose to filter paralogues or poor quality paired alignment\n",
    "filtering_flag = \"1\" if filtering_val == \"True\" else \"0\"\n",
    "\n",
    "identity_filt = 95.0  # @param {type:\"number\"}\n",
    "# @markdown - Identity to filter out highly similar sequences for modeling\n",
    "\n",
    "lineage = \"All\"  # @param ['All','Mammalia (Mammal)','Chordata'] {type: \"raw\"}\n",
    "lineage = lineage.split()[0]\n",
    "# @markdown - Select lineage to include for modeling\n",
    "\n",
    "# @markdown #### Advanced model settings\n",
    "model_type = \"alphafold2_multimer_v3\"  # @param [\"auto\", \"alphafold2_ptm\", \"alphafold2_multimer_v1\", \"alphafold2_multimer_v2\", \"alphafold2_multimer_v3\", \"deepfold_v1\"]\n",
    "# @markdown Any of the mode_types can be used (regardless if input is monomer or complex).\n",
    "# @markdown - if `auto` selected, will use `alphafold2_ptm` for monomer prediction and `alphafold2_multimer_v3` for complex prediction.\n",
    "num_models = 1  # @param [\"1\", \"2\", \"3\", \"4\", \"5\"] {type:\"raw\"}\n",
    "# @markdown - number of models to model\n",
    "\n",
    "num_recycles = \"3\"  # @param [\"auto\", \"0\", \"1\", \"3\", \"6\", \"12\", \"24\", \"48\"]\n",
    "# @markdown - if `auto` selected, will use `num_recycles=20` if `model_type=alphafold2_multimer_v3`, else `num_recycles=3` .\n",
    "recycle_early_stop_tolerance = \"auto\"  # @param [\"auto\", \"0.0\", \"0.5\", \"1.0\"]\n",
    "# @markdown - if `auto` selected, will use `tol=0.5` if `model_type=alphafold2_multimer_v3` else `tol=0.0`.\n",
    "relax_max_iterations = 200  # @param [0, 200, 2000] {type:\"raw\"}\n",
    "# @markdown - max amber relax iterations, `0` = unlimited (AlphaFold2 default, can take very long)\n",
    "\n",
    "\n",
    "# @markdown #### Sample settings\n",
    "# @markdown -  enable dropouts and increase number of seeds to sample predictions from uncertainty of the model.\n",
    "# @markdown -  decrease `max_msa` to increase uncertainity\n",
    "max_msa = \"auto\"  # @param [\"auto\", \"512:1024\", \"256:512\", \"64:128\", \"32:64\", \"16:32\"]\n",
    "num_seeds = 1  # @param [1,2,4,8,16] {type:\"raw\"}\n",
    "use_dropout = False  # @param {type:\"boolean\"}\n",
    "\n",
    "num_recycles = None if num_recycles == \"auto\" else int(num_recycles)\n",
    "recycle_early_stop_tolerance = (\n",
    "    None\n",
    "    if recycle_early_stop_tolerance == \"auto\"\n",
    "    else float(recycle_early_stop_tolerance)\n",
    ")\n",
    "if max_msa == \"auto\":\n",
    "    max_msa = None\n",
    "\n",
    "# @markdown #### Save settings\n",
    "save_all = True  # @param {type:\"boolean\"}\n",
    "save_recycles = False  # @param {type:\"boolean\"}\n",
    "# @markdown -  if the save_to_google_drive option was selected, the result zip will be uploaded to your Google Drive\n",
    "dpi = 200  # @param {type:\"integer\"}\n",
    "# @markdown - set dpi for image resolution\n",
    "os.system(\n",
    "    \"wget --no-check-certificate -qnc http://prodata.swmed.edu/download/pub/test_fetch/bad_pairs\"\n",
    ")\n",
    "\n",
    "# @markdown Don't forget to hit `Runtime` -> `Run all` after updating the form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IJ7Ek4HaRUB"
   },
   "outputs": [],
   "source": [
    "#@title Download and generate MSA\n",
    "import os, copy,shutil, time\n",
    "from itertools import combinations\n",
    "def pair_msa(queries_def, tmpdir, lineage, MSA_pair_mode, pairing_strategy, a3m_file, filtering_flag):\n",
    "    prots = set([i[0] for i in queries_def])\n",
    "    bad_pairs = {}\n",
    "    f=open('bad_pairs')\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        if line[0] in prots and line[1] in prots:\n",
    "            bad_pairs[frozenset([line[0],line[1]])] = line[-1].split(',')\n",
    "    f.close()\n",
    "    def read_fasta(filename):\n",
    "        with open(filename) as f:\n",
    "            header = ''\n",
    "            sequence = ''\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>'):\n",
    "                    if header and sequence:\n",
    "                        yield (header, sequence)\n",
    "                    header = line[1:]\n",
    "                    sequence = ''\n",
    "                else:\n",
    "                    sequence += line\n",
    "            if header and sequence:\n",
    "                yield (header, sequence)\n",
    "\n",
    "    def write_header(f, queries_def):\n",
    "        lengths = []\n",
    "        copies = []\n",
    "        queries_order = []\n",
    "        for protein, copy, path in queries_def:\n",
    "            for header, sequence in read_fasta(path):\n",
    "                lengths.append(str(len(sequence)))\n",
    "                break\n",
    "            copies.append(copy)\n",
    "            queries_order.append(protein)\n",
    "        f.write(f\"#{','.join(lengths)}\\t{','.join(copies)}\\n\")\n",
    "        f.write('>' + '\\t'.join(queries_order) + '\\n')\n",
    "        for ent in queries_def:\n",
    "            for _, sequence in read_fasta(ent[-1]):\n",
    "                f.write(sequence)\n",
    "                break\n",
    "        f.write('\\n')\n",
    "        return queries_order, {p: l for p, l in zip(queries_order, lengths)}\n",
    "\n",
    "    if lineage != 'All':\n",
    "        for ent in queries_def:\n",
    "            if not os.path.exists(f'{tmpdir}/{ent[0]}_{lineage}.fas'):\n",
    "                filename = get_msa_lineage(ent[0], lineage, tmpdir)\n",
    "                ent.append(filename)\n",
    "            else:\n",
    "                ent.append(f'{tmpdir}/{ent[0]}_{lineage}.fas')\n",
    "    else:\n",
    "        for ent in queries_def:\n",
    "            shutil.copy(f\"{tmpdir}/{ent[0]}.fas\", f'{tmpdir}/{ent[0]}_all.fas')\n",
    "            ent.append(f'{tmpdir}/{ent[0]}_all.fas')\n",
    "\n",
    "    taxon_count = {}\n",
    "    for prot_index, protein in enumerate(queries_def):\n",
    "        for header, sequence in read_fasta(protein[-1]):\n",
    "            if header != 'query':\n",
    "                taxon = header.split()[0]\n",
    "                if taxon not in taxon_count:\n",
    "                    taxon_count[taxon] = []\n",
    "                taxon_count[taxon].append(prot_index)\n",
    "    protindex2taxons = {}\n",
    "    for taxon in taxon_count:\n",
    "        for k in range(1,len(taxon_count[taxon])+1):\n",
    "            combs = combinations(taxon_count[taxon], k)\n",
    "            for i in combs:\n",
    "                i=sorted(i)\n",
    "                try:\n",
    "                    protindex2taxons[tuple(i)].add(taxon)\n",
    "                except:\n",
    "                    protindex2taxons[tuple(i)] = set([taxon])\n",
    "\n",
    "\n",
    "    done_taxons = set()\n",
    "    fw = open(a3m_file, 'w')\n",
    "    queries_order, length_dic = write_header(fw, queries_def)\n",
    "    if MSA_pair_mode in ['paired', 'paired_unpaired']:\n",
    "        queries_index = [i for i in range(len(queries_def))]\n",
    "        print(queries_index)\n",
    "        for i in range(len(queries_def), 1, -1):\n",
    "            comb = combinations(queries_index, i)\n",
    "            all_good_taxons = set()\n",
    "            for j in comb:\n",
    "                j = tuple(sorted(j))\n",
    "                pairs4exclusion = set()\n",
    "                if filtering_flag == '1':\n",
    "                    pairs = combinations(j, 2)\n",
    "                    for pair in pairs:\n",
    "                        prot_pair = frozenset([queries_def[p][0] for p in pair])\n",
    "                        if prot_pair in bad_pairs:\n",
    "                            pairs4exclusion = pairs4exclusion.union(bad_pairs[prot_pair])\n",
    "                if j in protindex2taxons:\n",
    "                    good_taxons = protindex2taxons[j] - pairs4exclusion - done_taxons\n",
    "                    all_good_taxons = all_good_taxons.union(good_taxons)\n",
    "                    seqs = {}\n",
    "                    for taxon in good_taxons:\n",
    "                        seqs[taxon] = [0]*len(queries_order)\n",
    "                    for index, p in enumerate(queries_def):\n",
    "                        if index not in j:\n",
    "                            for taxon in good_taxons:\n",
    "                                seqs[taxon][index] = '-'*int(length_dic[p[0]])\n",
    "                        else:\n",
    "                            for header, sequence in read_fasta(p[-1]):\n",
    "                                taxon = header.split()[0]\n",
    "                                if taxon in good_taxons:\n",
    "                                    seqs[taxon][index] = sequence\n",
    "                    defline = []\n",
    "                    for taxon in good_taxons:\n",
    "                        defline = [taxon+'_'+queries_def[k][0] if k in j else 'DUMMY' for k in range(len(queries_def))]\n",
    "                        defline = '\\t'.join(defline)\n",
    "                        fw.write('>' + defline + '\\n' + ''.join(seqs[taxon]).upper()+'\\n')\n",
    "            done_taxons = done_taxons.union(all_good_taxons)\n",
    "            if pairing_strategy == 'complete':\n",
    "                break\n",
    "    print(done_taxons)\n",
    "    if MSA_pair_mode in ['unpaired', 'paired_unpaired']:\n",
    "        unpaired_taxons = set(taxon_count.keys()) - done_taxons\n",
    "\n",
    "        for index, query in enumerate(queries_def):\n",
    "            padding_left = ''\n",
    "            padding_right = ''\n",
    "            for i in range(len(queries_def)):\n",
    "                if i<index:\n",
    "                    padding_left = padding_left + '-'*int(length_dic[queries_def[i][0]])\n",
    "                elif i>index:\n",
    "                    padding_right = padding_right + '-'*int(length_dic[queries_def[i][0]])\n",
    "            file_name = query[-1]\n",
    "            for head, sequence in read_fasta(file_name):\n",
    "                if head == 'query':\n",
    "                    fw.write('>' + query[0] + '\\n')\n",
    "                    fw.write(f'{padding_left}{sequence}{padding_right}\\n')\n",
    "                elif head.split()[0] in unpaired_taxons:\n",
    "                    fw.write(f'>{head}\\n{padding_left}{sequence.upper()}{padding_right}\\n')\n",
    "    fw.close()\n",
    "    return a3m_file, length_dic\n",
    "\n",
    "def get_msa_lineage(pid, lineage, tmpdir):\n",
    "    f = open(f\"{tmpdir}/{pid}.fas\", 'r')\n",
    "    fw = open(f\"{tmpdir}/{pid}_{lineage}.fas\", 'w')\n",
    "    flag = 0\n",
    "    fw.write(next(f))\n",
    "    fw.write(next(f))\n",
    "    for line in f:\n",
    "        if line[0] == '>' and lineage in line.split()[1].split(':'):\n",
    "            flag =1\n",
    "            header = line\n",
    "        elif line[0] == '>' and lineage not in line.split()[1].split(':'):\n",
    "            flag = 0\n",
    "            header = line\n",
    "        elif line[0] != '>':\n",
    "            if flag == 1 :\n",
    "                fw.write(header)\n",
    "                fw.write(line)\n",
    "    fw.close()\n",
    "    f.close()\n",
    "    return f\"{tmpdir}/{pid}_{lineage}.fas\"\n",
    "\n",
    "\n",
    "a3m_file = os.path.join(jobname,f\"{jobname}.custom.a3m\")\n",
    "queries_path = a3m_file\n",
    "tmpdir = f'./{jobname}/tmp_msa'\n",
    "if not os.path.exists(tmpdir):\n",
    "    os.makedirs(tmpdir)\n",
    "\n",
    "for i in collected_protein_data:\n",
    "    os.system(f'wget --no-check-certificate -qnc  https://conglab.swmed.edu/humanPPI/MSAs/{i[0]}.fas -O ./{jobname}/tmp_msa/{i[0]}.fas')\n",
    "\n",
    "a3m_file, length_dic=pair_msa([list(i) for i in collected_protein_data], tmpdir, lineage, MSA_pair_mode, pairing_strategy, a3m_file, filtering_flag)\n",
    "time.sleep(2)\n",
    "cmd = f'../bin/hhfilter -i {a3m_file} -o {a3m_file[:-4]}.i{identity_filt}.a3m -id {identity_filt} -M first >& tmplog'\n",
    "!{cmd}\n",
    "os.system(f'mv {a3m_file} {a3m_file[:-4]}.noidentfilter.fas')\n",
    "os.system(f'mv {a3m_file[:-4]}.i{identity_filt}.a3m {a3m_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbaIO9pWjaN0"
   },
   "outputs": [],
   "source": [
    "# @title Run Prediction\n",
    "display_images = True  # @param {type:\"boolean\"}\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "from pathlib import Path\n",
    "from colabfold.download import download_alphafold_params, default_data_dir\n",
    "from colabfold.utils import setup_logging\n",
    "from colabfold.batch import get_queries, run, set_model_type\n",
    "from colabfold.plot import plot_msa_v2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\n",
    "except:\n",
    "    K80_chk = \"0\"\n",
    "    pass\n",
    "if \"1\" in K80_chk:\n",
    "    print(\"WARNING: found GPU Tesla K80: limited to total length < 1000\")\n",
    "    if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n",
    "        del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n",
    "    if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n",
    "        del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n",
    "\n",
    "from colabfold.colabfold import plot_protein\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def input_features_callback(input_features):\n",
    "    if display_images:\n",
    "        plot_msa_v2(input_features)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def prediction_callback(protein_obj, length, prediction_result, input_features, mode):\n",
    "    model_name, relaxed = mode\n",
    "    if not relaxed:\n",
    "        if display_images:\n",
    "            fig = plot_protein(protein_obj, Ls=length, dpi=150)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def calculate_contactprob_and_interactprob(jobname, collected_protein_data, length_dic):\n",
    "    import pickle, scipy, os\n",
    "    import numpy as np\n",
    "\n",
    "    chain2prot = {}\n",
    "    prot2chain = {}\n",
    "    resid2index = {}\n",
    "    allchains = []\n",
    "\n",
    "    lens = []\n",
    "    CHARS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n",
    "    index_count = 0\n",
    "    chain_count = 0\n",
    "    for i in range(len(collected_protein_data)):\n",
    "        prot = collected_protein_data[i]\n",
    "        for j in range(int(prot[1])):\n",
    "            lens.append(int(length_dic[prot[0]]))\n",
    "            allchains.append(CHARS[chain_count])\n",
    "            chain2prot[CHARS[chain_count]] = prot[0]\n",
    "            resid2index[CHARS[chain_count]] = {}\n",
    "            try:\n",
    "                prot2chain[prot[0]].append(CHARS[i * j + j])\n",
    "            except:\n",
    "                prot2chain[prot[0]] = [CHARS[i * j + j]]\n",
    "            for k in range(int(length_dic[prot[0]])):\n",
    "                resid2index[CHARS[chain_count]][k + 1] = index_count\n",
    "                index_count = index_count + 1\n",
    "            chain_count = +1\n",
    "\n",
    "    total_lens = sum(lens)\n",
    "\n",
    "    pickle_files = [i[:-1] for i in os.popen(f\"ls -1 {jobname}/*.pickle\")]\n",
    "    for name in pickle_files:\n",
    "        with open(name, \"rb\") as f:\n",
    "            prediction_result = pickle.load(f)\n",
    "        pdist = prediction_result[\"distogram\"][\"logits\"][:total_lens][:, :total_lens]\n",
    "        pdist = scipy.special.softmax(pdist, axis=-1)\n",
    "        prob12 = np.sum(pdist[:, :, :32], axis=-1)\n",
    "        prediction_result[\"contact_prob\"] = prob12.astype(np.float16)\n",
    "        prediction_result[\"interaction_prob\"] = {}\n",
    "        #    pickle.dump(prediction_result, open(name, 'wb'))\n",
    "\n",
    "        print(allchains)\n",
    "        resid2CAcoor = {chain: {} for chain in allchains}\n",
    "        resid2coors = {chain: {} for chain in allchains}\n",
    "        fp = open(\n",
    "            name.replace(\".pickle\", \".pdb\").replace(\"all_rank\", \"unrelaxed_rank\"), \"r\"\n",
    "        )\n",
    "        for line in fp:\n",
    "            if len(line) >= 50:\n",
    "                if line[:4] == \"ATOM\":\n",
    "                    atom = line[12:16].strip()\n",
    "                    resid = int(line[22:26])\n",
    "                    chainid = line[21]\n",
    "                    coorx = float(line[30:38])\n",
    "                    coory = float(line[38:46])\n",
    "                    coorz = float(line[46:54])\n",
    "                    resid2CAcoor\n",
    "                    if atom == \"CA\":\n",
    "                        resid2CAcoor[chainid][resid] = [coorx, coory, coorz]\n",
    "                    try:\n",
    "                        resid2coors[chainid][resid].append([coorx, coory, coorz])\n",
    "                    except KeyError:\n",
    "                        resid2coors[chainid][resid] = [[coorx, coory, coorz]]\n",
    "        fp.close()\n",
    "\n",
    "        for idx1, chainid1 in enumerate(allchains):\n",
    "            for idx2, chainid2 in enumerate(allchains):\n",
    "                if idx1 < idx2:\n",
    "                    mask = np.zeros((total_lens, total_lens), dtype=bool)\n",
    "                    residsA = resid2CAcoor[chainid1].keys()\n",
    "                    residsB = resid2CAcoor[chainid2].keys()\n",
    "                    for residA in residsA:\n",
    "                        for residB in residsB:\n",
    "                            caA = resid2CAcoor[chainid1][residA]\n",
    "                            caB = resid2CAcoor[chainid2][residB]\n",
    "                            cadist = (\n",
    "                                (caA[0] - caB[0]) ** 2\n",
    "                                + (caA[1] - caB[1]) ** 2\n",
    "                                + (caA[2] - caB[2]) ** 2\n",
    "                            ) ** 0.5\n",
    "\n",
    "                            if cadist < 20:\n",
    "                                dists = []\n",
    "                                coorsA = resid2coors[chainid1][residA]\n",
    "                                coorsB = resid2coors[chainid2][residB]\n",
    "                                for coorA in coorsA:\n",
    "                                    for coorB in coorsB:\n",
    "                                        dist = (\n",
    "                                            (coorA[0] - coorB[0]) ** 2\n",
    "                                            + (coorA[1] - coorB[1]) ** 2\n",
    "                                            + (coorA[2] - coorB[2]) ** 2\n",
    "                                        ) ** 0.5\n",
    "                                        dists.append(dist)\n",
    "                                if min(dists) < 8:\n",
    "                                    mask[\n",
    "                                        resid2index[chainid1][residA],\n",
    "                                        resid2index[chainid2][residB],\n",
    "                                    ] = True\n",
    "\n",
    "                    try:\n",
    "                        probs = np.copy(prediction_result[\"contact_prob\"])\n",
    "                        IFprobs = probs[mask]\n",
    "                        if len(IFprobs) > 0:\n",
    "                            maxprob = np.max(IFprobs)\n",
    "                        else:\n",
    "                            maxprob = 0.0\n",
    "                        prediction_result[\"interaction_prob\"][\n",
    "                            chainid1 + chainid2\n",
    "                        ] = maxprob\n",
    "                    except:\n",
    "                        prediction_result[\"interaction_prob\"][\n",
    "                            chainid1 + chainid2\n",
    "                        ] = \"error\"\n",
    "                        print(\"error in getting interaction probability\")\n",
    "        pickle.dump(prediction_result, open(name, \"wb\"))\n",
    "    return\n",
    "\n",
    "\n",
    "result_dir = jobname\n",
    "log_filename = os.path.join(jobname, \"log.txt\")\n",
    "setup_logging(Path(log_filename))\n",
    "\n",
    "queries, is_complex = get_queries(queries_path)\n",
    "model_type = set_model_type(is_complex, model_type)\n",
    "\n",
    "if \"multimer\" in model_type and max_msa is not None:\n",
    "    use_cluster_profile = False\n",
    "else:\n",
    "    use_cluster_profile = True\n",
    "\n",
    "download_alphafold_params(model_type, Path(\".\"))\n",
    "results = run(\n",
    "    queries=queries,\n",
    "    result_dir=result_dir,\n",
    "    use_templates=use_templates,\n",
    "    custom_template_path=custom_template_path,\n",
    "    num_relax=num_relax,\n",
    "    msa_mode=msa_mode,\n",
    "    model_type=model_type,\n",
    "    num_models=5,\n",
    "    num_recycles=num_recycles,\n",
    "    relax_max_iterations=relax_max_iterations,\n",
    "    recycle_early_stop_tolerance=recycle_early_stop_tolerance,\n",
    "    num_seeds=num_seeds,\n",
    "    use_dropout=use_dropout,\n",
    "    model_order=[3, 1, 2, 4, 5],\n",
    "    is_complex=is_complex,\n",
    "    data_dir=Path(\".\"),\n",
    "    keep_existing_results=False,\n",
    "    rank_by=\"auto\",\n",
    "    pair_mode=MSA_pair_mode,\n",
    "    pairing_strategy=pairing_strategy,\n",
    "    stop_at_score=float(100),\n",
    "    prediction_callback=prediction_callback,\n",
    "    dpi=dpi,\n",
    "    zip_results=False,\n",
    "    save_all=save_all,\n",
    "    max_msa=max_msa,\n",
    "    use_cluster_profile=use_cluster_profile,\n",
    "    input_features_callback=input_features_callback,\n",
    "    save_recycles=save_recycles,\n",
    "    user_agent=\"colabfold/google-colab-main\",\n",
    ")\n",
    "\n",
    "calculate_contactprob_and_interactprob(jobname, collected_protein_data, length_dic)\n",
    "results_zip = f\"{jobname}.result.zip\"\n",
    "os.system(f\"zip -r {results_zip} {jobname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "KK7X9T44pWb7"
   },
   "outputs": [],
   "source": [
    "# @title Display 3D structure {run: \"auto\"}\n",
    "import py3Dmol\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from colabfold.colabfold import plot_plddt_legend\n",
    "from colabfold.colabfold import pymol_color_list, alphabet_list\n",
    "\n",
    "rank_num = 1  # @param [\"1\", \"2\", \"3\", \"4\", \"5\"] {type:\"raw\"}\n",
    "color = \"lDDT\"  # @param [\"chain\", \"lDDT\", \"rainbow\"]\n",
    "show_sidechains = False  # @param {type:\"boolean\"}\n",
    "show_mainchains = False  # @param {type:\"boolean\"}\n",
    "\n",
    "tag = results[\"rank\"][0][rank_num - 1]\n",
    "jobname_prefix = \".custom\" if msa_mode == \"custom\" else \"\"\n",
    "pdb_filename = f\"{jobname}/{jobname}{jobname_prefix}_unrelaxed_{tag}.pdb\"\n",
    "pdb_file = glob.glob(pdb_filename)\n",
    "\n",
    "\n",
    "def show_pdb(rank_num=1, show_sidechains=False, show_mainchains=False, color=\"lDDT\"):\n",
    "    model_name = f\"rank_{rank_num}\"\n",
    "    view = py3Dmol.view(\n",
    "        js=\"https://3dmol.org/build/3Dmol.js\",\n",
    "    )\n",
    "    view.addModel(open(pdb_file[0], \"r\").read(), \"pdb\")\n",
    "\n",
    "    if color == \"lDDT\":\n",
    "        view.setStyle(\n",
    "            {\n",
    "                \"cartoon\": {\n",
    "                    \"colorscheme\": {\n",
    "                        \"prop\": \"b\",\n",
    "                        \"gradient\": \"roygb\",\n",
    "                        \"min\": 50,\n",
    "                        \"max\": 90,\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    elif color == \"rainbow\":\n",
    "        view.setStyle({\"cartoon\": {\"color\": \"spectrum\"}})\n",
    "    elif color == \"chain\":\n",
    "        chains = len(queries[0][1]) + 1 if is_complex else 1\n",
    "        for n, chain, color in zip(range(chains), alphabet_list, pymol_color_list):\n",
    "            view.setStyle({\"chain\": chain}, {\"cartoon\": {\"color\": color}})\n",
    "\n",
    "    if show_sidechains:\n",
    "        BB = [\"C\", \"O\", \"N\"]\n",
    "        view.addStyle(\n",
    "            {\n",
    "                \"and\": [\n",
    "                    {\"resn\": [\"GLY\", \"PRO\"], \"invert\": True},\n",
    "                    {\"atom\": BB, \"invert\": True},\n",
    "                ]\n",
    "            },\n",
    "            {\"stick\": {\"colorscheme\": f\"WhiteCarbon\", \"radius\": 0.3}},\n",
    "        )\n",
    "        view.addStyle(\n",
    "            {\"and\": [{\"resn\": \"GLY\"}, {\"atom\": \"CA\"}]},\n",
    "            {\"sphere\": {\"colorscheme\": f\"WhiteCarbon\", \"radius\": 0.3}},\n",
    "        )\n",
    "        view.addStyle(\n",
    "            {\"and\": [{\"resn\": \"PRO\"}, {\"atom\": [\"C\", \"O\"], \"invert\": True}]},\n",
    "            {\"stick\": {\"colorscheme\": f\"WhiteCarbon\", \"radius\": 0.3}},\n",
    "        )\n",
    "    if show_mainchains:\n",
    "        BB = [\"C\", \"O\", \"N\", \"CA\"]\n",
    "        view.addStyle(\n",
    "            {\"atom\": BB}, {\"stick\": {\"colorscheme\": f\"WhiteCarbon\", \"radius\": 0.3}}\n",
    "        )\n",
    "\n",
    "    view.zoomTo()\n",
    "    return view\n",
    "\n",
    "\n",
    "show_pdb(rank_num, show_sidechains, show_mainchains, color).show()\n",
    "if color == \"lDDT\":\n",
    "    plot_plddt_legend().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11l8k--10q0C"
   },
   "outputs": [],
   "source": [
    "# @title Plots {run: \"auto\"}\n",
    "from IPython.display import display, HTML\n",
    "import base64\n",
    "from html import escape\n",
    "\n",
    "\n",
    "# see: https://stackoverflow.com/a/53688522\n",
    "def image_to_data_url(filename):\n",
    "    ext = filename.split(\".\")[-1]\n",
    "    prefix = f\"data:image/{ext};base64,\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        img = f.read()\n",
    "    return prefix + base64.b64encode(img).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "pae = \"\"\n",
    "pae_file = os.path.join(jobname, f\"{jobname}{jobname_prefix}_pae.png\")\n",
    "if os.path.isfile(pae_file):\n",
    "    pae = image_to_data_url(pae_file)\n",
    "cov = image_to_data_url(\n",
    "    os.path.join(jobname, f\"{jobname}{jobname_prefix}_coverage.png\")\n",
    ")\n",
    "plddt = image_to_data_url(os.path.join(jobname, f\"{jobname}{jobname_prefix}_plddt.png\"))\n",
    "display(\n",
    "    HTML(\n",
    "        f\"\"\"\n",
    "<style>\n",
    "  img {{\n",
    "    float:left;\n",
    "  }}\n",
    "  .full {{\n",
    "    max-width:100%;\n",
    "  }}\n",
    "  .half {{\n",
    "    max-width:50%;\n",
    "  }}\n",
    "  @media (max-width:640px) {{\n",
    "    .half {{\n",
    "      max-width:100%;\n",
    "    }}\n",
    "  }}\n",
    "</style>\n",
    "<div style=\"max-width:90%; padding:2em;\">\n",
    "  <h1>Plots for {escape(jobname)}</h1>\n",
    "  { '<!--' if pae == '' else '' }<img src=\"{pae}\" class=\"full\" />{ '-->' if pae == '' else '' }\n",
    "  <img src=\"{cov}\" class=\"half\" />\n",
    "  <img src=\"{plddt}\" class=\"half\" />\n",
    "</div>\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z88qVoiO68i1"
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33g5IIegij5R"
   },
   "outputs": [],
   "source": [
    "# @title Package and download results\n",
    "# @markdown If you are having issues downloading the result archive, try disabling your adblocker and run this cell again. If that fails click on the little folder icon to the left, navigate to file: `jobname.result.zip`, right-click and select \\\"Download\\\" (see [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
    "\n",
    "if msa_mode == \"custom\":\n",
    "    print(\"Don't forget to cite your custom MSA generation method.\")\n",
    "\n",
    "files.download(f\"{jobname}.result.zip\")\n",
    "\n",
    "if save_to_google_drive == True and drive:\n",
    "    uploaded = drive.CreateFile({\"title\": f\"{jobname}.result.zip\"})\n",
    "    uploaded.SetContentFile(f\"{jobname}.result.zip\")\n",
    "    uploaded.Upload()\n",
    "    print(f\"Uploaded {jobname}.result.zip to Google Drive with ID {uploaded.get('id')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGUBLzB3C6WN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Instructions <a name=\"Instructions\"></a>\n",
    "\n",
    "**Quick start**\n",
    "\n",
    "1. Paste your protein sequence(s) in the input field.\n",
    "2. Press \"Runtime\" -> \"Run all\".\n",
    "3. The pipeline consists of 5 steps. The currently running step is indicated by\n",
    "   a circle with a stop sign next to it.\n",
    "\n",
    "**Result zip file contents**\n",
    "\n",
    "1. PDB formatted structures sorted by avg. pLDDT and complexes are sorted by\n",
    "   pTMscore. (unrelaxed and relaxed if `use_amber` is enabled).\n",
    "2. Plots of the model quality.\n",
    "3. Plots of the MSA coverage.\n",
    "4. Parameter log file.\n",
    "5. fasta formatted input MSA.\n",
    "6. A `predicted_aligned_error_v1.json` using\n",
    "   [AlphaFold-DB's format](https://alphafold.ebi.ac.uk/faq#faq-7) and a\n",
    "   `scores.json` for each model which contains an array (list of lists) for PAE,\n",
    "   a list with the average pLDDT and the pTMscore.\n",
    "7. BibTeX file with citations for all used tools and databases.\n",
    "\n",
    "At the end of the job a download modal box will pop up with a\n",
    "`jobname.result.zip` file. Additionally, if the `save_to_google_drive` option\n",
    "was selected, the `jobname.result.zip` will be uploaded to your Google Drive.\n",
    "\n",
    "**Using custom templates** <a name=\"custom_templates\"></a>\n",
    "\n",
    "To predict the structure with a custom template (PDB or mmCIF formatted): (1)\n",
    "change the `template_mode` to \"custom\" in the execute cell and (2) wait for an\n",
    "upload box to appear at the end of the \"Input Protein\" box. Select and upload\n",
    "your templates (multiple choices are possible).\n",
    "\n",
    "- Templates must follow the four letter PDB naming with lower case letters.\n",
    "\n",
    "- Templates in mmCIF format must contain `_entity_poly_seq`. An error is thrown\n",
    "  if this field is not present. The field\n",
    "  `_pdbx_audit_revision_history.revision_date` is automatically generated if it\n",
    "  is not present.\n",
    "\n",
    "- Templates in PDB format are automatically converted to the mmCIF format.\n",
    "  `_entity_poly_seq` and `_pdbx_audit_revision_history.revision_date` are\n",
    "  automatically generated.\n",
    "\n",
    "**Troubleshooting**\n",
    "\n",
    "- Check that the runtime type is set to GPU at \"Runtime\" -> \"Change runtime\n",
    "  type\".\n",
    "- Try to restart the session \"Runtime\" -> \"Factory reset runtime\".\n",
    "- Check your input sequence.\n",
    "\n",
    "**Known issues**\n",
    "\n",
    "- Google Colab assigns different types of GPUs with varying amount of memory.\n",
    "  Some might not have enough memory to predict the structure for a long\n",
    "  sequence.\n",
    "- Your browser can block the pop-up for downloading the result file. You can\n",
    "  choose the `save_to_google_drive` option to upload to Google Drive instead or\n",
    "  manually download the result file: Click on the little folder icon to the\n",
    "  left, navigate to file: `jobname.result.zip`, right-click and select\n",
    "  \\\"Download\\\" (see\n",
    "  [screenshot](https://pbs.twimg.com/media/E6wRW2lWUAEOuoe?format=jpg&name=small)).\n",
    "\n",
    "**Description of the plots**\n",
    "\n",
    "- **Number of sequences per position** - We want to see at least 30 sequences\n",
    "  per position, for best performance, ideally 100 sequences.\n",
    "- **Predicted lDDT per position** - model confidence (out of 100) at each\n",
    "  position. The higher the better.\n",
    "- **Predicted Alignment Error** - For homooligomers, this could be a useful\n",
    "  metric to assess how confident the model is about the interface. The lower the\n",
    "  better.\n",
    "\n",
    "**License**\n",
    "\n",
    "The source code of ColabFold is licensed under\n",
    "[MIT](https://raw.githubusercontent.com/sokrypton/ColabFold/main/LICENSE).\n",
    "Additionally, this notebook uses the AlphaFold2 source code and its parameters\n",
    "licensed under\n",
    "[Apache 2.0](https://raw.githubusercontent.com/deepmind/alphafold/main/LICENSE)\n",
    "and [CC BY 4.0](https://creativecommons.org/licenses/by-sa/4.0/) respectively.\n",
    "Read more about the AlphaFold license\n",
    "[here](https://github.com/deepmind/alphafold).\n",
    "\n",
    "**Acknowledgments**\n",
    "\n",
    "- We thank the AlphaFold and ColabFold team for developing an excellent model\n",
    "  and open sourcing the software.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "collabfold",
   "language": "python",
   "name": "collab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
