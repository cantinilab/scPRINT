{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "086c0ecc",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [12]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b39090e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:24:46.939359Z",
     "iopub.status.busy": "2025-09-17T15:24:46.938763Z",
     "iopub.status.idle": "2025-09-17T15:24:57.092400Z",
     "shell.execute_reply": "2025-09-17T15:24:57.091499Z"
    },
    "papermill": {
     "duration": 10.172254,
     "end_time": "2025-09-17T15:24:57.094996",
     "exception": false,
     "start_time": "2025-09-17T15:24:46.922742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml4ig1/Documents code/scPRINT/.venv/lib/python3.11/site-packages/louvain/__init__.py:54: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution, DistributionNotFound\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mâ†’\u001b[0m connected lamindb: jkobject/scprint_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ml4ig1/Documents code/simpler_flash/src/simpler_flash/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/ml4ig1/Documents code/simpler_flash/src/simpler_flash/layer_norm.py:1107: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "from scprint import scPrint\n",
    "from scdataloader import Preprocessor\n",
    "from scdataloader.utils import load_genes\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "from huggingface_hub import hf_hub_download\n",
    "import lamindb as ln\n",
    "\n",
    "from scprint.tasks import Embedder\n",
    "from scprint.tasks.cell_emb import display_confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "from scib_metrics.benchmark import Benchmarker, BioConservation, BatchCorrection\n",
    "from anndata import AnnData\n",
    "from scdataloader.utils import translate\n",
    "import bionty as bt\n",
    "from scprint.tasks.cell_emb import compute_classification\n",
    "\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from scdataloader import SimpleAnnDataset, Collator, DataModule\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import lamindb as ln\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import scipy.sparse\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "829c1ea0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:24:57.124244Z",
     "iopub.status.busy": "2025-09-17T15:24:57.123471Z",
     "iopub.status.idle": "2025-09-17T15:24:57.182515Z",
     "shell.execute_reply": "2025-09-17T15:24:57.181561Z"
    },
    "papermill": {
     "duration": 0.075487,
     "end_time": "2025-09-17T15:24:57.185346",
     "exception": false,
     "start_time": "2025-09-17T15:24:57.109859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_checkpoint_file = hf_hub_download(\n",
    "#    repo_id=\"jkobject/scPRINT\", filename=f\"v2-medium.ckpt\"\n",
    "# )\n",
    "# model_checkpoint_file = ../data/\n",
    "model_checkpoint_file = \"../../../1lzuxvg0.ckpt\"\n",
    "# w937u4o1.ckpt'\n",
    "# da6ao55o.ckpt # 649\n",
    "# 1lzuxvg0.ckpt # 677"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "327b650f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:24:57.213837Z",
     "iopub.status.busy": "2025-09-17T15:24:57.213211Z",
     "iopub.status.idle": "2025-09-17T15:27:47.486800Z",
     "shell.execute_reply": "2025-09-17T15:27:47.485977Z"
    },
    "papermill": {
     "duration": 170.289846,
     "end_time": "2025-09-17T15:27:47.489107",
     "exception": false,
     "start_time": "2025-09-17T15:24:57.199261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene position encoding has changed in the dataloader compared to last time, trying to revert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FYI: scPrint is not attached to a `Trainer`.\n"
     ]
    }
   ],
   "source": [
    "model = scPrint.load_from_checkpoint(\n",
    "    model_checkpoint_file, precpt_gene_emb=None, attention=\"normal\"\n",
    ")\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "593d6235",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:27:47.516313Z",
     "iopub.status.busy": "2025-09-17T15:27:47.516015Z",
     "iopub.status.idle": "2025-09-17T15:27:48.212017Z",
     "shell.execute_reply": "2025-09-17T15:27:48.211051Z"
    },
    "papermill": {
     "duration": 0.711081,
     "end_time": "2025-09-17T15:27:48.214457",
     "exception": false,
     "start_time": "2025-09-17T15:27:47.503376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "da = sc.read(\"./data/task_3_embed.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45447a9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:27:48.242471Z",
     "iopub.status.busy": "2025-09-17T15:27:48.242108Z",
     "iopub.status.idle": "2025-09-17T15:27:48.321337Z",
     "shell.execute_reply": "2025-09-17T15:27:48.320450Z"
    },
    "papermill": {
     "duration": 0.094543,
     "end_time": "2025-09-17T15:27:48.323683",
     "exception": false,
     "start_time": "2025-09-17T15:27:48.229140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_to_val = {n: i for i, n in enumerate(set(da.obs[\"batch\"].unique()))}\n",
    "da.obs[\"batch\"] = da.obs[\"batch\"].map(map_to_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc07e5ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:27:48.352264Z",
     "iopub.status.busy": "2025-09-17T15:27:48.351711Z",
     "iopub.status.idle": "2025-09-17T15:28:02.377363Z",
     "shell.execute_reply": "2025-09-17T15:28:02.376478Z"
    },
    "papermill": {
     "duration": 14.041652,
     "end_time": "2025-09-17T15:28:02.379841",
     "exception": false,
     "start_time": "2025-09-17T15:27:48.338189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (21760, 57186)\n",
      "Validation data: (5440, 57186)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for fine-tuning (using the cat/tiger dataset from above)\n",
    "# Split data into train/val\n",
    "n_train = int(0.8 * len(da))\n",
    "train_idx = np.random.choice(len(da), n_train, replace=False)\n",
    "val_idx = np.setdiff1d(np.arange(len(da)), train_idx)\n",
    "\n",
    "train_data = da[train_idx].copy()\n",
    "val_data = da[val_idx].copy()\n",
    "\n",
    "print(f\"Training data: {train_data.shape}\")\n",
    "print(f\"Validation data: {val_data.shape}\")\n",
    "\n",
    "mencoders = {}\n",
    "for k, v in model.label_decoders.items():\n",
    "    mencoders[k] = {va: ke for ke, va in v.items()}\n",
    "# this needs to remain its original name as it is expect like that by collator, otherwise need to send org_to_id as params\n",
    "mencoders.pop(\"organism_ontology_term_id\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SimpleAnnDataset(\n",
    "    train_data,\n",
    "    obs_to_output=[\"cell_type_ontology_term_id\", \"batch\", \"organism_ontology_term_id\"],\n",
    "    get_knn_cells=model.expr_emb_style == \"metacell\",\n",
    "    encoder=mencoders,\n",
    ")\n",
    "\n",
    "val_dataset = SimpleAnnDataset(\n",
    "    val_data,\n",
    "    obs_to_output=[\"cell_type_ontology_term_id\", \"batch\", \"organism_ontology_term_id\"],\n",
    "    get_knn_cells=model.expr_emb_style == \"metacell\",\n",
    "    encoder=mencoders,\n",
    ")\n",
    "\n",
    "# Create collator\n",
    "collator = Collator(\n",
    "    organisms=model.organisms,\n",
    "    valid_genes=model.genes,\n",
    "    class_names=[\"cell_type_ontology_term_id\", \"batch\"],\n",
    "    how=\"random expr\",  # or \"all expr\" for full expression\n",
    "    max_len=2800,\n",
    "    add_zero_genes=0,\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collator,\n",
    "    batch_size=32,  # Adjust based on GPU memory\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=collator,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce25257b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:28:02.408190Z",
     "iopub.status.busy": "2025-09-17T15:28:02.407911Z",
     "iopub.status.idle": "2025-09-17T15:28:02.473750Z",
     "shell.execute_reply": "2025-09-17T15:28:02.472891Z"
    },
    "papermill": {
     "duration": 0.081442,
     "end_time": "2025-09-17T15:28:02.476123",
     "exception": false,
     "start_time": "2025-09-17T15:28:02.394681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_model_org = 8\n",
    "batch_cls = torch.nn.Sequential(\n",
    "    torch.nn.Linear(d_model_org, d_model_org * 8),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(d_model_org * 8, len(set(da.obs[\"batch\"].unique()))),\n",
    ")\n",
    "batch_cls = batch_cls.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5544b3a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:28:02.504874Z",
     "iopub.status.busy": "2025-09-17T15:28:02.504582Z",
     "iopub.status.idle": "2025-09-17T15:28:02.571619Z",
     "shell.execute_reply": "2025-09-17T15:28:02.570813Z"
    },
    "papermill": {
     "duration": 0.083185,
     "end_time": "2025-09-17T15:28:02.573912",
     "exception": false,
     "start_time": "2025-09-17T15:28:02.490727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for val in model.parameters():\n",
    "    val.requires_grad = True\n",
    "    # setting all to TRUE\n",
    "\n",
    "for val in model.cell_transformer.parameters():\n",
    "    val.requires_grad = True\n",
    "for val in model.transformer.blocks[7].parameters():\n",
    "    val.requires_grad = True\n",
    "for i in model.transformer.blocks:\n",
    "    i.cross_attn.requires_grad = True\n",
    "for val in model.compressor.parameters():\n",
    "    val.requires_grad = True\n",
    "for val in model.cls_decoders[\"cell_type_ontology_term_id\"].parameters():\n",
    "    val.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9eb3daa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:28:02.602111Z",
     "iopub.status.busy": "2025-09-17T15:28:02.601679Z",
     "iopub.status.idle": "2025-09-17T15:28:02.695650Z",
     "shell.execute_reply": "2025-09-17T15:28:02.694763Z"
    },
    "papermill": {
     "duration": 0.109561,
     "end_time": "2025-09-17T15:28:02.697942",
     "exception": false,
     "start_time": "2025-09-17T15:28:02.588381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mmd_loss(X, Y):\n",
    "    \"\"\"\n",
    "    Compute Maximum Mean Discrepancy (MMD) loss between two 2D embedding matrices.\n",
    "\n",
    "    Args:\n",
    "        X: Tensor of shape (n1, emb_dim) - first set of embeddings\n",
    "        Y: Tensor of shape (n2, emb_dim) - second set of embeddings\n",
    "\n",
    "    Returns:\n",
    "        MMD loss value (negative to encourage dissimilarity)\n",
    "    \"\"\"\n",
    "\n",
    "    def rbf_kernel(x, y, sigma):\n",
    "        \"\"\"Compute RBF kernel between two sets of vectors\"\"\"\n",
    "        distance = torch.cdist(x, y, p=2) ** 2\n",
    "        return torch.exp(-distance / (2 * sigma**2))\n",
    "\n",
    "    # Use multiple kernel bandwidths for better performance\n",
    "    sigmas = [0.1, 1.0, 10.0]\n",
    "    mmd_loss = 0.0\n",
    "\n",
    "    for sigma in sigmas:\n",
    "        # K(X, X) - kernel matrix within first group (n1 x n1)\n",
    "        k_xx = rbf_kernel(X, X, sigma)\n",
    "        # K(Y, Y) - kernel matrix within second group (n2 x n2)\n",
    "        k_yy = rbf_kernel(Y, Y, sigma)\n",
    "        # K(X, Y) - kernel matrix between groups (n1 x n2)\n",
    "        k_xy = rbf_kernel(X, Y, sigma)\n",
    "\n",
    "        # Unbiased MMD estimation\n",
    "        n1 = X.shape[0]\n",
    "        n2 = Y.shape[0]\n",
    "\n",
    "        # Remove diagonal elements for unbiased estimation of K(X,X) and K(Y,Y)\n",
    "        # For K(X,X): exclude diagonal\n",
    "        if n1 > 1:\n",
    "            mask_xx = 1 - torch.eye(n1, device=X.device)\n",
    "            k_xx_term = (k_xx * mask_xx).sum() / (n1 * (n1 - 1))\n",
    "        else:\n",
    "            k_xx_term = 0.0\n",
    "\n",
    "        # For K(Y,Y): exclude diagonal\n",
    "        if n2 > 1:\n",
    "            mask_yy = 1 - torch.eye(n2, device=Y.device)\n",
    "            k_yy_term = (k_yy * mask_yy).sum() / (n2 * (n2 - 1))\n",
    "        else:\n",
    "            k_yy_term = 0.0\n",
    "\n",
    "        # For K(X,Y): use all elements (no diagonal to exclude)\n",
    "        k_xy_term = k_xy.mean()\n",
    "\n",
    "        # MMD^2 = E[K(X,X)] + E[K(Y,Y)] - 2*E[K(X,Y)]\n",
    "        mmd_squared = k_xx_term + k_yy_term - 2 * k_xy_term\n",
    "        mmd_loss += mmd_squared\n",
    "\n",
    "    # Return negative MMD to encourage dissimilarity (higher MMD = more different)\n",
    "    return mmd_loss / len(sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c28ce78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:28:02.727698Z",
     "iopub.status.busy": "2025-09-17T15:28:02.727097Z",
     "iopub.status.idle": "2025-09-17T15:28:02.828638Z",
     "shell.execute_reply": "2025-09-17T15:28:02.827760Z"
    },
    "papermill": {
     "duration": 0.118252,
     "end_time": "2025-09-17T15:28:02.831046",
     "exception": false,
     "start_time": "2025-09-17T15:28:02.712794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_corr_pass(batch):\n",
    "    gene_pos = batch[\"genes\"].to(model.device)\n",
    "    expression = batch[\"x\"].to(model.device)\n",
    "    depth = batch[\"depth\"].to(model.device)\n",
    "    class_elem = batch[\"class\"].long().to(model.device)\n",
    "    total_loss = 0\n",
    "\n",
    "    # Forward pass with automatic mixed precisio^n\n",
    "    with torch.cuda.amp.autocast():\n",
    "        # Forward pass\n",
    "        output = model.forward(\n",
    "            gene_pos,\n",
    "            expression,\n",
    "            req_depth=depth,\n",
    "            depth_mult=expression.sum(1),\n",
    "            do_class=True,\n",
    "            metacell_token=torch.zeros_like(depth),\n",
    "        )\n",
    "        ## adaptor on ct_emb\n",
    "        # ctpos = model.classes.index(\"cell_type_ontology_term_id\") + 1\n",
    "        # emb = output[\"output_cell_embs\"][:, ctpos, :]\n",
    "        #\n",
    "        # output[\"output_cell_embs\"][:, ctpos, :] = adaptor_layer(\n",
    "        #    torch.cat([emb, class_elem[:, 1].unsqueeze(1).float()], dim=1)\n",
    "        # )\n",
    "\n",
    "        ## generate expr loss\n",
    "        output_gen = model._generate(\n",
    "            cell_embs=output[\"output_cell_embs\"],\n",
    "            gene_pos=gene_pos,\n",
    "            depth_mult=expression.sum(1),\n",
    "            req_depth=depth,\n",
    "        )\n",
    "        if \"zero_logits\" in output_gen:\n",
    "            loss_expr = loss.zinb(\n",
    "                theta=output_gen[\"disp\"],\n",
    "                pi=output_gen[\"zero_logits\"],\n",
    "                mu=output_gen[\"mean\"],\n",
    "                target=expression,\n",
    "            )\n",
    "            if model.zinb_and_mse:\n",
    "                loss_expr += (\n",
    "                    loss.mse(\n",
    "                        input=torch.log(output_gen[\"mean\"] + 1)\n",
    "                        * (1 - torch.sigmoid(output_gen[\"zero_logits\"])),\n",
    "                        target=torch.log(expression + 1),\n",
    "                    )\n",
    "                    / 10  # scale to make it more similar to the zinb\n",
    "                )\n",
    "        else:\n",
    "            loss_expr = loss.mse(\n",
    "                input=torch.log(output_gen[\"mean\"] + 1),\n",
    "                target=torch.log(expression + 1),\n",
    "            )\n",
    "        # Add expression loss to total\n",
    "        total_loss += loss_expr\n",
    "\n",
    "        # ct clss\n",
    "        cls_output = output.get(\"cls_output_cell_type_ontology_term_id\")\n",
    "        # ct_output = output[\"output_cell_embs\"][:, ctpos, :]\n",
    "        # cls_output = model.cls_decoders[\"cell_type_ontology_term_id\"](ct_output)\n",
    "        cls_loss = loss.hierarchical_classification(\n",
    "            pred=cls_output,\n",
    "            cl=class_elem[:, 0],\n",
    "            labels_hierarchy=model.mat_labels_hierarchy.get(\n",
    "                \"cell_type_ontology_term_id\"\n",
    "            ).to(\"cuda\"),\n",
    "        )\n",
    "\n",
    "        # organ class\n",
    "        org_emb = output[\"compressed_cell_embs\"][\n",
    "            model.classes.index(\"organism_ontology_term_id\") + 1\n",
    "        ]\n",
    "        cls_loss += F.cross_entropy(\n",
    "            input=batch_cls(org_emb),\n",
    "            target=class_elem[:, 1],\n",
    "        )\n",
    "        total_loss += cls_loss\n",
    "\n",
    "        pos = model.classes.index(\"cell_type_ontology_term_id\") + 1\n",
    "        # Apply gradient reversal to the input embedding\n",
    "        selected_emb = (\n",
    "            output[\"compressed_cell_embs\"][pos]\n",
    "            if model.compressor is not None\n",
    "            else output[\"input_cell_embs\"][:, pos, :]\n",
    "        )\n",
    "        X, Y = selected_emb[class_elem[:, 1] == 1], selected_emb[class_elem[:, 1] == 0]\n",
    "\n",
    "        mmd = mmd_loss(X, Y)\n",
    "        if torch.isnan(mmd):\n",
    "            print(\"mmd nan\")\n",
    "        mmd = mmd.item() if not torch.isnan(mmd) else 0\n",
    "\n",
    "        # Add adversarial loss to total loss\n",
    "        total_loss += mmd * 3\n",
    "        total_loss += output[\"vae_kl_loss\"] * 0.001\n",
    "    return total_loss, cls_loss, mmd, loss_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "771be887",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:28:02.859972Z",
     "iopub.status.busy": "2025-09-17T15:28:02.859561Z",
     "iopub.status.idle": "2025-09-17T15:28:02.952098Z",
     "shell.execute_reply": "2025-09-17T15:28:02.951179Z"
    },
    "papermill": {
     "duration": 0.108652,
     "end_time": "2025-09-17T15:28:02.954535",
     "exception": false,
     "start_time": "2025-09-17T15:28:02.845883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3707924/2531489886.py:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n"
     ]
    }
   ],
   "source": [
    "# Alternative: Manual Training Loop (for more control)\n",
    "# If you prefer to have more control over the training process\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from scprint.model import loss\n",
    "\n",
    "num_epochs = 10\n",
    "lr = 0.0004\n",
    "\n",
    "# Setup optimizer\n",
    "all_params = (\n",
    "    list(model.parameters()) + list(batch_cls.parameters())\n",
    "    # + list(batch_vector.parameters())\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    all_params, lr=lr, weight_decay=0.01, betas=(0.9, 0.999), eps=1e-8\n",
    ")\n",
    "\n",
    "# Setup scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.2, patience=1\n",
    ")\n",
    "\n",
    "# Setup automatic mixed precision\n",
    "scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "_ = model.train()\n",
    "\n",
    "for k, i in model.mat_labels_hierarchy.items():\n",
    "    model.mat_labels_hierarchy[k] = i.to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7375aba4",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a06917e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:28:02.983375Z",
     "iopub.status.busy": "2025-09-17T15:28:02.983070Z",
     "iopub.status.idle": "2025-09-17T15:28:06.015938Z",
     "shell.execute_reply": "2025-09-17T15:28:06.014732Z"
    },
    "papermill": {
     "duration": 3.049173,
     "end_time": "2025-09-17T15:28:06.018723",
     "exception": true,
     "start_time": "2025-09-17T15:28:02.969550",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Current learning rate: 4.00e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3707924/3469757012.py:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 88.00 MiB. GPU 0 has a total capacity of 19.57 GiB of which 71.94 MiB is free. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Of the allocated memory 18.15 GiB is allocated by PyTorch, and 162.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[32m     15\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     total_loss, cls_loss, mmd, loss_expr = \u001b[43mbatch_corr_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m     18\u001b[39m     scaler.scale(total_loss).backward()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mbatch_corr_pass\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m     11\u001b[39m output = model.forward(\n\u001b[32m     12\u001b[39m     gene_pos,\n\u001b[32m     13\u001b[39m     expression,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     metacell_token=torch.zeros_like(depth),\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m## adaptor on ct_emb\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# ctpos = model.classes.index(\"cell_type_ontology_term_id\") + 1\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# emb = output[\"output_cell_embs\"][:, ctpos, :]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m \n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m## generate expr loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m output_gen = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcell_embs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_cell_embs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgene_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgene_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdepth_mult\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpression\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreq_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mzero_logits\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output_gen:\n\u001b[32m     35\u001b[39m     loss_expr = loss.zinb(\n\u001b[32m     36\u001b[39m         theta=output_gen[\u001b[33m\"\u001b[39m\u001b[33mdisp\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     37\u001b[39m         pi=output_gen[\u001b[33m\"\u001b[39m\u001b[33mzero_logits\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     38\u001b[39m         mu=output_gen[\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     39\u001b[39m         target=expression,\n\u001b[32m     40\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents code/scPRINT/scprint/model/model.py:1090\u001b[39m, in \u001b[36mscPrint._generate\u001b[39m\u001b[34m(self, cell_embs, gene_pos, depth_mult, req_depth, metacell_token, **decoder_kwargs)\u001b[39m\n\u001b[32m   1084\u001b[39m _, encoding = \u001b[38;5;28mself\u001b[39m._encoder(\n\u001b[32m   1085\u001b[39m     cell_embs=cell_embs,\n\u001b[32m   1086\u001b[39m     gene_pos=gene_pos,\n\u001b[32m   1087\u001b[39m     metacell_token=metacell_token,\n\u001b[32m   1088\u001b[39m )\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cell_transformer:\n\u001b[32m-> \u001b[39m\u001b[32m1090\u001b[39m     transformer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_kv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcell_embs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1092\u001b[39m     encoding = torch.cat([cell_embs, encoding], dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents code/scPRINT/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents code/scPRINT/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents code/simpler_flash/src/simpler_flash/flashformer.py:207\u001b[39m, in \u001b[36mFlashTransformer.forward\u001b[39m\u001b[34m(self, hidden_states, x_kv, return_qkv, bias, bias_layer, mask_zeros)\u001b[39m\n\u001b[32m    201\u001b[39m     latent_kv = \u001b[38;5;28mself\u001b[39m.learnt_latent(\n\u001b[32m    202\u001b[39m         torch.arange(\n\u001b[32m    203\u001b[39m             \u001b[38;5;28mself\u001b[39m.sketcher_size, dtype=torch.long, device=hidden_states.device\n\u001b[32m    204\u001b[39m         ).repeat(hidden_states.shape[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m)\n\u001b[32m    205\u001b[39m     )\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.blocks):\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     hidden_states = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlatent_kv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_type\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcriss-cross\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_qkv\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_qkv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbias_layer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_zeros\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m return_qkv:\n\u001b[32m    217\u001b[39m         qkvs.append(hidden_states[-\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents code/scPRINT/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents code/scPRINT/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents code/simpler_flash/src/simpler_flash/block.py:234\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, hidden_states, x_kv, latent_kv, residual, bias, is_causal, src_key_padding_mask, mixer_subset, mixer_kwargs, return_qkv)\u001b[39m\n\u001b[32m    232\u001b[39m     mixer_kwargs[\u001b[33m\"\u001b[39m\u001b[33mmixer_subset\u001b[39m\u001b[33m\"\u001b[39m] = mixer_subset\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cross_attn \u001b[38;5;129;01mand\u001b[39;00m x_kv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx_kv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_qkv\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmixer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fused_dropout_add_ln:\n\u001b[32m    242\u001b[39m         dropped = \u001b[38;5;28mself\u001b[39m.drop_path3(\u001b[38;5;28mself\u001b[39m.dropout3(hidden_states))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents code/scPRINT/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents code/scPRINT/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents code/simpler_flash/src/simpler_flash/mha.py:915\u001b[39m, in \u001b[36mMHA.forward\u001b[39m\u001b[34m(self, x, x_kv, key_padding_mask, cu_seqlens, max_seqlen, mixer_subset, inference_params, return_qkv, **kwargs)\u001b[39m\n\u001b[32m    913\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.attn_type == \u001b[33m\"\u001b[39m\u001b[33mcriss-cross\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    914\u001b[39m         cc_context = \u001b[38;5;28mself\u001b[39m.inner_criss_cross_attn(cc_q, kv, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m     context = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minner_cross_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_type\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcriss-cross\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcc_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.attn_type == \u001b[33m\"\u001b[39m\u001b[33mcriss-cross\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents code/scPRINT/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents code/scPRINT/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents code/simpler_flash/src/simpler_flash/mha.py:363\u001b[39m, in \u001b[36mCrossAttention.forward\u001b[39m\u001b[34m(self, q, kv, causal, bias)\u001b[39m\n\u001b[32m    361\u001b[39m q = q.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.softpick:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     output = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    366\u001b[39m     output = output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 88.00 MiB. GPU 0 has a total capacity of 19.57 GiB of which 71.94 MiB is free. Including non-PyTorch memory, this process has 18.53 GiB memory in use. Of the allocated memory 18.15 GiB is allocated by PyTorch, and 162.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Current learning rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    # Training phase\n",
    "    train_loss = 0.0\n",
    "    train_steps = 0\n",
    "    avg_adv = 0\n",
    "    avg_expr = 0\n",
    "    avg_cls = 0\n",
    "    avg_mmd = 0\n",
    "\n",
    "    # pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        total_loss, cls_loss, mmd, loss_expr = batch_corr_pass(batch)\n",
    "        # Backward pass\n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += total_loss.item()\n",
    "        train_steps += 1\n",
    "        avg_cls += cls_loss.item()\n",
    "        avg_expr += loss_expr.item()\n",
    "        avg_mmd += mmd\n",
    "        # Update progress bar\n",
    "        # if batch_idx % 35 == 0:\n",
    "        # print(\n",
    "        #    f\"avg_loss {train_loss / train_steps:.4f}, avg_cls {avg_cls / train_steps:.4f}, avg_expr {avg_expr / train_steps:.4f}, avg_adv {avg_mmd/ train_steps:.4f}\"\n",
    "        # )\n",
    "        # pbar.set_postfix(\n",
    "        #    {\n",
    "        #        \"loss\": f\"{total_loss.item():.4f}\",\n",
    "        #        \"avg_loss\": f\"{train_loss / train_steps:.4f}\",\n",
    "        #        \"cls_loss\": f\"{cls_loss.item():.4f}\",\n",
    "        #        \"mmd_loss\": f\"{mmd:.4f}\",\n",
    "        #        \"expr_loss\": f\"{loss_expr.item():.4f}\",\n",
    "        #    }\n",
    "        # )\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_steps = 0\n",
    "    val_loss_expr = 0.0\n",
    "    val_mmd = 0.0\n",
    "    val_cls = 0.0\n",
    "    val_loss_to_prt = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:  # tqdm(val_loader, desc=\"Validation\"):\n",
    "            loss_val, cls_loss, mmd, loss_expr = batch_corr_pass(batch)\n",
    "            val_loss_to_prt += loss_val.item()\n",
    "            val_loss += loss_val.item()\n",
    "            val_steps += 1\n",
    "            val_loss_expr += loss_expr.item()\n",
    "            val_mmd += mmd\n",
    "            val_cls += cls_loss.item()\n",
    "    try:\n",
    "        avg_val_loss = val_loss_to_prt / val_steps\n",
    "        avg_train_loss = train_loss / train_steps\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Error: Division by zero occurred while calculating average losses.\")\n",
    "        avg_train_loss = 0\n",
    "    print(\n",
    "        \"cls_loss: {:.4f}, mmd_loss: {:.4f}, expr_loss: {:.4f}\".format(\n",
    "            val_cls / val_steps, val_mmd / val_steps, val_loss_expr / val_steps\n",
    "        )\n",
    "    )\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Store LR before scheduler step for comparison\n",
    "    lr_before = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Check if LR was reduced\n",
    "    lr_after = optimizer.param_groups[0][\"lr\"]\n",
    "    if lr_after < lr_before:\n",
    "        print(\n",
    "            f\"ðŸ”» Learning rate reduced from {lr_before:.2e} to {lr_after:.2e} (factor: {lr_after / lr_before:.3f})\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"âœ… Learning rate unchanged: {lr_after:.2e}\")\n",
    "\n",
    "    # Early stopping check (simple implementation)\n",
    "    if epoch > 3 and val_loss / val_steps > 1.3 * avg_train_loss:\n",
    "        print(\"Early stopping due to overfitting\")\n",
    "        break\n",
    "\n",
    "print(\"Manual fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98646e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:38:23.612531Z",
     "iopub.status.busy": "2025-09-17T14:38:23.611849Z",
     "iopub.status.idle": "2025-09-17T14:38:25.894081Z",
     "shell.execute_reply": "2025-09-17T14:38:25.892351Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "checkpoint = {\n",
    "    \"epoch\": epoch,\n",
    "    \"global_step\": (1 + epoch) * batch_idx,\n",
    "    \"pytorch-lightning_version\": L.__version__,\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"optimizer_states\": [optimizer.state_dict()],\n",
    "    \"lr_schedulers\": [scheduler.state_dict()],\n",
    "    \"hparams_name\": None,\n",
    "    \"loops\": None,\n",
    "    \"callbacks\": None,\n",
    "    \"hyper_parameters\": model.hparams,\n",
    "}\n",
    "torch.save(checkpoint, \"fit_2.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048ab6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:38:25.931554Z",
     "iopub.status.busy": "2025-09-17T14:38:25.930923Z",
     "iopub.status.idle": "2025-09-17T14:38:26.028879Z",
     "shell.execute_reply": "2025-09-17T14:38:26.027330Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"fit_2.ckpt\")[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daa4847",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:38:26.065779Z",
     "iopub.status.busy": "2025-09-17T14:38:26.065174Z",
     "iopub.status.idle": "2025-09-17T14:38:44.231575Z",
     "shell.execute_reply": "2025-09-17T14:38:44.230393Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = scPrint.load_from_checkpoint(\n",
    "    \"fit_2.ckpt\", precpt_gene_emb=None, attention=\"normal\"\n",
    ")\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f5c43a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:38:44.267886Z",
     "iopub.status.busy": "2025-09-17T14:38:44.267679Z",
     "iopub.status.idle": "2025-09-17T14:38:44.327038Z",
     "shell.execute_reply": "2025-09-17T14:38:44.326200Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "da.obs = da.obs.iloc[:, :-15]\n",
    "for i in [\n",
    "    \"scprint_emb\",\n",
    "    \"scprint_emb_age_group\",\n",
    "    \"scprint_emb_assay_ontology_term_id\",\n",
    "    \"scprint_emb_cell_culture\",\n",
    "    \"scprint_emb_cell_type_ontology_term_id\",\n",
    "    \"scprint_emb_disease_ontology_term_id\",\n",
    "    \"scprint_emb_organism_ontology_term_id\",\n",
    "    \"scprint_emb_other\",\n",
    "    \"scprint_emb_self_reported_ethnicity_ontology_term_id\",\n",
    "    \"scprint_emb_sex_ontology_term_id\",\n",
    "    \"scprint_emb_tissue_ontology_term_id\",\n",
    "]:\n",
    "    da.obsm.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a13f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:38:44.361828Z",
     "iopub.status.busy": "2025-09-17T14:38:44.361452Z",
     "iopub.status.idle": "2025-09-17T14:38:44.436825Z",
     "shell.execute_reply": "2025-09-17T14:38:44.435986Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed = Embedder(\n",
    "    how=\"random expr\",\n",
    "    max_len=2800,\n",
    "    num_workers=8,\n",
    "    pred_embedding=[\"all\"],\n",
    "    doplot=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb7590",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:38:44.472931Z",
     "iopub.status.busy": "2025-09-17T14:38:44.472394Z",
     "iopub.status.idle": "2025-09-17T14:45:12.987034Z",
     "shell.execute_reply": "2025-09-17T14:45:12.981321Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_adata, metrics = embed(model, da.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56806e1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:45:13.110599Z",
     "iopub.status.busy": "2025-09-17T14:45:13.109553Z",
     "iopub.status.idle": "2025-09-17T14:45:17.369742Z",
     "shell.execute_reply": "2025-09-17T14:45:17.368196Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_classification(\n",
    "    n_adata,\n",
    "    [\"cell_type_ontology_term_id\"],\n",
    "    label_decoders=model.label_decoders,\n",
    "    labels_hierarchy=model.labels_hierarchy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bbeed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:45:17.484969Z",
     "iopub.status.busy": "2025-09-17T14:45:17.484342Z",
     "iopub.status.idle": "2025-09-17T14:46:11.816463Z",
     "shell.execute_reply": "2025-09-17T14:46:11.814993Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pp.neighbors(n_adata, use_rep=\"scprint_emb_cell_type_ontology_term_id\")\n",
    "sc.tl.umap(n_adata)\n",
    "sc.pl.umap(\n",
    "    n_adata,\n",
    "    color=[\"conv_pred_cell_type_ontology_term_id\", \"cell_type\", \"batch\"],\n",
    "    ncols=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887818f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:46:11.964168Z",
     "iopub.status.busy": "2025-09-17T14:46:11.963493Z",
     "iopub.status.idle": "2025-09-17T14:46:31.623052Z",
     "shell.execute_reply": "2025-09-17T14:46:31.622259Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pp.neighbors(n_adata, use_rep=\"scprint_emb\")\n",
    "sc.tl.umap(n_adata)\n",
    "sc.pl.umap(\n",
    "    n_adata,\n",
    "    color=[\"conv_pred_cell_type_ontology_term_id\", \"cell_type\", \"batch\"],\n",
    "    ncols=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58cd187",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:46:31.799300Z",
     "iopub.status.busy": "2025-09-17T14:46:31.798644Z",
     "iopub.status.idle": "2025-09-17T14:48:58.178129Z",
     "shell.execute_reply": "2025-09-17T14:48:58.176431Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bm = Benchmarker(\n",
    "    n_adata,\n",
    "    batch_key=\"batch\",  # batch, tech, assay_ontology_term_id, donor_id\n",
    "    label_key=\"cell_type_ontology_term_id\",  # celltype\n",
    "    embedding_obsm_keys=[\"scprint_emb_cell_type_ontology_term_id\"],\n",
    "    bio_conservation_metrics=BioConservation(),\n",
    "    batch_correction_metrics=BatchCorrection(),\n",
    "    n_jobs=10,\n",
    ")\n",
    "bm.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813cd5c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:48:58.369818Z",
     "iopub.status.busy": "2025-09-17T14:48:58.369166Z",
     "iopub.status.idle": "2025-09-17T14:48:58.703378Z",
     "shell.execute_reply": "2025-09-17T14:48:58.701807Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# after fine tuning\n",
    "bm.plot_results_table(min_max_scale=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scPRINT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 203.438373,
   "end_time": "2025-09-17T15:28:09.167342",
   "environment_variables": {},
   "exception": true,
   "input_path": "fine_tuning_cross_species_emb_mmd.ipynb",
   "output_path": "fine_tuning_cross_species_emb_mmd.ipynb",
   "parameters": {},
   "start_time": "2025-09-17T15:24:45.728969",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
