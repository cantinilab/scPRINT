{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b39090e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:36:21.663538Z",
     "iopub.status.busy": "2025-09-12T08:36:21.663392Z",
     "iopub.status.idle": "2025-09-12T08:36:45.780197Z",
     "shell.execute_reply": "2025-09-12T08:36:45.779722Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-09-12T08:38:56.015877",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "from scprint import scPrint\n",
    "from scdataloader import Preprocessor\n",
    "from scdataloader.utils import load_genes\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "from huggingface_hub import hf_hub_download\n",
    "import lamindb as ln\n",
    "\n",
    "from scprint.tasks import Embedder\n",
    "from scprint.tasks.cell_emb import display_confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "from scib_metrics.benchmark import Benchmarker, BioConservation, BatchCorrection\n",
    "from anndata import AnnData\n",
    "from scdataloader.utils import translate\n",
    "import bionty as bt\n",
    "from scprint.tasks.cell_emb import compute_classification\n",
    "\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from scdataloader import SimpleAnnDataset, Collator, DataModule\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import lamindb as ln\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import scipy.sparse\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c1ea0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:36:45.794287Z",
     "iopub.status.busy": "2025-09-12T08:36:45.793727Z",
     "iopub.status.idle": "2025-09-12T08:36:49.469930Z",
     "shell.execute_reply": "2025-09-12T08:36:49.469486Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_checkpoint_file = hf_hub_download(\n",
    "#    repo_id=\"jkobject/scPRINT\", filename=f\"v2-medium.ckpt\"\n",
    "# )\n",
    "# model_checkpoint_file = ../data/\n",
    "model_checkpoint_file = \"../../../1lzuxvg0.ckpt\"\n",
    "# w937u4o1.ckpt'\n",
    "# da6ao55o.ckpt # 649\n",
    "# 1lzuxvg0.ckpt # 677"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b650f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T08:36:49.482478Z",
     "iopub.status.busy": "2025-09-12T08:36:49.482319Z",
     "iopub.status.idle": "2025-09-12T08:36:56.171604Z",
     "shell.execute_reply": "2025-09-12T08:36:56.170922Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = scPrint.load_from_checkpoint(\n",
    "    model_checkpoint_file, precpt_gene_emb=None, attention=\"normal\"\n",
    ")\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166863e5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(da.X>0).sum(1).mean(), (da.X>0).sum(1).max(), (da.X>0).sum(1).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4f0453",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "((da.X>0).sum(1)>2800).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b21f38",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "da.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d6235",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:27:47.516313Z",
     "iopub.status.busy": "2025-09-17T15:27:47.516015Z",
     "iopub.status.idle": "2025-09-17T15:27:48.212017Z",
     "shell.execute_reply": "2025-09-17T15:27:48.211051Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "da = sc.read(\"./data/task_3_embed.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45447a9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:27:48.242471Z",
     "iopub.status.busy": "2025-09-17T15:27:48.242108Z",
     "iopub.status.idle": "2025-09-17T15:27:48.321337Z",
     "shell.execute_reply": "2025-09-17T15:27:48.320450Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_to_val = {n: i for i, n in enumerate(set(da.obs[\"batch\"].unique()))}\n",
    "da.obs[\"batch\"] = da.obs[\"batch\"].map(map_to_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc07e5ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:27:48.352264Z",
     "iopub.status.busy": "2025-09-17T15:27:48.351711Z",
     "iopub.status.idle": "2025-09-17T15:28:02.377363Z",
     "shell.execute_reply": "2025-09-17T15:28:02.376478Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare data for fine-tuning (using the cat/tiger dataset from above)\n",
    "# Split data into train/val\n",
    "n_train = int(0.8 * len(da))\n",
    "train_idx = np.random.choice(len(da), n_train, replace=False)\n",
    "val_idx = np.setdiff1d(np.arange(len(da)), train_idx)\n",
    "\n",
    "train_data = da[train_idx].copy()\n",
    "val_data = da[val_idx].copy()\n",
    "\n",
    "print(f\"Training data: {train_data.shape}\")\n",
    "print(f\"Validation data: {val_data.shape}\")\n",
    "\n",
    "mencoders = {}\n",
    "for k, v in model.label_decoders.items():\n",
    "    mencoders[k] = {va: ke for ke, va in v.items()}\n",
    "# this needs to remain its original name as it is expect like that by collator, otherwise need to send org_to_id as params\n",
    "mencoders.pop(\"organism_ontology_term_id\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SimpleAnnDataset(\n",
    "    train_data,\n",
    "    obs_to_output=[\"cell_type_ontology_term_id\", \"batch\", \"organism_ontology_term_id\"],\n",
    "    get_knn_cells=model.expr_emb_style == \"metacell\",\n",
    "    encoder=mencoders,\n",
    ")\n",
    "\n",
    "val_dataset = SimpleAnnDataset(\n",
    "    val_data,\n",
    "    obs_to_output=[\"cell_type_ontology_term_id\", \"batch\", \"organism_ontology_term_id\"],\n",
    "    get_knn_cells=model.expr_emb_style == \"metacell\",\n",
    "    encoder=mencoders,\n",
    ")\n",
    "\n",
    "# Create collator\n",
    "collator = Collator(\n",
    "    organisms=model.organisms,\n",
    "    valid_genes=model.genes,\n",
    "    class_names=[\"cell_type_ontology_term_id\", \"batch\"],\n",
    "    how=\"random expr\",  # or \"all expr\" for full expression\n",
    "    max_len=2200,\n",
    "    add_zero_genes=0,\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collator,\n",
    "    batch_size=32,  # Adjust based on GPU memory\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=collator,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce25257b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:28:02.408190Z",
     "iopub.status.busy": "2025-09-17T15:28:02.407911Z",
     "iopub.status.idle": "2025-09-17T15:28:02.473750Z",
     "shell.execute_reply": "2025-09-17T15:28:02.472891Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_model_org = 8\n",
    "d_module_cell = 128\n",
    "batch_cls = torch.nn.Sequential(\n",
    "    torch.nn.Linear(d_model_org, d_model_org * 8),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(d_model_org * 8, len(set(da.obs[\"batch\"].unique()))),\n",
    ")\n",
    "batch_cls = batch_cls.to(model.device)\n",
    "\n",
    "batch_emb = torch.nn.Embedding(len(set(da.obs[\"batch\"].unique())), d_module_cell).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5544b3a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:28:02.504874Z",
     "iopub.status.busy": "2025-09-17T15:28:02.504582Z",
     "iopub.status.idle": "2025-09-17T15:28:02.571619Z",
     "shell.execute_reply": "2025-09-17T15:28:02.570813Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for val in model.parameters():\n",
    "    val.requires_grad = True\n",
    "    # setting all to TRUE\n",
    "\n",
    "for val in model.cell_transformer.parameters():\n",
    "    val.requires_grad = True\n",
    "for val in model.transformer.blocks[7].parameters():\n",
    "    val.requires_grad = True\n",
    "for i in model.transformer.blocks:\n",
    "    i.cross_attn.requires_grad = True\n",
    "for val in model.compressor.parameters():\n",
    "    val.requires_grad = True\n",
    "for val in model.cls_decoders[\"cell_type_ontology_term_id\"].parameters():\n",
    "    val.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb3daa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:28:02.602111Z",
     "iopub.status.busy": "2025-09-17T15:28:02.601679Z",
     "iopub.status.idle": "2025-09-17T15:28:02.695650Z",
     "shell.execute_reply": "2025-09-17T15:28:02.694763Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mmd_loss(X, Y):\n",
    "    \"\"\"\n",
    "    Compute Maximum Mean Discrepancy (MMD) loss between two 2D embedding matrices.\n",
    "\n",
    "    Args:\n",
    "        X: Tensor of shape (n1, emb_dim) - first set of embeddings\n",
    "        Y: Tensor of shape (n2, emb_dim) - second set of embeddings\n",
    "\n",
    "    Returns:\n",
    "        MMD loss value (negative to encourage dissimilarity)\n",
    "    \"\"\"\n",
    "\n",
    "    def rbf_kernel(x, y, sigma):\n",
    "        \"\"\"Compute RBF kernel between two sets of vectors\"\"\"\n",
    "        distance = torch.cdist(x, y, p=2) ** 2\n",
    "        return torch.exp(-distance / (2 * sigma**2))\n",
    "    \n",
    "    def energy_kernel(x,y):\n",
    "        \"\"\"Compute Energy kernel between two sets of vectors\"\"\"\n",
    "        distance = torch.cdist(x, y, p=2)\n",
    "        return -distance\n",
    "\n",
    "    # Use multiple kernel bandwidths for better performance\n",
    "    sigmas = [0] #[0.1, 1.0, 10.0]\n",
    "    mmd_loss = 0.0\n",
    "\n",
    "    for sigma in sigmas:\n",
    "        # K(X, X) - kernel matrix within first group (n1 x n1)\n",
    "        #k_xx = rbf_kernel(X, X, sigma)\n",
    "        k_xx = energy_kernel(X, X)\n",
    "        # K(Y, Y) - kernel matrix within second group (n2 x n2)\n",
    "        # k_yy = rbf_kernel(Y, Y, sigma)\n",
    "        k_yy = energy_kernel(Y, Y)\n",
    "        # K(X, Y) - kernel matrix between groups (n1 x n2)\n",
    "        # k_xy = rbf_kernel(X, Y, sigma)\n",
    "        k_xy = energy_kernel(X, Y)\n",
    "\n",
    "        # Unbiased MMD estimation\n",
    "        n1 = X.shape[0]\n",
    "        n2 = Y.shape[0]\n",
    "\n",
    "        # Remove diagonal elements for unbiased estimation of K(X,X) and K(Y,Y)\n",
    "        # For K(X,X): exclude diagonal\n",
    "        if n1 > 1:\n",
    "            mask_xx = 1 - torch.eye(n1, device=X.device)\n",
    "            k_xx_term = (k_xx * mask_xx).sum() / (n1 * (n1 - 1))\n",
    "        else:\n",
    "            k_xx_term = 0.0\n",
    "\n",
    "        # For K(Y,Y): exclude diagonal\n",
    "        if n2 > 1:\n",
    "            mask_yy = 1 - torch.eye(n2, device=Y.device)\n",
    "            k_yy_term = (k_yy * mask_yy).sum() / (n2 * (n2 - 1))\n",
    "        else:\n",
    "            k_yy_term = 0.0\n",
    "\n",
    "        # For K(X,Y): use all elements (no diagonal to exclude)\n",
    "        k_xy_term = k_xy.mean()\n",
    "\n",
    "        # MMD^2 = E[K(X,X)] + E[K(Y,Y)] - 2*E[K(X,Y)]\n",
    "        mmd_squared = k_xx_term + k_yy_term - 2 * k_xy_term\n",
    "        mmd_loss += mmd_squared\n",
    "\n",
    "    # Return negative MMD to encourage dissimilarity (higher MMD = more different)\n",
    "    return mmd_loss / len(sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c28ce78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T15:28:02.727698Z",
     "iopub.status.busy": "2025-09-17T15:28:02.727097Z",
     "iopub.status.idle": "2025-09-17T15:28:02.828638Z",
     "shell.execute_reply": "2025-09-17T15:28:02.827760Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_corr_pass(batch):\n",
    "    gene_pos = batch[\"genes\"].to(model.device)\n",
    "    expression = batch[\"x\"].to(model.device)\n",
    "    depth = batch[\"depth\"].to(model.device)\n",
    "    class_elem = batch[\"class\"].long().to(model.device)\n",
    "    total_loss = 0\n",
    "\n",
    "    # Forward pass with automatic mixed precisio^n\n",
    "    with torch.cuda.amp.autocast():\n",
    "        # Forward pass\n",
    "        output = model.forward(\n",
    "            gene_pos,\n",
    "            expression,\n",
    "            req_depth=depth,\n",
    "            depth_mult=expression.sum(1),\n",
    "            do_class=True,\n",
    "            metacell_token=torch.zeros_like(depth),\n",
    "        )\n",
    "        ## adaptor on ct_emb\n",
    "        # ctpos = model.classes.index(\"cell_type_ontology_term_id\") + 1\n",
    "        # emb = output[\"output_cell_embs\"][:, ctpos, :]\n",
    "        #\n",
    "        # output[\"output_cell_embs\"][:, ctpos, :] = adaptor_layer(\n",
    "        #    torch.cat([emb, class_elem[:, 1].unsqueeze(1).float()], dim=1)\n",
    "        # )\n",
    "\n",
    "        #batch_pos = model.classes.index(\"organism_ontology_term_id\") + 1\n",
    "        #output[\"output_cell_embs\"][:, batch_pos, :] = batch_emb(\n",
    "        #    class_elem[:, 1]\n",
    "        #)\n",
    "\n",
    "        ## generate expr loss\n",
    "        output_gen = model._generate(\n",
    "            cell_embs=output[\"output_cell_embs\"],\n",
    "            gene_pos=gene_pos,\n",
    "            depth_mult=expression.sum(1),\n",
    "            req_depth=depth,\n",
    "        )\n",
    "        if \"zero_logits\" in output_gen:\n",
    "            loss_expr = loss.zinb(\n",
    "                theta=output_gen[\"disp\"],\n",
    "                pi=output_gen[\"zero_logits\"],\n",
    "                mu=output_gen[\"mean\"],\n",
    "                target=expression,\n",
    "            )\n",
    "            if model.zinb_and_mse:\n",
    "                loss_expr += (\n",
    "                    loss.mse(\n",
    "                        input=torch.log(output_gen[\"mean\"] + 1)\n",
    "                        * (1 - torch.sigmoid(output_gen[\"zero_logits\"])),\n",
    "                        target=torch.log(expression + 1),\n",
    "                    )\n",
    "                    / 10  # scale to make it more similar to the zinb\n",
    "                )\n",
    "        else:\n",
    "            loss_expr = loss.mse(\n",
    "                input=torch.log(output_gen[\"mean\"] + 1),\n",
    "                target=torch.log(expression + 1),\n",
    "            )\n",
    "        # Add expression loss to total\n",
    "        total_loss += loss_expr\n",
    "\n",
    "        # ct clss\n",
    "        cls_output = output.get(\"cls_output_cell_type_ontology_term_id\")\n",
    "        # ct_output = output[\"output_cell_embs\"][:, ctpos, :]\n",
    "        # cls_output = model.cls_decoders[\"cell_type_ontology_term_id\"](ct_output)\n",
    "        cls_loss = loss.hierarchical_classification(\n",
    "            pred=cls_output,\n",
    "            cl=class_elem[:, 0],\n",
    "            labels_hierarchy=model.mat_labels_hierarchy.get(\n",
    "                \"cell_type_ontology_term_id\"\n",
    "            ).to(\"cuda\"),\n",
    "        )\n",
    "\n",
    "        # organ class\n",
    "        org_emb = output[\"compressed_cell_embs\"][\n",
    "            model.classes.index(\"organism_ontology_term_id\") + 1\n",
    "        ]\n",
    "        cls_loss += F.cross_entropy(\n",
    "            input=batch_cls(org_emb),\n",
    "            target=class_elem[:, 1],\n",
    "        )\n",
    "        total_loss += cls_loss\n",
    "\n",
    "        pos = model.classes.index(\"cell_type_ontology_term_id\") + 1\n",
    "        # Apply gradient reversal to the input embedding\n",
    "        selected_emb = (\n",
    "            output[\"compressed_cell_embs\"][pos]\n",
    "            if model.compressor is not None\n",
    "            else output[\"input_cell_embs\"][:, pos, :]\n",
    "        )\n",
    "        X, Y = selected_emb[class_elem[:, 1] == 1], selected_emb[class_elem[:, 1] == 0]\n",
    "\n",
    "        mmd = mmd_loss(X, Y)\n",
    "        if torch.isnan(mmd):\n",
    "            print(\"mmd nan\")\n",
    "        mmd = mmd.item() if not torch.isnan(mmd) else 0\n",
    "\n",
    "        # Add adversarial loss to total loss\n",
    "        total_loss += mmd * 3\n",
    "        total_loss += output[\"vae_kl_loss\"] * 0.5\n",
    "    return total_loss, cls_loss, mmd, loss_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771be887",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Alternative: Manual Training Loop (for more control)\n",
    "# If you prefer to have more control over the training process\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from scprint.model import loss\n",
    "\n",
    "num_epochs = 8\n",
    "lr = 0.0002\n",
    "\n",
    "# Setup optimizer\n",
    "all_params = (\n",
    "    list(model.parameters()) + list(batch_cls.parameters())\n",
    "    # + list(batch_vector.parameters())\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    all_params, lr=lr, weight_decay=0.01, betas=(0.9, 0.999), eps=1e-8\n",
    ")\n",
    "\n",
    "# Setup scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.2, patience=1\n",
    ")\n",
    "\n",
    "# Setup automatic mixed precision\n",
    "scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "_ = model.train()\n",
    "\n",
    "for k, i in model.mat_labels_hierarchy.items():\n",
    "    model.mat_labels_hierarchy[k] = i.to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7375aba4",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06917e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T09:52:20.405972Z",
     "iopub.status.busy": "2025-09-29T09:52:20.405829Z",
     "iopub.status.idle": "2025-09-29T10:08:55.187257Z",
     "shell.execute_reply": "2025-09-29T10:08:55.186662Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Current learning rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    # Training phase\n",
    "    train_loss = 0.0\n",
    "    train_steps = 0\n",
    "    avg_adv = 0\n",
    "    avg_expr = 0\n",
    "    avg_cls = 0\n",
    "    avg_mmd = 0\n",
    "\n",
    "    # pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        total_loss, cls_loss, mmd, loss_expr = batch_corr_pass(batch)\n",
    "        # Backward pass\n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += total_loss.item()\n",
    "        train_steps += 1\n",
    "        avg_cls += cls_loss.item()\n",
    "        avg_expr += loss_expr.item()\n",
    "        avg_mmd += mmd\n",
    "        # Update progress bar\n",
    "        # if batch_idx % 35 == 0:\n",
    "        # print(\n",
    "        #    f\"avg_loss {train_loss / train_steps:.4f}, avg_cls {avg_cls / train_steps:.4f}, avg_expr {avg_expr / train_steps:.4f}, avg_adv {avg_mmd/ train_steps:.4f}\"\n",
    "        # )\n",
    "        # pbar.set_postfix(\n",
    "        #    {\n",
    "        #        \"loss\": f\"{total_loss.item():.4f}\",\n",
    "        #        \"avg_loss\": f\"{train_loss / train_steps:.4f}\",\n",
    "        #        \"cls_loss\": f\"{cls_loss.item():.4f}\",\n",
    "        #        \"mmd_loss\": f\"{mmd:.4f}\",\n",
    "        #        \"expr_loss\": f\"{loss_expr.item():.4f}\",\n",
    "        #    }\n",
    "        # )\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_steps = 0\n",
    "    val_loss_expr = 0.0\n",
    "    val_mmd = 0.0\n",
    "    val_cls = 0.0\n",
    "    val_loss_to_prt = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:  # tqdm(val_loader, desc=\"Validation\"):\n",
    "            loss_val, cls_loss, mmd, loss_expr = batch_corr_pass(batch)\n",
    "            val_loss_to_prt += loss_val.item()\n",
    "            val_loss += loss_val.item()\n",
    "            val_steps += 1\n",
    "            val_loss_expr += loss_expr.item()\n",
    "            val_mmd += mmd\n",
    "            val_cls += cls_loss.item()\n",
    "    try:\n",
    "        avg_val_loss = val_loss_to_prt / val_steps\n",
    "        avg_train_loss = train_loss / train_steps\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Error: Division by zero occurred while calculating average losses.\")\n",
    "        avg_train_loss = 0\n",
    "    print(\n",
    "        \"cls_loss: {:.4f}, mmd_loss: {:.4f}, expr_loss: {:.4f}\".format(\n",
    "            val_cls / val_steps, val_mmd / val_steps, val_loss_expr / val_steps\n",
    "        )\n",
    "    )\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Store LR before scheduler step for comparison\n",
    "    lr_before = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Check if LR was reduced\n",
    "    lr_after = optimizer.param_groups[0][\"lr\"]\n",
    "    if lr_after < lr_before:\n",
    "        print(\n",
    "            f\"ðŸ”» Learning rate reduced from {lr_before:.2e} to {lr_after:.2e} (factor: {lr_after / lr_before:.3f})\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"âœ… Learning rate unchanged: {lr_after:.2e}\")\n",
    "\n",
    "    # Early stopping check (simple implementation)\n",
    "    if epoch > 3 and val_loss / val_steps > 1.3 * avg_train_loss:\n",
    "        print(\"Early stopping due to overfitting\")\n",
    "        break\n",
    "\n",
    "print(\"Manual fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98646e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T10:08:55.205316Z",
     "iopub.status.busy": "2025-09-29T10:08:55.205148Z",
     "iopub.status.idle": "2025-09-29T10:09:00.399271Z",
     "shell.execute_reply": "2025-09-29T10:09:00.398745Z"
    },
    "papermill": {
     "duration": 5.201081,
     "end_time": "2025-09-29T10:09:00.400217",
     "exception": false,
     "start_time": "2025-09-29T10:08:55.199136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "checkpoint = {\n",
    "    \"epoch\": epoch,\n",
    "    \"global_step\": (1 + epoch) * batch_idx,\n",
    "    \"pytorch-lightning_version\": L.__version__,\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"optimizer_states\": [optimizer.state_dict()],\n",
    "    \"lr_schedulers\": [scheduler.state_dict()],\n",
    "    \"hparams_name\": None,\n",
    "    \"loops\": None,\n",
    "    \"callbacks\": None,\n",
    "    \"hyper_parameters\": model.hparams,\n",
    "}\n",
    "torch.save(checkpoint, \"fit_2.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048ab6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:38:25.931554Z",
     "iopub.status.busy": "2025-09-17T14:38:25.930923Z",
     "iopub.status.idle": "2025-09-17T14:38:26.028879Z",
     "shell.execute_reply": "2025-09-17T14:38:26.027330Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"fit_2.ckpt\")[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daa4847",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:38:26.065779Z",
     "iopub.status.busy": "2025-09-17T14:38:26.065174Z",
     "iopub.status.idle": "2025-09-17T14:38:44.231575Z",
     "shell.execute_reply": "2025-09-17T14:38:44.230393Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = scPrint.load_from_checkpoint(\n",
    "    \"fit_2.ckpt\", precpt_gene_emb=None, attention=\"normal\"\n",
    ")\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f5c43a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:38:44.267886Z",
     "iopub.status.busy": "2025-09-17T14:38:44.267679Z",
     "iopub.status.idle": "2025-09-17T14:38:44.327038Z",
     "shell.execute_reply": "2025-09-17T14:38:44.326200Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "da.obs = da.obs.iloc[:, :-15]\n",
    "for i in [\n",
    "    \"scprint_emb\",\n",
    "    \"scprint_emb_age_group\",\n",
    "    \"scprint_emb_assay_ontology_term_id\",\n",
    "    \"scprint_emb_cell_culture\",\n",
    "    \"scprint_emb_cell_type_ontology_term_id\",\n",
    "    \"scprint_emb_disease_ontology_term_id\",\n",
    "    \"scprint_emb_organism_ontology_term_id\",\n",
    "    \"scprint_emb_other\",\n",
    "    \"scprint_emb_self_reported_ethnicity_ontology_term_id\",\n",
    "    \"scprint_emb_sex_ontology_term_id\",\n",
    "    \"scprint_emb_tissue_ontology_term_id\",\n",
    "]:\n",
    "    da.obsm.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a13f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:38:44.361828Z",
     "iopub.status.busy": "2025-09-17T14:38:44.361452Z",
     "iopub.status.idle": "2025-09-17T14:38:44.436825Z",
     "shell.execute_reply": "2025-09-17T14:38:44.435986Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed = Embedder(\n",
    "    how=\"random expr\",\n",
    "    max_len=2800,\n",
    "    num_workers=8,\n",
    "    pred_embedding=[\"all\"],\n",
    "    doplot=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb7590",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:38:44.472931Z",
     "iopub.status.busy": "2025-09-17T14:38:44.472394Z",
     "iopub.status.idle": "2025-09-17T14:45:12.987034Z",
     "shell.execute_reply": "2025-09-17T14:45:12.981321Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_adata, metrics = embed(model, da.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56806e1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:45:13.110599Z",
     "iopub.status.busy": "2025-09-17T14:45:13.109553Z",
     "iopub.status.idle": "2025-09-17T14:45:17.369742Z",
     "shell.execute_reply": "2025-09-17T14:45:17.368196Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_classification(\n",
    "    n_adata,\n",
    "    [\"cell_type_ontology_term_id\"],\n",
    "    label_decoders=model.label_decoders,\n",
    "    labels_hierarchy=model.labels_hierarchy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bbeed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:45:17.484969Z",
     "iopub.status.busy": "2025-09-17T14:45:17.484342Z",
     "iopub.status.idle": "2025-09-17T14:46:11.816463Z",
     "shell.execute_reply": "2025-09-17T14:46:11.814993Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pp.neighbors(n_adata, use_rep=\"scprint_emb_cell_type_ontology_term_id\")\n",
    "sc.tl.umap(n_adata)\n",
    "sc.pl.umap(\n",
    "    n_adata,\n",
    "    color=[\"conv_pred_cell_type_ontology_term_id\", \"cell_type\", \"batch\"],\n",
    "    ncols=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887818f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:46:11.964168Z",
     "iopub.status.busy": "2025-09-17T14:46:11.963493Z",
     "iopub.status.idle": "2025-09-17T14:46:31.623052Z",
     "shell.execute_reply": "2025-09-17T14:46:31.622259Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.pp.neighbors(n_adata, use_rep=\"scprint_emb\")\n",
    "sc.tl.umap(n_adata)\n",
    "sc.pl.umap(\n",
    "    n_adata,\n",
    "    color=[\"conv_pred_cell_type_ontology_term_id\", \"cell_type\", \"batch\"],\n",
    "    ncols=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58cd187",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:46:31.799300Z",
     "iopub.status.busy": "2025-09-17T14:46:31.798644Z",
     "iopub.status.idle": "2025-09-17T14:48:58.178129Z",
     "shell.execute_reply": "2025-09-17T14:48:58.176431Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bm = Benchmarker(\n",
    "    n_adata,\n",
    "    batch_key=\"batch\",  # batch, tech, assay_ontology_term_id, donor_id\n",
    "    label_key=\"cell_type_ontology_term_id\",  # celltype\n",
    "    embedding_obsm_keys=[\"scprint_emb_cell_type_ontology_term_id\"],\n",
    "    bio_conservation_metrics=BioConservation(),\n",
    "    batch_correction_metrics=BatchCorrection(),\n",
    "    n_jobs=10,\n",
    ")\n",
    "bm.benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813cd5c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T14:48:58.369818Z",
     "iopub.status.busy": "2025-09-17T14:48:58.369166Z",
     "iopub.status.idle": "2025-09-17T14:48:58.703378Z",
     "shell.execute_reply": "2025-09-17T14:48:58.701807Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# after fine tuning\n",
    "bm.plot_results_table(min_max_scale=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scPRINT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 203.438373,
   "end_time": "2025-09-17T15:28:09.167342",
   "environment_variables": {},
   "exception": null,
   "input_path": "fine_tuning_cross_species_emb_mmd.ipynb",
   "output_path": "fine_tuning_cross_species_emb_mmd.ipynb",
   "parameters": {},
   "start_time": "2025-09-12T08:38:54.200138",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
