{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import gseapy as gp\n",
    "from gears import PertData, GEARS\n",
    "\n",
    "from scipy.sparse import issparse\n",
    "import scipy as sp\n",
    "from einops import rearrange\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import scgpt as scg\n",
    "from scgpt.tasks import GeneEmbedding\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.utils import set_seed\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch\n",
    "from scgpt.preprocess import Preprocessor\n",
    "\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pytz\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import BertConfig, BertForMaskedLM, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geneformer import GeneformerPretrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_num = 0\n",
    "random.seed(seed_num)\n",
    "np.random.seed(seed_num)\n",
    "seed_val = 42\n",
    "torch.manual_seed(seed_val)\n",
    "# torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# set local time/directories\n",
    "timezone = pytz.timezone(\"US/Eastern\")\n",
    "rootdir = \"/parent_ouput_directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model parameters\n",
    "# model type\n",
    "model_type = \"bert\"\n",
    "# max input size\n",
    "max_input_size = 2**11  # 2048\n",
    "# number of layers\n",
    "num_layers = 2\n",
    "# number of attention heads\n",
    "num_attn_heads = 1\n",
    "# number of embedding dimensions\n",
    "num_embed_dim = 128\n",
    "# intermediate size\n",
    "intermed_size = num_embed_dim * 2\n",
    "# activation function\n",
    "activ_fn = \"relu\"\n",
    "# initializer range, layer norm, dropout\n",
    "initializer_range = 0.02\n",
    "layer_norm_eps = 1e-12\n",
    "attention_probs_dropout_prob = 0.1\n",
    "hidden_dropout_prob = 0.1\n",
    "\n",
    "# set training parameters\n",
    "# total number of examples in Genecorpus-30M after QC filtering:\n",
    "num_examples = 27_406_208\n",
    "# number gpus\n",
    "num_gpus = 1\n",
    "# batch size for training and eval\n",
    "geneformer_batch_size = 4\n",
    "# max learning rate\n",
    "max_lr = 1e-3\n",
    "# learning schedule\n",
    "lr_schedule_fn = \"linear\"\n",
    "# warmup steps\n",
    "warmup_steps = 10_000\n",
    "# number of epochs\n",
    "epochs = 3\n",
    "# optimizer\n",
    "optimizer = \"adamw\"\n",
    "# weight_decay\n",
    "weight_decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output directories\n",
    "current_date = datetime.datetime.now(tz=timezone)\n",
    "datestamp = f\"{str(current_date.year)[-2:]}{current_date.month:02d}{current_date.day:02d}_{current_date.strftime('%X').replace(':','')}\"\n",
    "run_name = f\"{datestamp}_geneformer_30M_L{num_layers}_emb{num_embed_dim}_SL{max_input_size}_E{epochs}_B{geneformer_batch_size}_LR{max_lr}_LS{lr_schedule_fn}_WU{warmup_steps}_O{optimizer}_DS{num_gpus}\"\n",
    "training_output_dir = f\"{rootdir}/models/{run_name}/\"\n",
    "logging_dir = f\"{rootdir}/runs/{run_name}/\"\n",
    "model_output_dir = os.path.join(training_output_dir, \"models/\")\n",
    "\n",
    "\n",
    "# ensure not overwriting previously saved model\n",
    "model_output_file = os.path.join(model_output_dir, \"pytorch_model.bin\")\n",
    "if os.path.isfile(model_output_file) is True:\n",
    "    raise Exception(\"Model already saved to this directory.\")\n",
    "\n",
    "\n",
    "# make training and model output directories\n",
    "subprocess.call(f\"mkdir {training_output_dir}\", shell=True)\n",
    "subprocess.call(f\"mkdir {model_output_dir}\", shell=True)\n",
    "\n",
    "\n",
    "# load gene_ensembl_id:token dictionary (e.g. https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/blob/main/token_dictionary.pkl)\n",
    "with open(\"token_dictionary.pkl\", \"rb\") as fp:\n",
    "    token_dictionary = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configuration\n",
    "config = {\n",
    "    \"hidden_size\": num_embed_dim,\n",
    "    \"num_hidden_layers\": num_layers,\n",
    "    \"initializer_range\": initializer_range,\n",
    "    \"layer_norm_eps\": layer_norm_eps,\n",
    "    \"attention_probs_dropout_prob\": attention_probs_dropout_prob,\n",
    "    \"hidden_dropout_prob\": hidden_dropout_prob,\n",
    "    \"intermediate_size\": intermed_size,\n",
    "    \"hidden_act\": activ_fn,\n",
    "    \"max_position_embeddings\": max_input_size,\n",
    "    \"model_type\": model_type,\n",
    "    \"num_attention_heads\": num_attn_heads,\n",
    "    \"pad_token_id\": token_dictionary.get(\"<pad>\"),\n",
    "    \"vocab_size\": len(token_dictionary),  # genes+2 for <mask> and <pad> tokens\n",
    "}\n",
    "\n",
    "config = BertConfig(**config)\n",
    "model = BertForMaskedLM(config)\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training arguments\n",
    "training_args = {\n",
    "    \"learning_rate\": max_lr,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": False,\n",
    "    \"group_by_length\": True,\n",
    "    \"length_column_name\": \"length\",\n",
    "    \"disable_tqdm\": False,\n",
    "    \"lr_scheduler_type\": lr_schedule_fn,\n",
    "    \"warmup_steps\": warmup_steps,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"per_device_train_batch_size\": geneformer_batch_size,\n",
    "    \"num_train_epochs\": epochs,\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": np.floor(\n",
    "        num_examples / geneformer_batch_size / 8\n",
    "    ),  # 8 saves per epoch\n",
    "    \"logging_steps\": 1000,\n",
    "    \"output_dir\": training_output_dir,\n",
    "    \"logging_dir\": logging_dir,\n",
    "}\n",
    "training_args = TrainingArguments(**training_args)\n",
    "\n",
    "print(\"Starting training.\")\n",
    "\n",
    "# define the trainer\n",
    "trainer = GeneformerPretrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    # pretraining corpus (e.g. https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/tree/main/genecorpus_30M_2048.dataset)\n",
    "    train_dataset=load_from_disk(\"genecorpus_30M_2048.dataset\"),\n",
    "    # file of lengths of each example cell (e.g. https://huggingface.co/datasets/ctheodoris/Genecorpus-30M/blob/main/genecorpus_30M_2048_lengths.pkl)\n",
    "    example_lengths_file=\"genecorpus_30M_2048_lengths.pkl\",\n",
    "    token_dictionary=token_dictionary,\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()\n",
    "\n",
    "# save model\n",
    "trainer.save_model(model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
