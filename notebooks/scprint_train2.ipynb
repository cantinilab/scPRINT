{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mâ†’\u001b[0m connected lamindb: jkobject/scprint2\n"
     ]
    }
   ],
   "source": [
    "! lamin load jkobject/scprint2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, StochasticWeightAveraging, EarlyStopping, LearningRateMonitor, LearningRateFinder\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "\n",
    "from scprint import scPrint\n",
    "from scprint.trainer import TrainingMode\n",
    "from scdataloader import DataModule \n",
    "import pandas as pd\n",
    "from scdataloader.utils import load_genes\n",
    "import lamindb as ln\n",
    "\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: drop tissue & dev stage until part or is taken in account\n",
    "\n",
    "hierarchical_clss = [\n",
    "    \"cell_type_ontology_term_id\",  # 1\n",
    "    \"tissue_ontology_term_id\",\n",
    "    \"disease_ontology_term_id\",  # 2\n",
    "    \"simplified_dev_stage\",\n",
    "    \"assay_ontology_term_id\",  # 3\n",
    "    'self_reported_ethnicity_ontology_term_id',  # 4\n",
    "]\n",
    "clss_to_predict = hierarchical_clss+[\n",
    "    'sex_ontology_term_id',  # 5\n",
    "    \"organism_ontology_term_id\",  # 6\n",
    "    \"cell_culture\"\n",
    "]\n",
    "clss_to_weight = [\n",
    "    \"tissue_ontology_term_id\",\n",
    "    \"disease_ontology_term_id\",\n",
    "    \"simplified_dev_stage\",\n",
    "    \"assay_ontology_term_id\",\n",
    "    \"organism_ontology_term_id\",\n",
    "    \"clust_cell_type\",\n",
    "    # 'dataset_id',\n",
    "    # 'cell_culture',\n",
    "    #  \"heat_diff\",\n",
    "    #  \"total_counts\",\n",
    "    \"nnz\",\n",
    "    #  \"dpt_group\",\n",
    "]\n",
    "\n",
    "gene_emb = '../data/main/gene_embeddings.parquet'\n",
    "d_model = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m!\u001b[0m no run & transform got linked, call `ln.track()` & re-run\n",
      "\u001b[93m!\u001b[0m run input wasn't tracked, call `ln.track()` and re-run\n",
      "\u001b[93m!\u001b[0m run input wasn't tracked, call `ln.track()` and re-run\n",
      "won't do any check but we recommend to have your dataset coming from local storage\n",
      "100.0% are aligned\n",
      "seeing a string: loading gene positions as biomart parquet file\n"
     ]
    }
   ],
   "source": [
    "datamodule = DataModule(\n",
    "    collection_name=\"scPRINT-V2 test\", #some, all, preprocessed dataset, all no zhang, \n",
    "    gene_embeddings=gene_emb,\n",
    "    clss_to_weight=clss_to_weight,\n",
    "    metacell_mode=0.2,\n",
    "    clss_to_predict=clss_to_predict,\n",
    "    hierarchical_clss=hierarchical_clss,\n",
    "    organisms=[\"NCBITaxon:9606\"],#, \"NCBITaxon:10090\"],\n",
    "    how=\"random expr\",\n",
    "    max_len=3200,\n",
    "    add_zero_genes=0,\n",
    "    # how much more you will see the most present vs less present category\n",
    "    weight_scaler=100,\n",
    "    batch_size=8,\n",
    "    num_workers=20,\n",
    "    prefetch_factor=3,\n",
    "    train_oversampling_per_epoch=2,\n",
    "    validation_split=0.05,\n",
    "    do_gene_pos='../data/main/biomart_pos.parquet',\n",
    "    pin_memory=True,\n",
    "    test_split=0.05)\n",
    "testfiles = datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "model = scPrint(\n",
    "    genes=datamodule.genes,\n",
    "    d_model=d_model*2,\n",
    "    nhead=2*2,\n",
    "    num_heads_kv=2,\n",
    "    nlayers=8,\n",
    "    layers_cls = [d_model],\n",
    "    classes = datamodule.classes,\n",
    "    labels_hierarchy = datamodule.labels_hierarchy,\n",
    "    dropout=0.1,\n",
    "    transformer=\"flash\",\n",
    "    precpt_gene_emb=gene_emb,\n",
    "    gene_pos_enc=datamodule.gene_pos,\n",
    "    mvc_decoder=\"inner product\",\n",
    "    label_decoders = datamodule.decoders,\n",
    "    num_batch_labels = datamodule.num_datasets,\n",
    "    checkpointing=True,\n",
    "    prenorm=True,\n",
    "    cell_specific_blocks=True,\n",
    "    #weight_decay=0.01,\n",
    "    #zinb=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjkobject\u001b[0m (\u001b[33mml4ig\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../data/tensorboard/wandb/run-20241205_171025-0g3z31yt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ml4ig/scprint_test/runs/0g3z31yt' target=\"_blank\">clean-dragon-11</a></strong> to <a href='https://wandb.ai/ml4ig/scprint_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ml4ig/scprint_test' target=\"_blank\">https://wandb.ai/ml4ig/scprint_test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ml4ig/scprint_test/runs/0g3z31yt' target=\"_blank\">https://wandb.ai/ml4ig/scprint_test/runs/0g3z31yt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    }
   ],
   "source": [
    "# from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"scprint_test\",\n",
    "                           save_dir=\"../data/tensorboard\")\n",
    "wandb_logger.watch(model, log='all', log_freq=50, log_graph=True)\n",
    "\n",
    "# tlogger = TensorBoardLogger(save_dir=\"../data/tensorboard\")\n",
    "# tlogger.log_graph(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n"
     ]
    }
   ],
   "source": [
    "chckp = ModelCheckpoint(monitor=\"val_loss\", save_top_k=-1)\n",
    "trainingmode = TrainingMode(\n",
    "    do_denoise=True,\n",
    "    noise=[0.6],\n",
    "    do_mvc=False,\n",
    "    do_adv_cls=False,\n",
    "    run_full_forward=True,\n",
    "    mask_ratio=[\"TF\"],\n",
    "    warmup_duration=100,\n",
    "    fused_adam=False,\n",
    "    lr_reduce_patience=200,\n",
    ")\n",
    "trainer = Trainer(precision=\"16-mixed\", gradient_clip_val=30, gradient_clip_algorithm=\"norm\", max_time={\"hours\": 2}, limit_val_batches=1, callbacks=[\n",
    "                  trainingmode], accumulate_grad_batches=1, check_val_every_n_epoch=1, reload_dataloaders_every_n_epochs=1000000, \n",
    "                  #logger=wandb_logger\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                            | Type                         | Params | Mode \n",
      "------------------------------------------------------------------------------------------\n",
      "0  | gene_encoder                    | GeneEncoder                  | 5.9 M  | train\n",
      "1  | expr_encoder                    | ContinuousValueEncoder       | 66.8 K | train\n",
      "2  | pos_encoder                     | PositionalEncoding           | 0      | train\n",
      "3  | class_encoder                   | CategoryValueEncoder         | 2.6 K  | train\n",
      "4  | depth_encoder                   | ContinuousValueEncoder       | 66.8 K | train\n",
      "5  | transformer                     | FlashTransformer             | 11.6 M | train\n",
      "6  | cell_transformer                | FlashTransformer             | 8.7 M  | train\n",
      "7  | expr_decoder                    | ExprDecoder                  | 133 K  | train\n",
      "8  | cls_decoders                    | ModuleDict                   | 315 K  | train\n",
      "9  | grad_reverse_discriminator_loss | AdversarialDiscriminatorLoss | 134 K  | train\n",
      "10 | mvc_decoder                     | MVCDecoder                   | 262 K  | train\n",
      "------------------------------------------------------------------------------------------\n",
      "21.3 M    Trainable params\n",
      "5.9 M     Non-trainable params\n",
      "27.2 M    Total params\n",
      "108.718   Total estimated model params size (MB)\n",
      "607       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_fit_start\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14bdd8183ee34e0c98979a7c74ff63a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a4e9e99e274be38f8aaa60567e9812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n",
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "masking\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "denoising\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n",
      "compute loss\n",
      "generate\n",
      "encoder\n",
      "decoder\n",
      "backward\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward end\n",
      "new batch\n",
      "full forward\n",
      "encoder\n",
      "transformer\n",
      "cell transformer\n",
      "decoder\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents code/scPRINT/scprint/model/model.py:1121\u001b[0m, in \u001b[0;36mscPrint.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;66;03m# update params\u001b[39;00m\n\u001b[0;32m-> 1121\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# manually warm up lr without a scheduler\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# making sure that we don't do this during lrfinder\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/plugins/precision/amp.py:78\u001b[0m, in \u001b[0;36mMixedPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 78\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# If backward was skipped in automatic optimization (return None), unscaling is not needed\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:129\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 129\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/loops/optimization/automatic.py:317\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m--> 317\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents code/scPRINT/scprint/model/model.py:731\u001b[0m, in \u001b[0;36mscPrint.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;124;03mtraining_step defines the train loop. It is independent of forward\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;124;03m    _type_: _description_\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 731\u001b[0m total_loss, losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_full_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_denoise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_denoise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnoise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_next_tp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_next_tp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_cce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_cce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcce_temp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcce_temp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_ecs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_ecs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_mvc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_mvc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_adv_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_adv_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_adv_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_adv_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_full_forward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_full_forward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, total_loss, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sync_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents code/scPRINT/scprint/model/model.py:811\u001b[0m, in \u001b[0;36mscPrint._full_training\u001b[0;34m(self, batch, do_denoise, noise, do_next_tp, do_cce, cce_temp, do_ecs, do_mvc, do_adv_cls, do_adv_batch, do_cls, do_generate, run_full_forward, mask_ratio)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_full_forward:\n\u001b[0;32m--> 811\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgene_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreq_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_mvc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_mvc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output:\n",
      "File \u001b[0;32m~/Documents code/scPRINT/scprint/model/model.py:623\u001b[0m, in \u001b[0;36mscPrint.forward\u001b[0;34m(self, gene_pos, expression, mask, req_depth, timepoint, get_gene_emb, depth_mult, do_sample, do_mvc, do_class, get_attention_layer)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransformer_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdepth_mult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_gene_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_mvc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreq_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents code/scPRINT/scprint/model/model.py:493\u001b[0m, in \u001b[0;36mscPrint._decoder\u001b[0;34m(self, transformer_output, depth_mult, get_gene_emb, do_sample, do_mvc, do_class, req_depth)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m do_class:\n\u001b[1;32m    492\u001b[0m     output\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m--> 493\u001b[0m         {\n\u001b[1;32m    494\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_output_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m clsname: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_decoders[clsname](\n\u001b[1;32m    495\u001b[0m                 output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_embs\u001b[39m\u001b[38;5;124m\"\u001b[39m][\n\u001b[1;32m    496\u001b[0m                     :, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m i, :\n\u001b[1;32m    497\u001b[0m                 ]  \u001b[38;5;66;03m# the first elem is the base cell embedding\u001b[39;00m\n\u001b[1;32m    498\u001b[0m             )\n\u001b[1;32m    499\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i, clsname \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses)\n\u001b[1;32m    500\u001b[0m         }\n\u001b[1;32m    501\u001b[0m     )  \u001b[38;5;66;03m# (minibatch, n_cls)\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_mvc:\n",
      "File \u001b[0;32m~/Documents code/scPRINT/scprint/model/model.py:494\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m do_class:\n\u001b[1;32m    492\u001b[0m     output\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    493\u001b[0m         {\n\u001b[0;32m--> 494\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls_output_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m clsname: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls_decoders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclsname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m                \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcell_embs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# the first elem is the base cell embedding\u001b[39;49;00m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i, clsname \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses)\n\u001b[1;32m    500\u001b[0m         }\n\u001b[1;32m    501\u001b[0m     )  \u001b[38;5;66;03m# (minibatch, n_cls)\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_mvc:\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents code/scPRINT/scprint/model/decoders.py:242\u001b[0m, in \u001b[0;36mClsDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    241\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x)\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(overfit_batches=1)` was configured so 1 batch will be used.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(overfit_batches=1)` was configured so 1 batch will be used.\n"
     ]
    }
   ],
   "source": [
    "trainingmode = TrainingMode(\n",
    "    do_denoise=True,\n",
    "    noise=[0.6],\n",
    "    do_cce=True,\n",
    "    cce_sim=0.6,\n",
    "    do_ecs=False,\n",
    "    ecs_threshold=0.4,\n",
    "    ecs_scale=0.05,\n",
    "    class_scale=0.08,\n",
    "    do_cls=True,\n",
    "    do_mvc=False,\n",
    "    do_adv_cls=False,\n",
    "    run_full_forward=True,\n",
    "    do_next_tp=False,\n",
    "    mask_ratio=[\"TF\"],\n",
    "    fused_adam=False,\n",
    "    lr_reduce_monitor=None,\n",
    "    \n",
    ")\n",
    "overfit_trainer = Trainer(precision=\"16-mixed\", gradient_clip_val=10, max_time={\"hours\": 2}, limit_val_batches=1, callbacks=[\n",
    "                  trainingmode], accumulate_grad_batches=1, check_val_every_n_epoch=10_000, overfit_batches=1, \n",
    "                  reload_dataloaders_every_n_epochs=1_000_000, num_sanity_val_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'overfit_trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moverfit_trainer\u001b[49m\u001b[38;5;241m.\u001b[39mfit(model, datamodule\u001b[38;5;241m=\u001b[39mdatamodule)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'overfit_trainer' is not defined"
     ]
    }
   ],
   "source": [
    "overfit_trainer.fit(model, datamodule=datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scprint2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
