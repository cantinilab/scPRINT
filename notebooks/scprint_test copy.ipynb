{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! lamin load scprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 42\n",
      "2024-01-28 15:26:28,774:INFO - Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'triton'\n",
      "can't use flash attention and triton kernel,        you likely don't have the right hardware or didn't         make the right installation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/nscprint/lib/python3.10/site-packages/gget/gget_setup.py:252: DeprecationWarning: invalid escape sequence '\\/'\n",
      "  command = \"\"\"\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/nscprint/lib/python3.10/site-packages/gget/gget_setup.py:263: DeprecationWarning: invalid escape sequence '\\/'\n",
      "  \"/\", \"\\/\"\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/nscprint/lib/python3.10/site-packages/gget/gget_setup.py:271: DeprecationWarning: invalid escape sequence '\\/'\n",
      "  command = \"\"\"\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/nscprint/lib/python3.10/site-packages/gget/gget_setup.py:282: DeprecationWarning: invalid escape sequence '\\/'\n",
      "  \"/\", \"\\/\"\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_genes' from 'scdataloader.utils' (/opt/homebrew/Caskroom/miniconda/base/envs/nscprint/lib/python3.10/site-packages/scdataloader/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, seed_everything\n\u001b[1;32m     10\u001b[0m seed_everything(\u001b[38;5;241m42\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscprint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scPrint\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscprint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getBiomartTable\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscdataloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[0;32m~/Documents/code/scPRINT/scprint/__init__.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m     handler\u001b[38;5;241m.\u001b[39msetFormatter(formatter)\n\u001b[1;32m     18\u001b[0m     logger\u001b[38;5;241m.\u001b[39maddHandler(handler)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scPrint\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# from .data_collator import DataCollator\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# from .data_sampler import SubsetsBatchSampler\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# from .trainer import (\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#    test,\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# )#\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code/scPRINT/scprint/model/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Documents/code/scPRINT/scprint/model/model.py:40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdsbn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DomainSpecificBatchNorm1d\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m loss\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tokenizer\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mscPrint\u001b[39;00m(L\u001b[38;5;241m.\u001b[39mLightningModule):\n",
      "File \u001b[0;32m~/Documents/code/scPRINT/scprint/dataloader/__init__.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotein_embedder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PROTBERT\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m embed\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Collator\n",
      "File \u001b[0;32m~/Documents/code/scPRINT/scprint/dataloader/collator.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlnschema_bionty\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlb\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscdataloader\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_genes\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCollator\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_genes' from 'scdataloader.utils' (/opt/homebrew/Caskroom/miniconda/base/envs/nscprint/lib/python3.10/site-packages/scdataloader/utils.py)"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import lamindb as ln\n",
    "import lnschema_bionty as lb\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "from scprint import scPrint\n",
    "from scprint.utils import getBiomartTable\n",
    "\n",
    "from scdataloader import Dataset\n",
    "from scdataloader import DataModule\n",
    "from scprint.dataloader import embed\n",
    "from scdataloader.utils import load_genes\n",
    "from scprint.dataloader import Collator\n",
    "\n",
    "import torch \n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "lb.settings.organism = \"human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Gene embeddings\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# embeddings = embed(genedf=genedf,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     organism=\"homo_sapiens\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     embedding_size=1024,)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# embeddings.to_parquet('../data/temp/embeddings.parquet')\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/temp/embeddings.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m embeddings\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memb_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# and annotations\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "## Gene embeddings\n",
    "# embeddings = embed(genedf=genedf,\n",
    "#     organism=\"homo_sapiens\",\n",
    "#     cache=True,\n",
    "#     fasta_path=\"/tmp/data/fasta/\",\n",
    "#     embedding_size=1024,)\n",
    "# embeddings.to_parquet('../data/temp/embeddings.parquet')\n",
    "embeddings = pd.read_parquet('../data/temp/embeddings.parquet')\n",
    "embeddings.columns = ['emb_'+str(i) for i in embeddings.columns]\n",
    "# and annotations\n",
    "biomart = getBiomartTable(attributes=['start_position', 'chromosome_name']).set_index('ensembl_gene_id')\n",
    "biomart = biomart.loc[~biomart.index.duplicated(keep='first')]\n",
    "biomart = biomart.sort_values(by=['chromosome_name', 'start_position'])\n",
    "# and location\n",
    "c = []\n",
    "i = 0\n",
    "prev_position = -100000\n",
    "prev_chromosome = None\n",
    "for _, r in biomart.iterrows():\n",
    "    if r['chromosome_name'] != prev_chromosome or r['start_position'] - prev_position > 10_000:\n",
    "        i += 1\n",
    "    c.append(i)\n",
    "    prev_position = r['start_position']\n",
    "    prev_chromosome = r['chromosome_name']\n",
    "print(f'reduced the size to {len(set(c))/len(biomart)}')\n",
    "biomart['pos'] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OR directly load the dataset\n",
    "name=\"preprocessed dataset\"\n",
    "dataset = ln.Collection.filter(name=name).first()\n",
    "dataset.artifacts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: drop tissue & dev stage until part or is taken in account\n",
    "\n",
    "hierarchical_labels = [\n",
    "    \"cell_type_ontology_term_id\",\n",
    "    #\"tissue_ontology_term_id\",\n",
    "    \"disease_ontology_term_id\",\n",
    "    #\"development_stage_ontology_term_id\",\n",
    "    \"assay_ontology_term_id\",\n",
    "    'self_reported_ethnicity_ontology_term_id',\n",
    "]\n",
    "\n",
    "labels_weighted_sampling = hierarchical_labels+[\n",
    "    'sex_ontology_term_id',\n",
    "    \"organism_ontology_term_id\",\n",
    "]\n",
    "\n",
    "all_labels = labels_weighted_sampling+[\n",
    "    #'dataset_id',\n",
    "    #'cell_culture',\n",
    "    \"heat_diff\",\n",
    "    \"total_counts\",\n",
    "    \"nnz\",\n",
    "    \"dpt_group\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "won't do any check but we recommend to have your dataset coming from local storage\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mdataset = Dataset(dataset, organisms=[\"NCBITaxon:9606\"], obs=all_labels, clss_to_pred=labels_weighted_sampling, hierarchical_clss=hierarchical_labels)\n",
    "print(mdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we might want not to order the genes by expression (or do it?)\n",
    "# we might want to not introduce zeros and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = Collator(organisms=[\"NCBITaxon:9606\",], labels=all_labels, genelist=embeddings.index.tolist(), max_len=1000, add_zero_genes=100, org_to_id={'NCBITaxon:9606': mdataset.encoder['organism_ontology_term_id']['NCBITaxon:9606']})#mdataset.encoder['organism_ontology_term_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = DataModule(mdataset, label_to_weight=labels_weighted_sampling, collate_fn=col, batch_size=32, num_workers=4)\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in datamodule.train_dataloader():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 128\n",
    "m = torch.nn.AdaptiveAvgPool1d(d_model)\n",
    "sembeddings = pd.DataFrame(\n",
    "    data=m(torch.tensor(embeddings.values)), index=embeddings.index, columns=[f'emb_{i}' for i in range(d_model)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {k: len(v) for k, v in mdataset.class_topred.items()}\n",
    "\n",
    "cls_hierarchies = {}\n",
    "for k, dic in mdataset.class_groupings.items():\n",
    "    rdic = {}\n",
    "    for sk, v in dic.items():\n",
    "        rdic[mdataset.encoder[k][sk]] = [mdataset.encoder[k][i] for i in list(v)]\n",
    "    cls_hierarchies[k] = rdic\n",
    "\n",
    "df = sembeddings.join(biomart,how=\"inner\")\n",
    "\n",
    "genedf = load_genes(['NCBITaxon:9606'])\n",
    "df = df.loc[genedf[genedf.index.isin(df.index)].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-27 23:28:52,033:INFO - Created a temporary directory at /tmp/tmp82od2gry\n",
      "2024-01-27 23:28:52,035:INFO - Writing /tmp/tmp82od2gry/_remote_module_non_scriptable.py\n",
      "2024-01-27 23:28:52,035:INFO - Writing /tmp/tmp82od2gry/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "model = scPrint(\n",
    "    genes = df.index.tolist(),\n",
    "    d_model = d_model,\n",
    "    nhead = 4,\n",
    "    d_hid = d_model,\n",
    "    nlayers = 2,\n",
    "    layers_cls = [],\n",
    "    labels = labels,\n",
    "    cls_hierarchy = cls_hierarchies,\n",
    "    dropout= 0.2,\n",
    "    transformer = \"fast\",\n",
    "    use_precpt_gene_emb = df.iloc[:,:d_model].values.astype(float),\n",
    "    gene_pos_enc = df['pos'].tolist(),\n",
    "    mvc_decoder = \"inner product\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to transform an scGPT checkpoint to an scPrint's\n",
    "# ckpt = torch.load(\"../../scGPT/save/model_e6.pt\")\n",
    "# scPrint.load_from_checkpoint(\"../../scGPT/save/model_e6.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "#wandb_logger = WandbLogger(project=\"scprint_test\", save_dir=\"../data/tensorboard\")\n",
    "#wandb_logger.watch(model)\n",
    "\n",
    "tlogger = TensorBoardLogger(save_dir=\"../data/tensorboard\")\n",
    "tlogger.log_graph(model, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.profilers import PyTorchProfiler\n",
    "pytorch_prof = PyTorchProfiler(\"../data/tensorboard\", emit_nvtx=False, group_by_input_shape=True, record_shapes=True, profile_memory=True, with_stack=True, on_trace_ready=torch.profiler.tensorboard_trace_handler(\"../data/tensorboard/\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using 16bit Automatic Mixed Precision (AMP)\n",
      "2024-01-27 23:28:56,458:INFO - Using 16bit Automatic Mixed Precision (AMP)\n",
      "2024-01-27 23:28:56,458:INFO - Using 16bit Automatic Mixed Precision (AMP)\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "2024-01-27 23:28:56,794:INFO - GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "2024-01-27 23:28:56,797:INFO - TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "2024-01-27 23:28:56,800:INFO - IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "2024-01-27 23:28:56,803:INFO - HPU available: False, using: 0 HPUs\n",
      "INFO: Running in `fast_dev_run` mode: will run the requested loop using 1000 batch(es). Logging and checkpointing is suppressed.\n",
      "2024-01-27 23:28:56,807:INFO - Running in `fast_dev_run` mode: will run the requested loop using 1000 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    }
   ],
   "source": [
    "# sets seeds for numpy, torch and python.random.\n",
    "trainer = Trainer(precision=16, profiler=\"simple\", fast_dev_run=1000, logger=tlogger)#, wandb_logger], limit_train_batches=0.1, limit_test_batches=0.03, limit_val_batches=0.03)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.labels = {}\n",
    "model.expr_decoder.nfirst_labels_to_skip=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "2024-01-27 23:42:54,990:INFO - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name          | Type                   | Params\n",
      "----------------------------------------------------------\n",
      "0  | gene_encoder  | GeneEncoder            | 4.3 M \n",
      "1  | expr_encoder  | ContinuousValueEncoder | 512   \n",
      "2  | pos_encoder   | PositionalEncoding     | 0     \n",
      "3  | label_encoder | BatchLabelEncoder      | 1.4 K \n",
      "4  | time_encoder  | ContinuousValueEncoder | 512   \n",
      "5  | depth_encoder | ContinuousValueEncoder | 512   \n",
      "6  | transformer   | TransformerEncoder     | 796 K \n",
      "7  | expr_decoder  | ExprDecoder            | 50.1 K\n",
      "8  | cls_decoders  | ModuleDict             | 29.9 K\n",
      "9  | mvc_decoder   | MVCDecoder             | 65.7 K\n",
      "10 | sim           | Similarity             | 0     \n",
      "----------------------------------------------------------\n",
      "945 K     Trainable params\n",
      "4.3 M     Non-trainable params\n",
      "5.3 M     Total params\n",
      "21.134    Total estimated model params size (MB)\n",
      "2024-01-27 23:42:54,996:INFO - \n",
      "   | Name          | Type                   | Params\n",
      "----------------------------------------------------------\n",
      "0  | gene_encoder  | GeneEncoder            | 4.3 M \n",
      "1  | expr_encoder  | ContinuousValueEncoder | 512   \n",
      "2  | pos_encoder   | PositionalEncoding     | 0     \n",
      "3  | label_encoder | BatchLabelEncoder      | 1.4 K \n",
      "4  | time_encoder  | ContinuousValueEncoder | 512   \n",
      "5  | depth_encoder | ContinuousValueEncoder | 512   \n",
      "6  | transformer   | TransformerEncoder     | 796 K \n",
      "7  | expr_decoder  | ExprDecoder            | 50.1 K\n",
      "8  | cls_decoders  | ModuleDict             | 29.9 K\n",
      "9  | mvc_decoder   | MVCDecoder             | 65.7 K\n",
      "10 | sim           | Similarity             | 0     \n",
      "----------------------------------------------------------\n",
      "945 K     Trainable params\n",
      "4.3 M     Non-trainable params\n",
      "5.3 M     Total params\n",
      "21.134    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f61dfa5e03400ead8ee73b36336771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=\"../data/tensorboard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: log embeddings of N cells + cell type & disease & tissue & sex & predicted version as scanpy plot and log them to wandb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.66 - 128 - noclass - 64 - 8\n",
    "4.22 - 128 - noclass - 32 - 4\n",
    "4.50 - 128 - noclass - 32 - 8\n",
    "9.00 - 128 - 1 MLM   - 32 - 8\n",
    "5.80 - 256 - 1 MLM   - 32 - 8\n",
    "2.80 - 256 - noclass - 32 - 8\n",
    "2.82 - 256 - noclass - 32 - 4\n",
    "1.70 - 256 - all   - 32 - 8\n",
    "# I could go 2 times faster with better engineering\n",
    "# I need to rework the classification task\n",
    " - 128 - all - 32 - 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger.finalize(status=\"aborted\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.tuner import Tuner\n",
    "tuner = Tuner(trainer)\n",
    "tuner.scale_batch_size(model, mode=\"power\") #default\n",
    "tuner.lr_find(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: debug how much time is spent on class prediction (0.5 day)\n",
    "# TODO: test unseen genes (do we see much being kept after filtering and stuff) (0.5 day)\n",
    "# TODO: visualize and assess the embeddings (0.5 day)\n",
    "----\n",
    "# TODO: add model checkpointing (0.5 day) \n",
    "# TODO: add method to run the model as a script (0.5 day) ---\n",
    "# TODO: create env and copy data to maestro (0.5 day)\n",
    "# TODO: create script and test it on maestro (0.5 day)\n",
    "# TODO: connect with maestro people to ask for longer compute time \n",
    "# TODO: do the same to jean zay (0.5 day)\n",
    "------\n",
    "# TODO: make a model benchmark package (continue from where I left off) (4 days)\n",
    "# TODO: make a task function & make a benchmark function (1 day) (*denoising, *classification, *embeddings, *perturbation prediction)\n",
    "# TODO: define the test set as a subcollection of datasets (print them and save as a file somewhere too) (0.5 day)\n",
    "------\n",
    "# TODO: debug the gene embedding creation\n",
    "# TODO: create embedding & make it work for the 4-5 species in the dataset (1 days) \n",
    "# TODO: debug the timepoint problem (2 days)\n",
    "# TODO: find the neighboors and next time point cells (1 days)\n",
    "# TODO: create a version with next time point and neighboors task (1 days)\n",
    "# TODO: make a trajectory prediction task (predict future cell type/s, expression) and benchmark (similarity to known future cell, similarity to known future expression) (1 days)\n",
    "------\n",
    "# TODO: log more info with .log() during training (the model architecture) (some weights? some gradients?) (0.5 day)\n",
    "# TODO: run a large training on maestro (0.5 day)\n",
    "------\n",
    "# TODO: add KO & drug datasets\n",
    "# TODO: create a version with KO and drug effect prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scprint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
