{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ml4ig1/miniconda3/envs/scprint/bin/lamin\", line 5, in <module>\n",
      "    from lamin_cli.__main__ import main\n",
      "  File \"/home/ml4ig1/miniconda3/envs/scprint/lib/python3.10/site-packages/lamin_cli/__main__.py\", line 11, in <module>\n",
      "    import rich_click as click\n",
      "  File \"/home/ml4ig1/miniconda3/envs/scprint/lib/python3.10/site-packages/rich_click/__init__.py\", line 73, in <module>\n",
      "    from . import rich_click as rich_click\n",
      "  File \"/home/ml4ig1/miniconda3/envs/scprint/lib/python3.10/site-packages/rich_click/rich_click.py\", line 6, in <module>\n",
      "    import rich.columns\n",
      "  File \"/home/ml4ig1/miniconda3/envs/scprint/lib/python3.10/site-packages/rich/columns.py\", line 7, in <module>\n",
      "    from .console import Console, ConsoleOptions, RenderableType, RenderResult\n",
      "  File \"/home/ml4ig1/miniconda3/envs/scprint/lib/python3.10/site-packages/rich/console.py\", line 57, in <module>\n",
      "    from .markup import render as render_markup\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 674, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\n",
      "  File \"<frozen importlib._bootstrap>\", line 541, in _init_module_attrs\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! lamin load scprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/pasteur/appa/homes/jkalfon/miniconda3/envs/scprint17/lib/python3.10/site-packages/bitsandbytes/cextension.py:31: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pasteur/appa/homes/jkalfon/miniconda3/envs/scprint17/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "ðŸ’¡ connected lamindb: jkobject/scprint\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, StochasticWeightAveraging, EarlyStopping, LearningRateMonitor, LearningRateFinder\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "from scprint import scPrint\n",
    "from scprint.trainer import TrainingMode\n",
    "from scdataloader import DataModule \n",
    "import pandas as pd\n",
    "from scdataloader.utils import load_genes\n",
    "\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: drop tissue & dev stage until part or is taken in account\n",
    "\n",
    "hierarchical_labels = [\n",
    "    \"cell_type_ontology_term_id\", #1\n",
    "    # \"tissue_ontology_term_id\",\n",
    "    \"disease_ontology_term_id\", # 2\n",
    "#    \"development_stage_ontology_term_id\",\n",
    "    \"assay_ontology_term_id\", #3\n",
    "    'self_reported_ethnicity_ontology_term_id', #4\n",
    "]\n",
    "labels_to_pred = hierarchical_labels+[\n",
    "    'sex_ontology_term_id', #5\n",
    "    \"organism_ontology_term_id\", #6\n",
    "]\n",
    "all_labels = labels_to_pred+[\n",
    "    #'dataset_id',\n",
    "    'cell_culture',\n",
    "    \"heat_diff\",\n",
    "    \"total_counts\",\n",
    "    \"nnz\",\n",
    "    \"dpt_group\",\n",
    "]\n",
    "\n",
    "gene_emb = '../data/main/gene_embeddings.parquet'\n",
    "d_model=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "won't do any check but we recommend to have your dataset coming from local storage\n",
      "100.0% are aligned\n",
      "total dataset size is 97.032938749 Gb\n",
      "---\n",
      "dataset contains:\n",
      "     4926521 cells\n",
      "     127057 genes\n",
      "     11 labels\n",
      "     6 clss_to_pred\n",
      "     4 hierarchical_clss\n",
      "     2 organisms\n",
      "dataset contains 229 classes to predict\n",
      "\n",
      "seeing a string: loading gene positions as biomart parquet file\n",
      "these files will be considered test datasets:\n",
      "    /pasteur/zeus/projets/p02/ml4ig_hot/Users/jkalfon/scprint/.lamindb/BljRloq1xjcxRNDpejzI.h5ad\n",
      "    /pasteur/zeus/projets/p02/ml4ig_hot/Users/jkalfon/scprint/.lamindb/yBCKp6HmXuHa0cZptMo7.h5ad\n",
      "perc test:  0.0057480725242011555\n"
     ]
    }
   ],
   "source": [
    "datamodule = DataModule(\n",
    "    collection_name=\"preprocessed dataset\",\n",
    "    gene_embeddings=gene_emb,\n",
    "    all_labels=all_labels,\n",
    "    hierarchical_labels=hierarchical_labels,\n",
    "    organisms=[\"NCBITaxon:9606\", \"NCBITaxon:10090\"],\n",
    "    how=\"random expr\",\n",
    "    max_len=1200,\n",
    "    add_zero_genes=0,\n",
    "    # how much more you will see the most present vs less present category \n",
    "    weight_scaler=10,\n",
    "    label_to_weight=labels_to_pred,\n",
    "    label_to_pred=labels_to_pred,\n",
    "    batch_size=64,\n",
    "    num_workers=16,\n",
    "    #train_oversampling=2,\n",
    "    validation_split=0.05,\n",
    "    do_gene_pos='../data/main/biomart_pos.parquet',\n",
    "    test_split=0.05)\n",
    "testfiles = datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NCBITaxon:9606', 'NCBITaxon:9606', 'NCBITaxon:9606', ...,\n",
       "       'NCBITaxon:9606', 'NCBITaxon:9606', 'NCBITaxon:9606'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.dataset.mapped_dataset.get_merged_labels(\"organism_ontology_term_id\").astype(str).astype(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'NCBITaxon:9606': 4100357, 'NCBITaxon:10090': 826164})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[ 1.,  9.,  1.,  ...,  0.,  0.,  0.],\n",
      "        [ 1.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        ...,\n",
      "        [ 1.,  1.,  1.,  ...,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  ...,  0.,  0.,  0.],\n",
      "        [ 2., 62.,  2.,  ...,  1.,  1.,  1.]]), 'genes': tensor([[ 1974, 10542, 12127,  ..., 17762, 23034, 15146],\n",
      "        [ 8748,  8113, 10403,  ...,  7110, 11736,  9473],\n",
      "        [ 2368,   167,  4574,  ...,   212,  8655, 14602],\n",
      "        ...,\n",
      "        [ 5043,  5342,  4305,  ...,  5301,   903,  6058],\n",
      "        [ 5568,  9731, 14907,  ...,  3789,  4733, 10995],\n",
      "        [ 4685, 16960, 12874,  ...,  7018, 10614,   441]], dtype=torch.int32), 'class': tensor([[255,   8,   5,  -1,   0,   1],\n",
      "        [119,   8,   5,  -1,   0,   1],\n",
      "        [181,   8,   5,   1,   1,   1],\n",
      "        [208,   3,   9,  -1,   1,   1],\n",
      "        [104,   8,   2,   4,   1,   1],\n",
      "        [201,   8,   7,  -1,   1,   1],\n",
      "        [ 66,   8,   7,  -1,   1,   1],\n",
      "        [234,   3,   5,  -1,   0,   1],\n",
      "        [ 36,   2,   7,   1,   0,   1],\n",
      "        [255,   2,   7,   3,   1,   1],\n",
      "        [123,   2,   7,   1,   1,   1],\n",
      "        [128,   8,   7,  -1,   0,   1],\n",
      "        [240,   8,   6,  -1,   1,   1],\n",
      "        [210,   8,   9,  -1,   1,   1],\n",
      "        [ 58,   8,   9,  -1,   1,   1],\n",
      "        [146,  13,   5,   0,   0,   1],\n",
      "        [234,   8,   7,  -1,   1,   0],\n",
      "        [225,   8,   7,  -1,   1,   0],\n",
      "        [242,   8,   1,  -1,   0,   1],\n",
      "        [151,   8,   9,  -1,   0,   1],\n",
      "        [190,   2,   7,   6,   1,   1],\n",
      "        [  2,  19,   5,   1,   1,   1],\n",
      "        [277,   8,   7,  -1,   0,   1],\n",
      "        [ 55,   8,   7,   1,   0,   1],\n",
      "        [211,   8,   7,  -1,   0,   1],\n",
      "        [273,   8,   6,  -1,   1,   1],\n",
      "        [ 31,   8,   2,   4,   1,   1],\n",
      "        [211,   8,   7,  -1,   1,   0],\n",
      "        [177,   9,   7,   1,   0,   1],\n",
      "        [164,   9,   7,   3,   0,   1],\n",
      "        [255,   8,   5,  -1,   0,   1],\n",
      "        [261,   8,   7,  -1,   0,   1],\n",
      "        [ 55,   8,   5,   1,   0,   1],\n",
      "        [ 67,   8,   5,   1,   0,   1],\n",
      "        [ 31,   8,   2,   4,   0,   1],\n",
      "        [164,   8,   7,   1,   1,   1],\n",
      "        [288,   8,   6,  -1,   1,   1],\n",
      "        [ 79,   8,   5,   1,   0,   1],\n",
      "        [ 67,  13,   5,   0,   0,   1],\n",
      "        [149,   9,   5,   5,   0,   1],\n",
      "        [ 82,   8,   6,  -1,   0,   1],\n",
      "        [194,   8,   7,  -1,   1,   1],\n",
      "        [167,   9,   5,   1,   1,   1],\n",
      "        [217,   8,   6,  -1,   0,   1],\n",
      "        [287,   8,   2,   4,   0,   1],\n",
      "        [123,   8,   5,   1,   1,   1],\n",
      "        [109,   8,   9,  -1,   0,   1],\n",
      "        [176,   9,   5,   5,   1,   1],\n",
      "        [217,   8,   5,  -1,   0,   1],\n",
      "        [ 53,   9,   7,   2,   0,   1],\n",
      "        [272,  10,   5,   1,   0,   1],\n",
      "        [115,   8,   5,   1,   0,   1],\n",
      "        [291,   8,   2,   4,   0,   1],\n",
      "        [285,   8,   5,   1,   1,   1],\n",
      "        [208,   3,   5,  -1,   0,   1],\n",
      "        [112,   8,   3,  -1,   0,   0],\n",
      "        [177,   2,   7,   6,   1,   1],\n",
      "        [190,   8,   9,  -1,   0,   1],\n",
      "        [ 87,  13,   5,   5,   1,   1],\n",
      "        [ 12,   8,   5,  -1,   1,   1],\n",
      "        [237,   9,   7,  -1,   1,   1],\n",
      "        [ 87,   8,   5,   5,   1,   1],\n",
      "        [253,   8,   9,  -1,   1,   1],\n",
      "        [103,   8,   5,  -1,   0,   1]], dtype=torch.int32), 'tp': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 'depth': tensor([2.4010e+03, 2.6330e+03, 2.3780e+03, 3.7000e+02, 4.6100e+02, 1.1678e+04,\n",
      "        1.3431e+04, 6.6750e+03, 7.6100e+02, 3.4030e+03, 3.3050e+03, 1.6130e+03,\n",
      "        1.7630e+04, 3.3650e+03, 1.3549e+04, 1.0040e+03, 4.2722e+04, 8.1850e+03,\n",
      "        1.8530e+03, 4.9040e+03, 7.2800e+02, 1.0630e+03, 1.4150e+03, 9.6160e+03,\n",
      "        1.0681e+04, 5.0730e+03, 5.5700e+02, 2.9310e+03, 2.2510e+03, 3.0830e+03,\n",
      "        1.6110e+03, 6.8920e+03, 2.7494e+04, 1.5580e+03, 3.0400e+02, 2.7870e+03,\n",
      "        3.8870e+03, 7.6830e+03, 5.4200e+02, 8.0900e+03, 7.7960e+03, 8.9250e+03,\n",
      "        4.0254e+04, 3.0660e+03, 4.1800e+02, 8.6640e+03, 1.2279e+04, 4.4970e+03,\n",
      "        5.1260e+03, 1.4810e+03, 8.1900e+02, 3.3640e+03, 1.3020e+03, 8.2900e+02,\n",
      "        5.0780e+03, 6.0225e+05, 6.9600e+02, 3.4190e+03, 1.1349e+04, 3.9220e+03,\n",
      "        4.7540e+03, 6.9700e+03, 1.7590e+03, 4.4880e+03])}\n"
     ]
    }
   ],
   "source": [
    "for i in datamodule.train_dataloader():\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to transform an scGPT checkpoint to an scPrint's\n",
    "# ckpt = torch.load(\"../../scGPT/save/model_e6.pt\")\n",
    "# scPrint.load_from_checkpoint(\"../../scGPT/save/model_e6.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 09:16:21,556:ERROR - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: jkobject (ml4ig). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../data/tensorboard/wandb/run-20240314_091623-heig0ijx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ml4ig/scprint_test/runs/heig0ijx' target=\"_blank\">buttermilk-flambee-65</a></strong> to <a href='https://wandb.ai/ml4ig/scprint_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ml4ig/scprint_test' target=\"_blank\">https://wandb.ai/ml4ig/scprint_test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ml4ig/scprint_test/runs/heig0ijx' target=\"_blank\">https://wandb.ai/ml4ig/scprint_test/runs/heig0ijx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WandbLogger\n\u001b[1;32m      4\u001b[0m wandb_logger \u001b[38;5;241m=\u001b[39m WandbLogger(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscprint_test\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/tensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m wandb_logger\u001b[38;5;241m.\u001b[39mwatch(\u001b[43mmodel\u001b[49m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, log_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, log_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#tlogger = TensorBoardLogger(save_dir=\"../data/tensorboard\")\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#tlogger.log_graph(model)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"scprint_test\", save_dir=\"../data/tensorboard\")\n",
    "wandb_logger.watch(model, log='all', log_freq=50, log_graph=True)\n",
    "\n",
    "#tlogger = TensorBoardLogger(save_dir=\"../data/tensorboard\")\n",
    "#tlogger.log_graph(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from lightning.pytorch.profilers import PyTorchProfiler\n",
    "#pytorch_prof = PyTorchProfiler(\"../data/tensorboard\", emit_nvtx=False, group_by_input_shape=True, record_shapes=True, profile_memory=True, with_stack=True, on_trace_ready=torch.profiler.tensorboard_trace_handler(\"../data/tensorboard/\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pasteur/appa/homes/jkalfon/miniconda3/envs/scprint17/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /pasteur/appa/homes/jkalfon/miniconda3/envs/scprint1 ...\n",
      "/pasteur/appa/homes/jkalfon/miniconda3/envs/scprint17/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:552: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "chckp = ModelCheckpoint(monitor=\"val_loss\", save_top_k=-1)\n",
    "trainingmode = TrainingMode(\n",
    "    do_denoise=True, \n",
    "    noise=[0.4],\n",
    "    do_cce=True, \n",
    "    cce_sim=0.6, \n",
    "    do_ecs=True, \n",
    "    ecs_threshold = 0.4, \n",
    "    ecs_scale = 0.05,\n",
    "    class_scale = 0.08,\n",
    "    do_mvc=False, \n",
    "    do_adv_cls=True,\n",
    "    do_next_tp=False, \n",
    "    mask_ratio=[0.3], \n",
    "    warmup_duration= 500, \n",
    "    weight_decay= 0.01, \n",
    "    fused_adam= True,)\n",
    "es = EarlyStopping(patience=2, monitor='val_loss')\n",
    "swa = StochasticWeightAveraging(swa_lrs= 0.01)\n",
    "lrm = LearningRateMonitor(logging_interval=\"step\")\n",
    "#lrf = LearningRateFinder(mode=\"exponential\",)\n",
    "# TODO: to check that the class hierarchy are really ordered from 1-2-3-4... as well (oredered dict)\n",
    "trainer = Trainer(precision=\"16-mixed\", gradient_clip_val=100, max_time={\"hours\": 2}, limit_train_batches=50, limit_val_batches=10, callbacks=[chckp, es, lrm,  swa, trainingmode], accumulate_grad_batches=1, reload_dataloaders_every_n_epochs=1)#, logger=tlogger) #detect_anomaly=True, fast_dev_run=20, overfit_batches=10, limit_train_batches=1, limit_val_batches=0\n",
    "#logger=wandb_logger,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<lightning.pytorch.callbacks.early_stopping.EarlyStopping object at 0x7fe6fbc64a30>, <lightning.pytorch.callbacks.lr_monitor.LearningRateMonitor object at 0x7fe6fbc647f0>, <lightning.pytorch.callbacks.stochastic_weight_avg.StochasticWeightAveraging object at 0x7fe6fbc64520>, TrainingMode(do_denoise=True, noise=[0.4], do_cce=True, cce_sim=0.6, cce_scale=0.002, do_ecs=True, ecs_threshold=0.4, ecs_scale=0.05, do_mvc=False, do_adv_cls=True, adv_class_scale=0.005, do_next_tp=False, do_generate=False, class_scale=0.08, mask_ratio=[0.3], warmup_duration=500, weight_decay=0.01, fused_adam=True, lr_reduce_patience=1, lr_reduce_factor=0.6, optim=adam, mvc_scale=0.2, do_cls=True, do_adv_batch=False, run_full_forward=False), <lightning.pytorch.callbacks.timer.Timer object at 0x7fe6fbc64460>, <lightning.pytorch.callbacks.progress.tqdm_progress.TQDMProgressBar object at 0x7fe6fbc64b20>, <lightning.pytorch.callbacks.model_summary.ModelSummary object at 0x7fe6fbc643d0>, <lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint object at 0x7fe6fbc647c0>]\n"
     ]
    }
   ],
   "source": [
    "print(trainer.callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 2 (1379575640.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 2\n"
     ]
    }
   ],
   "source": [
    "for i in trainer.callbacks:\n",
    "  if isinstance(i, TrainingMode):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cell_type_ontology_term_id',\n",
       " 'disease_ontology_term_id',\n",
       " 'assay_ontology_term_id',\n",
       " 'self_reported_ethnicity_ontology_term_id',\n",
       " 'sex_ontology_term_id',\n",
       " 'organism_ontology_term_id']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "You have turned on `Trainer(detect_anomaly=True)`. This will significantly slow down compute speed and is recommended only for model debugging.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# sanity. should be overfiting.\n",
    "trainer = Trainer(precision=\"16-mixed\", max_epochs=1000, limit_val_batches=0, check_val_every_n_epoch=1000, log_every_n_steps=1000, detect_anomaly=True, overfit_batches=30, gradient_clip_val=100,\n",
    "reload_dataloaders_every_n_epochs=1000) #logger=wandb_logger) limit_train_batches=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 11:20:42,459:INFO - Created a temporary directory at /local/scratch/tmp/tmpeztdpiv1\n",
      "2024-03-26 11:20:42,460:INFO - Writing /local/scratch/tmp/tmpeztdpiv1/_remote_module_non_scriptable.py\n",
      "2024-03-26 11:20:42,460:INFO - Writing /local/scratch/tmp/tmpeztdpiv1/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "model = scPrint(\n",
    "    genes = datamodule.genes,\n",
    "    d_model = d_model,\n",
    "    nhead = 4,\n",
    "    nlayers = 4,\n",
    "    layers_cls = [d_model],\n",
    "    labels = datamodule.labels,\n",
    "    cls_hierarchy = datamodule.cls_hierarchy,\n",
    "    dropout= 0.1,\n",
    "    transformer = \"flash\",\n",
    "    precpt_gene_emb = gene_emb,\n",
    "    gene_pos_enc = datamodule.gene_pos,\n",
    "    mvc_decoder = \"inner product\",\n",
    "    label_decoders = datamodule.decoders,\n",
    "    fused_dropout_add_ln = False,\n",
    "    num_batch_labels = datamodule.num_datasets,\n",
    "    checkpointing=True,\n",
    "    prenorm=True,\n",
    "    num_heads_kv=None,\n",
    ")\n",
    "\n",
    "model.do_denoise = True\n",
    "model.noise = [.5]\n",
    "model.do_cce = True\n",
    "model.do_mvc = False\n",
    "model.do_adv_cls = False\n",
    "model.do_generate = True\n",
    "model.weight_decay = 1e-4\n",
    "model.fused_adam = True\n",
    "model.lr_patience = 1\n",
    "model.optim = \"adamW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 23149, 1: 0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.kwargs['collate_fn'].start_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these files will be considered test datasets:\n",
      "    /home/ml4ig1/scprint/.lamindb/BljRloq1xjcxRNDpejzI.h5ad\n",
      "    /home/ml4ig1/scprint/.lamindb/yBCKp6HmXuHa0cZptMo7.h5ad\n",
      "perc test:  0.0057480725242011555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                            | Type                         | Params\n",
      "----------------------------------------------------------------------------------\n",
      "0  | gene_encoder                    | GeneEncoder                  | 5.7 M \n",
      "1  | expr_encoder                    | ContinuousValueEncoder       | 17.0 K\n",
      "2  | pos_encoder                     | PositionalEncoding           | 0     \n",
      "3  | label_encoder                   | CategoryValueEncoder         | 1.0 K \n",
      "4  | depth_encoder                   | ContinuousValueEncoder       | 17.0 K\n",
      "5  | norm_and_dropout                | Sequential                   | 256   \n",
      "6  | transformer                     | FlashTransformerEncoder      | 793 K \n",
      "7  | expr_decoder                    | ExprDecoder                  | 33.7 K\n",
      "8  | cls_decoders                    | ModuleDict                   | 130 K \n",
      "9  | grad_reverse_discriminator_loss | AdversarialDiscriminatorLoss | 45.0 K\n",
      "10 | mvc_decoder                     | MVCDecoder                   | 65.7 K\n",
      "----------------------------------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "5.7 M     Non-trainable params\n",
      "6.8 M     Total params\n",
      "27.328    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f45a335b2f14f2baa034679f80306c9",
       "version_major": 2,
       "version_minor": 0
      },
      "nbformat": 4,
      "nbformat_minor": 2
}
